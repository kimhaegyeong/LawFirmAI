# -*- coding: utf-8 -*-
"""
State Reduction ?±ëŠ¥ ?ŒìŠ¤??
ë©”ëª¨ë¦??¬ìš©?? ì²˜ë¦¬ ?ë„, ?°ì´???„ì†¡??ë¹„êµ
"""

import asyncio
import logging
import sys
import time
from pathlib import Path

# ?„ë¡œ?íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê?
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    import psutil
except ImportError:
    psutil = None

try:
    import pytest
except ImportError:
    pytest = None

from source.agents.node_input_output_spec import get_all_node_names
from source.agents.state_adapter import StateAdapter
from source.agents.state_reduction import (
    StateReducer,
    reduce_state_for_node,
    reduce_state_size,
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_test_state() -> dict:
    """?ŒìŠ¤?¸ìš© ?€?©ëŸ‰ State ?ì„±"""
    return {
        "query": "ê³„ì•½???‘ì„± ??ì£¼ì˜?¬í•­?€?",
        "session_id": "test_session_123",
        "query_type": "general_question",
        "confidence": 0.85,
        "legal_field": "civil_law",
        "legal_domain": "?¼ë°˜",
        "urgency_level": "medium",
        "urgency_reasoning": "ê¸´ê¸‰???‰ê? ê²°ê³¼",
        "emergency_type": None,
        "complexity_level": "simple",
        "requires_expert": False,
        "expert_subgraph": None,
        "is_multi_turn": False,
        "multi_turn_confidence": 1.0,
        "conversation_history": [],
        "conversation_context": None,
        "search_query": "ê³„ì•½???‘ì„± ??ì£¼ì˜?¬í•­",
        "extracted_keywords": ["ê³„ì•½??, "ì£¼ì˜?¬í•­", "ë²•ë¥ "],
        "ai_keyword_expansion": {
            "keywords": ["ê³„ì•½??, "ì£¼ì˜?¬í•­", "ë²•ë¥ "],
            "expanded": ["ê³„ì•½", "ê³„ì•½??, "ì£¼ì˜?¬í•­", "ë²•ë¥ ", "ë²•ë ¹"],
            "confidence": 0.9
        },
        "retrieved_docs": [
            {"content": "?ŒìŠ¤??ë¬¸ì„œ ?´ìš© " * 500, "source": f"doc_{i}", "metadata": {"title": f"ë¬¸ì„œ {i}"}}
            for i in range(20)
        ],
        "analysis": "ë²•ë¥  ë¶„ì„ ê²°ê³¼",
        "legal_references": ["ë¯¼ë²•", "ê³„ì•½ë²?],
        "legal_citations": [{"law": "ë¯¼ë²•", "article": "??ì¡?}],
        "answer": "ê³„ì•½???‘ì„± ??ì£¼ìš” ì£¼ì˜?¬í•­?€ ?¤ìŒê³?ê°™ìŠµ?ˆë‹¤...",
        "sources": ["doc_1", "doc_2"],
        "structure_confidence": 0.95,
        "document_type": None,
        "document_analysis": None,
        "key_clauses": [],
        "potential_issues": [],
        "legal_validity_check": True,
        "legal_basis_validation": {"confidence": 0.9},
        "outdated_laws": [],
        "quality_check_passed": True,
        "quality_score": 0.85,
        "retry_count": 0,
        "needs_enhancement": False,
        "processing_steps": [f"?¨ê³„ {i}" for i in range(50)],
        "errors": [],
        "metadata": {"version": "1.0"},
        "processing_time": 0.0,
        "tokens_used": 1000
    }


def estimate_size(obj) -> int:
    """ê°ì²´ ?¬ê¸° ì¶”ì • (bytes)"""
    return sys.getsizeof(str(obj))


class TestStateReductionPerformance:
    """State Reduction ?±ëŠ¥ ?ŒìŠ¤??""

    def test_memory_usage_reduction(self):
        """ë©”ëª¨ë¦??¬ìš©??ê°ì†Œ ?ŒìŠ¤??""
        full_state = create_test_state()
        full_size = estimate_size(full_state)

        reducer = StateReducer(aggressive_reduction=True)

        # ê°??¸ë“œë³„ë¡œ State Reduction ?ìš©
        nodes = [
            "classify_query",
            "assess_urgency",
            "resolve_multi_turn",
            "route_expert",
            "retrieve_documents",
            "generate_answer_enhanced"
        ]

        total_reduced_size = 0
        for node_name in nodes:
            reduced = reducer.reduce_state_for_node(full_state, node_name)
            reduced_size = estimate_size(reduced)
            reduction_pct = (1 - reduced_size / full_size) * 100 if full_size > 0 else 0

            logger.info(
                f"{node_name}: {reduction_pct:.1f}% reduction "
                f"({full_size:.0f} ??{reduced_size:.0f} bytes)"
            )

            total_reduced_size += reduced_size

            # ê°ì†Œ??ê²€ì¦?
            assert reduction_pct > 0, f"{node_name}?ì„œ ê°ì†Œê°€ ë°œìƒ?˜ì? ?ŠìŒ"

        # ?‰ê·  ê°ì†Œ??ê³„ì‚°
        avg_reduction = sum([
            (1 - estimate_size(reducer.reduce_state_for_node(full_state, node)) / full_size) * 100
            for node in nodes
        ]) / len(nodes)

        logger.info(f"?‰ê·  State Reduction: {avg_reduction:.1f}%")
        assert avg_reduction > 50, f"?‰ê·  ê°ì†Œ?¨ì´ 50% ë¯¸ë§Œ: {avg_reduction:.1f}%"

    def test_processing_speed(self):
        """ì²˜ë¦¬ ?ë„ ê°œì„  ?ŒìŠ¤??""
        full_state = create_test_state()
        reducer = StateReducer(aggressive_reduction=True)

        nodes = get_all_node_names()

        # State Reduction ?†ì´ ì²˜ë¦¬ ?œê°„ ì¸¡ì •
        start = time.time()
        for node_name in nodes[:5]:
            _ = full_state  # ëª¨ì˜ ì²˜ë¦¬
        time_without_reduction = time.time() - start

        # State Reduction ?ìš© ??ì²˜ë¦¬ ?œê°„ ì¸¡ì •
        start = time.time()
        for node_name in nodes[:5]:
            reduced = reducer.reduce_state_for_node(full_state, node_name)
            _ = reduced  # ëª¨ì˜ ì²˜ë¦¬
        time_with_reduction = time.time() - start

        logger.info(
            f"ì²˜ë¦¬ ?œê°„: Reduction ?†ì´ {time_without_reduction:.4f}s, "
            f"Reduction ?ìš© {time_with_reduction:.4f}s"
        )

    def test_state_size_reduction(self):
        """State ?¬ê¸° ?œí•œ ?ŒìŠ¤??""
        large_state = {
            "retrieved_docs": [
                {"content": "test " * 1000} for _ in range(50)
            ],
            "conversation_history": [f"turn_{i}" for i in range(20)]
        }

        reduced = reduce_state_size(large_state, max_docs=10, max_content_per_doc=500)

        assert len(reduced["retrieved_docs"]) <= 10
        for doc in reduced["retrieved_docs"]:
            assert len(doc.get("content", "")) <= 503  # 500 + "..."

        assert len(reduced["conversation_history"]) <= 5

    def test_flat_vs_nested_conversion(self):
        """Flat ??Nested ë³€???±ëŠ¥ ?ŒìŠ¤??""
        flat_state = create_test_state()

        start = time.time()
        nested_state = StateAdapter.to_nested(flat_state)
        to_nested_time = time.time() - start

        start = time.time()
        flat_again = StateAdapter.to_flat(nested_state)
        to_flat_time = time.time() - start

        logger.info(f"Flat ??Nested: {to_nested_time*1000:.2f}ms")
        logger.info(f"Nested ??Flat: {to_flat_time*1000:.2f}ms")

        # ì£¼ìš” ?„ë“œ ?™ì¼???•ì¸
        assert flat_again["query"] == flat_state["query"]
        assert flat_again["query_type"] == flat_state["query_type"]


class TestWorkflowIntegration:
    """?Œí¬?Œë¡œ???µí•© ?±ëŠ¥ ?ŒìŠ¤??""

    def test_full_workflow_performance(self):
        """?„ì²´ ?Œí¬?Œë¡œ???±ëŠ¥ ?ŒìŠ¤??""
        # ?œë??ˆì´?? ?„ì²´ ?Œí¬?Œë¡œ???¤í–‰
        initial_state = create_test_state()
        reducer = StateReducer(aggressive_reduction=True)

        workflow_nodes = [
            "classify_query",
            "assess_urgency",
            "resolve_multi_turn",
            "route_expert",
            "expand_keywords_ai",
            "retrieve_documents",
            "process_legal_terms",
            "generate_answer_enhanced",
            "validate_answer_quality"
        ]

        start_time = time.time()
        total_memory_reduction = 0

        for node_name in workflow_nodes:
            reduced = reducer.reduce_state_for_node(initial_state, node_name)
            reduced_size = estimate_size(reduced)
            original_size = estimate_size(initial_state)
            reduction = (1 - reduced_size / original_size) * 100 if original_size > 0 else 0

            total_memory_reduction += reduction

        total_time = time.time() - start_time
        avg_reduction = total_memory_reduction / len(workflow_nodes)

        logger.info(f"?„ì²´ ?Œí¬?Œë¡œ???¤í–‰ ?œê°„: {total_time:.4f}s")
        logger.info(f"?‰ê·  ë©”ëª¨ë¦?ê°ì†Œ?? {avg_reduction:.1f}%")

        assert total_time < 1.0, "?¨í¬?Œë¡œ???¤í–‰ ?œê°„???ˆë¬´ ê¹€"
        assert avg_reduction > 40, f"?‰ê·  ê°ì†Œ?¨ì´ ??Œ: {avg_reduction:.1f}%"


def benchmark_state_operations():
    """State ?°ì‚° ë²¤ì¹˜ë§ˆí¬"""
    logger.info("?” State Reduction ?±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ?œì‘")

    state = create_test_state()
    reducer = StateReducer(aggressive_reduction=True)

    results = []

    for node_name in get_all_node_names():
        start = time.time()
        reduced = reducer.reduce_state_for_node(state, node_name)
        elapsed = time.time() - start

        original_size = estimate_size(state)
        reduced_size = estimate_size(reduced)
        reduction_pct = (1 - reduced_size / original_size) * 100 if original_size > 0 else 0

        results.append({
            "node": node_name,
            "time_ms": elapsed * 1000,
            "original_size": original_size,
            "reduced_size": reduced_size,
            "reduction_pct": reduction_pct
        })

    # ê²°ê³¼ ì¶œë ¥
    logger.info("=" * 80)
    logger.info(f"{'Node':<30} {'Time(ms)':<12} {'Size(bytes)':<15} {'Reduction(%)':<15}")
    logger.info("-" * 80)

    for r in sorted(results, key=lambda x: x["reduction_pct"], reverse=True):
        logger.info(
            f"{r['node']:<30} {r['time_ms']:<12.2f} "
            f"{r['original_size']}??r['reduced_size']:<4} {r['reduction_pct']:<15.1f}"
        )

    # ?µê³„
    avg_time = sum(r["time_ms"] for r in results) / len(results)
    avg_reduction = sum(r["reduction_pct"] for r in results) / len(results)
    total_original = sum(r["original_size"] for r in results)
    total_reduced = sum(r["reduced_size"] for r in results)

    logger.info("-" * 80)
    logger.info(f"?‰ê·  ì²˜ë¦¬ ?œê°„: {avg_time:.2f}ms")
    logger.info(f"?‰ê·  ê°ì†Œ?? {avg_reduction:.1f}%")
    logger.info(f"?„ì²´ ?¬ê¸°: {total_original:,} ??{total_reduced:,} bytes")

    return results


if __name__ == "__main__":
    # ê°„ë‹¨???ŒìŠ¤???¤í–‰
    benchmark_state_operations()
