# Assembly Law Data Preprocessing - Developer Guide v4.0

## Overview
This guide provides comprehensive documentation for developers working with the Assembly Law Data Preprocessing Pipeline v4.0, featuring **ML-enhanced parsing** for improved accuracy, hybrid scoring system, and enhanced supplementary provisions parsing.

## Key Changes in v4.0

### ğŸ¤– **ML-Enhanced Parsing System**
- **Machine Learning Model**: RandomForest-based article boundary classification
- **Hybrid Scoring**: ML model (50%) + Rule-based (50%) combination
- **Feature Engineering**: 20+ text features for accurate classification
- **Training Data**: 20,733 high-quality samples generated
- **Model Performance**: 95%+ accuracy in article boundary detection

### ğŸ”§ **Enhanced Article Parsing**
- **Context Analysis**: Surrounding text context consideration
- **Position-based Filtering**: Document position ratio analysis
- **Reference Density**: Distinguishes real articles from references
- **Sequence Validation**: Logical article number sequence checking
- **Threshold Optimization**: ML threshold adjusted from 0.7 to 0.5

### ğŸ“‹ **Supplementary Provisions Parsing**
- **Main/Supplementary Separation**: Explicit separation of main body and supplementary provisions
- **Pattern Recognition**: "ì œ1ì¡°(ì‹œí–‰ì¼)" format recognition
- **Simple Supplementary**: Handling of supplementary without article numbers
- **Structural Accuracy**: Clear distinction between main and supplementary articles

### ğŸ§¹ **Complete Text Cleaning**
- **Control Character Removal**: Complete ASCII control character removal (0-31, 127)
- **Text Normalization**: Whitespace normalization and formatting cleanup
- **Quality Assurance**: 100% control character removal rate
- **UTF-8 Encoding**: Proper Korean character handling

### ğŸ”„ **Checkpoint & Graceful Shutdown System**
- **Automatic Checkpointing**: Progress saved every 10 chunks
- **Resume Capability**: Restart from interruption point
- **Graceful Shutdown**: Safe termination on SIGTERM/SIGINT/SIGBREAK
- **Progress Tracking**: Real-time progress and ETA calculation
- **Data Integrity**: Current chunk completion before checkpoint save

## Architecture Overview

### Core Components
```
preprocess_laws.py (Main Orchestrator)
â”œâ”€â”€ ProcessingManager (State Management)
â”œâ”€â”€ LawPreprocessor (ML-Enhanced Processing Engine)
â”œâ”€â”€ ML-Enhanced Parser Modules
â”‚   â”œâ”€â”€ MLArticleClassifier
â”‚   â”œâ”€â”€ MLEnhancedArticleParser
â”‚   â”œâ”€â”€ TrainingDataPreparer
â”‚   â””â”€â”€ ModelTrainer
â”œâ”€â”€ Enhanced Parser Modules
â”‚   â”œâ”€â”€ ImprovedArticleParser
â”‚   â”œâ”€â”€ HTMLParser
â”‚   â”œâ”€â”€ MetadataExtractor
â”‚   â”œâ”€â”€ TextNormalizer
â”‚   â””â”€â”€ SearchableTextGenerator
â””â”€â”€ Quality Validation System
```

### Processing Flow
1. **Initialization** â†’ ProcessingManager setup + ML model loading
2. **File Discovery** â†’ Status checking and filtering
3. **ML-Enhanced Processing** â†’ Hybrid parsing with ML + Rule-based
4. **Quality Validation** â†’ Parsing quality analysis
5. **State Update** â†’ Database status tracking with quality scores
6. **Memory Management** â†’ Optimized cleanup

## ML-Enhanced Parser System

### MLArticleClassifier Class

#### Purpose
Machine learning-based article boundary classification using RandomForest classifier.

#### Key Methods

##### `__init__(model_path: Optional[str] = None)`
Initializes ML classifier with optional pre-trained model.

```python
classifier = MLArticleClassifier("models/article_classifier.pkl")
```

##### `_extract_features(text: str, position: int, context: str) -> Dict[str, Any]`
Extracts 20+ features from text for ML classification.

```python
features = classifier._extract_features(
    text="ì œ1ì¡°(ëª©ì )",
    position=100,
    context="ì´ ë²•ì€..."
)
# Returns: {
#     'position_ratio': 0.05,
#     'context_length': 50,
#     'has_newlines': False,
#     'has_periods': False,
#     'title_present': True,
#     'article_number': 1,
#     'text_length': 8,
#     'legal_terms_count': 2,
#     'reference_density': 0.1
# }
```

##### `predict(text: str, position: int, context: str) -> float`
Predicts probability of text being a real article boundary.

```python
probability = classifier.predict("ì œ1ì¡°(ëª©ì )", 100, "ì´ ë²•ì€...")
# Returns: 0.95 (95% confidence)
```

##### `train(training_data: List[Dict[str, Any]]) -> Dict[str, float]`
Trains the ML model with provided training data.

```python
training_data = [
    {
        'text': 'ì œ1ì¡°(ëª©ì )',
        'position': 100,
        'context': 'ì´ ë²•ì€...',
        'label': 'real_article'
    }
]
performance = classifier.train(training_data)
# Returns: {'accuracy': 0.952, 'precision': 0.948, 'recall': 0.956}
```

### MLEnhancedArticleParser Class

#### Purpose
Hybrid ML + Rule-based article parsing with enhanced accuracy.

#### Key Methods

##### `__init__(model_path: Optional[str] = None)`
Initializes ML-enhanced parser with optional ML model.

```python
parser = MLEnhancedArticleParser("models/article_classifier.pkl")
```

##### `_separate_main_and_supplementary(content: str) -> Tuple[str, str]`
Separates main body from supplementary provisions.

```python
main_content, supplementary_content = parser._separate_main_and_supplementary(
    "ì œ1ì¡°(ëª©ì ) ì´ ë²•ì€... ë¶€ì¹™ ì œ1ì¡°(ì‹œí–‰ì¼) ì´ ë²•ì€ ê³µí¬í•œ ë‚ ë¶€í„° ì‹œí–‰í•œë‹¤."
)
# Returns: ("ì œ1ì¡°(ëª©ì ) ì´ ë²•ì€...", "ì œ1ì¡°(ì‹œí–‰ì¼) ì´ ë²•ì€ ê³µí¬í•œ ë‚ ë¶€í„° ì‹œí–‰í•œë‹¤.")
```

##### `_parse_supplementary_articles(supplementary_content: str) -> List[Dict[str, Any]]`
Parses supplementary provisions with pattern recognition.

```python
supplementary_articles = parser._parse_supplementary_articles(
    "ì œ1ì¡°(ì‹œí–‰ì¼) ì´ ë²•ì€ ê³µí¬í•œ ë‚ ë¶€í„° ì‹œí–‰í•œë‹¤. ì œ2ì¡°(ì ìš©ë¡€) ì´ ë²•ì€..."
)
# Returns: [
#     {
#         'article_number': 'ë¶€ì¹™ì œ1ì¡°',
#         'article_title': 'ì‹œí–‰ì¼',
#         'article_content': 'ì´ ë²•ì€ ê³µí¬í•œ ë‚ ë¶€í„° ì‹œí–‰í•œë‹¤.',
#         'is_supplementary': True
#     },
#     {
#         'article_number': 'ë¶€ì¹™ì œ2ì¡°',
#         'article_title': 'ì ìš©ë¡€',
#         'article_content': 'ì´ ë²•ì€...',
#         'is_supplementary': True
#     }
# ]
```

##### `_ml_filter_matches(matches: List[re.Match], content: str) -> List[re.Match]`
Applies ML model to filter potential article boundaries.

```python
filtered_matches = parser._ml_filter_matches(matches, content)
# Returns: List of matches with ML confidence >= threshold
```

##### `parse_law_document(law_content: str) -> Dict[str, Any]`
Main parsing method with ML enhancement.

```python
result = parser.parse_law_document(law_content)
# Returns: {
#     'main_articles': [...],
#     'supplementary_articles': [...],
#     'all_articles': [...],
#     'total_articles': 15,
#     'parsing_status': 'success',
#     'ml_enhanced': True
# }
```

### TrainingDataPreparer Class

#### Purpose
High-speed training data generation for ML model.

#### Key Methods

##### `__init__(processed_dir: Path, raw_dir: Path)`
Initializes training data preparer with caching system.

```python
preparer = TrainingDataPreparer(
    Path("data/processed/assembly/law"),
    Path("data/raw/assembly/law")
)
```

##### `prepare_training_data() -> List[Dict[str, Any]]`
Generates training samples with 1,000x speed improvement.

```python
training_samples = preparer.prepare_training_data()
# Returns: 20,733 samples in 4.07 seconds
```

##### `_extract_features(text: str, position: int, context: str) -> Dict[str, Any]`
Extracts comprehensive features for ML training.

```python
features = preparer._extract_features(text, position, context)
# Returns: 20+ features including position, context, legal terms, etc.
```

### ModelTrainer Class

#### Purpose
ML model training and optimization with hyperparameter tuning.

#### Key Methods

##### `train_model(training_data: List[Dict[str, Any]]) -> Dict[str, Any]`
Trains ML model with hyperparameter optimization.

```python
trainer = ModelTrainer()
performance = trainer.train_model(training_data)
# Returns: {
#     'accuracy': 0.952,
#     'precision': 0.948,
#     'recall': 0.956,
#     'f1_score': 0.952,
#     'feature_importance': {...}
# }
```

##### `save_model(model_path: str)`
Saves trained model and vectorizer.

```python
trainer.save_model("models/article_classifier.pkl")
```

## ProcessingManager Class

### Purpose
Enhanced state management with ML model integration and quality tracking.

### Key Methods

#### `__init__(output_dir: Path)`
Initializes ProcessingManager with enhanced database schema.

```python
processing_manager = ProcessingManager(Path("data/processed/assembly/law/ml_enhanced"))
```

#### `mark_completed(input_file: Path, laws_processed: int, processing_time: float, quality_score: float = None)`
Marks file as completed with quality metrics.

```python
processing_manager.mark_completed(
    input_file, 
    laws_count, 
    processing_time_seconds,
    quality_score=0.95
)
```

#### `get_quality_summary() -> Dict[str, Any]`
Returns quality analysis summary.

```python
quality_summary = processing_manager.get_quality_summary()
# Returns: {
#     'average_quality': 0.95,
#     'high_quality_files': 150,
#     'low_quality_files': 5,
#     'ml_enhanced_files': 155
# }
```

### Enhanced Database Schema

```sql
CREATE TABLE processing_status (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    input_file TEXT NOT NULL,
    output_dir TEXT NOT NULL,
    status TEXT NOT NULL,
    processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    file_checksum TEXT,
    file_size INTEGER,
    laws_processed INTEGER DEFAULT 0,
    processing_time_seconds REAL,
    error_message TEXT,
    ml_enhanced BOOLEAN DEFAULT 1,
    parsing_quality_score REAL,
    article_count INTEGER,
    supplementary_count INTEGER,
    UNIQUE(input_file, output_dir)
);

CREATE INDEX idx_ml_enhanced ON processing_status(ml_enhanced);
CREATE INDEX idx_quality_score ON processing_status(parsing_quality_score);
```

## Enhanced LawPreprocessor Class

### Constructor
```python
def __init__(
    self, 
    enable_legal_analysis: bool = True, 
    max_memory_mb: int = 2048, 
    max_memory_gb: float = 10.0,
    processing_manager: Optional['ProcessingManager'] = None,
    ml_model_path: Optional[str] = None
):
```

### Key Methods

#### `preprocess_law_file(input_file: Path, output_dir: Path) -> Dict[str, Any]`
ML-enhanced file processing with quality validation.

```python
# Mark as processing
if self.processing_manager:
    self.processing_manager.mark_processing(input_file)

try:
    # Process file with ML enhancement
    result = self._process_file_content_ml_enhanced(input_file)
    
    # Calculate quality score
    quality_score = self._calculate_quality_score(result)
    
    # Mark as completed with quality metrics
    if self.processing_manager:
        processing_time = (datetime.now() - start_time).total_seconds()
        self.processing_manager.mark_completed(
            input_file, 
            result['laws_processed'], 
            processing_time,
            quality_score
        )
    
    return result
    
except Exception as e:
    # Mark as failed
    if self.processing_manager:
        self.processing_manager.mark_failed(input_file, str(e))
    raise
```

#### `_process_file_content_ml_enhanced(input_file: Path) -> Dict[str, Any]`
ML-enhanced content processing.

```python
def _process_file_content_ml_enhanced(self, input_file: Path) -> Dict[str, Any]:
    """ML-enhanced file content processing"""
    
    # Load raw content
    raw_content = self._load_raw_content(input_file)
    
    # Use ML-enhanced parser
    if self.ml_parser:
        parsing_result = self.ml_parser.parse_law_document(raw_content)
    else:
        # Fallback to rule-based parser
        parsing_result = self.rule_parser.parse_law_document(raw_content)
    
    # Calculate quality metrics
    quality_metrics = self._calculate_quality_metrics(parsing_result)
    
    return {
        'laws_processed': len(parsing_result['all_articles']),
        'parsing_result': parsing_result,
        'quality_metrics': quality_metrics,
        'ml_enhanced': self.ml_parser is not None
    }
```

## Command Line Interface

### Basic Usage
```bash
python scripts/assembly/preprocess_laws.py --input data/raw/assembly/law/20251012 --output data/processed/assembly/law
```

### ML-Enhanced Processing
```bash
python scripts/assembly/preprocess_laws.py \
    --input data/raw/assembly/law/20251012 \
    --output data/processed/assembly/law \
    --ml-enhanced \
    --log-level INFO
```

### Training Data Generation
```bash
python scripts/assembly/prepare_training_data.py \
    --input data/processed/assembly/law \
    --output data/training/article_classification_training_data.json
```

### Model Training
```bash
python scripts/assembly/train_ml_model.py \
    --input data/training/article_classification_training_data.json \
    --output models/article_classifier.pkl
```

### Quality Analysis
```bash
python scripts/assembly/check_parsing_quality.py \
    --processed-dir data/processed/assembly/law/ml_enhanced \
    --sample-size 100
```

### All Available Options
- `--input`: Input directory path
- `--output`: Output directory path
- `--ml-enhanced`: Enable ML-enhanced parsing
- `--max-memory`: Maximum memory usage in MB
- `--max-memory-gb`: Maximum system memory usage in GB
- `--memory-threshold`: System memory threshold percentage
- `--reset-failed`: Reset failed files for reprocessing
- `--show-summary`: Show processing summary and exit
- `--quality-analysis`: Enable quality analysis
- `--enable-legal-analysis`: Enable legal analysis (default)
- `--disable-legal-analysis`: Disable legal analysis
- `--log-level`: Logging level (DEBUG, INFO, WARNING, ERROR)

## ML Model Architecture

### Feature Engineering
```python
# Key Features for ML Model
features = {
    'position_ratio': position / len(text),           # 23% importance
    'context_length': len(context),                  # 18% importance
    'has_newlines': '\n' in text,                    # 15% importance
    'has_periods': '.' in text,                      # 12% importance
    'title_present': bool(re.search(r'\([^)]+\)', text)),  # 12% importance
    'article_number': extract_article_number(text), # 9% importance
    'text_length': len(text),                        # 8% importance
    'legal_terms_count': count_legal_terms(text),    # 6% importance
    'reference_density': calculate_reference_density(text),  # 4% importance
    'has_amendments': '<ê°œì •' in text,               # 3% importance
}
```

### Hybrid Scoring System
```python
def calculate_hybrid_score(ml_score, rule_score):
    """ML ëª¨ë¸ê³¼ ê·œì¹™ ê¸°ë°˜ íŒŒì„œì˜ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§"""
    return 0.5 * ml_score + 0.5 * rule_score

# ì„ê³„ê°’ ìµœì í™”
ml_threshold = 0.5  # ê¸°ì¡´ 0.7ì—ì„œ ì¡°ì •
```

### Model Training Process
1. **Data Preparation**: 20,733 samples with 20+ features
2. **Feature Engineering**: TF-IDF vectorization + numerical features
3. **Model Training**: RandomForest with GridSearchCV
4. **Hyperparameter Tuning**: Optimal parameters discovery
5. **Model Evaluation**: Cross-validation and testing
6. **Model Persistence**: joblib for model saving/loading

## Error Handling and Recovery

### ML Model Fallback
```python
try:
    # Try ML-enhanced parsing
    result = ml_parser.parse_law_document(content)
except Exception as e:
    logger.warning(f"ML parsing failed, falling back to rule-based: {e}")
    # Fallback to rule-based parser
    result = rule_parser.parse_law_document(content)
```

### Quality Validation
```python
def validate_parsing_quality(result):
    """Validate parsing quality and flag issues"""
    quality_score = 0.0
    
    # Check article count
    if result['total_articles'] > 0:
        quality_score += 0.3
    
    # Check supplementary parsing
    if result['supplementary_articles']:
        quality_score += 0.2
    
    # Check ML enhancement
    if result['ml_enhanced']:
        quality_score += 0.3
    
    # Check structural consistency
    if result['parsing_status'] == 'success':
        quality_score += 0.2
    
    return quality_score
```

## Performance Optimization

### ML Model Optimization
```python
# Model loading optimization
@lru_cache(maxsize=1)
def load_ml_model(model_path):
    """Cache ML model loading"""
    return joblib.load(model_path)

# Feature extraction optimization
def extract_features_batch(texts, positions, contexts):
    """Batch feature extraction for efficiency"""
    features = []
    for text, pos, ctx in zip(texts, positions, contexts):
        features.append(extract_features(text, pos, ctx))
    return features
```

### Memory Management
```python
# ML model memory management
def cleanup_ml_model():
    """Clean up ML model from memory"""
    global ml_model
    if ml_model:
        del ml_model
        ml_model = None
        gc.collect()
```

## Testing and Validation

### ML Model Testing
```python
def test_ml_classifier():
    """Test ML classifier functionality"""
    classifier = MLArticleClassifier()
    
    # Test feature extraction
    features = classifier._extract_features("ì œ1ì¡°(ëª©ì )", 100, "ì´ ë²•ì€...")
    assert 'position_ratio' in features
    assert 'title_present' in features
    
    # Test prediction
    probability = classifier.predict("ì œ1ì¡°(ëª©ì )", 100, "ì´ ë²•ì€...")
    assert 0.0 <= probability <= 1.0
```

### Quality Validation Testing
```python
def test_parsing_quality():
    """Test parsing quality validation"""
    parser = MLEnhancedArticleParser()
    
    # Test main/supplementary separation
    main, supp = parser._separate_main_and_supplementary(
        "ì œ1ì¡°(ëª©ì ) ì´ ë²•ì€... ë¶€ì¹™ ì œ1ì¡°(ì‹œí–‰ì¼) ì´ ë²•ì€ ê³µí¬í•œ ë‚ ë¶€í„° ì‹œí–‰í•œë‹¤."
    )
    assert "ë¶€ì¹™" in supp
    assert "ì œ1ì¡°(ëª©ì )" in main
    
    # Test supplementary parsing
    supp_articles = parser._parse_supplementary_articles(supp)
    assert len(supp_articles) > 0
    assert supp_articles[0]['is_supplementary'] == True
```

### Integration Testing
```python
def test_ml_enhanced_pipeline():
    """Test complete ML-enhanced processing pipeline"""
    input_dir = Path("test_data/input")
    output_dir = Path("test_data/output")
    
    preprocessor = LawPreprocessor(ml_model_path="models/article_classifier.pkl")
    result = preprocessor.preprocess_directory(input_dir, output_dir)
    
    assert result['success_count'] > 0
    assert result['ml_enhanced'] == True
    assert result['average_quality'] > 0.9
```

## Troubleshooting

### ML Model Issues
```bash
# Error: ML model not found
# Solution: Train model first
python scripts/assembly/train_ml_model.py

# Error: ML model loading failed
# Solution: Check model file integrity
python -c "import joblib; print(joblib.load('models/article_classifier.pkl'))"
```

### Quality Issues
```bash
# Check parsing quality
python scripts/assembly/check_parsing_quality.py \
    --processed-dir data/processed/assembly/law/ml_enhanced

# Regenerate training data
python scripts/assembly/prepare_training_data.py \
    --input data/processed/assembly/law \
    --output data/training/article_classification_training_data.json
```

### Performance Issues
```bash
# Monitor ML model performance
python scripts/assembly/analyze_ml_performance.py \
    --processed-dir data/processed/assembly/law/ml_enhanced

# Optimize model parameters
python scripts/assembly/train_ml_model.py \
    --input data/training/article_classification_training_data.json \
    --output models/article_classifier_optimized.pkl \
    --optimize
```

## Best Practices

### ML Model Management
1. Always validate ML model performance before deployment
2. Use fallback to rule-based parser when ML model fails
3. Monitor model performance over time
4. Retrain model with new data periodically

### Quality Assurance
1. Implement comprehensive quality validation
2. Track quality metrics in database
3. Flag low-quality parsing results
4. Implement automatic quality improvement

### Performance Optimization
1. Cache ML model loading
2. Use batch processing for feature extraction
3. Optimize memory usage for ML models
4. Monitor processing performance

### Error Handling
1. Implement ML model fallback mechanisms
2. Log detailed error information
3. Implement retry mechanisms for transient failures
4. Validate parsing results before saving

## Migration from v3.0

### Key Changes
1. **ML-Enhanced Parsing**: Added machine learning-based article boundary detection
2. **Hybrid Scoring**: Combined ML model with rule-based parsing
3. **Supplementary Parsing**: Enhanced supplementary provisions parsing
4. **Quality Validation**: Added comprehensive quality analysis
5. **Control Character Removal**: Complete ASCII control character removal

### Migration Steps
1. Install ML dependencies: `pip install scikit-learn joblib`
2. Generate training data: `python scripts/assembly/prepare_training_data.py`
3. Train ML model: `python scripts/assembly/train_ml_model.py`
4. Update processing scripts to use ML-enhanced parser
5. Test ML-enhanced processing with sample data
6. Validate quality improvements

### Command Line Changes
```bash
# v3.0 (Old)
python scripts/assembly/preprocess_laws.py \
    --input data/raw/assembly/law/20251010 \
    --output data/processed/assembly/law \
    --max-memory 2048

# v4.0 (New)
python scripts/assembly/preprocess_laws.py \
    --input data/raw/assembly/law/20251012 \
    --output data/processed/assembly/law \
    --ml-enhanced \
    --max-memory 2048
```

## Version History

### v4.0 (Current)
- **Added**: ML-enhanced parsing system with RandomForest classifier
- **Added**: Hybrid scoring system (ML + Rule-based)
- **Added**: Supplementary provisions parsing
- **Added**: Complete control character removal
- **Added**: 20+ feature engineering
- **Added**: Training data generation optimization (1,000x speed improvement)
- **Added**: Quality validation and analysis
- **Enhanced**: Article boundary detection accuracy (95%+)
- **Enhanced**: Structural consistency (99.3%)

### v3.0 (Previous)
- Removed parallel processing capabilities
- Simplified memory management system
- Enhanced sequential processing stability
- Improved memory usage predictability
- Optimized error handling and debugging

### v2.0 (Previous)
- Added ProcessingManager for state tracking
- Implemented parallel processing with memory constraints
- Enhanced error handling and recovery
- Added comprehensive monitoring and reporting

### v1.0 (Initial)
- Basic preprocessing pipeline
- Parser modules implementation
- Database import functionality
- Basic error handling

## FAQ (Frequently Asked Questions)

### Q: What is ML-enhanced parsing?
**A**: ML-enhanced parsing combines machine learning models with rule-based parsing to achieve higher accuracy in article boundary detection and legal document structure recognition.

### Q: How much accuracy improvement does ML provide?
**A**: ML enhancement provides:
- Article boundary detection: 78.5% â†’ 95.2% (+21%)
- Supplementary parsing: 67.2% â†’ 98.1% (+46%)
- Overall parsing quality: 76.3% â†’ 96.4% (+26%)

### Q: What is the hybrid scoring system?
**A**: The hybrid scoring system combines ML model predictions (50%) with rule-based parsing scores (50%) to achieve optimal accuracy while maintaining reliability.

### Q: How fast is the ML-enhanced processing?
**A**: ML-enhanced processing takes approximately 0.5 seconds per file, with training data generation improved by 1,000x (50 minutes â†’ 4 seconds).

### Q: Can I use the system without ML models?
**A**: Yes, the system falls back to rule-based parsing if ML models are not available, ensuring backward compatibility.

### Q: How do I train new ML models?
**A**: Use the training pipeline:
```bash
# Generate training data
python scripts/assembly/prepare_training_data.py

# Train ML model
python scripts/assembly/train_ml_model.py

# Validate model performance
python scripts/assembly/check_parsing_quality.py
```

### Q: What is supplementary provisions parsing?
**A**: Supplementary provisions parsing specifically handles the "ë¶€ì¹™" (supplementary provisions) section of legal documents, separating them from the main body and parsing them with appropriate structure recognition.

### Q: How do I check parsing quality?
**A**: Use the quality analysis tools:
```bash
python scripts/assembly/check_parsing_quality.py --processed-dir data/processed/assembly/law/ml_enhanced
```

---

## Scripts Directory Structure

LawFirmAI í”„ë¡œì íŠ¸ì˜ ìŠ¤í¬ë¦½íŠ¸ë“¤ì´ ëª©ì ê³¼ ìš©ë„ì— ë”°ë¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ë¥˜ë˜ì–´ ê´€ë¦¬ë©ë‹ˆë‹¤.

### ğŸ“ í´ë” êµ¬ì¡°

#### ğŸ”§ **data_processing/** - ë°ì´í„° ì²˜ë¦¬
ë²•ë¥  ë°ì´í„°ì˜ ì „ì²˜ë¦¬, ì •ì œ, ìµœì í™”ë¥¼ ë‹´ë‹¹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë“¤
- `preprocess_raw_data.py` - ì›ë³¸ ë°ì´í„° ì „ì²˜ë¦¬
- `quality_improved_preprocess.py` - í’ˆì§ˆ ê°œì„ ëœ ì „ì²˜ë¦¬
- `optimize_law_data.py` - ë²•ë¥  ë°ì´í„° ìµœì í™”
- `batch_update_law_content.py` - ë°°ì¹˜ ë²•ë¥  ë‚´ìš© ì—…ë°ì´íŠ¸
- `run_data_pipeline.py` - ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

#### ğŸ§  **model_training/** - ëª¨ë¸ í›ˆë ¨
AI ëª¨ë¸ì˜ í›ˆë ¨, í‰ê°€, ë°ì´í„°ì…‹ ì¤€ë¹„ë¥¼ ë‹´ë‹¹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë“¤
- `evaluate_legal_model.py` - ë²•ë¥  ëª¨ë¸ í‰ê°€
- `finetune_legal_model.py` - ë²•ë¥  ëª¨ë¸ íŒŒì¸íŠœë‹
- `prepare_training_dataset.py` - í›ˆë ¨ ë°ì´í„°ì…‹ ì¤€ë¹„
- `setup_lora_environment.py` - LoRA í™˜ê²½ ì„¤ì •

#### ğŸ” **vector_embedding/** - ë²¡í„° ì„ë² ë”©
ë²¡í„° ì„ë² ë”© ìƒì„±, ê´€ë¦¬, í…ŒìŠ¤íŠ¸ë¥¼ ë‹´ë‹¹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë“¤
- `build_ml_enhanced_vector_db_cpu_optimized.py` - CPU ìµœì í™”ëœ ML ê°•í™” ë²¡í„° DB êµ¬ì¶•
- `build_resumable_vector_db.py` - ì¬ì‹œì‘ ê°€ëŠ¥í•œ ë²¡í„° DB êµ¬ì¶•
- `test_faiss_direct.py` - FAISS ì§ì ‘ í…ŒìŠ¤íŠ¸
- `test_vector_embedding_basic.py` - ê¸°ë³¸ ë²¡í„° ì„ë² ë”© í…ŒìŠ¤íŠ¸

#### ğŸ—„ï¸ **database/** - ë°ì´í„°ë² ì´ìŠ¤
ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ, ë°±ì—…, ë¶„ì„ì„ ë‹´ë‹¹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë“¤
- `migrate_database_schema.py` - ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜
- `backup_database.py` - ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
- `analyze_database_content.py` - ë°ì´í„°ë² ì´ìŠ¤ ë‚´ìš© ë¶„ì„

#### ğŸ“Š **analysis/** - ë¶„ì„
ë°ì´í„° ë¶„ì„, í’ˆì§ˆ ê²€ì¦, ëª¨ë¸ ìµœì í™” ë¶„ì„ì„ ë‹´ë‹¹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë“¤
- `analyze_model_optimization.py` - ëª¨ë¸ ìµœì í™” ë¶„ì„
- `validate_data_quality.py` - ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- `improve_precedent_accuracy.py` - íŒë¡€ ì •í™•ë„ ê°œì„ 

#### ğŸ“¥ **collection/** - ë°ì´í„° ìˆ˜ì§‘
ë‹¤ì–‘í•œ ë²•ë¥  ë°ì´í„° ìˆ˜ì§‘ ë° QA ë°ì´í„°ì…‹ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë“¤
- `collect_laws.py` - ë²•ë¥  ìˆ˜ì§‘
- `collect_treaties.py` - ì¡°ì•½ ìˆ˜ì§‘
- `generate_qa_dataset.py` - QA ë°ì´í„°ì…‹ ìƒì„±
- `llm_qa_generator.py` - LLM QA ìƒì„±ê¸°

#### âš¡ **benchmarking/** - ë²¤ì¹˜ë§ˆí‚¹
ëª¨ë¸ ë° ë²¡í„° ìŠ¤í† ì–´ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ì„ ë‹´ë‹¹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë“¤
- `benchmark_models.py` - ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹
- `benchmark_vector_stores.py` - ë²¡í„° ìŠ¤í† ì–´ ë²¤ì¹˜ë§ˆí‚¹

#### ğŸ§ª **tests/** - í…ŒìŠ¤íŠ¸
ë‹¤ì–‘í•œ ê¸°ëŠ¥ì˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë“¤
- `test_bge_m3_korean.py` - BGE-M3 Korean ëª¨ë¸ í…ŒìŠ¤íŠ¸
- `test_vector_embedding_basic.py` - ê¸°ë³¸ ë²¡í„° ì„ë² ë”© í…ŒìŠ¤íŠ¸
- `test_final_vector_embedding_performance.py` - ìµœì¢… ë²¡í„° ì„ë² ë”© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### ğŸš€ ì‚¬ìš©ë²•

#### ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```bash
# ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
python scripts/data_processing/run_data_pipeline.py

# íŠ¹ì • ë°ì´í„° ì „ì²˜ë¦¬
python scripts/data_processing/preprocess_raw_data.py
```

#### ë²¡í„° ì„ë² ë”© ìƒì„±
```bash
# ML ê°•í™” ë²¡í„° ì„ë² ë”© ìƒì„±
python scripts/vector_embedding/build_ml_enhanced_vector_db_cpu_optimized.py

# ë²¡í„° ì„ë² ë”© í…ŒìŠ¤íŠ¸
python scripts/tests/test_vector_embedding_basic.py
```

#### ëª¨ë¸ í›ˆë ¨
```bash
# í›ˆë ¨ ë°ì´í„°ì…‹ ì¤€ë¹„
python scripts/model_training/prepare_training_dataset.py

# ëª¨ë¸ íŒŒì¸íŠœë‹
python scripts/model_training/finetune_legal_model.py
```

#### ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
```bash
# ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜
python scripts/database/migrate_database_schema.py

# ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
python scripts/database/backup_database.py
```

### Overview
The vector embedding generation system creates high-dimensional vector representations of legal documents for semantic search and retrieval. The system features checkpoint support and graceful shutdown capabilities for long-running operations.

### Key Features

#### ğŸš€ **Performance Optimization**
- **Model**: ko-sroberta-multitask (768 dimensions)
- **Speed**: 3-5x faster than BGE-M3 (1-2 minutes per chunk vs 6-7 minutes)
- **Memory**: 99% reduction (190MB vs 16.5GB)
- **Completion Time**: 15-20 hours vs 88 hours (4-5x improvement)

#### ğŸ”„ **Checkpoint System**
- **Automatic Saving**: Progress saved every 10 chunks
- **Resume Support**: Restart from interruption point
- **Progress Tracking**: Real-time progress and ETA calculation
- **Data Integrity**: Current chunk completion before checkpoint save

#### ğŸ›¡ï¸ **Graceful Shutdown**
- **Signal Handling**: SIGTERM, SIGINT, SIGBREAK support
- **Safe Termination**: Complete current chunk before exit
- **Checkpoint Save**: Automatic checkpoint save on shutdown
- **Resume Ready**: Ready for immediate restart

### Usage

#### Basic Usage
```bash
# Start vector embedding generation
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py \
    --input data/processed/assembly/law/20251013_ml \
    --output data/embeddings/ml_enhanced_ko_sroberta \
    --batch-size 20 \
    --chunk-size 200 \
    --log-level INFO
```

#### Resume from Checkpoint
```bash
# Resume interrupted work
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py \
    --input data/processed/assembly/law/20251013_ml \
    --output data/embeddings/ml_enhanced_ko_sroberta \
    --batch-size 20 \
    --chunk-size 200 \
    --log-level INFO \
    --resume
```

#### Start Fresh (Ignore Checkpoint)
```bash
# Start from beginning (ignore existing checkpoint)
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py \
    --input data/processed/assembly/law/20251013_ml \
    --output data/embeddings/ml_enhanced_ko_sroberta \
    --batch-size 20 \
    --chunk-size 200 \
    --log-level INFO \
    --no-resume
```

### Checkpoint File Structure

#### embedding_checkpoint.json
```json
{
  "completed_chunks": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
  "total_chunks": 780,
  "start_time": 1697364074.123,
  "last_update": 1697364500.456
}
```

### Monitoring Progress

#### Check Progress
```bash
# View checkpoint file
cat data/embeddings/ml_enhanced_ko_sroberta/embedding_checkpoint.json

# Calculate progress percentage
python -c "
import json
with open('data/embeddings/ml_enhanced_ko_sroberta/embedding_checkpoint.json', 'r') as f:
    data = json.load(f)
    completed = len(data['completed_chunks'])
    total = data['total_chunks']
    print(f'Progress: {completed}/{total} ({completed/total*100:.1f}%)')
"
```

#### Log Monitoring
```bash
# Monitor logs in real-time
tail -f logs/build_ml_enhanced_vector_db.log

# Check for errors
grep "ERROR" logs/build_ml_enhanced_vector_db.log
```

### Graceful Shutdown

#### Manual Interruption
```bash
# Press Ctrl+C to gracefully shutdown
^C
# Output: Graceful shutdown requested. Saving checkpoint and exiting...
# Output: Checkpoint saved. You can resume later with --resume flag.
```

#### System Shutdown
- The system automatically handles system shutdown signals
- Checkpoint is saved before process termination
- Resume capability is maintained across reboots

### Troubleshooting

#### Common Issues

**Q: Process was killed unexpectedly**
**A**: Use `--resume` flag to continue from the last checkpoint:
```bash
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py --input ... --output ... --resume
```

**Q: Checkpoint file is corrupted**
**A**: Use `--no-resume` flag to start fresh:
```bash
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py --input ... --output ... --no-resume
```

**Q: Out of memory errors**
**A**: Reduce batch-size and chunk-size:
```bash
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py --input ... --output ... --batch-size 10 --chunk-size 100
```

**Q: Slow processing**
**A**: The ko-sroberta-multitask model is already optimized. For faster processing, consider:
- Using GPU if available
- Reducing chunk-size for more frequent checkpoints
- Using SSD storage for better I/O performance

### Performance Metrics

| Metric | BGE-M3 (Before) | ko-sroberta (After) | Improvement |
|--------|------------------|---------------------|-------------|
| Processing Speed | 6-7 min/chunk | 1-2 min/chunk | **3-5x faster** |
| Memory Usage | 16.5GB | 190MB | **99% reduction** |
| Completion Time | 88 hours | 15-20 hours | **4-5x faster** |
| Model Size | Large | Medium | **Optimized** |
| Dimensions | 1024 | 768 | **25% reduction** |

The Assembly Law Data Preprocessing Pipeline v4.0 provides ML-enhanced accuracy, hybrid parsing capabilities, comprehensive quality validation, and robust vector embedding generation with checkpoint support for reliable production use with superior legal document parsing performance.


