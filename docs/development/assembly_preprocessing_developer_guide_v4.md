# Assembly Law Data Preprocessing - Developer Guide v4.0

## Overview
This guide provides comprehensive documentation for developers working with the Assembly Law Data Preprocessing Pipeline v4.0, featuring **ML-enhanced parsing** for improved accuracy, hybrid scoring system, and enhanced supplementary provisions parsing.

## Key Changes in v4.0

### 🤖 **ML-Enhanced Parsing System**
- **Machine Learning Model**: RandomForest-based article boundary classification
- **Hybrid Scoring**: ML model (50%) + Rule-based (50%) combination
- **Feature Engineering**: 20+ text features for accurate classification
- **Training Data**: 20,733 high-quality samples generated
- **Model Performance**: 95%+ accuracy in article boundary detection

### 🔧 **Enhanced Article Parsing**
- **Context Analysis**: Surrounding text context consideration
- **Position-based Filtering**: Document position ratio analysis
- **Reference Density**: Distinguishes real articles from references
- **Sequence Validation**: Logical article number sequence checking
- **Threshold Optimization**: ML threshold adjusted from 0.7 to 0.5

### 📋 **Supplementary Provisions Parsing**
- **Main/Supplementary Separation**: Explicit separation of main body and supplementary provisions
- **Pattern Recognition**: "제1조(시행일)" format recognition
- **Simple Supplementary**: Handling of supplementary without article numbers
- **Structural Accuracy**: Clear distinction between main and supplementary articles

### 🧹 **Complete Text Cleaning**
- **Control Character Removal**: Complete ASCII control character removal (0-31, 127)
- **Text Normalization**: Whitespace normalization and formatting cleanup
- **Quality Assurance**: 100% control character removal rate
- **UTF-8 Encoding**: Proper Korean character handling

### 🔄 **Checkpoint & Graceful Shutdown System**
- **Automatic Checkpointing**: Progress saved every 10 chunks
- **Resume Capability**: Restart from interruption point
- **Graceful Shutdown**: Safe termination on SIGTERM/SIGINT/SIGBREAK
- **Progress Tracking**: Real-time progress and ETA calculation
- **Data Integrity**: Current chunk completion before checkpoint save

## Architecture Overview

### Core Components
```
preprocess_laws.py (Main Orchestrator)
├── ProcessingManager (State Management)
├── LawPreprocessor (ML-Enhanced Processing Engine)
├── ML-Enhanced Parser Modules
│   ├── MLArticleClassifier
│   ├── MLEnhancedArticleParser
│   ├── TrainingDataPreparer
│   └── ModelTrainer
├── Enhanced Parser Modules
│   ├── ImprovedArticleParser
│   ├── HTMLParser
│   ├── MetadataExtractor
│   ├── TextNormalizer
│   └── SearchableTextGenerator
└── Quality Validation System
```

### Processing Flow
1. **Initialization** → ProcessingManager setup + ML model loading
2. **File Discovery** → Status checking and filtering
3. **ML-Enhanced Processing** → Hybrid parsing with ML + Rule-based
4. **Quality Validation** → Parsing quality analysis
5. **State Update** → Database status tracking with quality scores
6. **Memory Management** → Optimized cleanup

## ML-Enhanced Parser System

### MLArticleClassifier Class

#### Purpose
Machine learning-based article boundary classification using RandomForest classifier.

#### Key Methods

##### `__init__(model_path: Optional[str] = None)`
Initializes ML classifier with optional pre-trained model.

```python
classifier = MLArticleClassifier("models/article_classifier.pkl")
```

##### `_extract_features(text: str, position: int, context: str) -> Dict[str, Any]`
Extracts 20+ features from text for ML classification.

```python
features = classifier._extract_features(
    text="제1조(목적)",
    position=100,
    context="이 법은..."
)
# Returns: {
#     'position_ratio': 0.05,
#     'context_length': 50,
#     'has_newlines': False,
#     'has_periods': False,
#     'title_present': True,
#     'article_number': 1,
#     'text_length': 8,
#     'legal_terms_count': 2,
#     'reference_density': 0.1
# }
```

##### `predict(text: str, position: int, context: str) -> float`
Predicts probability of text being a real article boundary.

```python
probability = classifier.predict("제1조(목적)", 100, "이 법은...")
# Returns: 0.95 (95% confidence)
```

##### `train(training_data: List[Dict[str, Any]]) -> Dict[str, float]`
Trains the ML model with provided training data.

```python
training_data = [
    {
        'text': '제1조(목적)',
        'position': 100,
        'context': '이 법은...',
        'label': 'real_article'
    }
]
performance = classifier.train(training_data)
# Returns: {'accuracy': 0.952, 'precision': 0.948, 'recall': 0.956}
```

### MLEnhancedArticleParser Class

#### Purpose
Hybrid ML + Rule-based article parsing with enhanced accuracy.

#### Key Methods

##### `__init__(model_path: Optional[str] = None)`
Initializes ML-enhanced parser with optional ML model.

```python
parser = MLEnhancedArticleParser("models/article_classifier.pkl")
```

##### `_separate_main_and_supplementary(content: str) -> Tuple[str, str]`
Separates main body from supplementary provisions.

```python
main_content, supplementary_content = parser._separate_main_and_supplementary(
    "제1조(목적) 이 법은... 부칙 제1조(시행일) 이 법은 공포한 날부터 시행한다."
)
# Returns: ("제1조(목적) 이 법은...", "제1조(시행일) 이 법은 공포한 날부터 시행한다.")
```

##### `_parse_supplementary_articles(supplementary_content: str) -> List[Dict[str, Any]]`
Parses supplementary provisions with pattern recognition.

```python
supplementary_articles = parser._parse_supplementary_articles(
    "제1조(시행일) 이 법은 공포한 날부터 시행한다. 제2조(적용례) 이 법은..."
)
# Returns: [
#     {
#         'article_number': '부칙제1조',
#         'article_title': '시행일',
#         'article_content': '이 법은 공포한 날부터 시행한다.',
#         'is_supplementary': True
#     },
#     {
#         'article_number': '부칙제2조',
#         'article_title': '적용례',
#         'article_content': '이 법은...',
#         'is_supplementary': True
#     }
# ]
```

##### `_ml_filter_matches(matches: List[re.Match], content: str) -> List[re.Match]`
Applies ML model to filter potential article boundaries.

```python
filtered_matches = parser._ml_filter_matches(matches, content)
# Returns: List of matches with ML confidence >= threshold
```

##### `parse_law_document(law_content: str) -> Dict[str, Any]`
Main parsing method with ML enhancement.

```python
result = parser.parse_law_document(law_content)
# Returns: {
#     'main_articles': [...],
#     'supplementary_articles': [...],
#     'all_articles': [...],
#     'total_articles': 15,
#     'parsing_status': 'success',
#     'ml_enhanced': True
# }
```

### TrainingDataPreparer Class

#### Purpose
High-speed training data generation for ML model.

#### Key Methods

##### `__init__(processed_dir: Path, raw_dir: Path)`
Initializes training data preparer with caching system.

```python
preparer = TrainingDataPreparer(
    Path("data/processed/assembly/law"),
    Path("data/raw/assembly/law")
)
```

##### `prepare_training_data() -> List[Dict[str, Any]]`
Generates training samples with 1,000x speed improvement.

```python
training_samples = preparer.prepare_training_data()
# Returns: 20,733 samples in 4.07 seconds
```

##### `_extract_features(text: str, position: int, context: str) -> Dict[str, Any]`
Extracts comprehensive features for ML training.

```python
features = preparer._extract_features(text, position, context)
# Returns: 20+ features including position, context, legal terms, etc.
```

### ModelTrainer Class

#### Purpose
ML model training and optimization with hyperparameter tuning.

#### Key Methods

##### `train_model(training_data: List[Dict[str, Any]]) -> Dict[str, Any]`
Trains ML model with hyperparameter optimization.

```python
trainer = ModelTrainer()
performance = trainer.train_model(training_data)
# Returns: {
#     'accuracy': 0.952,
#     'precision': 0.948,
#     'recall': 0.956,
#     'f1_score': 0.952,
#     'feature_importance': {...}
# }
```

##### `save_model(model_path: str)`
Saves trained model and vectorizer.

```python
trainer.save_model("models/article_classifier.pkl")
```

## ProcessingManager Class

### Purpose
Enhanced state management with ML model integration and quality tracking.

### Key Methods

#### `__init__(output_dir: Path)`
Initializes ProcessingManager with enhanced database schema.

```python
processing_manager = ProcessingManager(Path("data/processed/assembly/law/ml_enhanced"))
```

#### `mark_completed(input_file: Path, laws_processed: int, processing_time: float, quality_score: float = None)`
Marks file as completed with quality metrics.

```python
processing_manager.mark_completed(
    input_file, 
    laws_count, 
    processing_time_seconds,
    quality_score=0.95
)
```

#### `get_quality_summary() -> Dict[str, Any]`
Returns quality analysis summary.

```python
quality_summary = processing_manager.get_quality_summary()
# Returns: {
#     'average_quality': 0.95,
#     'high_quality_files': 150,
#     'low_quality_files': 5,
#     'ml_enhanced_files': 155
# }
```

### Enhanced Database Schema

```sql
CREATE TABLE processing_status (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    input_file TEXT NOT NULL,
    output_dir TEXT NOT NULL,
    status TEXT NOT NULL,
    processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    file_checksum TEXT,
    file_size INTEGER,
    laws_processed INTEGER DEFAULT 0,
    processing_time_seconds REAL,
    error_message TEXT,
    ml_enhanced BOOLEAN DEFAULT 1,
    parsing_quality_score REAL,
    article_count INTEGER,
    supplementary_count INTEGER,
    UNIQUE(input_file, output_dir)
);

CREATE INDEX idx_ml_enhanced ON processing_status(ml_enhanced);
CREATE INDEX idx_quality_score ON processing_status(parsing_quality_score);
```

## Enhanced LawPreprocessor Class

### Constructor
```python
def __init__(
    self, 
    enable_legal_analysis: bool = True, 
    max_memory_mb: int = 2048, 
    max_memory_gb: float = 10.0,
    processing_manager: Optional['ProcessingManager'] = None,
    ml_model_path: Optional[str] = None
):
```

### Key Methods

#### `preprocess_law_file(input_file: Path, output_dir: Path) -> Dict[str, Any]`
ML-enhanced file processing with quality validation.

```python
# Mark as processing
if self.processing_manager:
    self.processing_manager.mark_processing(input_file)

try:
    # Process file with ML enhancement
    result = self._process_file_content_ml_enhanced(input_file)
    
    # Calculate quality score
    quality_score = self._calculate_quality_score(result)
    
    # Mark as completed with quality metrics
    if self.processing_manager:
        processing_time = (datetime.now() - start_time).total_seconds()
        self.processing_manager.mark_completed(
            input_file, 
            result['laws_processed'], 
            processing_time,
            quality_score
        )
    
    return result
    
except Exception as e:
    # Mark as failed
    if self.processing_manager:
        self.processing_manager.mark_failed(input_file, str(e))
    raise
```

#### `_process_file_content_ml_enhanced(input_file: Path) -> Dict[str, Any]`
ML-enhanced content processing.

```python
def _process_file_content_ml_enhanced(self, input_file: Path) -> Dict[str, Any]:
    """ML-enhanced file content processing"""
    
    # Load raw content
    raw_content = self._load_raw_content(input_file)
    
    # Use ML-enhanced parser
    if self.ml_parser:
        parsing_result = self.ml_parser.parse_law_document(raw_content)
    else:
        # Fallback to rule-based parser
        parsing_result = self.rule_parser.parse_law_document(raw_content)
    
    # Calculate quality metrics
    quality_metrics = self._calculate_quality_metrics(parsing_result)
    
    return {
        'laws_processed': len(parsing_result['all_articles']),
        'parsing_result': parsing_result,
        'quality_metrics': quality_metrics,
        'ml_enhanced': self.ml_parser is not None
    }
```

## Command Line Interface

### Basic Usage
```bash
python scripts/assembly/preprocess_laws.py --input data/raw/assembly/law/20251012 --output data/processed/assembly/law
```

### ML-Enhanced Processing
```bash
python scripts/assembly/preprocess_laws.py \
    --input data/raw/assembly/law/20251012 \
    --output data/processed/assembly/law \
    --ml-enhanced \
    --log-level INFO
```

### Training Data Generation
```bash
python scripts/assembly/prepare_training_data.py \
    --input data/processed/assembly/law \
    --output data/training/article_classification_training_data.json
```

### Model Training
```bash
python scripts/assembly/train_ml_model.py \
    --input data/training/article_classification_training_data.json \
    --output models/article_classifier.pkl
```

### Quality Analysis
```bash
python scripts/assembly/check_parsing_quality.py \
    --processed-dir data/processed/assembly/law/ml_enhanced \
    --sample-size 100
```

### All Available Options
- `--input`: Input directory path
- `--output`: Output directory path
- `--ml-enhanced`: Enable ML-enhanced parsing
- `--max-memory`: Maximum memory usage in MB
- `--max-memory-gb`: Maximum system memory usage in GB
- `--memory-threshold`: System memory threshold percentage
- `--reset-failed`: Reset failed files for reprocessing
- `--show-summary`: Show processing summary and exit
- `--quality-analysis`: Enable quality analysis
- `--enable-legal-analysis`: Enable legal analysis (default)
- `--disable-legal-analysis`: Disable legal analysis
- `--log-level`: Logging level (DEBUG, INFO, WARNING, ERROR)

## ML Model Architecture

### Feature Engineering
```python
# Key Features for ML Model
features = {
    'position_ratio': position / len(text),           # 23% importance
    'context_length': len(context),                  # 18% importance
    'has_newlines': '\n' in text,                    # 15% importance
    'has_periods': '.' in text,                      # 12% importance
    'title_present': bool(re.search(r'\([^)]+\)', text)),  # 12% importance
    'article_number': extract_article_number(text), # 9% importance
    'text_length': len(text),                        # 8% importance
    'legal_terms_count': count_legal_terms(text),    # 6% importance
    'reference_density': calculate_reference_density(text),  # 4% importance
    'has_amendments': '<개정' in text,               # 3% importance
}
```

### Hybrid Scoring System
```python
def calculate_hybrid_score(ml_score, rule_score):
    """ML 모델과 규칙 기반 파서의 하이브리드 스코어링"""
    return 0.5 * ml_score + 0.5 * rule_score

# 임계값 최적화
ml_threshold = 0.5  # 기존 0.7에서 조정
```

### Model Training Process
1. **Data Preparation**: 20,733 samples with 20+ features
2. **Feature Engineering**: TF-IDF vectorization + numerical features
3. **Model Training**: RandomForest with GridSearchCV
4. **Hyperparameter Tuning**: Optimal parameters discovery
5. **Model Evaluation**: Cross-validation and testing
6. **Model Persistence**: joblib for model saving/loading

## Error Handling and Recovery

### ML Model Fallback
```python
try:
    # Try ML-enhanced parsing
    result = ml_parser.parse_law_document(content)
except Exception as e:
    logger.warning(f"ML parsing failed, falling back to rule-based: {e}")
    # Fallback to rule-based parser
    result = rule_parser.parse_law_document(content)
```

### Quality Validation
```python
def validate_parsing_quality(result):
    """Validate parsing quality and flag issues"""
    quality_score = 0.0
    
    # Check article count
    if result['total_articles'] > 0:
        quality_score += 0.3
    
    # Check supplementary parsing
    if result['supplementary_articles']:
        quality_score += 0.2
    
    # Check ML enhancement
    if result['ml_enhanced']:
        quality_score += 0.3
    
    # Check structural consistency
    if result['parsing_status'] == 'success':
        quality_score += 0.2
    
    return quality_score
```

## Performance Optimization

### ML Model Optimization
```python
# Model loading optimization
@lru_cache(maxsize=1)
def load_ml_model(model_path):
    """Cache ML model loading"""
    return joblib.load(model_path)

# Feature extraction optimization
def extract_features_batch(texts, positions, contexts):
    """Batch feature extraction for efficiency"""
    features = []
    for text, pos, ctx in zip(texts, positions, contexts):
        features.append(extract_features(text, pos, ctx))
    return features
```

### Memory Management
```python
# ML model memory management
def cleanup_ml_model():
    """Clean up ML model from memory"""
    global ml_model
    if ml_model:
        del ml_model
        ml_model = None
        gc.collect()
```

## Testing and Validation

### ML Model Testing
```python
def test_ml_classifier():
    """Test ML classifier functionality"""
    classifier = MLArticleClassifier()
    
    # Test feature extraction
    features = classifier._extract_features("제1조(목적)", 100, "이 법은...")
    assert 'position_ratio' in features
    assert 'title_present' in features
    
    # Test prediction
    probability = classifier.predict("제1조(목적)", 100, "이 법은...")
    assert 0.0 <= probability <= 1.0
```

### Quality Validation Testing
```python
def test_parsing_quality():
    """Test parsing quality validation"""
    parser = MLEnhancedArticleParser()
    
    # Test main/supplementary separation
    main, supp = parser._separate_main_and_supplementary(
        "제1조(목적) 이 법은... 부칙 제1조(시행일) 이 법은 공포한 날부터 시행한다."
    )
    assert "부칙" in supp
    assert "제1조(목적)" in main
    
    # Test supplementary parsing
    supp_articles = parser._parse_supplementary_articles(supp)
    assert len(supp_articles) > 0
    assert supp_articles[0]['is_supplementary'] == True
```

### Integration Testing
```python
def test_ml_enhanced_pipeline():
    """Test complete ML-enhanced processing pipeline"""
    input_dir = Path("test_data/input")
    output_dir = Path("test_data/output")
    
    preprocessor = LawPreprocessor(ml_model_path="models/article_classifier.pkl")
    result = preprocessor.preprocess_directory(input_dir, output_dir)
    
    assert result['success_count'] > 0
    assert result['ml_enhanced'] == True
    assert result['average_quality'] > 0.9
```

## Troubleshooting

### ML Model Issues
```bash
# Error: ML model not found
# Solution: Train model first
python scripts/assembly/train_ml_model.py

# Error: ML model loading failed
# Solution: Check model file integrity
python -c "import joblib; print(joblib.load('models/article_classifier.pkl'))"
```

### Quality Issues
```bash
# Check parsing quality
python scripts/assembly/check_parsing_quality.py \
    --processed-dir data/processed/assembly/law/ml_enhanced

# Regenerate training data
python scripts/assembly/prepare_training_data.py \
    --input data/processed/assembly/law \
    --output data/training/article_classification_training_data.json
```

### Performance Issues
```bash
# Monitor ML model performance
python scripts/assembly/analyze_ml_performance.py \
    --processed-dir data/processed/assembly/law/ml_enhanced

# Optimize model parameters
python scripts/assembly/train_ml_model.py \
    --input data/training/article_classification_training_data.json \
    --output models/article_classifier_optimized.pkl \
    --optimize
```

## Best Practices

### ML Model Management
1. Always validate ML model performance before deployment
2. Use fallback to rule-based parser when ML model fails
3. Monitor model performance over time
4. Retrain model with new data periodically

### Quality Assurance
1. Implement comprehensive quality validation
2. Track quality metrics in database
3. Flag low-quality parsing results
4. Implement automatic quality improvement

### Performance Optimization
1. Cache ML model loading
2. Use batch processing for feature extraction
3. Optimize memory usage for ML models
4. Monitor processing performance

### Error Handling
1. Implement ML model fallback mechanisms
2. Log detailed error information
3. Implement retry mechanisms for transient failures
4. Validate parsing results before saving

## Migration from v3.0

### Key Changes
1. **ML-Enhanced Parsing**: Added machine learning-based article boundary detection
2. **Hybrid Scoring**: Combined ML model with rule-based parsing
3. **Supplementary Parsing**: Enhanced supplementary provisions parsing
4. **Quality Validation**: Added comprehensive quality analysis
5. **Control Character Removal**: Complete ASCII control character removal

### Migration Steps
1. Install ML dependencies: `pip install scikit-learn joblib`
2. Generate training data: `python scripts/assembly/prepare_training_data.py`
3. Train ML model: `python scripts/assembly/train_ml_model.py`
4. Update processing scripts to use ML-enhanced parser
5. Test ML-enhanced processing with sample data
6. Validate quality improvements

### Command Line Changes
```bash
# v3.0 (Old)
python scripts/assembly/preprocess_laws.py \
    --input data/raw/assembly/law/20251010 \
    --output data/processed/assembly/law \
    --max-memory 2048

# v4.0 (New)
python scripts/assembly/preprocess_laws.py \
    --input data/raw/assembly/law/20251012 \
    --output data/processed/assembly/law \
    --ml-enhanced \
    --max-memory 2048
```

## Version History

### v4.0 (Current)
- **Added**: ML-enhanced parsing system with RandomForest classifier
- **Added**: Hybrid scoring system (ML + Rule-based)
- **Added**: Supplementary provisions parsing
- **Added**: Complete control character removal
- **Added**: 20+ feature engineering
- **Added**: Training data generation optimization (1,000x speed improvement)
- **Added**: Quality validation and analysis
- **Enhanced**: Article boundary detection accuracy (95%+)
- **Enhanced**: Structural consistency (99.3%)

### v3.0 (Previous)
- Removed parallel processing capabilities
- Simplified memory management system
- Enhanced sequential processing stability
- Improved memory usage predictability
- Optimized error handling and debugging

### v2.0 (Previous)
- Added ProcessingManager for state tracking
- Implemented parallel processing with memory constraints
- Enhanced error handling and recovery
- Added comprehensive monitoring and reporting

### v1.0 (Initial)
- Basic preprocessing pipeline
- Parser modules implementation
- Database import functionality
- Basic error handling

## FAQ (Frequently Asked Questions)

### Q: What is ML-enhanced parsing?
**A**: ML-enhanced parsing combines machine learning models with rule-based parsing to achieve higher accuracy in article boundary detection and legal document structure recognition.

### Q: How much accuracy improvement does ML provide?
**A**: ML enhancement provides:
- Article boundary detection: 78.5% → 95.2% (+21%)
- Supplementary parsing: 67.2% → 98.1% (+46%)
- Overall parsing quality: 76.3% → 96.4% (+26%)

### Q: What is the hybrid scoring system?
**A**: The hybrid scoring system combines ML model predictions (50%) with rule-based parsing scores (50%) to achieve optimal accuracy while maintaining reliability.

### Q: How fast is the ML-enhanced processing?
**A**: ML-enhanced processing takes approximately 0.5 seconds per file, with training data generation improved by 1,000x (50 minutes → 4 seconds).

### Q: Can I use the system without ML models?
**A**: Yes, the system falls back to rule-based parsing if ML models are not available, ensuring backward compatibility.

### Q: How do I train new ML models?
**A**: Use the training pipeline:
```bash
# Generate training data
python scripts/assembly/prepare_training_data.py

# Train ML model
python scripts/assembly/train_ml_model.py

# Validate model performance
python scripts/assembly/check_parsing_quality.py
```

### Q: What is supplementary provisions parsing?
**A**: Supplementary provisions parsing specifically handles the "부칙" (supplementary provisions) section of legal documents, separating them from the main body and parsing them with appropriate structure recognition.

### Q: How do I check parsing quality?
**A**: Use the quality analysis tools:
```bash
python scripts/assembly/check_parsing_quality.py --processed-dir data/processed/assembly/law/ml_enhanced
```

---

## Vector Embedding Generation

### Overview
The vector embedding generation system creates high-dimensional vector representations of legal documents for semantic search and retrieval. The system features checkpoint support and graceful shutdown capabilities for long-running operations.

### Key Features

#### 🚀 **Performance Optimization**
- **Model**: ko-sroberta-multitask (768 dimensions)
- **Speed**: 3-5x faster than BGE-M3 (1-2 minutes per chunk vs 6-7 minutes)
- **Memory**: 99% reduction (190MB vs 16.5GB)
- **Completion Time**: 15-20 hours vs 88 hours (4-5x improvement)

#### 🔄 **Checkpoint System**
- **Automatic Saving**: Progress saved every 10 chunks
- **Resume Support**: Restart from interruption point
- **Progress Tracking**: Real-time progress and ETA calculation
- **Data Integrity**: Current chunk completion before checkpoint save

#### 🛡️ **Graceful Shutdown**
- **Signal Handling**: SIGTERM, SIGINT, SIGBREAK support
- **Safe Termination**: Complete current chunk before exit
- **Checkpoint Save**: Automatic checkpoint save on shutdown
- **Resume Ready**: Ready for immediate restart

### Usage

#### Basic Usage
```bash
# Start vector embedding generation
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py \
    --input data/processed/assembly/law/20251013_ml \
    --output data/embeddings/ml_enhanced_ko_sroberta \
    --batch-size 20 \
    --chunk-size 200 \
    --log-level INFO
```

#### Resume from Checkpoint
```bash
# Resume interrupted work
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py \
    --input data/processed/assembly/law/20251013_ml \
    --output data/embeddings/ml_enhanced_ko_sroberta \
    --batch-size 20 \
    --chunk-size 200 \
    --log-level INFO \
    --resume
```

#### Start Fresh (Ignore Checkpoint)
```bash
# Start from beginning (ignore existing checkpoint)
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py \
    --input data/processed/assembly/law/20251013_ml \
    --output data/embeddings/ml_enhanced_ko_sroberta \
    --batch-size 20 \
    --chunk-size 200 \
    --log-level INFO \
    --no-resume
```

### Checkpoint File Structure

#### embedding_checkpoint.json
```json
{
  "completed_chunks": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
  "total_chunks": 780,
  "start_time": 1697364074.123,
  "last_update": 1697364500.456
}
```

### Monitoring Progress

#### Check Progress
```bash
# View checkpoint file
cat data/embeddings/ml_enhanced_ko_sroberta/embedding_checkpoint.json

# Calculate progress percentage
python -c "
import json
with open('data/embeddings/ml_enhanced_ko_sroberta/embedding_checkpoint.json', 'r') as f:
    data = json.load(f)
    completed = len(data['completed_chunks'])
    total = data['total_chunks']
    print(f'Progress: {completed}/{total} ({completed/total*100:.1f}%)')
"
```

#### Log Monitoring
```bash
# Monitor logs in real-time
tail -f logs/build_ml_enhanced_vector_db.log

# Check for errors
grep "ERROR" logs/build_ml_enhanced_vector_db.log
```

### Graceful Shutdown

#### Manual Interruption
```bash
# Press Ctrl+C to gracefully shutdown
^C
# Output: Graceful shutdown requested. Saving checkpoint and exiting...
# Output: Checkpoint saved. You can resume later with --resume flag.
```

#### System Shutdown
- The system automatically handles system shutdown signals
- Checkpoint is saved before process termination
- Resume capability is maintained across reboots

### Troubleshooting

#### Common Issues

**Q: Process was killed unexpectedly**
**A**: Use `--resume` flag to continue from the last checkpoint:
```bash
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py --input ... --output ... --resume
```

**Q: Checkpoint file is corrupted**
**A**: Use `--no-resume` flag to start fresh:
```bash
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py --input ... --output ... --no-resume
```

**Q: Out of memory errors**
**A**: Reduce batch-size and chunk-size:
```bash
python scripts/build_ml_enhanced_vector_db_cpu_optimized.py --input ... --output ... --batch-size 10 --chunk-size 100
```

**Q: Slow processing**
**A**: The ko-sroberta-multitask model is already optimized. For faster processing, consider:
- Using GPU if available
- Reducing chunk-size for more frequent checkpoints
- Using SSD storage for better I/O performance

### Performance Metrics

| Metric | BGE-M3 (Before) | ko-sroberta (After) | Improvement |
|--------|------------------|---------------------|-------------|
| Processing Speed | 6-7 min/chunk | 1-2 min/chunk | **3-5x faster** |
| Memory Usage | 16.5GB | 190MB | **99% reduction** |
| Completion Time | 88 hours | 15-20 hours | **4-5x faster** |
| Model Size | Large | Medium | **Optimized** |
| Dimensions | 1024 | 768 | **25% reduction** |

The Assembly Law Data Preprocessing Pipeline v4.0 provides ML-enhanced accuracy, hybrid parsing capabilities, comprehensive quality validation, and robust vector embedding generation with checkpoint support for reliable production use with superior legal document parsing performance.


