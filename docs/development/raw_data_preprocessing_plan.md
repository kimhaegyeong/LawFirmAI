# Raw ë°ì´í„° ì „ì²˜ë¦¬ ê³„íšì„œ v1.0

## ğŸ“‹ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” LawFirmAI í”„ë¡œì íŠ¸ì—ì„œ ìˆ˜ì§‘ëœ raw ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ê³„íšì„ ì œì‹œí•©ë‹ˆë‹¤. êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° OpenAPIë¥¼ í†µí•´ ìˆ˜ì§‘ëœ ë²•ë¥  ë°ì´í„°ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ê³¼ RAG ì‹œìŠ¤í…œì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ¯ ì „ì²˜ë¦¬ ëª©í‘œ

### ì£¼ìš” ëª©í‘œ
- **ë°ì´í„° í‘œì¤€í™”**: ìˆ˜ì§‘ëœ raw ë°ì´í„°ë¥¼ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- **í’ˆì§ˆ í–¥ìƒ**: í…ìŠ¤íŠ¸ ì •ë¦¬, ìš©ì–´ ì •ê·œí™”, ì¤‘ë³µ ì œê±°ë¥¼ í†µí•œ ë°ì´í„° í’ˆì§ˆ ê°œì„ 
- **êµ¬ì¡°í™”**: ë²¡í„° ê²€ìƒ‰ê³¼ RAG ì‹œìŠ¤í…œì— ìµœì í™”ëœ ë°ì´í„° êµ¬ì¡° ìƒì„±
- **í™•ì¥ì„±**: í–¥í›„ ì¶”ê°€ ë°ì´í„° ìœ í˜•ì— ëŒ€í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í™•ì¥ ê°€ëŠ¥

### ì„±ëŠ¥ ëª©í‘œ
- **ì²˜ë¦¬ ì†ë„**: ì‹œê°„ë‹¹ 1,000ê°œ ë¬¸ì„œ ì´ìƒ ì²˜ë¦¬
- **í’ˆì§ˆ ì§€í‘œ**: ì™„ì„±ë„ 95%, ì •í™•ë„ 98%, ì¼ê´€ì„± 90% ì´ìƒ
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 16GB ì´í•˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- **ì—ëŸ¬ìœ¨**: 5% ì´í•˜ ì²˜ë¦¬ ì‹¤íŒ¨ìœ¨

---

## ğŸ“Š í˜„ì¬ ë°ì´í„° í˜„í™©

### ìˆ˜ì§‘ëœ ë°ì´í„° ìœ í˜•ë³„ í˜„í™©

| ë°ì´í„° ìœ í˜• | ìˆ˜ì§‘ ìƒíƒœ | íŒŒì¼ ìˆ˜ | ì˜ˆìƒ í¬ê¸° | ìš°ì„ ìˆœìœ„ |
|------------|----------|---------|-----------|----------|
| **ë²•ë ¹ ë°ì´í„°** | âœ… ì™„ë£Œ | 21ê°œ | ~50MB | Critical |
| **íŒë¡€ ë°ì´í„°** | âœ… ì™„ë£Œ | ì—°ë„ë³„ ìˆ˜ì§‘ | ~500MB | Critical |
| **í—Œì¬ê²°ì •ë¡€** | âœ… ì™„ë£Œ | 2024-2025ë…„ | ~100MB | High |
| **ë²•ë ¹í•´ì„ë¡€** | âœ… ì™„ë£Œ | ë°°ì¹˜ë³„ ìˆ˜ì§‘ | ~200MB | High |
| **ë²•ë¥  ìš©ì–´** | âœ… ì™„ë£Œ | ì„¸ì…˜ë³„ ìˆ˜ì§‘ | ~10MB | Medium |
| **í–‰ì •ê·œì¹™** | â³ ëŒ€ê¸° | 0ê°œ | - | Low |
| **ìì¹˜ë²•ê·œ** | â³ ëŒ€ê¸° | 0ê°œ | - | Low |
| **ìœ„ì›íšŒê²°ì •ë¬¸** | â³ ëŒ€ê¸° | 0ê°œ | - | Low |
| **ì¡°ì•½** | â³ ëŒ€ê¸° | 0ê°œ | - | Low |

### ë°ì´í„° êµ¬ì¡° ë¶„ì„

#### ë²•ë ¹ ë°ì´í„° êµ¬ì¡°
```json
{
  "basic_info": {
    "id": "ë²•ë¥ ID",
    "name": "ë²•ë¥ ëª…",
    "mst": "ë§ˆìŠ¤í„°ID",
    "effective_date": "ì‹œí–‰ì¼ì",
    "promulgation_date": "ê³µí¬ì¼ì",
    "ministry": "ì†Œê´€ë¶€ì²˜",
    "category": "ë¶„ë¥˜"
  },
  "current_text": {
    "response": {
      "body": {
        "items": {
          "item": {
            "ì¡°ë¬¸ë‚´ìš©": "ë²•ë ¹ ë³¸ë¬¸",
            "ì¡°ë¬¸ì œëª©": "ì¡°ë¬¸ ì œëª©"
          }
        }
      }
    }
  },
  "history": [
    {
      "ì—°í˜ID": "ID",
      "ì‹œí–‰ì¼ì": "ë‚ ì§œ",
      "ê³µí¬ì¼ì": "ë‚ ì§œ",
      "ì œê°œì •êµ¬ë¶„": "êµ¬ë¶„",
      "ë‚´ìš©": "ë‚´ìš©",
      "ì œê°œì •ì´ìœ ": "ì´ìœ "
    }
  ]
}
```

#### íŒë¡€ ë°ì´í„° êµ¬ì¡°
```json
{
  "basic_info": {
    "íŒë¡€ì¼ë ¨ë²ˆí˜¸": "ID",
    "ì‚¬ê±´ëª…": "ì‚¬ê±´ëª…",
    "ì‚¬ê±´ë²ˆí˜¸": "ì‚¬ê±´ë²ˆí˜¸",
    "ë²•ì›ëª…": "ë²•ì›ëª…",
    "ë²•ì›ì½”ë“œ": "ì½”ë“œ",
    "ì„ ê³ ì¼ì": "ë‚ ì§œ",
    "ì‚¬ê±´ìœ í˜•ëª…": "ìœ í˜•",
    "ì‚¬ê±´ìœ í˜•ì½”ë“œ": "ì½”ë“œ",
    "íŒê²°ìœ í˜•": "ìœ í˜•"
  },
  "detail_info": {
    "response": {
      "body": {
        "items": {
          "item": {
            "íŒì‹œì‚¬í•­": "íŒì‹œì‚¬í•­",
            "íŒê²°ìš”ì§€": "íŒê²°ìš”ì§€",
            "ì‚¬ê±´ê°œìš”": "ì‚¬ê±´ê°œìš”",
            "ìŸì ": "ìŸì ",
            "ê²°ë¡ ": "ê²°ë¡ ",
            "ì°¸ì¡°ì¡°ë¬¸": "ì°¸ì¡°ì¡°ë¬¸",
            "ì°¸ì¡°íŒë¡€": "ì°¸ì¡°íŒë¡€",
            "í‚¤ì›Œë“œ": "í‚¤ì›Œë“œ",
            "ë¶„ë¥˜": "ë¶„ë¥˜"
          }
        }
      }
    }
  }
}
```

---

## ğŸ› ï¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ê³„

### 3ë‹¨ê³„ ì „ì²˜ë¦¬ ì•„í‚¤í…ì²˜

```mermaid
graph TD
    A[Raw Data] --> B[Phase 1: ê¸°ë³¸ ì •ë¦¬]
    B --> C[Phase 2: êµ¬ì¡°í™”]
    C --> D[Phase 3: ìµœì í™”]
    D --> E[Processed Data]
    
    B --> B1[í…ìŠ¤íŠ¸ ì •ë¦¬]
    B --> B2[ì¸ì½”ë”© ì •ê·œí™”]
    B --> B3[HTML íƒœê·¸ ì œê±°]
    
    C --> C1[ì—”í‹°í‹° ì¶”ì¶œ]
    C --> C2[ìš©ì–´ ì •ê·œí™”]
    C --> C3[ì²­í‚¹ ìƒì„±]
    C --> C4[ë©”íƒ€ë°ì´í„° ìƒì„±]
    
    D --> D1[ì¤‘ë³µ ì œê±°]
    D --> D2[í’ˆì§ˆ ê²€ì¦]
    D --> D3[ì¸ë±ì‹±]
    D --> D4[í†µí•©]
```

### í•µì‹¬ ì „ì²˜ë¦¬ ëª¨ë“ˆ

#### 1. LegalDataProcessor
```python
class LegalDataProcessor:
    """ë²•ë¥  ë°ì´í„° ì „ì²˜ë¦¬ í•µì‹¬ í´ë˜ìŠ¤"""
    
    def __init__(self, enable_term_normalization=True):
        self.term_normalizer = LegalTermNormalizer()
        self.text_chunker = LegalTextChunker()
        self.quality_validator = QualityValidator()
    
    def process_law_data(self, law_data: Dict) -> Dict:
        """ë²•ë ¹ ë°ì´í„° ì „ì²˜ë¦¬"""
        pass
    
    def process_precedent_data(self, precedent_data: Dict) -> Dict:
        """íŒë¡€ ë°ì´í„° ì „ì²˜ë¦¬"""
        pass
    
    def process_batch(self, data_list: List[Dict], data_type: str) -> List[Dict]:
        """ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬"""
        pass
```

#### 2. LegalTermNormalizer
```python
class LegalTermNormalizer:
    """ë²•ë¥  ìš©ì–´ ì •ê·œí™” í´ë˜ìŠ¤"""
    
    def normalize_text(self, text: str, context: str = None) -> Dict:
        """ë‹¤ì¸µ ì •ê·œí™” íŒŒì´í”„ë¼ì¸"""
        # Level 1: ê¸°ë³¸ ì •ê·œí™”
        # Level 2: ë²•ë¥  ìš©ì–´ í‘œì¤€í™”
        # Level 3: ì˜ë¯¸ì  ì •ê·œí™”
        # Level 4: êµ¬ì¡°ì  ì •ê·œí™”
        pass
```

#### 3. QualityValidator
```python
class QualityValidator:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í´ë˜ìŠ¤"""
    
    def validate_document(self, document: Dict) -> Tuple[bool, List[str]]:
        """ë¬¸ì„œ ìœ íš¨ì„± ê²€ì‚¬"""
        pass
    
    def check_completeness(self, document: Dict) -> float:
        """ì™„ì„±ë„ ê²€ì‚¬"""
        pass
    
    def check_consistency(self, document: Dict) -> float:
        """ì¼ê´€ì„± ê²€ì‚¬"""
        pass
```

---

## ğŸ“… ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš

### Phase 1: í•µì‹¬ ë°ì´í„° ì „ì²˜ë¦¬ (1-2ì¼)

#### 1.1 ë²•ë ¹ ë°ì´í„° ì „ì²˜ë¦¬
**ëª©í‘œ**: 21ê°œ ë²•ë ¹ íŒŒì¼ ì „ì²˜ë¦¬ ì™„ë£Œ
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 30ë¶„
**ì²˜ë¦¬ ê³¼ì •**:
1. JSON íŒŒì¼ ë¡œë“œ ë° íŒŒì‹±
2. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ (ë²•ë¥ ëª…, ì‹œí–‰ì¼ì, ì†Œê´€ë¶€ì²˜ ë“±)
3. ì¡°ë¬¸ ë‚´ìš© ì¶”ì¶œ ë° ì •ë¦¬
4. í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì²­í‚¹
5. ë²•ë¥  ìš©ì–´ ì •ê·œí™” ì ìš©
6. ë©”íƒ€ë°ì´í„° ìƒì„±

**ì¶œë ¥ í˜•ì‹**:
```json
{
  "id": "law_ë¯¼ë²•",
  "law_name": "ë¯¼ë²•",
  "law_id": "ë²•ë¥ ì œ00000í˜¸",
  "effective_date": "2024-01-01",
  "ministry": "ë²•ë¬´ë¶€",
  "category": "ë¯¼ì‚¬ë²•",
  "cleaned_content": "ì •ë¦¬ëœ ë²•ë ¹ ë³¸ë¬¸",
  "chunks": [
    {
      "id": "chunk_0",
      "text": "ì²­í¬ ë‚´ìš©",
      "start_pos": 0,
      "end_pos": 1000,
      "entities": {...}
    }
  ],
  "entities": {
    "laws": ["ë¯¼ë²•", "ìƒë²•"],
    "articles": ["ì œ1ì¡°", "ì œ2ì¡°"],
    "keywords": ["ê³„ì•½", "ì±„ê¶Œ", "ì±„ë¬´"]
  },
  "document_hash": "md5_hash",
  "processed_at": "2024-01-01T00:00:00Z"
}
```

#### 1.2 íŒë¡€ ë°ì´í„° ì „ì²˜ë¦¬
**ëª©í‘œ**: ì—°ë„ë³„ ìˆ˜ì§‘ëœ íŒë¡€ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2-3ì‹œê°„
**ì²˜ë¦¬ ê³¼ì •**:
1. ì—°ë„ë³„ í´ë” ìˆœíšŒ
2. JSON íŒŒì¼ ë°°ì¹˜ ë¡œë“œ
3. íŒë¡€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
4. íŒì‹œì‚¬í•­, íŒê²°ìš”ì§€, ì‚¬ê±´ê°œìš” í†µí•©
5. í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì²­í‚¹
6. ë²•ë¥  ì—”í‹°í‹° ì¶”ì¶œ

**ì¶œë ¥ í˜•ì‹**:
```json
{
  "id": "precedent_2024_001",
  "case_name": "ì‚¬ê±´ëª…",
  "case_number": "2024ë‹¤12345",
  "court": "ì„œìš¸ê³ ë“±ë²•ì›",
  "decision_date": "2024-01-01",
  "case_type": "ë¯¼ì‚¬",
  "issue": "íŒì‹œì‚¬í•­",
  "reasoning": "íŒê²°ìš”ì§€",
  "case_summary": "ì‚¬ê±´ê°œìš”",
  "cleaned_content": "ì •ë¦¬ëœ íŒë¡€ ë‚´ìš©",
  "chunks": [...],
  "entities": {...},
  "document_hash": "md5_hash",
  "processed_at": "2024-01-01T00:00:00Z"
}
```

### Phase 2: í™•ì¥ ë°ì´í„° ì „ì²˜ë¦¬ (2-3ì¼)

#### 2.1 í—Œì¬ê²°ì •ë¡€ ë°ì´í„° ì „ì²˜ë¦¬
**ëª©í‘œ**: 2024-2025ë…„ í—Œì¬ê²°ì •ë¡€ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1ì‹œê°„
**ì²˜ë¦¬ ê³¼ì •**:
1. ì—°ë„ë³„ í´ë” ìˆœíšŒ
2. í—Œì¬ê²°ì •ë¡€ íŠ¹í™” í•„ë“œ ì²˜ë¦¬
3. ê´€ë ¨ë²•ë ¹, ê´€ë ¨íŒë¡€ ì¶”ì¶œ
4. í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì²­í‚¹

#### 2.2 ë²•ë ¹í•´ì„ë¡€ ë°ì´í„° ì „ì²˜ë¦¬
**ëª©í‘œ**: ë°°ì¹˜ë³„ ìˆ˜ì§‘ëœ ë²•ë ¹í•´ì„ë¡€ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1-2ì‹œê°„
**ì²˜ë¦¬ ê³¼ì •**:
1. ë°°ì¹˜ë³„ í´ë” ìˆœíšŒ
2. ë²•ë ¹í•´ì„ë¡€ íŠ¹í™” í•„ë“œ ì²˜ë¦¬
3. ë¶€ì²˜ë³„ ë¶„ë¥˜ ì •ë³´ ì¶”ì¶œ
4. í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì²­í‚¹

#### 2.3 ë²•ë¥  ìš©ì–´ ë°ì´í„° ì „ì²˜ë¦¬
**ëª©í‘œ**: ì„¸ì…˜ë³„ ìˆ˜ì§‘ëœ ë²•ë¥  ìš©ì–´ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 30ë¶„
**ì²˜ë¦¬ ê³¼ì •**:
1. ì„¸ì…˜ë³„ í´ë” ìˆœíšŒ
2. ìš©ì–´ ì‚¬ì „ ë°ì´í„° ì •ë¦¬
3. ë™ì˜ì–´ ê·¸ë£¹ ë§¤í•‘
4. ìš©ì–´ ì •ì˜ í‘œì¤€í™”

### Phase 3: í’ˆì§ˆ ê²€ì¦ ë° í†µí•© (1ì¼)

#### 3.1 ë°ì´í„° í’ˆì§ˆ ê²€ì¦
**ëª©í‘œ**: ì „ì²˜ë¦¬ëœ ëª¨ë“  ë°ì´í„°ì˜ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ
**ê²€ì¦ í•­ëª©**:
- [ ] í•„ìˆ˜ í•„ë“œ ì™„ì„±ë„ ê²€ì‚¬
- [ ] í…ìŠ¤íŠ¸ ê¸¸ì´ ë° í˜•ì‹ ê²€ì‚¬
- [ ] ì²­í‚¹ í’ˆì§ˆ ê²€ì‚¬
- [ ] ìš©ì–´ ì •ê·œí™” ì •í™•ë„ ê²€ì‚¬
- [ ] ì¤‘ë³µ ë°ì´í„° ê²€ì‚¬

#### 3.2 í†µí•© ì¸ë±ìŠ¤ ìƒì„±
**ëª©í‘œ**: ëª¨ë“  ì „ì²˜ë¦¬ëœ ë°ì´í„°ì˜ í†µí•© ì¸ë±ìŠ¤ ìƒì„±
**ìƒì„± í•­ëª©**:
- [ ] ë¬¸ì„œë³„ ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤
- [ ] ìš©ì–´ë³„ ì—­ì¸ë±ìŠ¤
- [ ] ë‚ ì§œë³„ ì¸ë±ìŠ¤
- [ ] ì¹´í…Œê³ ë¦¬ë³„ ì¸ë±ìŠ¤

#### 3.3 ë²¡í„°í™” ì¤€ë¹„
**ëª©í‘œ**: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
**ì¤€ë¹„ í•­ëª©**:
- [ ] ì²­í¬ë³„ í…ìŠ¤íŠ¸ ì •ë¦¬
- [ ] ì„ë² ë”© ìƒì„±ìš© ë©”íƒ€ë°ì´í„° ì¤€ë¹„
- [ ] ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ì¤€ë¹„

---

## ğŸ”§ ê¸°ìˆ  êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ êµ¬ì¡°

```
scripts/
â”œâ”€â”€ preprocess_raw_data.py          # ë©”ì¸ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ batch_preprocess.py             # ë°°ì¹˜ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ validate_processed_data.py      # ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ consolidate_results.py          # ê²°ê³¼ í†µí•© ìŠ¤í¬ë¦½íŠ¸
```

### ë©”ì¸ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

```python
#!/usr/bin/env python3
"""
Raw ë°ì´í„° ì „ì²˜ë¦¬ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
from pathlib import Path
import logging
from datetime import datetime
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from source.data.data_processor import LegalDataProcessor
from source.data.legal_term_normalizer import LegalTermNormalizer

class RawDataPreprocessingPipeline:
    def __init__(self, enable_term_normalization=True):
        self.processor = LegalDataProcessor(enable_term_normalization)
        self.output_dir = Path("data/processed")
        self.output_dir.mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        # í†µê³„ ì´ˆê¸°í™”
        self.stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "by_type": {}
        }
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(
                    f'logs/preprocessing_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
                ),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def run_full_preprocessing(self):
        """ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("=== Raw ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")
        
        start_time = datetime.now()
        
        try:
            # Phase 1: í•µì‹¬ ë°ì´í„° ì „ì²˜ë¦¬
            self.logger.info("Phase 1: í•µì‹¬ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
            self.process_laws()
            self.process_precedents()
            
            # Phase 2: í™•ì¥ ë°ì´í„° ì „ì²˜ë¦¬
            self.logger.info("Phase 2: í™•ì¥ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
            self.process_constitutional_decisions()
            self.process_legal_interpretations()
            self.process_legal_terms()
            
            # Phase 3: í’ˆì§ˆ ê²€ì¦ ë° í†µí•©
            self.logger.info("Phase 3: í’ˆì§ˆ ê²€ì¦ ë° í†µí•©")
            self.validate_processed_data()
            self.consolidate_results()
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            self.logger.info(f"=== ì „ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration}) ===")
            self.print_statistics()
            
        except Exception as e:
            self.logger.error(f"ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def process_laws(self):
        """ë²•ë ¹ ë°ì´í„° ì „ì²˜ë¦¬"""
        self.logger.info("ë²•ë ¹ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        
        law_files = list(Path("data/raw/laws").glob("*.json"))
        processed_laws = []
        
        for law_file in law_files:
            try:
                with open(law_file, 'r', encoding='utf-8') as f:
                    law_data = json.load(f)
                
                processed_law = self.processor.process_law_data(law_data)
                
                if processed_law.get('status') == 'success':
                    processed_laws.append(processed_law)
                    self.stats['successful'] += 1
                else:
                    self.stats['failed'] += 1
                
                self.stats['total_processed'] += 1
                
            except Exception as e:
                self.logger.error(f"ë²•ë ¹ ì „ì²˜ë¦¬ ì‹¤íŒ¨ {law_file}: {e}")
                self.stats['failed'] += 1
                self.stats['total_processed'] += 1
        
        # ê²°ê³¼ ì €ì¥
        self.save_processed_data(processed_laws, "laws")
        self.stats['by_type']['laws'] = len(processed_laws)
        
        self.logger.info(f"ë²•ë ¹ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_laws)}ê°œ")
    
    def process_precedents(self):
        """íŒë¡€ ë°ì´í„° ì „ì²˜ë¦¬"""
        self.logger.info("íŒë¡€ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        
        precedent_dirs = list(Path("data/raw/precedents").glob("yearly_*"))
        all_processed_precedents = []
        
        for precedent_dir in precedent_dirs:
            json_files = list(precedent_dir.glob("*.json"))
            
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        precedent_data = json.load(f)
                    
                    # íŒë¡€ ë°ì´í„°ëŠ” ë°°ì—´ í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
                    if isinstance(precedent_data, list):
                        processed_precedents = self.processor.process_batch(
                            precedent_data, 'precedent'
                        )
                    else:
                        processed_precedents = [self.processor.process_precedent_data(precedent_data)]
                    
                    all_processed_precedents.extend(processed_precedents)
                    
                    self.stats['total_processed'] += len(processed_precedents)
                    self.stats['successful'] += len([p for p in processed_precedents if p.get('status') == 'success'])
                    self.stats['failed'] += len([p for p in processed_precedents if p.get('status') != 'success'])
                    
                except Exception as e:
                    self.logger.error(f"íŒë¡€ ì „ì²˜ë¦¬ ì‹¤íŒ¨ {json_file}: {e}")
                    self.stats['failed'] += 1
                    self.stats['total_processed'] += 1
        
        # ê²°ê³¼ ì €ì¥
        self.save_processed_data(all_processed_precedents, "precedents")
        self.stats['by_type']['precedents'] = len(all_processed_precedents)
        
        self.logger.info(f"íŒë¡€ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(all_processed_precedents)}ê°œ")
    
    def process_constitutional_decisions(self):
        """í—Œì¬ê²°ì •ë¡€ ë°ì´í„° ì „ì²˜ë¦¬"""
        self.logger.info("í—Œì¬ê²°ì •ë¡€ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        
        constitutional_dirs = list(Path("data/raw/constitutional_decisions").glob("yearly_*"))
        all_processed_decisions = []
        
        for constitutional_dir in constitutional_dirs:
            json_files = list(constitutional_dir.glob("*.json"))
            
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        decision_data = json.load(f)
                    
                    if isinstance(decision_data, list):
                        processed_decisions = self.processor.process_batch(
                            decision_data, 'constitutional_decision'
                        )
                    else:
                        processed_decisions = [self.processor.process_constitutional_decision_data(decision_data)]
                    
                    all_processed_decisions.extend(processed_decisions)
                    
                    self.stats['total_processed'] += len(processed_decisions)
                    self.stats['successful'] += len([p for p in processed_decisions if p.get('status') == 'success'])
                    self.stats['failed'] += len([p for p in processed_decisions if p.get('status') != 'success'])
                    
                except Exception as e:
                    self.logger.error(f"í—Œì¬ê²°ì •ë¡€ ì „ì²˜ë¦¬ ì‹¤íŒ¨ {json_file}: {e}")
                    self.stats['failed'] += 1
                    self.stats['total_processed'] += 1
        
        # ê²°ê³¼ ì €ì¥
        self.save_processed_data(all_processed_decisions, "constitutional_decisions")
        self.stats['by_type']['constitutional_decisions'] = len(all_processed_decisions)
        
        self.logger.info(f"í—Œì¬ê²°ì •ë¡€ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(all_processed_decisions)}ê°œ")
    
    def process_legal_interpretations(self):
        """ë²•ë ¹í•´ì„ë¡€ ë°ì´í„° ì „ì²˜ë¦¬"""
        self.logger.info("ë²•ë ¹í•´ì„ë¡€ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        
        interpretation_dirs = list(Path("data/raw/legal_interpretations").glob("yearly_*"))
        all_processed_interpretations = []
        
        for interpretation_dir in interpretation_dirs:
            json_files = list(interpretation_dir.glob("*.json"))
            
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        interpretation_data = json.load(f)
                    
                    if isinstance(interpretation_data, list):
                        processed_interpretations = self.processor.process_batch(
                            interpretation_data, 'legal_interpretation'
                        )
                    else:
                        processed_interpretations = [self.processor.process_legal_interpretation_data(interpretation_data)]
                    
                    all_processed_interpretations.extend(processed_interpretations)
                    
                    self.stats['total_processed'] += len(processed_interpretations)
                    self.stats['successful'] += len([p for p in processed_interpretations if p.get('status') == 'success'])
                    self.stats['failed'] += len([p for p in processed_interpretations if p.get('status') != 'success'])
                    
                except Exception as e:
                    self.logger.error(f"ë²•ë ¹í•´ì„ë¡€ ì „ì²˜ë¦¬ ì‹¤íŒ¨ {json_file}: {e}")
                    self.stats['failed'] += 1
                    self.stats['total_processed'] += 1
        
        # ê²°ê³¼ ì €ì¥
        self.save_processed_data(all_processed_interpretations, "legal_interpretations")
        self.stats['by_type']['legal_interpretations'] = len(all_processed_interpretations)
        
        self.logger.info(f"ë²•ë ¹í•´ì„ë¡€ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(all_processed_interpretations)}ê°œ")
    
    def process_legal_terms(self):
        """ë²•ë¥  ìš©ì–´ ë°ì´í„° ì „ì²˜ë¦¬"""
        self.logger.info("ë²•ë¥  ìš©ì–´ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        
        term_dirs = list(Path("data/raw/legal_terms").glob("session_*"))
        all_processed_terms = []
        
        for term_dir in term_dirs:
            json_files = list(term_dir.glob("*.json"))
            
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        term_data = json.load(f)
                    
                    # ìš©ì–´ ë°ì´í„°ëŠ” íŠ¹ë³„í•œ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ
                    processed_terms = self.process_legal_term_data(term_data)
                    all_processed_terms.extend(processed_terms)
                    
                    self.stats['total_processed'] += len(processed_terms)
                    self.stats['successful'] += len(processed_terms)
                    
                except Exception as e:
                    self.logger.error(f"ë²•ë¥  ìš©ì–´ ì „ì²˜ë¦¬ ì‹¤íŒ¨ {json_file}: {e}")
                    self.stats['failed'] += 1
                    self.stats['total_processed'] += 1
        
        # ê²°ê³¼ ì €ì¥
        self.save_processed_data(all_processed_terms, "legal_terms")
        self.stats['by_type']['legal_terms'] = len(all_processed_terms)
        
        self.logger.info(f"ë²•ë¥  ìš©ì–´ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(all_processed_terms)}ê°œ")
    
    def process_legal_term_data(self, term_data):
        """ë²•ë¥  ìš©ì–´ ë°ì´í„° ì²˜ë¦¬"""
        processed_terms = []
        
        if isinstance(term_data, dict) and 'terms' in term_data:
            for term in term_data['terms']:
                processed_term = {
                    'id': term.get('term_sequence_number', ''),
                    'term_name_korean': term.get('term_name_korean', ''),
                    'term_name_chinese': term.get('term_name_chinese', ''),
                    'definition': term.get('definition', ''),
                    'source': term.get('source', ''),
                    'category': 'legal_term',
                    'status': 'success',
                    'processed_at': datetime.now().isoformat()
                }
                processed_terms.append(processed_term)
        
        return processed_terms
    
    def save_processed_data(self, data, data_type):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        output_dir = self.output_dir / data_type
        output_dir.mkdir(exist_ok=True)
        
        output_file = output_dir / f"{data_type}_processed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"{data_type} ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
    
    def validate_processed_data(self):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²€ì¦"""
        self.logger.info("ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²€ì¦ ì‹œì‘")
        
        validation_results = {}
        
        for data_type in self.stats['by_type'].keys():
            validation_results[data_type] = self.validate_data_type(data_type)
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        validation_file = self.output_dir / f"validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(validation_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {validation_file}")
    
    def validate_data_type(self, data_type):
        """íŠ¹ì • ë°ì´í„° ìœ í˜• ê²€ì¦"""
        # ì‹¤ì œ ê²€ì¦ ë¡œì§ êµ¬í˜„
        return {
            "total_documents": self.stats['by_type'].get(data_type, 0),
            "validation_passed": True,
            "issues": []
        }
    
    def consolidate_results(self):
        """ê²°ê³¼ í†µí•©"""
        self.logger.info("ì „ì²˜ë¦¬ ê²°ê³¼ í†µí•© ì‹œì‘")
        
        # í†µí•© ì¸ë±ìŠ¤ ìƒì„±
        consolidated_index = {
            "metadata": {
                "total_processed": self.stats['total_processed'],
                "successful": self.stats['successful'],
                "failed": self.stats['failed'],
                "by_type": self.stats['by_type'],
                "processed_at": datetime.now().isoformat()
            },
            "data_types": list(self.stats['by_type'].keys()),
            "file_locations": {}
        }
        
        # íŒŒì¼ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
        for data_type in self.stats['by_type'].keys():
            data_dir = self.output_dir / data_type
            if data_dir.exists():
                files = list(data_dir.glob("*.json"))
                consolidated_index["file_locations"][data_type] = [str(f) for f in files]
        
        # í†µí•© ì¸ë±ìŠ¤ ì €ì¥
        index_file = self.output_dir / "consolidated_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(consolidated_index, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ê²°ê³¼ í†µí•© ì™„ë£Œ: {index_file}")
    
    def print_statistics(self):
        """í†µê³„ ì¶œë ¥"""
        self.logger.info("=== ì „ì²˜ë¦¬ í†µê³„ ===")
        self.logger.info(f"ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ")
        self.logger.info(f"ì„±ê³µ: {self.stats['successful']}ê°œ")
        self.logger.info(f"ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
        self.logger.info(f"ì„±ê³µë¥ : {self.stats['successful']/self.stats['total_processed']*100:.2f}%")
        
        self.logger.info("=== ë°ì´í„° ìœ í˜•ë³„ í†µê³„ ===")
        for data_type, count in self.stats['by_type'].items():
            self.logger.info(f"{data_type}: {count}ê°œ")

if __name__ == "__main__":
    pipeline = RawDataPreprocessingPipeline(enable_term_normalization=True)
    pipeline.run_full_preprocessing()
```

### ë°°ì¹˜ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

```python
#!/usr/bin/env python3
"""
ë°°ì¹˜ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ - íŠ¹ì • ë°ì´í„° ìœ í˜•ë§Œ ì²˜ë¦¬
"""

import argparse
import sys
from pathlib import Path

def main():
    parser = argparse.ArgumentParser(description="Raw ë°ì´í„° ë°°ì¹˜ ì „ì²˜ë¦¬")
    parser.add_argument("--data-type", required=True, 
                       choices=["laws", "precedents", "constitutional", "interpretations", "terms", "all"],
                       help="ì „ì²˜ë¦¬í•  ë°ì´í„° ìœ í˜•")
    parser.add_argument("--enable-normalization", action="store_true", default=True,
                       help="ë²•ë¥  ìš©ì–´ ì •ê·œí™” í™œì„±í™”")
    parser.add_argument("--output-dir", default="data/processed",
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--dry-run", action="store_true",
                       help="ì‹¤ì œ ì²˜ë¦¬ ì—†ì´ ê³„íšë§Œ ì¶œë ¥")
    
    args = parser.parse_args()
    
    pipeline = RawDataPreprocessingPipeline(args.enable_normalization)
    
    if args.dry_run:
        pipeline.dry_run(args.data_type)
    else:
        if args.data_type == "all":
            pipeline.run_full_preprocessing()
        else:
            pipeline.process_specific_type(args.data_type)

if __name__ == "__main__":
    main()
```

---

## ğŸš€ ì‹¤í–‰ ê³„íš

### ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª…ë ¹ì–´

```bash
# 1. ì „ì²´ ì „ì²˜ë¦¬ ì‹¤í–‰
python scripts/preprocess_raw_data.py

# 2. íŠ¹ì • ë°ì´í„° ìœ í˜•ë§Œ ì „ì²˜ë¦¬
python scripts/batch_preprocess.py --data-type laws
python scripts/batch_preprocess.py --data-type precedents
python scripts/batch_preprocess.py --data-type constitutional

# 3. ë“œë¼ì´ëŸ° ëª¨ë“œ (ì‹¤ì œ ì²˜ë¦¬ ì—†ì´ ê³„íš í™•ì¸)
python scripts/batch_preprocess.py --data-type all --dry-run

# 4. ìš©ì–´ ì •ê·œí™” ë¹„í™œì„±í™”
python scripts/batch_preprocess.py --data-type laws --no-normalization
```

### ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„

| ë°ì´í„° ìœ í˜• | íŒŒì¼ ìˆ˜ | ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ |
|------------|---------|----------------|---------------|
| ë²•ë ¹ ë°ì´í„° | 21ê°œ | 30ë¶„ | 2GB |
| íŒë¡€ ë°ì´í„° | ì—°ë„ë³„ | 2-3ì‹œê°„ | 4GB |
| í—Œì¬ê²°ì •ë¡€ | 2024-2025ë…„ | 1ì‹œê°„ | 2GB |
| ë²•ë ¹í•´ì„ë¡€ | ë°°ì¹˜ë³„ | 1-2ì‹œê°„ | 3GB |
| ë²•ë¥  ìš©ì–´ | ì„¸ì…˜ë³„ | 30ë¶„ | 1GB |
| **ì „ì²´** | **ëª¨ë“  ë°ì´í„°** | **5-7ì‹œê°„** | **8GB** |

---

## ğŸ“Š í’ˆì§ˆ ê´€ë¦¬

### í’ˆì§ˆ ì§€í‘œ

| ì§€í‘œ | ëª©í‘œê°’ | ì¸¡ì • ë°©ë²• |
|------|--------|-----------|
| **ì™„ì„±ë„** | 95% ì´ìƒ | í•„ìˆ˜ í•„ë“œ ëˆ„ë½ë¥  ì¸¡ì • |
| **ì •í™•ë„** | 98% ì´ìƒ | ì›ë³¸ ë°ì´í„°ì™€ì˜ ì¼ì¹˜ë„ ì¸¡ì • |
| **ì¼ê´€ì„±** | 90% ì´ìƒ | ë°ì´í„° í˜•ì‹ í†µì¼ë„ ì¸¡ì • |
| **ìš©ì–´ ì •ê·œí™”** | 90% ì´ìƒ | ìš©ì–´ ì •ê·œí™” ì„±ê³µë¥  ì¸¡ì • |

### ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ëª¨ë“  JSON íŒŒì¼ì´ ì˜¬ë°”ë¥´ê²Œ íŒŒì‹±ë˜ëŠ”ê°€?
- [ ] í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì§€ ì•Šì•˜ëŠ”ê°€?
- [ ] í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì²­í‚¹ì´ ì˜¬ë°”ë¥´ê²Œ ìˆ˜í–‰ë˜ì—ˆëŠ”ê°€?
- [ ] ë²•ë¥  ìš©ì–´ ì •ê·œí™”ê°€ ì •í™•í•˜ê²Œ ì ìš©ë˜ì—ˆëŠ”ê°€?
- [ ] ì¤‘ë³µ ë°ì´í„°ê°€ ì œê±°ë˜ì—ˆëŠ”ê°€?
- [ ] ë©”íƒ€ë°ì´í„°ê°€ ì™„ì „í•˜ê²Œ ìƒì„±ë˜ì—ˆëŠ”ê°€?
- [ ] ë²¡í„°í™”ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜ë˜ì—ˆëŠ”ê°€?

### ì—ëŸ¬ ì²˜ë¦¬ ì „ëµ

1. **íŒŒì¼ ë ˆë²¨ ì—ëŸ¬**: ê°œë³„ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ê¸°ë¡ í›„ ê³„ì† ì§„í–‰
2. **ë°°ì¹˜ ë ˆë²¨ ì—ëŸ¬**: ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¡œì§ ì ìš©
3. **ì‹œìŠ¤í…œ ë ˆë²¨ ì—ëŸ¬**: ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì •
4. **ë°ì´í„° ë ˆë²¨ ì—ëŸ¬**: ì˜ëª»ëœ ë°ì´í„° í˜•ì‹ ì‹œ ê¸°ë³¸ê°’ ì ìš©

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### ë©”ëª¨ë¦¬ ìµœì í™”

1. **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
2. **ê°€ë¹„ì§€ ì»¬ë ‰ì…˜**: ë¶ˆí•„ìš”í•œ ê°ì²´ ì¦‰ì‹œ í•´ì œ
3. **ë°°ì¹˜ í¬ê¸° ì¡°ì •**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¥¸ ë™ì  ì¡°ì •
4. **ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 

### ì²˜ë¦¬ ì†ë„ ìµœì í™”

1. **ë³‘ë ¬ ì²˜ë¦¬**: ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í†µí•œ ë™ì‹œ ì²˜ë¦¬
2. **ìºì‹±**: ì¤‘ë³µ ì—°ì‚° ë°©ì§€
3. **ì¸ë±ì‹±**: ë¹ ë¥¸ ë°ì´í„° ì ‘ê·¼ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
4. **ì••ì¶•**: ë°ì´í„° ì €ì¥ ê³µê°„ ìµœì í™”

---

## ğŸ”„ í–¥í›„ í™•ì¥ ê³„íš

### ë‹¨ê¸° í™•ì¥ (1-2ê°œì›”)

- [ ] í–‰ì •ê·œì¹™ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¶”ê°€
- [ ] ìì¹˜ë²•ê·œ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¶”ê°€
- [ ] ìœ„ì›íšŒê²°ì •ë¬¸ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¶”ê°€
- [ ] ì¡°ì•½ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¶”ê°€

### ì¤‘ê¸° í™•ì¥ (3-6ê°œì›”)

- [ ] AI ê¸°ë°˜ ìš©ì–´ ì •ê·œí™” ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ë‹¤êµ­ì–´ ë²•ë¥  ìš©ì–´ ì •ê·œí™” ì§€ì›
- [ ] ì‹¤ì‹œê°„ ë°ì´í„° ì „ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€
- [ ] í´ë¼ìš°ë“œ ê¸°ë°˜ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

### ì¥ê¸° í™•ì¥ (6ê°œì›” ì´ìƒ)

- [ ] ë¶„ì‚° ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬
- [ ] ìë™ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
- [ ] ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ë°ì´í„° í’ˆì§ˆ ê°œì„ 

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- [TASKë³„ ìƒì„¸ ê°œë°œ ê³„íš](../TASK/TASKë³„ ìƒì„¸ ê°œë°œ ê³„íš_v1.0.md)
- [ë²•ë¥  ìš©ì–´ ì •ê·œí™” ì „ëµ](legal_term_normalization_strategy.md)
- [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì•„í‚¤í…ì²˜](../architecture/hybrid_search_architecture.md)
- [ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ](../database_schema.md)

---

*ì´ ë¬¸ì„œëŠ” LawFirmAI í”„ë¡œì íŠ¸ì˜ raw ë°ì´í„° ì „ì²˜ë¦¬ ê³„íšì„ ì œì‹œí•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ ì§„í–‰ì— ë”°ë¼ í•„ìš”ì— ë”°ë¼ ìˆ˜ì • ë° ë³´ì™„í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.*
