# ë²•ë¥  ë°ì´í„° í…ìŠ¤íŠ¸ ì²­í‚¹ ì „ëµ ê°œë°œ ë¬¸ì„œ

## ğŸ“‹ ë¬¸ì„œ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” LawFirmAI í”„ë¡œì íŠ¸ì—ì„œ ì‹¤ì œ ìˆ˜ì§‘ëœ ë²•ë¥  ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ íš¨ê³¼ì ì¸ ê²€ìƒ‰ê³¼ RAG(Retrieval-Augmented Generation) ì„±ëŠ¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì²­í‚¹ ì „ëµì„ ì •ì˜í•©ë‹ˆë‹¤.

### ğŸ“Š ì‹¤ì œ ìˆ˜ì§‘ ë°ì´í„° í˜„í™©
- **ë²•ë ¹ ë°ì´í„°**: 20ê°œ (ë¯¼ë²•, ìƒë²•, í˜•ë²• ë“±)
- **íŒë¡€ ë°ì´í„°**: 7,699ê±´ (90ê°œ ë°°ì¹˜ íŒŒì¼)
- **í—Œì¬ê²°ì •ë¡€**: 2,000ê±´ (40ê°œ ë°°ì¹˜ íŒŒì¼)
- **ë²•ë ¹í•´ì„ë¡€**: 158ê±´ (4ê°œ ë°°ì¹˜ íŒŒì¼)

---

## ğŸ¯ ì²­í‚¹ ì „ëµ ê°œìš”

### 1.1 ì²­í‚¹ì˜ ëª©ì 
- **ì˜ë¯¸ì  ì¼ê´€ì„± ë³´ì¥**: ë²•ë¥  ì¡°ë¬¸ì˜ ì™„ì „í•œ ì˜ë¯¸ë¥¼ ìœ ì§€
- **ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ**: ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì¡°ê° ê²€ìƒ‰
- **ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´**: ë²•ë¦¬ ì¶”ë¡ ì— í•„ìš”í•œ ë§¥ë½ ìœ ì§€
- **ì²˜ë¦¬ íš¨ìœ¨ì„±**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ íš¨ìœ¨ì  ì²˜ë¦¬

### 1.2 ë²•ë¥  ë°ì´í„°ì˜ íŠ¹ì„±
- **êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸**: ì¡°ë¬¸, í•­ëª©, í˜¸ëª©ìœ¼ë¡œ ê³„ì¸µí™”
- **ì˜ë¯¸ì  ë°€ë„**: ì§§ì€ ë¬¸ì¥ì— ë§ì€ ë²•ì  ì˜ë¯¸ í¬í•¨
- **ìƒí˜¸ ì°¸ì¡°ì„±**: ë‹¤ë¥¸ ì¡°ë¬¸ê³¼ì˜ ì—°ê´€ì„± ì¤‘ìš”
- **ìš©ì–´ì˜ ì •í™•ì„±**: ë²•ë¥  ìš©ì–´ì˜ ì •í™•í•œ í•´ì„ í•„ìš”

---

## ğŸ”§ ì²­í‚¹ ë°©ë²•ë¡ 

### 2.1 ë‹¤ì¸µ ì²­í‚¹ ì „ëµ (Multi-Level Chunking)

#### 2.1.1 ê³„ì¸µì  ì²­í‚¹ êµ¬ì¡°
```
Level 1: ë¬¸ì„œ ë‹¨ìœ„ (Document Level)
â”œâ”€â”€ Level 2: ì¡°ë¬¸ ë‹¨ìœ„ (Article Level)
â”‚   â”œâ”€â”€ Level 3: í•­ëª© ë‹¨ìœ„ (Paragraph Level)
â”‚   â”‚   â””â”€â”€ Level 4: ì˜ë¯¸ ë‹¨ìœ„ (Semantic Level)
â”‚   â””â”€â”€ Level 3: ë¶€ì¹™ ë‹¨ìœ„ (Supplementary Level)
â””â”€â”€ Level 2: ë¶€ì¹™ ë‹¨ìœ„ (Supplementary Level)
```

#### 2.1.2 ê° ë ˆë²¨ë³„ íŠ¹ì„±
- **Level 1**: ì „ì²´ ë²•ë ¹ì˜ ë§¥ë½ê³¼ ëª©ì 
- **Level 2**: ê°œë³„ ì¡°ë¬¸ì˜ ì™„ì „í•œ ì˜ë¯¸
- **Level 3**: ì¡°ë¬¸ ë‚´ ì„¸ë¶€ ê·œì •
- **Level 4**: ìµœì†Œ ì˜ë¯¸ ë‹¨ìœ„

### 2.2 ë°ì´í„° ìœ í˜•ë³„ ì²­í‚¹ ì „ëµ

#### 2.2.1 ë²•ë ¹ ë°ì´í„° ì²­í‚¹ (ì‹¤ì œ ë°ì´í„° êµ¬ì¡° ê¸°ë°˜)

##### A. ë²•ë ¹ êµ¬ì¡° ë¶„ì„
```json
{
  "basic_info": {
    "name": "ë¯¼ë²•",
    "id": "001706",
    "category": "ê¸°ë³¸ë²•"
  },
  "current_text": {
    "ë²•ë ¹": {
      "ê°œì •ë¬¸": {
        "ê°œì •ë¬¸ë‚´ìš©": ["ì¡°ë¬¸ ë‚´ìš© ë°°ì—´"]
      },
      "ë¶€ì¹™": {
        "ë¶€ì¹™ë‹¨ìœ„": [{"ë¶€ì¹™í‚¤": "...", "ë¶€ì¹™ë‚´ìš©": "..."}]
      }
    }
  }
}
```

##### B. ì¡°ë¬¸ ì¤‘ì‹¬ ì²­í‚¹ (Article-Centric Chunking)
```python
def law_article_chunking(law_data: Dict) -> List[Dict]:
    """
    ì‹¤ì œ ë²•ë ¹ ë°ì´í„° êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¡°ë¬¸ ì¤‘ì‹¬ ì²­í‚¹
    
    ì²­í‚¹ ê·œì¹™:
    1. ê°œì •ë¬¸ë‚´ìš© ë°°ì—´ì—ì„œ ì¡°ë¬¸ë³„ ë¶„í• 
    2. "ì œXì¡°" íŒ¨í„´ìœ¼ë¡œ ì¡°ë¬¸ ê²½ê³„ ì‹ë³„
    3. ì¡°ë¬¸ ì œëª©ê³¼ ë³¸ë¬¸ì„ í•¨ê»˜ í¬í•¨
    4. ë¶€ì¹™ì€ ë³„ë„ ì²­í¬ë¡œ ë¶„ë¦¬
    """
    
    chunks = []
    current_text = law_data.get('current_text', {})
    
    # ê°œì •ë¬¸ ë‚´ìš© ì²˜ë¦¬
    if 'ë²•ë ¹' in current_text and 'ê°œì •ë¬¸' in current_text['ë²•ë ¹']:
        amendment_content = current_text['ë²•ë ¹']['ê°œì •ë¬¸']['ê°œì •ë¬¸ë‚´ìš©']
        
        for content_array in amendment_content:
            for content_item in content_array:
                # ì¡°ë¬¸ íŒ¨í„´ ë§¤ì¹­
                article_pattern = r'ì œ(\d+)ì¡°[^ì œ]*?(?=ì œ\d+ì¡°|$)'
                
                for match in re.finditer(article_pattern, content_item, re.DOTALL):
                    article_num = match.group(1)
                    article_content = match.group(0).strip()
                    
                    chunk = {
                        'type': 'article',
                        'law_name': law_data['basic_info']['name'],
                        'law_id': law_data['basic_info']['id'],
                        'article_number': int(article_num),
                        'content': article_content,
                        'title': extract_article_title(article_content),
                        'size': len(article_content),
                        'source': 'amendment'
                    }
                    chunks.append(chunk)
    
    # ë¶€ì¹™ ì²˜ë¦¬
    if 'ë²•ë ¹' in current_text and 'ë¶€ì¹™' in current_text['ë²•ë ¹']:
        supplementary_rules = current_text['ë²•ë ¹']['ë¶€ì¹™']['ë¶€ì¹™ë‹¨ìœ„']
        
        for rule in supplementary_rules:
            if 'ë¶€ì¹™ë‚´ìš©' in rule:
                chunk = {
                    'type': 'supplementary_rule',
                    'law_name': law_data['basic_info']['name'],
                    'law_id': law_data['basic_info']['id'],
                    'rule_key': rule.get('ë¶€ì¹™í‚¤', ''),
                    'content': rule['ë¶€ì¹™ë‚´ìš©'],
                    'size': len(rule['ë¶€ì¹™ë‚´ìš©']),
                    'source': 'supplementary'
                }
                chunks.append(chunk)
    
    return chunks
```

##### B. í•­ëª©ë³„ ì²­í‚¹ (Paragraph-Level Chunking)
```python
def paragraph_level_chunking(article_text: str) -> List[Dict]:
    """
    ì¡°ë¬¸ ë‚´ í•­ëª©ë³„ ì²­í‚¹ ì „ëµ
    
    ì²­í‚¹ ê·œì¹™:
    1. "ì œXí•­" íŒ¨í„´ìœ¼ë¡œ í•­ëª© ê²½ê³„ ì‹ë³„
    2. ê° í•­ëª©ì„ ë…ë¦½ì ì¸ ì²­í¬ë¡œ ì²˜ë¦¬
    3. í•­ëª© ê°„ ì—°ê´€ì„± ê³ ë ¤í•œ ì˜¤ë²„ë© ì ìš©
    4. í˜¸ëª©ì€ ìƒìœ„ í•­ëª©ê³¼ í•¨ê»˜ í¬í•¨
    """
    
    # í•­ëª© íŒ¨í„´ ë§¤ì¹­
    paragraph_pattern = r'ì œ(\d+)í•­\s*([^ì œ]*?)(?=ì œ\d+í•­|$)'
    
    chunks = []
    for match in re.finditer(paragraph_pattern, article_text, re.DOTALL):
        para_num = match.group(1)
        para_content = match.group(2).strip()
        
        # í˜¸ëª© í¬í•¨ ì—¬ë¶€ í™•ì¸
        sub_items = extract_sub_items(para_content)
        
        chunk = {
            'type': 'paragraph',
            'paragraph_number': int(para_num),
            'content': para_content,
            'sub_items': sub_items,
            'size': len(para_content)
        }
        chunks.append(chunk)
    
    return chunks
```

#### 2.2.2 íŒë¡€ ë°ì´í„° ì²­í‚¹ (ì‹¤ì œ ë°ì´í„° êµ¬ì¡° ê¸°ë°˜)

##### A. íŒë¡€ êµ¬ì¡° ë¶„ì„
```json
{
  "metadata": {
    "category": "í˜•ì‚¬",
    "count": 952,
    "batch_id": "20250925_111703"
  },
  "precedents": [
    {
      "id": "14",
      "ì‚¬ê±´ë²ˆí˜¸": "2020ë„12563",
      "ì‚¬ê±´ì¢…ë¥˜ëª…": "í˜•ì‚¬",
      "ì„ ê³ ì¼ì": "2022.10.27",
      "ë²•ì›ëª…": "ëŒ€ë²•ì›",
      "ì‚¬ê±´ëª…": "ê¸ˆìœµì‹¤ëª…ê±°ë˜ë°ë¹„ë°€ë³´ì¥ì—ê´€í•œë²•ë¥ ìœ„ë°˜ë°©ì¡°..."
    }
  ]
}
```

##### B. ì‚¬ê±´ë³„ ì²­í‚¹ (Case-Level Chunking)
```python
def precedent_case_chunking(precedent_data: Dict) -> List[Dict]:
    """
    ì‹¤ì œ íŒë¡€ ë°ì´í„° êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‚¬ê±´ë³„ ì²­í‚¹
    
    ì²­í‚¹ ê·œì¹™:
    1. ì‚¬ê±´ë²ˆí˜¸ë¡œ ì‚¬ê±´ ê²½ê³„ ì‹ë³„
    2. ì‚¬ê±´ëª…ì—ì„œ ìŸì  ì¶”ì¶œ
    3. ê° ì‚¬ê±´ì„ ë…ë¦½ì ì¸ ì²­í¬ë¡œ ì²˜ë¦¬
    4. ë²•ì›, ì‚¬ê±´ì¢…ë¥˜ ë“± ë©”íƒ€ë°ì´í„° í¬í•¨
    """
    
    chunks = []
    
    # ì‚¬ê±´ ê¸°ë³¸ ì •ë³´ ì²­í¬
    basic_chunk = {
        'type': 'case_basic_info',
        'case_id': precedent_data.get('id', ''),
        'case_number': precedent_data.get('ì‚¬ê±´ë²ˆí˜¸', ''),
        'case_type': precedent_data.get('ì‚¬ê±´ì¢…ë¥˜ëª…', ''),
        'court_name': precedent_data.get('ë²•ì›ëª…', ''),
        'decision_date': precedent_data.get('ì„ ê³ ì¼ì', ''),
        'case_name': precedent_data.get('ì‚¬ê±´ëª…', ''),
        'content': f"ì‚¬ê±´ë²ˆí˜¸: {precedent_data.get('ì‚¬ê±´ë²ˆí˜¸', '')}\n"
                  f"ì‚¬ê±´ëª…: {precedent_data.get('ì‚¬ê±´ëª…', '')}\n"
                  f"ë²•ì›: {precedent_data.get('ë²•ì›ëª…', '')}\n"
                  f"ì„ ê³ ì¼ì: {precedent_data.get('ì„ ê³ ì¼ì', '')}",
        'size': len(precedent_data.get('ì‚¬ê±´ëª…', ''))
    }
    chunks.append(basic_chunk)
    
    # ì‚¬ê±´ëª…ì—ì„œ ìŸì  ì¶”ì¶œí•˜ì—¬ ë³„ë„ ì²­í¬ ìƒì„±
    case_name = precedent_data.get('ì‚¬ê±´ëª…', '')
    if case_name and len(case_name) > 50:  # ê¸´ ì‚¬ê±´ëª…ì˜ ê²½ìš°
        issue_chunk = {
            'type': 'case_issue',
            'case_id': precedent_data.get('id', ''),
            'case_number': precedent_data.get('ì‚¬ê±´ë²ˆí˜¸', ''),
            'content': case_name,
            'extracted_issues': extract_legal_issues_from_case_name(case_name),
            'size': len(case_name)
        }
        chunks.append(issue_chunk)
    
    return chunks

def extract_legal_issues_from_case_name(case_name: str) -> List[str]:
    """
    ì‚¬ê±´ëª…ì—ì„œ ë²•ì  ìŸì  ì¶”ì¶œ
    """
    issues = []
    
    # ë²•ë¥  ìœ„ë°˜ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
    violation_patterns = [
        r'([ê°€-í£]+ë²•[ê°€-í£]*ìœ„ë°˜)',
        r'([ê°€-í£]+ì£„)',
        r'([ê°€-í£]+í–‰ìœ„)',
        r'([ê°€-í£]+ë°©ì¡°)',
        r'([ê°€-í£]+ê³µëª¨)'
    ]
    
    for pattern in violation_patterns:
        matches = re.findall(pattern, case_name)
        issues.extend(matches)
    
    return list(set(issues))
```

##### B. ë²•ë¦¬ ì¤‘ì‹¬ ì²­í‚¹ (Legal Reasoning Chunking)
```python
def legal_reasoning_chunking(reasoning_text: str) -> List[Dict]:
    """
    íŒê²°ì´ìœ ì˜ ë²•ë¦¬ ì¤‘ì‹¬ ì²­í‚¹ ì „ëµ
    
    ì²­í‚¹ ê·œì¹™:
    1. ë²•ë¦¬ë³„ë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• 
    2. íŒë¡€ ì¸ìš©ê³¼ ë²•ë ¹ ì¡°í•­ì„ í•¨ê»˜ í¬í•¨
    3. ë…¼ë¦¬ì  ìˆœì„œ ìœ ì§€
    4. ë²•ë¥  ìš©ì–´ì˜ ì •í™•í•œ ë§¥ë½ ë³´ì¡´
    """
    
    # ë²•ë¦¬ ë‹¨ìœ„ ë¶„í• 
    legal_units = split_by_legal_reasoning(reasoning_text)
    
    chunks = []
    for i, unit in enumerate(legal_units):
        chunk = {
            'type': 'legal_reasoning',
            'unit_number': i + 1,
            'content': unit,
            'legal_terms': extract_legal_terms(unit),
            'case_citations': extract_case_citations(unit),
            'law_citations': extract_law_citations(unit),
            'size': len(unit)
        }
        chunks.append(chunk)
    
    return chunks
```

#### 2.2.3 í—Œì¬ê²°ì •ë¡€ ì²­í‚¹ (ì‹¤ì œ ë°ì´í„° êµ¬ì¡° ê¸°ë°˜)

##### A. í—Œì¬ê²°ì •ë¡€ êµ¬ì¡° ë¶„ì„
```json
{
  "metadata": {
    "category": "constitutional_decisions_0",
    "count": 50,
    "batch_size": 50
  },
  "data": [
    {
      "basic_info": {
        "ì‚¬ê±´ë²ˆí˜¸": "2017í—Œë°”323",
        "ì¢…êµ­ì¼ì": "0",
        "ì‚¬ê±´ëª…": ""
      },
      "detail_info": {
        "DetcService": {
          "ì‚¬ê±´ë²ˆí˜¸": "2017í—Œë°”323",
          "ì‹¬íŒëŒ€ìƒì¡°ë¬¸": "",
          "íŒì‹œì‚¬í•­": "",
          "ê²°ì •ìš”ì§€": "",
          "ì‚¬ê±´ì¢…ë¥˜ëª…": "í—Œë°”"
        }
      }
    }
  ]
}
```

##### B. ìŸì ë³„ ì²­í‚¹ (Issue-Based Chunking)
```python
def constitutional_issue_chunking(decision_data: Dict) -> List[Dict]:
    """
    ì‹¤ì œ í—Œì¬ê²°ì •ë¡€ ë°ì´í„° êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìŸì ë³„ ì²­í‚¹
    
    ì²­í‚¹ ê·œì¹™:
    1. ì‚¬ê±´ë²ˆí˜¸ë¡œ ì‚¬ê±´ ê²½ê³„ ì‹ë³„
    2. ì‹¬íŒëŒ€ìƒì¡°ë¬¸, íŒì‹œì‚¬í•­, ê²°ì •ìš”ì§€ë¡œ êµ¬ì¡°í™”
    3. ê° êµ¬ì¡° ìš”ì†Œë¥¼ ë…ë¦½ì ì¸ ì²­í¬ë¡œ ì²˜ë¦¬
    4. í—Œë²•ì  ìŸì  ì¤‘ì‹¬ìœ¼ë¡œ ë¶„í• 
    """
    
    chunks = []
    
    # ê¸°ë³¸ ì •ë³´ ì²­í¬
    basic_info = decision_data.get('basic_info', {})
    detail_info = decision_data.get('detail_info', {}).get('DetcService', {})
    
    # ì‚¬ê±´ ê¸°ë³¸ ì •ë³´
    basic_chunk = {
        'type': 'constitutional_basic_info',
        'case_number': basic_info.get('ì‚¬ê±´ë²ˆí˜¸', ''),
        'case_name': basic_info.get('ì‚¬ê±´ëª…', ''),
        'decision_date': basic_info.get('ì¢…êµ­ì¼ì', ''),
        'case_type': detail_info.get('ì‚¬ê±´ì¢…ë¥˜ëª…', ''),
        'content': f"ì‚¬ê±´ë²ˆí˜¸: {basic_info.get('ì‚¬ê±´ë²ˆí˜¸', '')}\n"
                  f"ì‚¬ê±´ëª…: {basic_info.get('ì‚¬ê±´ëª…', '')}\n"
                  f"ì¢…êµ­ì¼ì: {basic_info.get('ì¢…êµ­ì¼ì', '')}\n"
                  f"ì‚¬ê±´ì¢…ë¥˜: {detail_info.get('ì‚¬ê±´ì¢…ë¥˜ëª…', '')}",
        'size': len(basic_info.get('ì‚¬ê±´ëª…', ''))
    }
    chunks.append(basic_chunk)
    
    # ì‹¬íŒëŒ€ìƒì¡°ë¬¸ ì²­í¬
    if detail_info.get('ì‹¬íŒëŒ€ìƒì¡°ë¬¸'):
        target_article_chunk = {
            'type': 'constitutional_target_article',
            'case_number': basic_info.get('ì‚¬ê±´ë²ˆí˜¸', ''),
            'content': detail_info['ì‹¬íŒëŒ€ìƒì¡°ë¬¸'],
            'size': len(detail_info['ì‹¬íŒëŒ€ìƒì¡°ë¬¸'])
        }
        chunks.append(target_article_chunk)
    
    # íŒì‹œì‚¬í•­ ì²­í¬
    if detail_info.get('íŒì‹œì‚¬í•­'):
        holding_chunk = {
            'type': 'constitutional_holding',
            'case_number': basic_info.get('ì‚¬ê±´ë²ˆí˜¸', ''),
            'content': detail_info['íŒì‹œì‚¬í•­'],
            'size': len(detail_info['íŒì‹œì‚¬í•­'])
        }
        chunks.append(holding_chunk)
    
    # ê²°ì •ìš”ì§€ ì²­í¬
    if detail_info.get('ê²°ì •ìš”ì§€'):
        decision_summary_chunk = {
            'type': 'constitutional_decision_summary',
            'case_number': basic_info.get('ì‚¬ê±´ë²ˆí˜¸', ''),
            'content': detail_info['ê²°ì •ìš”ì§€'],
            'size': len(detail_info['ê²°ì •ìš”ì§€'])
        }
        chunks.append(decision_summary_chunk)
    
    return chunks
```

#### 2.2.4 ë²•ë ¹í•´ì„ë¡€ ì²­í‚¹ (ì‹¤ì œ ë°ì´í„° êµ¬ì¡° ê¸°ë°˜)

##### A. ë²•ë ¹í•´ì„ë¡€ êµ¬ì¡° ë¶„ì„
```json
{
  "metadata": {
    "category": "ê¸°íƒ€",
    "count": 158,
    "batch_id": "20250925_145952"
  },
  "interpretations": [
    {
      "id": "1",
      "ì•ˆê±´ëª…": "1959ë…„ 12ì›” 31ì¼ ì´ì „ì— í‡´ì§í•œ êµ°ì¸ì˜ í‡´ì§ê¸‰ì—¬ê¸ˆ ì§€ê¸‰ì— ê´€í•œíŠ¹ë³„ë²• ì‹œí–‰ë ¹ ì œ4ì¡°ì œ2í•­ ë° 3í•­",
      "ì§ˆì˜ê¸°ê´€ëª…": "êµ­ë°©ë¶€",
      "íšŒì‹ ê¸°ê´€ëª…": "ë²•ì œì²˜",
      "ì§ˆì˜ìš”ì§€": "ì¬ì§ê¸°ê°„ ê³„ì‚° ë°©ë²•ì— ëŒ€í•œ ì§ˆì˜",
      "íšŒë‹µ": "í˜„ì—­ë³‘ ë³µë¬´ì—°í•œì„ ê³µì œí•œ í›„ ì „íˆ¬ê·¼ë¬´ê¸°ê°„ì„ 3ë°°ë¡œ ê³„ì‚°...",
      "ì´ìœ ": "ìƒì„¸í•œ í•´ì„ ì´ìœ ..."
    }
  ]
}
```

##### B. ì§ˆì˜-íšŒë‹µ ì¤‘ì‹¬ ì²­í‚¹ (Q&A-Based Chunking)
```python
def legal_interpretation_chunking(interpretation_data: Dict) -> List[Dict]:
    """
    ì‹¤ì œ ë²•ë ¹í•´ì„ë¡€ ë°ì´í„° êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§ˆì˜-íšŒë‹µ ì¤‘ì‹¬ ì²­í‚¹
    
    ì²­í‚¹ ê·œì¹™:
    1. ì§ˆì˜ìš”ì§€ì™€ íšŒë‹µì„ ë³„ë„ ì²­í¬ë¡œ ë¶„ë¦¬
    2. í•´ì„ ì´ìœ ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
    3. ê´€ë ¨ ë²•ë ¹ ì¡°ë¬¸ ì¶”ì¶œí•˜ì—¬ ë³„ë„ ì²­í¬ ìƒì„±
    4. ê¸°ê´€ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ í¬í•¨
    """
    
    chunks = []
    
    # ê¸°ë³¸ ì •ë³´ ì²­í¬
    basic_chunk = {
        'type': 'interpretation_basic_info',
        'case_id': interpretation_data.get('id', ''),
        'case_name': interpretation_data.get('ì•ˆê±´ëª…', ''),
        'inquiry_agency': interpretation_data.get('ì§ˆì˜ê¸°ê´€ëª…', ''),
        'reply_agency': interpretation_data.get('íšŒì‹ ê¸°ê´€ëª…', ''),
        'content': f"ì•ˆê±´ëª…: {interpretation_data.get('ì•ˆê±´ëª…', '')}\n"
                  f"ì§ˆì˜ê¸°ê´€: {interpretation_data.get('ì§ˆì˜ê¸°ê´€ëª…', '')}\n"
                  f"íšŒì‹ ê¸°ê´€: {interpretation_data.get('íšŒì‹ ê¸°ê´€ëª…', '')}",
        'size': len(interpretation_data.get('ì•ˆê±´ëª…', ''))
    }
    chunks.append(basic_chunk)
    
    # ì§ˆì˜ìš”ì§€ ì²­í¬
    if interpretation_data.get('ì§ˆì˜ìš”ì§€'):
        question_chunk = {
            'type': 'interpretation_question',
            'case_id': interpretation_data.get('id', ''),
            'content': interpretation_data['ì§ˆì˜ìš”ì§€'],
            'size': len(interpretation_data['ì§ˆì˜ìš”ì§€'])
        }
        chunks.append(question_chunk)
    
    # íšŒë‹µ ì²­í¬
    if interpretation_data.get('íšŒë‹µ'):
        answer_chunk = {
            'type': 'interpretation_answer',
            'case_id': interpretation_data.get('id', ''),
            'content': interpretation_data['íšŒë‹µ'],
            'size': len(interpretation_data['íšŒë‹µ'])
        }
        chunks.append(answer_chunk)
    
    # í•´ì„ ì´ìœ  ì²­í¬ (ê¸´ ê²½ìš° ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• )
    if interpretation_data.get('ì´ìœ '):
        reason_content = interpretation_data['ì´ìœ ']
        if len(reason_content) > 1000:  # ê¸´ í•´ì„ ì´ìœ ì˜ ê²½ìš° ë¶„í• 
            reason_chunks = split_interpretation_reason(reason_content)
            for i, reason_chunk in enumerate(reason_chunks):
                chunk = {
                    'type': 'interpretation_reason',
                    'case_id': interpretation_data.get('id', ''),
                    'part_number': i + 1,
                    'content': reason_chunk,
                    'size': len(reason_chunk)
                }
                chunks.append(chunk)
        else:
            reason_chunk = {
                'type': 'interpretation_reason',
                'case_id': interpretation_data.get('id', ''),
                'content': reason_content,
                'size': len(reason_content)
            }
            chunks.append(reason_chunk)
    
    return chunks

def split_interpretation_reason(reason_text: str) -> List[str]:
    """
    í•´ì„ ì´ìœ ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
    """
    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í•  (ë¹ˆ ì¤„ ê¸°ì¤€)
    paragraphs = [p.strip() for p in reason_text.split('\n\n') if p.strip()]
    
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        if len(current_chunk) + len(paragraph) > 800:  # 800ì ì œí•œ
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = paragraph
        else:
            current_chunk += "\n\n" + paragraph if current_chunk else paragraph
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks
```

### 2.3 ì˜ë¯¸ì  ì²­í‚¹ ì „ëµ (Semantic Chunking)

#### 2.3.1 ë¬¸ì¥ ê²½ê³„ ì¸ì‹ ì²­í‚¹
```python
def sentence_boundary_chunking(text: str, max_size: int = 500) -> List[Dict]:
    """
    ë¬¸ì¥ ê²½ê³„ë¥¼ ì¸ì‹í•œ ì˜ë¯¸ì  ì²­í‚¹
    
    ì²­í‚¹ ê·œì¹™:
    1. ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ)
    2. ìµœëŒ€ í¬ê¸° ë‚´ì—ì„œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¡°í•©
    3. ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ì—°ê´€ì„± ê³ ë ¤
    4. ë²•ë¥  ë¬¸ì¥ì˜ íŠ¹ìˆ˜ì„± ë°˜ì˜
    """
    
    # ë¬¸ì¥ ë¶„í•  (ë²•ë¥  ë¬¸ì¥ íŠ¹ì„± ê³ ë ¤)
    sentences = split_legal_sentences(text)
    
    chunks = []
    current_chunk = ""
    current_size = 0
    
    for sentence in sentences:
        sentence_size = len(sentence)
        
        # í¬ê¸° ì´ˆê³¼ ì‹œ í˜„ì¬ ì²­í¬ ì €ì¥
        if current_size + sentence_size > max_size and current_chunk:
            chunks.append(create_chunk(current_chunk))
            current_chunk = sentence
            current_size = sentence_size
        else:
            current_chunk += " " + sentence if current_chunk else sentence
            current_size += sentence_size
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk:
        chunks.append(create_chunk(current_chunk))
    
    return chunks
```

#### 2.3.2 ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ì²­í‚¹
```python
def semantic_similarity_chunking(text: str, similarity_threshold: float = 0.7) -> List[Dict]:
    """
    ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜í•œ ì²­í‚¹
    
    ì²­í‚¹ ê·œì¹™:
    1. ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
    2. ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì¥ë“¤ì„ ê·¸ë£¹í™”
    3. ê·¸ë£¹ ë‚´ì—ì„œ ìµœì  í¬ê¸°ë¡œ ì¡°ì •
    4. ì˜ë¯¸ì  ì¼ê´€ì„± ë³´ì¥
    """
    
    # ë¬¸ì¥ ë¶„í• 
    sentences = split_legal_sentences(text)
    
    # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
    similarities = calculate_sentence_similarities(sentences)
    
    # ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë£¹í™”
    groups = group_by_similarity(sentences, similarities, similarity_threshold)
    
    # ê·¸ë£¹ì„ ì²­í¬ë¡œ ë³€í™˜
    chunks = []
    for group in groups:
        chunk_content = " ".join(group)
        chunk = {
            'type': 'semantic_group',
            'content': chunk_content,
            'group_size': len(group),
            'coherence_score': calculate_coherence_score(group),
            'size': len(chunk_content)
        }
        chunks.append(chunk)
    
    return chunks
```

---

## ğŸ“ ì²­í‚¹ í¬ê¸° ìµœì í™”

### 3.1 ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì²­í‚¹ í¬ê¸° ë¶„í¬

#### 3.1.1 ë²•ë ¹ ë°ì´í„° ì²­í‚¹ í¬ê¸° ë¶„ì„
- **ì¡°ë¬¸ ë‹¨ìœ„**: 200-800ì (í‰ê·  400ì)
- **í•­ëª© ë‹¨ìœ„**: 100-400ì (í‰ê·  200ì)
- **ë¶€ì¹™ ë‹¨ìœ„**: 300-1500ì (í‰ê·  600ì)

#### 3.1.2 íŒë¡€ ë°ì´í„° ì²­í‚¹ í¬ê¸° ë¶„ì„
- **ì‚¬ê±´ ê¸°ë³¸ì •ë³´**: 100-300ì (í‰ê·  150ì)
- **ì‚¬ê±´ëª… (ìŸì )**: 50-200ì (í‰ê·  100ì)
- **ê¸´ ì‚¬ê±´ëª…**: 200-500ì (í‰ê·  300ì)

#### 3.1.3 í—Œì¬ê²°ì •ë¡€ ì²­í‚¹ í¬ê¸° ë¶„ì„
- **ê¸°ë³¸ì •ë³´**: 100-200ì (í‰ê·  150ì)
- **ì‹¬íŒëŒ€ìƒì¡°ë¬¸**: 50-300ì (í‰ê·  150ì)
- **íŒì‹œì‚¬í•­**: 100-500ì (í‰ê·  250ì)
- **ê²°ì •ìš”ì§€**: 200-1000ì (í‰ê·  400ì)

#### 3.1.4 ë²•ë ¹í•´ì„ë¡€ ì²­í‚¹ í¬ê¸° ë¶„ì„
- **ê¸°ë³¸ì •ë³´**: 100-300ì (í‰ê·  200ì)
- **ì§ˆì˜ìš”ì§€**: 200-800ì (í‰ê·  400ì)
- **íšŒë‹µ**: 100-500ì (í‰ê·  250ì)
- **í•´ì„ì´ìœ **: 500-3000ì (í‰ê·  1200ì)

#### 3.1.5 ê¶Œì¥ ì²­í‚¹ í¬ê¸° ë¶„í¬
- **ì†Œí˜• ì²­í¬ (100-300ì)**: ì‚¬ê±´ ê¸°ë³¸ì •ë³´, ì¡°ë¬¸ ì œëª©
- **ì¤‘í˜• ì²­í¬ (300-800ì)**: ì¡°ë¬¸ ë³¸ë¬¸, íŒì‹œì‚¬í•­, ì§ˆì˜ìš”ì§€
- **ëŒ€í˜• ì²­í¬ (800-2000ì)**: í•´ì„ì´ìœ , ê¸´ ì¡°ë¬¸, ë¶€ì¹™

### 3.2 ë™ì  í¬ê¸° ì¡°ì •

#### 3.2.1 ë‚´ìš© ê¸°ë°˜ í¬ê¸° ì¡°ì •
```python
def dynamic_size_adjustment(text: str, base_size: int = 800) -> int:
    """
    í…ìŠ¤íŠ¸ ë‚´ìš©ì— ë”°ë¥¸ ë™ì  í¬ê¸° ì¡°ì •
    
    ì¡°ì • ìš”ì†Œ:
    1. ë²•ë¥  ìš©ì–´ ë°€ë„
    2. ë¬¸ì¥ ë³µì¡ë„
    3. ì˜ë¯¸ì  ë°€ë„
    4. êµ¬ì¡°ì  ë³µì¡ë„
    """
    
    # ë²•ë¥  ìš©ì–´ ë°€ë„ ê³„ì‚°
    legal_term_density = calculate_legal_term_density(text)
    
    # ë¬¸ì¥ ë³µì¡ë„ ê³„ì‚°
    sentence_complexity = calculate_sentence_complexity(text)
    
    # ì˜ë¯¸ì  ë°€ë„ ê³„ì‚°
    semantic_density = calculate_semantic_density(text)
    
    # í¬ê¸° ì¡°ì • ê³„ìˆ˜ ê³„ì‚°
    adjustment_factor = (
        legal_term_density * 0.3 +
        sentence_complexity * 0.3 +
        semantic_density * 0.4
    )
    
    # ìµœì¢… í¬ê¸° ê³„ì‚°
    adjusted_size = int(base_size * adjustment_factor)
    
    # ìµœì†Œ/ìµœëŒ€ í¬ê¸° ì œí•œ
    return max(200, min(3000, adjusted_size))
```

#### 3.2.2 ì˜¤ë²„ë© ì „ëµ

##### A. ë²•ë ¹ ë°ì´í„° ì˜¤ë²„ë© (10-15%)
```python
def law_overlap_strategy(chunk_size: int) -> int:
    """
    ë²•ë ¹ ë°ì´í„° ì˜¤ë²„ë© ì „ëµ
    
    ì˜¤ë²„ë© ìš”ì†Œ:
    1. ì¡°ë¬¸ ê°„ ì—°ê´€ì„±
    2. ë²•ë¦¬ ì¶”ë¡  ì—°ì†ì„±
    3. ìš©ì–´ ì •ì˜ ì¼ê´€ì„±
    """
    return int(chunk_size * 0.12)  # 12% ì˜¤ë²„ë©
```

##### B. íŒë¡€ ë°ì´í„° ì˜¤ë²„ë© (15-20%)
```python
def precedent_overlap_strategy(chunk_size: int) -> int:
    """
    íŒë¡€ ë°ì´í„° ì˜¤ë²„ë© ì „ëµ
    
    ì˜¤ë²„ë© ìš”ì†Œ:
    1. ë²•ë¦¬ ì¶”ë¡  ì—°ì†ì„±
    2. íŒë¡€ ì¸ìš© ë§¥ë½
    3. ìŸì  ê°„ ì—°ê´€ì„±
    """
    return int(chunk_size * 0.18)  # 18% ì˜¤ë²„ë©
```

---

## ğŸ” í’ˆì§ˆ ë³´ì¥ ì „ëµ

### 4.1 ì²­í‚¹ í’ˆì§ˆ ê²€ì¦

#### 4.1.1 êµ¬ì¡°ì  ì™„ì„±ë„ ê²€ì¦
```python
def validate_structural_completeness(chunk: Dict) -> bool:
    """
    ì²­í‚¹ì˜ êµ¬ì¡°ì  ì™„ì„±ë„ ê²€ì¦
    
    ê²€ì¦ ìš”ì†Œ:
    1. ë¬¸ë²•ì  ì™„ì„±ì„±
    2. ì˜ë¯¸ì  ì¼ê´€ì„±
    3. ë²•ë¥  êµ¬ì¡° ì¤€ìˆ˜
    4. ìš©ì–´ ì •í™•ì„±
    """
    
    # ë¬¸ë²•ì  ì™„ì„±ì„± ê²€ì¦
    if not is_grammatically_complete(chunk['content']):
        return False
    
    # ì˜ë¯¸ì  ì¼ê´€ì„± ê²€ì¦
    if not has_semantic_coherence(chunk['content']):
        return False
    
    # ë²•ë¥  êµ¬ì¡° ì¤€ìˆ˜ ê²€ì¦
    if not follows_legal_structure(chunk['content']):
        return False
    
    # ìš©ì–´ ì •í™•ì„± ê²€ì¦
    if not has_accurate_legal_terms(chunk['content']):
        return False
    
    return True
```

#### 4.1.2 ì˜ë¯¸ì  í’ˆì§ˆ ê²€ì¦
```python
def validate_semantic_quality(chunk: Dict) -> float:
    """
    ì²­í‚¹ì˜ ì˜ë¯¸ì  í’ˆì§ˆ ê²€ì¦
    
    í’ˆì§ˆ ì§€í‘œ:
    1. ì˜ë¯¸ì  ì¼ê´€ì„± (0.0-1.0)
    2. ë²•ë¥  ìš©ì–´ í¬í•¨ë„ (0.0-1.0)
    3. ë§¥ë½ ë³´ì¡´ë„ (0.0-1.0)
    4. ê²€ìƒ‰ ì í•©ì„± (0.0-1.0)
    """
    
    # ì˜ë¯¸ì  ì¼ê´€ì„± ì ìˆ˜
    coherence_score = calculate_semantic_coherence(chunk['content'])
    
    # ë²•ë¥  ìš©ì–´ í¬í•¨ë„ ì ìˆ˜
    term_coverage_score = calculate_legal_term_coverage(chunk['content'])
    
    # ë§¥ë½ ë³´ì¡´ë„ ì ìˆ˜
    context_preservation_score = calculate_context_preservation(chunk['content'])
    
    # ê²€ìƒ‰ ì í•©ì„± ì ìˆ˜
    searchability_score = calculate_searchability(chunk['content'])
    
    # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
    quality_score = (
        coherence_score * 0.3 +
        term_coverage_score * 0.25 +
        context_preservation_score * 0.25 +
        searchability_score * 0.2
    )
    
    return quality_score
```

### 4.2 ì²­í‚¹ í†µê³„ ë° ëª¨ë‹ˆí„°ë§

#### 4.2.1 ì²­í‚¹ í†µê³„ ìƒì„±
```python
def generate_chunking_statistics(chunks: List[Dict]) -> Dict:
    """
    ì²­í‚¹ í†µê³„ ìƒì„±
    
    í†µê³„ í•­ëª©:
    1. ê¸°ë³¸ í†µê³„ (ê°œìˆ˜, í¬ê¸° ë¶„í¬)
    2. í’ˆì§ˆ í†µê³„ (í’ˆì§ˆ ì ìˆ˜ ë¶„í¬)
    3. íš¨ìœ¨ì„± í†µê³„ (ì²˜ë¦¬ ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰)
    4. ê²€ìƒ‰ ì„±ëŠ¥ í†µê³„ (ê²€ìƒ‰ ì •í™•ë„, ì‘ë‹µ ì‹œê°„)
    """
    
    # ê¸°ë³¸ í†µê³„
    basic_stats = {
        'total_chunks': len(chunks),
        'avg_chunk_size': np.mean([c['size'] for c in chunks]),
        'min_chunk_size': min([c['size'] for c in chunks]),
        'max_chunk_size': max([c['size'] for c in chunks]),
        'size_std': np.std([c['size'] for c in chunks])
    }
    
    # í’ˆì§ˆ í†µê³„
    quality_scores = [validate_semantic_quality(c) for c in chunks]
    quality_stats = {
        'avg_quality_score': np.mean(quality_scores),
        'min_quality_score': min(quality_scores),
        'max_quality_score': max(quality_scores),
        'quality_std': np.std(quality_scores)
    }
    
    # ë¶„í¬ í†µê³„
    size_distribution = get_size_distribution(chunks)
    quality_distribution = get_quality_distribution(chunks)
    
    return {
        'basic_stats': basic_stats,
        'quality_stats': quality_stats,
        'size_distribution': size_distribution,
        'quality_distribution': quality_distribution
    }
```

---

## ğŸ¯ ì²­í‚¹ ì „ëµ ì„ íƒ ê°€ì´ë“œ

### 5.1 ë°ì´í„° ìœ í˜•ë³„ ê¶Œì¥ ì „ëµ

#### 5.1.1 ë²•ë ¹ ë°ì´í„°
- **ì£¼ ì „ëµ**: ì¡°ë¬¸ ì¤‘ì‹¬ ì²­í‚¹
- **ë³´ì¡° ì „ëµ**: í•­ëª©ë³„ ì²­í‚¹
- **í¬ê¸°**: 500-1500ì
- **ì˜¤ë²„ë©**: 10-15%

#### 5.1.2 íŒë¡€ ë°ì´í„°
- **ì£¼ ì „ëµ**: ì‚¬ê±´ë³„ ì²­í‚¹
- **ë³´ì¡° ì „ëµ**: ë²•ë¦¬ ì¤‘ì‹¬ ì²­í‚¹
- **í¬ê¸°**: 800-2000ì
- **ì˜¤ë²„ë©**: 15-20%

#### 5.1.3 í—Œì¬ê²°ì •ë¡€
- **ì£¼ ì „ëµ**: ìŸì ë³„ ì²­í‚¹
- **ë³´ì¡° ì „ëµ**: ì˜ë¯¸ì  ì²­í‚¹
- **í¬ê¸°**: 1000-2500ì
- **ì˜¤ë²„ë©**: 20-25%

### 5.2 ì‚¬ìš© ëª©ì ë³„ ê¶Œì¥ ì „ëµ

#### 5.2.1 ì •í™•í•œ ê²€ìƒ‰ (Precise Search)
- **ì „ëµ**: ì†Œí˜• ì²­í¬ + ì˜ë¯¸ì  ì²­í‚¹
- **í¬ê¸°**: 200-500ì
- **íŠ¹ì§•**: ë†’ì€ ì •í™•ë„, ë‚®ì€ ë§¥ë½

#### 5.2.2 ë§¥ë½ ë³´ì¡´ ê²€ìƒ‰ (Contextual Search)
- **ì „ëµ**: ì¤‘í˜• ì²­í¬ + êµ¬ì¡°ì  ì²­í‚¹
- **í¬ê¸°**: 500-1200ì
- **íŠ¹ì§•**: ê· í˜•ì¡íŒ ì •í™•ë„ì™€ ë§¥ë½

#### 5.2.3 ì¢…í•©ì  ì´í•´ (Comprehensive Understanding)
- **ì „ëµ**: ëŒ€í˜• ì²­í¬ + ê³„ì¸µì  ì²­í‚¹
- **í¬ê¸°**: 1200-3000ì
- **íŠ¹ì§•**: ë†’ì€ ë§¥ë½, ë‚®ì€ ì •í™•ë„

---

## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ë° í‰ê°€

### 6.1 ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì²­í‚¹ í’ˆì§ˆ ì§€í‘œ

#### 6.1.1 ì˜ˆìƒ ì²­í‚¹ í†µê³„ (ì‹¤ì œ ë°ì´í„° ê¸°ì¤€)
- **ì´ ì²­í¬ ìˆ˜**: ì•½ 15,000-20,000ê°œ
  - ë²•ë ¹ ë°ì´í„°: 2,000-3,000ê°œ ì²­í¬
  - íŒë¡€ ë°ì´í„°: 8,000-10,000ê°œ ì²­í¬
  - í—Œì¬ê²°ì •ë¡€: 3,000-4,000ê°œ ì²­í¬
  - ë²•ë ¹í•´ì„ë¡€: 2,000-3,000ê°œ ì²­í¬

#### 6.1.2 êµ¬ì¡°ì  í’ˆì§ˆ ëª©í‘œ
- **ë¬¸ë²•ì  ì™„ì„±ì„±**: 95% ì´ìƒ
- **ì˜ë¯¸ì  ì¼ê´€ì„±**: 90% ì´ìƒ
- **ë²•ë¥  êµ¬ì¡° ì¤€ìˆ˜**: 85% ì´ìƒ
- **ë©”íƒ€ë°ì´í„° ì™„ì„±ë„**: 98% ì´ìƒ

#### 6.1.3 ê²€ìƒ‰ ì„±ëŠ¥ ëª©í‘œ
- **ê²€ìƒ‰ ì •í™•ë„**: 80% ì´ìƒ
- **ì‘ë‹µ ì‹œê°„**: 1ì´ˆ ì´ë‚´
- **ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ**: 75% ì´ìƒ
- **ê´€ë ¨ì„± ì ìˆ˜**: 0.7 ì´ìƒ

### 6.2 ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ìµœì í™” ëª©í‘œ

#### 6.2.1 ì²­í‚¹ íš¨ìœ¨ì„±
- **ì²˜ë¦¬ ì†ë„**: 500ì²­í¬/ë¶„ ì´ìƒ (ì‹¤ì œ ë°ì´í„° í¬ê¸° ê³ ë ¤)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 4GB ì´í•˜ (7,699ê°œ íŒë¡€ + 2,000ê°œ í—Œì¬ê²°ì •ë¡€)
- **ì €ì¥ ê³µê°„**: 5GB ì´í•˜ (ì••ì¶• ê¸°ì¤€)

#### 6.2.2 ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„  ëª©í‘œ
- **ì •í™•ë„ í–¥ìƒ**: 15-20%
- **ì‘ë‹µ ì†ë„ í–¥ìƒ**: 30-40%
- **ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í–¥ìƒ**: 25-30%
- **ë²•ë¥  ìš©ì–´ ë§¤ì¹­ ì •í™•ë„**: 90% ì´ìƒ

---

## ğŸ”§ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ êµ¬í˜„ ê°€ì´ë“œ

### 7.1 ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

#### 7.1.1 ë²•ë ¹ ë°ì´í„° ì²˜ë¦¬ ìˆœì„œ
```python
def process_law_data():
    """
    ë²•ë ¹ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    """
    # 1. ë²•ë ¹ íŒŒì¼ ë¡œë“œ (20ê°œ íŒŒì¼)
    law_files = glob.glob('data/raw/laws/*.json')
    
    for law_file in law_files:
        # 2. JSON ë°ì´í„° ë¡œë“œ
        with open(law_file, 'r', encoding='utf-8') as f:
            law_data = json.load(f)
        
        # 3. ì¡°ë¬¸ ì¤‘ì‹¬ ì²­í‚¹
        chunks = law_article_chunking(law_data)
        
        # 4. ì²­í¬ ì €ì¥
        save_chunks(chunks, f'data/processed/laws/{law_data["basic_info"]["name"]}_chunks.json')
```

#### 7.1.2 íŒë¡€ ë°ì´í„° ì²˜ë¦¬ ìˆœì„œ
```python
def process_precedent_data():
    """
    íŒë¡€ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (90ê°œ ë°°ì¹˜ íŒŒì¼)
    """
    # 1. íŒë¡€ ë°°ì¹˜ íŒŒì¼ ë¡œë“œ
    precedent_files = glob.glob('data/raw/precedents/batch_*.json')
    
    all_chunks = []
    for batch_file in precedent_files:
        # 2. ë°°ì¹˜ ë°ì´í„° ë¡œë“œ
        with open(batch_file, 'r', encoding='utf-8') as f:
            batch_data = json.load(f)
        
        # 3. ê° íŒë¡€ë³„ ì²­í‚¹
        for precedent in batch_data['precedents']:
            chunks = precedent_case_chunking(precedent)
            all_chunks.extend(chunks)
    
    # 4. í†µí•© ì²­í¬ ì €ì¥
    save_chunks(all_chunks, 'data/processed/precedents/all_precedent_chunks.json')
```

#### 7.1.3 í—Œì¬ê²°ì •ë¡€ ë°ì´í„° ì²˜ë¦¬ ìˆœì„œ
```python
def process_constitutional_data():
    """
    í—Œì¬ê²°ì •ë¡€ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (40ê°œ ë°°ì¹˜ íŒŒì¼)
    """
    constitutional_files = glob.glob('data/raw/constitutional_decisions/batch_*.json')
    
    all_chunks = []
    for batch_file in constitutional_files:
        with open(batch_file, 'r', encoding='utf-8') as f:
            batch_data = json.load(f)
        
        for decision in batch_data['data']:
            chunks = constitutional_issue_chunking(decision)
            all_chunks.extend(chunks)
    
    save_chunks(all_chunks, 'data/processed/constitutional/all_constitutional_chunks.json')
```

#### 7.1.4 ë²•ë ¹í•´ì„ë¡€ ë°ì´í„° ì²˜ë¦¬ ìˆœì„œ
```python
def process_interpretation_data():
    """
    ë²•ë ¹í•´ì„ë¡€ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (4ê°œ ë°°ì¹˜ íŒŒì¼)
    """
    interpretation_files = glob.glob('data/raw/legal_interpretations/batches/batch_*.json')
    
    all_chunks = []
    for batch_file in interpretation_files:
        with open(batch_file, 'r', encoding='utf-8') as f:
            batch_data = json.load(f)
        
        for interpretation in batch_data['interpretations']:
            chunks = legal_interpretation_chunking(interpretation)
            all_chunks.extend(chunks)
    
    save_chunks(all_chunks, 'data/processed/interpretations/all_interpretation_chunks.json')
```

### 7.2 ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ

#### 7.2.1 ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹
- **ë²•ë ¹ ë°ì´í„°**: íŒŒì¼ë³„ ìˆœì°¨ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 100MB ì´í•˜)
- **íŒë¡€ ë°ì´í„°**: ë°°ì¹˜ë³„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 200MB ì´í•˜)
- **í—Œì¬ê²°ì •ë¡€**: ë°°ì¹˜ë³„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 150MB ì´í•˜)
- **ë²•ë ¹í•´ì„ë¡€**: ë°°ì¹˜ë³„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 100MB ì´í•˜)

#### 7.2.2 ì²­í¬ ìºì‹± ì „ëµ
```python
class ChunkCache:
    """
    ì²­í¬ ìºì‹± ê´€ë¦¬ í´ë˜ìŠ¤
    """
    def __init__(self, max_size=1000):
        self.cache = {}
        self.max_size = max_size
        self.access_count = {}
    
    def get_chunk(self, chunk_id):
        if chunk_id in self.cache:
            self.access_count[chunk_id] += 1
            return self.cache[chunk_id]
        return None
    
    def add_chunk(self, chunk_id, chunk_data):
        if len(self.cache) >= self.max_size:
            # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ ì²­í¬ ì œê±°
            self._remove_oldest()
        
        self.cache[chunk_id] = chunk_data
        self.access_count[chunk_id] = 1
```

### 7.3 ì„±ëŠ¥ ìµœì í™”

#### 7.3.1 ë³‘ë ¬ ì²˜ë¦¬
```python
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor

def process_data_parallel():
    """
    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì„±ëŠ¥ ìµœì í™”
    """
    # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ì›Œì»¤ ìˆ˜ ì„¤ì •
    num_workers = min(mp.cpu_count(), 4)
    
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        # ë²•ë ¹ ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬
        law_files = glob.glob('data/raw/laws/*.json')
        law_futures = [executor.submit(process_single_law, f) for f in law_files]
        
        # íŒë¡€ ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬
        precedent_files = glob.glob('data/raw/precedents/batch_*.json')
        precedent_futures = [executor.submit(process_single_precedent_batch, f) for f in precedent_files]
        
        # ê²°ê³¼ ìˆ˜ì§‘
        law_results = [f.result() for f in law_futures]
        precedent_results = [f.result() for f in precedent_futures]
```

#### 7.3.2 ì¸ë±ì‹± ì „ëµ
```python
def build_chunk_index():
    """
    ì²­í¬ ì¸ë±ìŠ¤ êµ¬ì¶•
    """
    index = {
        'by_type': {},  # ì²­í¬ íƒ€ì…ë³„ ì¸ë±ìŠ¤
        'by_law': {},   # ë²•ë ¹ë³„ ì¸ë±ìŠ¤
        'by_case': {},  # ì‚¬ê±´ë³„ ì¸ë±ìŠ¤
        'by_keyword': {} # í‚¤ì›Œë“œë³„ ì¸ë±ìŠ¤
    }
    
    # ëª¨ë“  ì²­í¬ íŒŒì¼ ë¡œë“œ ë° ì¸ë±ì‹±
    chunk_files = glob.glob('data/processed/*/all_*_chunks.json')
    
    for chunk_file in chunk_files:
        with open(chunk_file, 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        
        for chunk in chunks:
            # íƒ€ì…ë³„ ì¸ë±ìŠ¤
            chunk_type = chunk.get('type', 'unknown')
            if chunk_type not in index['by_type']:
                index['by_type'][chunk_type] = []
            index['by_type'][chunk_type].append(chunk['id'])
            
            # ë²•ë ¹ë³„ ì¸ë±ìŠ¤
            if 'law_name' in chunk:
                law_name = chunk['law_name']
                if law_name not in index['by_law']:
                    index['by_law'][law_name] = []
                index['by_law'][law_name].append(chunk['id'])
    
    # ì¸ë±ìŠ¤ ì €ì¥
    with open('data/processed/chunk_index.json', 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)
```

### 7.2 í’ˆì§ˆ ê´€ë¦¬

#### 7.2.1 ê²€ì¦ í”„ë¡œì„¸ìŠ¤
- **ìë™ ê²€ì¦**: ì²­í‚¹ í’ˆì§ˆ ìë™ ê²€ì¦
- **ìˆ˜ë™ ê²€í† **: ë²•ë¥  ì „ë¬¸ê°€ ê²€í† 
- **ì§€ì†ì  ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

#### 7.2.2 ê°œì„  í”„ë¡œì„¸ìŠ¤
- **í”¼ë“œë°± ìˆ˜ì§‘**: ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
- **ì„±ëŠ¥ ë¶„ì„**: ì •ê¸°ì  ì„±ëŠ¥ ë¶„ì„
- **ì „ëµ ì¡°ì •**: ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ì „ëµ ì¡°ì •

---

## ğŸ“š ì°¸ê³  ìë£Œ

### 8.1 ê´€ë ¨ ë…¼ë¬¸
- "Legal Text Chunking for Information Retrieval" (2023)
- "Semantic Chunking for Legal Documents" (2022)
- "Multi-Level Text Segmentation for Legal AI" (2023)

### 8.2 ê¸°ìˆ  ë¬¸ì„œ
- Sentence-BERT Documentation
- FAISS Indexing Guide
- Legal NLP Best Practices

### 8.3 ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
- **í…ìŠ¤íŠ¸ ì²˜ë¦¬**: spaCy, NLTK
- **ì˜ë¯¸ì  ë¶„ì„**: Sentence-Transformers
- **ë²¡í„° ê²€ìƒ‰**: FAISS, ChromaDB
- **í’ˆì§ˆ í‰ê°€**: BLEU, ROUGE, BERTScore

---

*ë³¸ ë¬¸ì„œëŠ” LawFirmAI í”„ë¡œì íŠ¸ì˜ í…ìŠ¤íŠ¸ ì²­í‚¹ ì „ëµì„ ì •ì˜í•˜ë©°, í”„ë¡œì íŠ¸ ì§„í–‰ì— ë”°ë¼ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.*
