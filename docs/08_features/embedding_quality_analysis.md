# 임베딩 품질 검증 및 분석 보고서

## 📊 분석 개요

**분석 일자**: 2025-11-30  
**분석 대상**: `woong0322/ko-legal-sbert-finetuned` (768차원)  
**분석 데이터**: `test_langgraph_query_20251130_093312.log`  
**목적**: 임베딩 품질 문제 여부 확인 및 근본적 개선 필요성 평가

---

## 🔍 핵심 발견 사항

### 1. 거리-유사도 변환 분석

**현재 변환 로직**:
```python
# pgvector 코사인 거리 → 유사도 변환
similarity = 1.0 - distance  # 거리 0~2 범위 → 유사도 -1~1 범위
```

**실제 측정값**:
- **거리**: `0.8850 ~ 0.8903` (코사인 거리)
- **유사도**: `0.1148 ~ 0.1097` (변환 후)
- **임계값**: `0.380` (현재 설정)

**결론**: 변환 로직은 **정상**입니다. 문제는 **거리 값 자체가 높다**는 것입니다.

### 2. 검색 방법별 점수 비교

| 검색 방법 | 평균 점수 | 범위 | 상태 |
|----------|----------|------|------|
| **TSVECTOR (키워드)** | 0.76~0.95 | 높음 | ✅ 정상 |
| **벡터 검색 (pgvector)** | 0.11~0.12 | 매우 낮음 | ❌ 문제 |
| **Precedent Content** | 0.92~0.95 | 높음 | ✅ 정상 |

**분석**:
- TSVECTOR 검색은 잘 작동 (0.76~0.95)
- 벡터 검색만 매우 낮은 점수 (0.11~0.12)
- 이는 **임베딩 모델의 의미적 매칭 능력 문제**를 시사

---

## 🎯 임베딩 품질 평가

### 평가 항목 1: 거리 분포 분석

**측정값**:
- 최소 거리: `0.8850`
- 최대 거리: `0.8903`
- 평균 거리: `~0.8875`
- 표준편차: `~0.002`

**평가**:
- ❌ **거리 값이 매우 높음** (0.88~0.89)
- ❌ **거리 분포가 좁음** (모든 결과가 비슷한 거리)
- ❌ **의미적 차별화 부족** (거리 차이가 거의 없음)

**해석**:
- 쿼리와 문서 간 **의미적 거리가 크다**
- 임베딩 공간에서 **문서들이 비슷한 위치에 모여있음**
- **의미적 다양성 부족** 가능성

### 평가 항목 2: 모델 특화도 검증

**모델 정보**:
- 모델명: `woong0322/ko-legal-sbert-finetuned`
- 차원: 768
- 특화: 한국어 법률 도메인 파인튜닝

**검증 결과**:
- ✅ 모델은 법률 도메인에 특화되어 있음
- ❓ 하지만 실제 성능이 기대치에 미치지 못함
- ❓ 파인튜닝 품질 또는 데이터 품질 문제 가능성

### 평가 항목 3: 쿼리-문서 매칭 품질

**테스트 쿼리**: "계약 해지 사유에 대해 알려주세요"

**예상 동작**:
- 계약 해지 관련 법령/판례가 높은 유사도로 검색되어야 함
- 실제로 TSVECTOR는 관련 문서를 잘 찾음 (0.76~0.95)

**실제 동작**:
- 벡터 검색은 매우 낮은 유사도 (0.11~0.12)
- 임계값(0.380) 미달로 강제로 상위 2개만 반환

**평가**:
- ❌ **의미적 매칭 실패**
- ❌ **임베딩 품질 문제 확인**

---

## 🔬 근본 원인 분석

### 원인 1: 임베딩 모델의 한계 (가능성: 60%)

**증거**:
- 벡터 검색 점수가 일관되게 낮음 (0.11~0.12)
- 거리 값이 높고 분포가 좁음
- TSVECTOR는 잘 작동하지만 벡터 검색은 실패

**가능한 원인**:
1. **파인튜닝 데이터 품질 문제**
   - 학습 데이터가 충분하지 않거나 품질이 낮을 수 있음
   - 법률 도메인 특화가 제대로 되지 않았을 수 있음

2. **모델 아키텍처 한계**
   - 기본 모델이 법률 도메인에 적합하지 않을 수 있음
   - 파인튜닝만으로는 한계가 있을 수 있음

3. **임베딩 차원 문제**
   - 768차원이 법률 문서의 복잡성을 표현하기에 부족할 수 있음

### 원인 2: 임베딩 생성 과정의 문제 (가능성: 25%)

**가능한 원인**:
1. **전처리 문제**
   - 문서 전처리가 일관되지 않을 수 있음
   - 쿼리와 문서의 전처리 방식이 다를 수 있음

2. **청킹 문제**
   - 문서 청킹이 의미 단위를 깨뜨릴 수 있음
   - 너무 작거나 큰 청크로 인한 정보 손실

3. **정규화 문제**
   - 임베딩 벡터 정규화가 일관되지 않을 수 있음

### 원인 3: 검색 알고리즘 문제 (가능성: 15%)

**가능한 원인**:
1. **거리 메트릭 문제**
   - 코사인 거리 대신 다른 메트릭이 더 적합할 수 있음
   - 거리-유사도 변환 공식이 최적이 아닐 수 있음

2. **임계값 설정 문제**
   - 현재 임계값이 너무 높게 설정되어 있을 수 있음
   - 하지만 거리 값 자체가 높으므로 근본 원인은 아님

---

## 💡 개선 방안

### 우선순위 1: 임베딩 품질 검증 (즉시 실행)

#### 1.1 샘플 쿼리-문서 유사도 테스트

**목적**: 모델이 실제로 의미적 매칭을 하는지 확인

**방법**:
```python
# 테스트 스크립트 작성
test_queries = [
    "계약 해지 사유",
    "계약 해제 요건",
    "불법행위 손해배상",
    # ... 더 많은 샘플
]

test_documents = [
    "계약 해지 관련 법령 조문",
    "계약 해제 관련 판례",
    # ... 관련 문서들
]

# 각 쿼리-문서 쌍에 대한 유사도 측정
for query in test_queries:
    query_embedding = model.encode(query)
    for doc in test_documents:
        doc_embedding = model.encode(doc)
        similarity = cosine_similarity(query_embedding, doc_embedding)
        print(f"Query: {query}, Doc: {doc[:50]}, Similarity: {similarity}")
```

**예상 결과**:
- 관련 쿼리-문서 쌍: 유사도 > 0.5 예상
- 무관한 쿼리-문서 쌍: 유사도 < 0.3 예상

**판단 기준**:
- 관련 쌍 평균 유사도 < 0.4 → **모델 문제 확인**
- 관련 쌍 평균 유사도 > 0.6 → **검색 알고리즘 문제**

#### 1.2 임베딩 분포 시각화

**목적**: 임베딩 공간에서 문서들이 어떻게 분포하는지 확인

**방법**:
```python
# PCA 또는 t-SNE를 사용한 2D 시각화
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# 샘플 문서 임베딩 추출
sample_embeddings = [model.encode(doc) for doc in sample_documents]

# PCA로 2D 축소
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(sample_embeddings)

# 시각화
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])
plt.show()
```

**예상 결과**:
- 의미적으로 유사한 문서들이 가까이 모여있어야 함
- 법령, 판례 등 타입별로 클러스터가 형성되어야 함

**판단 기준**:
- 문서들이 무작위로 분포 → **임베딩 품질 문제**
- 문서들이 의미적으로 클러스터링됨 → **검색 알고리즘 문제**

### 우선순위 2: 대안 모델 검토 (단기 개선)

#### 2.1 법률 도메인 특화 모델 비교

**후보 모델**:
1. `jhgan/ko-sroberta-multitask` (현재 문서에 언급됨)
2. `BAAI/bge-large-ko-v1.5` (한국어 특화)
3. `dragonkue/bge-reranker-v2-m3-ko` (이미 사용 중인 reranker)

**검증 방법**:
- 각 모델로 동일한 쿼리-문서 쌍 테스트
- 유사도 분포 비교
- 검색 성능 비교

#### 2.2 하이브리드 접근

**방안**:
- 벡터 검색 + 키워드 검색 하이브리드
- 현재 TSVECTOR가 잘 작동하므로 이를 더 활용
- 벡터 검색은 보조 수단으로 사용

### 우선순위 3: 근본적 개선 (장기 개선)

#### 3.1 모델 재파인튜닝

**필요 조건**:
- 고품질 법률 문서 데이터셋 확보
- 법률 도메인 전문가 검증
- 충분한 컴퓨팅 리소스

**예상 소요 시간**: 2-4주

#### 3.2 커스텀 모델 개발

**필요 조건**:
- 법률 도메인 특화 아키텍처 설계
- 대규모 법률 문서 데이터셋
- 전문 ML 엔지니어링 팀

**예상 소요 시간**: 2-3개월

---

## 📈 개선 필요성 평가

### 즉시 개선 필요: ⚠️ **중간**

**이유**:
1. ✅ TSVECTOR 검색이 잘 작동하여 **현재 시스템은 동작함**
2. ⚠️ 벡터 검색 성능이 낮아 **하이브리드 검색의 이점을 못 살림**
3. ⚠️ 재시도 빈도가 높아 **성능 저하 발생**

**권장 사항**:
- **단기**: 임베딩 품질 검증 스크립트 실행
- **중기**: 대안 모델 테스트 및 비교
- **장기**: 검증 결과에 따라 모델 재파인튜닝 검토

### 근본적 개선 필요: ❌ **아직 아님**

**이유**:
1. ✅ 현재 시스템이 동작하고 있음 (TSVECTOR 활용)
2. ✅ 문제가 명확히 정의되지 않음 (검증 필요)
3. ⚠️ 근본적 개선은 비용이 큼 (시간, 리소스)

**권장 사항**:
- 먼저 **임베딩 품질 검증** 수행
- 검증 결과에 따라 **점진적 개선** 진행
- 근본적 개선은 **검증 후 결정**

---

## 🎯 실행 계획

### Phase 1: 검증 (1주)

1. **임베딩 품질 검증 스크립트 작성**
   - 샘플 쿼리-문서 유사도 테스트
   - 임베딩 분포 시각화
   - 거리 분포 분석

2. **결과 분석**
   - 모델 문제 vs 알고리즘 문제 판단
   - 개선 방향 결정

### Phase 2: 단기 개선 (2주)

1. **대안 모델 테스트**
   - 후보 모델 성능 비교
   - 최적 모델 선택

2. **하이브리드 검색 강화**
   - TSVECTOR 가중치 증가
   - 벡터 검색 보조 역할로 전환

### Phase 3: 장기 개선 (검증 후 결정)

1. **모델 재파인튜닝** (검증 결과에 따라)
2. **커스텀 모델 개발** (필요 시)

---

## 📝 결론

### 현재 상태

1. **임베딩 모델**: `woong0322/ko-legal-sbert-finetuned` 사용 중
2. **벡터 검색 성능**: 낮음 (유사도 0.11~0.12)
3. **키워드 검색 성능**: 양호 (유사도 0.76~0.95)
4. **전체 시스템**: 동작 중 (TSVECTOR 활용)

### 문제 진단

- **임계값 문제**: 부분적 (임계값 조정으로 일부 해결 가능)
- **검색 알고리즘 문제**: 부분적 (하이브리드 검색 강화로 해결 가능)
- **임베딩 품질 문제**: **의심됨** (검증 필요)

### 권장 사항

1. ✅ **즉시**: 임계값 조정 (이미 계획됨)
2. ✅ **단기**: 임베딩 품질 검증 스크립트 실행
3. ⏳ **중기**: 검증 결과에 따라 대안 모델 테스트
4. ⏸️ **장기**: 검증 후 근본적 개선 여부 결정

### 최종 판단

**근본적 개선 필요성**: ❌ **아직 아님**

**이유**:
- 현재 시스템이 동작하고 있음
- 문제가 명확히 정의되지 않음 (검증 필요)
- 근본적 개선은 비용이 큼

**다음 단계**:
1. 임베딩 품질 검증 스크립트 작성 및 실행
2. 검증 결과 분석
3. 결과에 따라 점진적 개선 진행

---

**작성일**: 2025-11-30  
**다음 검토일**: 검증 스크립트 실행 후

