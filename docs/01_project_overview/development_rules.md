# LawFirmAI ê°œë°œ ê·œì¹™ ë° ê°€ì´ë“œë¼ì¸

## ğŸ“‹ ë¬¸ì„œ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” LawFirmAI í”„ë¡œì íŠ¸ì˜ ê°œë°œ ê·œì¹™, ì½”ë”© ìŠ¤íƒ€ì¼, ìš´ì˜ ê°€ì´ë“œë¼ì¸ì„ ì •ì˜í•©ë‹ˆë‹¤. Phase 1-6ì´ ì™„ë£Œëœ ì§€ëŠ¥í˜• ëŒ€í™” ì‹œìŠ¤í…œê³¼ ì„±ëŠ¥ ìµœì í™”ëœ ì˜ë¯¸ì  ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ê°œë°œ ê°€ì´ë“œë¼ì¸ì„ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸš€ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ ê·œì¹™

### Gradio ì„œë²„ ê´€ë¦¬

#### ì„œë²„ ì‹œì‘
```bash
# Gradio ì„œë²„ ì‹œì‘ (LangChain ê¸°ë°˜)
cd gradio
python simple_langchain_app.py

# ë˜ëŠ” ìµœì‹  ì•± ì‹¤í–‰ (7ê°œ íƒ­ êµ¬ì„±)
python app.py
```

#### ì„œë²„ ì¢…ë£Œ (PID ê¸°ì¤€)
**âš ï¸ ì¤‘ìš”**: `taskkill /f /im python.exe` ì‚¬ìš© ê¸ˆì§€

**ì˜¬ë°”ë¥¸ ì¢…ë£Œ ë°©ë²•**:

1. **PID íŒŒì¼ ê¸°ë°˜ ì¢…ë£Œ** (ê¶Œì¥):
```bash
# Windows
python gradio/stop_server.py

# ë˜ëŠ” ë°°ì¹˜ íŒŒì¼ ì‚¬ìš©
gradio/stop_server.bat
```

2. **í¬íŠ¸ ê¸°ë°˜ ì¢…ë£Œ**:
```bash
# 7860 í¬íŠ¸ ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ í™•ì¸
netstat -ano | findstr :7860

# íŠ¹ì • PID ì¢…ë£Œ
taskkill /PID [PIDë²ˆí˜¸] /F
```

#### PID ê´€ë¦¬ êµ¬í˜„ ê·œì¹™

**ëª¨ë“  Gradio ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤**:

1. **PID íŒŒì¼ ìƒì„±**:
```python
import os
import signal
import atexit
from pathlib import Path

def save_pid():
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ PIDë¥¼ íŒŒì¼ì— ì €ì¥"""
    pid = os.getpid()
    pid_file = Path("gradio_server.pid")
    
    try:
        with open(pid_file, 'w') as f:
            f.write(str(pid))
        print(f"PID {pid} saved to {pid_file}")
    except Exception as e:
        print(f"Failed to save PID: {e}")

def cleanup_pid():
    """PID íŒŒì¼ ì •ë¦¬"""
    pid_file = Path("gradio_server.pid")
    if pid_file.exists():
        try:
            pid_file.unlink()
            print("PID file removed")
        except Exception as e:
            print(f"Failed to remove PID file: {e}")

# ì•± ì‹œì‘ ì‹œ
save_pid()

# ì•± ì¢…ë£Œ ì‹œ ì •ë¦¬
atexit.register(cleanup_pid)
signal.signal(signal.SIGINT, lambda s, f: cleanup_pid() or exit(0))
signal.signal(signal.SIGTERM, lambda s, f: cleanup_pid() or exit(0))
```

#### ê¸ˆì§€ ì‚¬í•­

**âŒ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ**:
```bash
# ëª¨ë“  Python í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ (ìœ„í—˜!)
taskkill /f /im python.exe
```

**âœ… ì˜¬ë°”ë¥¸ ë°©ë²•**:
```bash
# íŠ¹ì • PIDë§Œ ì¢…ë£Œ
taskkill /PID 12345 /F

# ë˜ëŠ” ì œê³µëœ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
python gradio/stop_server.py
```

## ğŸš€ Phaseë³„ ê°œë°œ ê°€ì´ë“œë¼ì¸

### Phase 1-3: ì§€ëŠ¥í˜• ëŒ€í™” ì‹œìŠ¤í…œ ê°œë°œ

#### Phase 1: ëŒ€í™” ë§¥ë½ ê°•í™”
```python
# í†µí•© ì„¸ì…˜ ê´€ë¦¬ êµ¬í˜„ ì˜ˆì‹œ
from source.services.integrated_session_manager import IntegratedSessionManager

class ChatService:
    def __init__(self):
        self.session_manager = IntegratedSessionManager()
    
    def process_message(self, message: str, session_id: str):
        # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
        context = self.session_manager.get_session_context(session_id)
        
        # ë‹¤ì¤‘ í„´ ì§ˆë¬¸ ì²˜ë¦¬
        processed_message = self.session_manager.process_multi_turn(message, context)
        
        # ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
        compressed_context = self.session_manager.compress_context(context)
        
        return processed_message, compressed_context
```

#### Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„
```python
# ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜ ê°œì¸í™” êµ¬í˜„ ì˜ˆì‹œ
from source.services.user_profile_manager import UserProfileManager
from source.services.emotion_intent_analyzer import EmotionIntentAnalyzer

class PersonalizedChatService:
    def __init__(self):
        self.profile_manager = UserProfileManager()
        self.emotion_analyzer = EmotionIntentAnalyzer()
    
    def get_personalized_response(self, message: str, user_id: str):
        # ì‚¬ìš©ì í”„ë¡œí•„ ë¡œë“œ
        profile = self.profile_manager.get_profile(user_id)
        
        # ê°ì • ë° ì˜ë„ ë¶„ì„
        emotion_result = self.emotion_analyzer.analyze(message)
        
        # ê°œì¸í™”ëœ ì‘ë‹µ ìƒì„±
        response = self.generate_response(message, profile, emotion_result)
        
        return response
```

#### Phase 3: ì¥ê¸° ê¸°ì–µ ë° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
```python
# ë§¥ë½ì  ë©”ëª¨ë¦¬ ê´€ë¦¬ êµ¬í˜„ ì˜ˆì‹œ
from source.services.contextual_memory_manager import ContextualMemoryManager
from source.services.conversation_quality_monitor import ConversationQualityMonitor

class AdvancedChatService:
    def __init__(self):
        self.memory_manager = ContextualMemoryManager()
        self.quality_monitor = ConversationQualityMonitor()
    
    def process_with_memory(self, message: str, user_id: str):
        # ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰
        relevant_memories = self.memory_manager.search_memories(message, user_id)
        
        # í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
        quality_score = self.quality_monitor.assess_quality(message, relevant_memories)
        
        # ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        self.memory_manager.update_memory(message, user_id, quality_score)
        
        return relevant_memories, quality_score
```

### Phase 5: ì„±ëŠ¥ ìµœì í™” ê°œë°œ

#### í†µí•© ìºì‹± ì‹œìŠ¤í…œ
```python
# ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„ ì˜ˆì‹œ
from source.services.integrated_cache_system import IntegratedCacheSystem

class OptimizedChatService:
    def __init__(self):
        self.cache_system = IntegratedCacheSystem()
    
    def get_cached_response(self, message: str, session_id: str):
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self.cache_system.generate_key(message, session_id)
        
        # ë‹¤ì¸µ ìºì‹œ ê²€ìƒ‰
        cached_result = self.cache_system.get(cache_key)
        
        if cached_result:
            return cached_result
        
        # ìºì‹œ ë¯¸ìŠ¤ ì‹œ ìƒˆë¡œ ìƒì„±
        result = self.generate_response(message)
        
        # ìºì‹œ ì €ì¥
        self.cache_system.set(cache_key, result)
        
        return result
```

#### ë³‘ë ¬ ê²€ìƒ‰ ì—”ì§„
```python
# ë³‘ë ¬ ê²€ìƒ‰ êµ¬í˜„ ì˜ˆì‹œ
import asyncio
from source.services.optimized_hybrid_search_engine import OptimizedHybridSearchEngine

class ParallelSearchService:
    def __init__(self):
        self.search_engine = OptimizedHybridSearchEngine()
    
    async def parallel_search(self, query: str):
        # ì •í™• ê²€ìƒ‰ê³¼ ì˜ë¯¸ ê²€ìƒ‰ì„ ë™ì‹œ ì‹¤í–‰
        exact_task = asyncio.create_task(self.search_engine.exact_search(query))
        semantic_task = asyncio.create_task(self.search_engine.semantic_search(query))
        
        # ê²°ê³¼ ë³‘í•©
        exact_results, semantic_results = await asyncio.gather(exact_task, semantic_task)
        
        return self.search_engine.merge_results(exact_results, semantic_results)
```

### Phase 6: ì˜ë¯¸ì  ê²€ìƒ‰ ì‹œìŠ¤í…œ ê°œë°œ

#### FAISS ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰
```python
# ì˜ë¯¸ì  ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„ ì˜ˆì‹œ
from source.services.semantic_search_engine import SemanticSearchEngine

class VectorSearchService:
    def __init__(self):
        self.semantic_engine = SemanticSearchEngine()
    
    def semantic_search(self, query: str, limit: int = 10):
        # ì¿¼ë¦¬ ë²¡í„°í™”
        query_vector = self.semantic_engine.encode_query(query)
        
        # FAISS ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰
        scores, indices = self.semantic_engine.search(query_vector, limit)
        
        # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ê²°ê³¼ ë°˜í™˜
        results = []
        for score, idx in zip(scores, indices):
            metadata = self.semantic_engine.get_metadata(idx)
            results.append({
                'text': metadata['text'],
                'score': float(score),
                'metadata': metadata
            })
        
        return results
```

#### ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›
```python
# ë‹¤ì¤‘ ëª¨ë¸ ê´€ë¦¬ì êµ¬í˜„ ì˜ˆì‹œ
from source.services.multi_model_manager import MultiModelManager

class MultiModelService:
    def __init__(self):
        self.model_manager = MultiModelManager()
    
    def search_with_multiple_models(self, query: str):
        results = {}
        
        # ko-sroberta-multitask ëª¨ë¸ë¡œ ê²€ìƒ‰
        kobart_results = self.model_manager.search_with_model(
            query, model_name="ko-sroberta-multitask"
        )
        results['kobart'] = kobart_results
        
        # BGE-M3-Korean ëª¨ë¸ë¡œ ê²€ìƒ‰
        bge_results = self.model_manager.search_with_model(
            query, model_name="BGE-M3-Korean"
        )
        results['bge'] = bge_results
        
        # ê²°ê³¼ í†µí•©
        return self.model_manager.merge_model_results(results)
```
```
LawFirmAI/
â”œâ”€â”€ gradio/                          # Gradio ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜
â”‚   â”œâ”€â”€ simple_langchain_app.py      # ë©”ì¸ LangChain ê¸°ë°˜ ì•±
â”‚   â”œâ”€â”€ app.py                       # ê¸°ë³¸ Gradio ì•±
â”‚   â”œâ”€â”€ stop_server.py               # ì„œë²„ ì¢…ë£Œ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ requirements.txt             # Gradio ì˜ì¡´ì„±
â”‚   â””â”€â”€ Dockerfile                   # Gradio Docker ì„¤ì •
â”œâ”€â”€ source/                          # í•µì‹¬ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ services/                    # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (80+ ì„œë¹„ìŠ¤)
â”‚   â”œâ”€â”€ data/                        # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ models/                      # AI ëª¨ë¸
â”‚   â””â”€â”€ utils/                       # ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ data/                            # ë°ì´í„° íŒŒì¼
â”‚   â”œâ”€â”€ lawfirm.db                   # SQLite ë°ì´í„°ë² ì´ìŠ¤
â”‚   â””â”€â”€ embeddings/                  # ë²¡í„° ì„ë² ë”©
â””â”€â”€ docs/                            # ë¬¸ì„œ
```

### ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ ê·œì¹™
```python
# í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë²¡í„° ì €ì¥ì†Œ
vector_store_paths = [
    "data/embeddings/ml_enhanced_ko_sroberta",  # ko-sroberta ë²¡í„°
    "data/embeddings/ml_enhanced_bge_m3",       # BGE-M3 ë²¡í„°
]
```

## ğŸ“ ë¡œê¹… ê·œì¹™

### Windows í™˜ê²½ ë¡œê¹… ì£¼ì˜ì‚¬í•­

**ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€** (Windows cp949 ì¸ì½”ë”© ë¬¸ì œ):
```python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ
logger.info("ğŸš€ Starting process...")
logger.info("âœ… Process completed")

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ  
logger.info("Starting process...")
logger.info("Process completed")
logger.info("[OK] Process completed")
logger.info("[ERROR] Process failed")
```

### í•œêµ­ì–´ ì¸ì½”ë”© ì²˜ë¦¬ ê·œì¹™

**âš ï¸ ì¤‘ìš”**: Windows í™˜ê²½ì—ì„œ í•œêµ­ì–´ ì½˜ì†” ì¶œë ¥ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ê·œì¹™

#### í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (í•„ìˆ˜)
```python
# ëª¨ë“  Python íŒŒì¼ ìƒë‹¨ì— ì¶”ê°€
import os
import sys

# ì¸ì½”ë”© ì„¤ì • (ìµœìš°ì„ )
os.environ['PYTHONIOENCODING'] = 'utf-8'
if sys.platform == 'win32':
    os.environ['PYTHONLEGACYWINDOWSSTDIO'] = 'utf-8'
```

#### ì•ˆì „í•œ ì½˜ì†” ì¶œë ¥
```python
def safe_print(message: str):
    """ì•ˆì „í•œ ì½˜ì†” ì¶œë ¥ (ì¸ì½”ë”© ì²˜ë¦¬)"""
    try:
        print(message)
    except UnicodeEncodeError:
        print(message.encode('utf-8', errors='replace').decode('utf-8'))

# ì‚¬ìš© ì˜ˆì‹œ
safe_print("ë²•ë¥  ë¬¸ì„œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
safe_print("ë²¡í„° ì €ì¥ì†Œ ë¡œë”© ì™„ë£Œ")
```

### í˜„ì¬ êµ¬í˜„ëœ ë¡œê¹… ì‹œìŠ¤í…œ
```python
# gradio/simple_langchain_app.pyì—ì„œ ì‚¬ìš© ì¤‘
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/simple_langchain_gradio.log')
    ]
)
logger = logging.getLogger(__name__)

# ì‚¬ìš© ì˜ˆì‹œ
logger.info("LawFirmAI service initialized")
logger.info("Vector store loaded successfully")
logger.warning("Configuration issue detected")
logger.error("Critical error occurred")
```

## ğŸ›¡ï¸ ë³´ì•ˆ ê·œì¹™

### í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
```python
import os
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ë¡œë“œ
env_file = Path(".env")
if env_file.exists():
    from dotenv import load_dotenv
    load_dotenv()

# API í‚¤ ê´€ë¦¬
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    logger.warning("OpenAI API key not found, using fallback")

# í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ í™˜ê²½ ë³€ìˆ˜
required_env_vars = [
    "OPENAI_API_KEY",      # OpenAI API í‚¤
    "GOOGLE_API_KEY",      # Google API í‚¤ (ì„ íƒì‚¬í•­)
    "DATABASE_URL",        # ë°ì´í„°ë² ì´ìŠ¤ URL
    "MODEL_PATH"           # ëª¨ë¸ ê²½ë¡œ
]
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê·œì¹™

### í˜„ì¬ êµ¬í˜„ëœ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
```python
# gradio/test_simple_query.pyì—ì„œ êµ¬í˜„
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

def test_vector_store_loading():
    """ë²¡í„° ì €ì¥ì†Œ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    from source.data.vector_store import LegalVectorStore
    
    vector_store = LegalVectorStore("test-model")
    assert vector_store is not None

def test_gradio_app_startup():
    """Gradio ì•± ì‹œì‘ í…ŒìŠ¤íŠ¸"""
    import subprocess
    import time
    
    # ì•± ì‹œì‘
    process = subprocess.Popen(['python', 'gradio/simple_langchain_app.py'])
    
    # ì ì‹œ ëŒ€ê¸°
    time.sleep(5)
    
    # í”„ë¡œì„¸ìŠ¤ ìƒíƒœ í™•ì¸
    assert process.poll() is None, "App should be running"
    
    # ì •ë¦¬
    process.terminate()
    process.wait()
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê·œì¹™

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
```python
import psutil
import time

def monitor_memory():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    process = psutil.Process()
    memory_info = process.memory_info()
    
    logger.info(f"Memory usage: {memory_info.rss / 1024 / 1024:.1f} MB")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ê²½ê³ 
    if memory_info.rss > 1024 * 1024 * 1024:  # 1GB
        logger.warning("High memory usage detected")
```

### ì‘ë‹µ ì‹œê°„ ì¸¡ì •
```python
import time
from functools import wraps

def measure_time(func):
    """ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        logger.info(f"{func.__name__} executed in {end_time - start_time:.3f}s")
        return result
    return wrapper

# ì‚¬ìš© ì˜ˆì‹œ
@measure_time
def search_documents(query):
    # ê²€ìƒ‰ ë¡œì§
    pass
```

## ğŸ”„ ë°°í¬ ê·œì¹™

### í˜„ì¬ êµ¬í˜„ëœ Docker ì„¤ì •
```dockerfile
# gradio/Dockerfile (í˜„ì¬ êµ¬í˜„)
FROM python:3.9-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY gradio/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë³µì‚¬
COPY gradio/ ./gradio/
COPY source/ ./source/

# ë¹„root ì‚¬ìš©ìë¡œ ì‹¤í–‰
RUN useradd --create-home --shell /bin/bash app
USER app

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 7860

# í—¬ìŠ¤ì²´í¬ ì¶”ê°€
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:7860/ || exit 1

CMD ["python", "gradio/simple_langchain_app.py"]
```

