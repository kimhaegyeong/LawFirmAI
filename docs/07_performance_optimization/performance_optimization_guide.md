# ğŸš€ LawFirmAI ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ì´ ê°€ì´ë“œëŠ” LawFirmAI ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ê³¼ ìµœì í™”ëœ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ¯ ìµœì í™” ëª©í‘œ

- **ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•**: 10ì´ˆ â†’ 2ì´ˆ ì´í•˜
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ëª¨ë¸ ì¬ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
- **ë™ì‹œ ì²˜ë¦¬**: ì—¬ëŸ¬ ìš”ì²­ ë³‘ë ¬ ì²˜ë¦¬
- **ìºì‹±**: ë™ì¼ ì§ˆë¬¸ì— ëŒ€í•œ ë¹ ë¥¸ ì‘ë‹µ

## ğŸ”§ ìµœì í™”ëœ ì»´í¬ë„ŒíŠ¸

### 1. OptimizedModelManager

#### ê°œìš”
AI ëª¨ë¸ì˜ ë¡œë”©ê³¼ ê´€ë¦¬ë¥¼ ìµœì í™”í•˜ëŠ” ì‹±ê¸€í†¤ íŒ¨í„´ ê¸°ë°˜ ëª¨ë¸ ê´€ë¦¬ìì…ë‹ˆë‹¤.

#### ì£¼ìš” ê¸°ëŠ¥
- **ì‹±ê¸€í†¤ íŒ¨í„´**: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©
- **ì§€ì—° ë¡œë”©**: í•„ìš” ì‹œì—ë§Œ ëª¨ë¸ ë¡œë”©
- **Float16 ì–‘ìí™”**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ê°ì†Œ
- **ë¯¸ë¦¬ ë¡œë”©**: ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”©

#### ì‚¬ìš©ë²•
```python
from source.services.optimized_model_manager import model_manager

# Sentence-BERT ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
model = model_manager.get_sentence_transformer(
    model_name="jhgan/ko-sroberta-multitask",
    enable_quantization=True,
    device="cpu"
)

# ì„±ëŠ¥ í†µê³„ í™•ì¸
stats = model_manager.get_stats()
print(f"ëª¨ë¸ ë¡œë”© íšŸìˆ˜: {stats['model_loads']}")
print(f"ìºì‹œ íˆíŠ¸ìœ¨: {stats['cache_hit_rate']:.1%}")
```

#### ì„¤ì • ì˜µì…˜
```python
# ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”©
model_configs = {
    'jhgan/ko-sroberta-multitask': {
        'device': 'cpu',
        'enable_quantization': True
    }
}
model_manager.preload_models(model_configs)
```

### 2. OptimizedHybridSearchEngine

#### ê°œìš”
ë³‘ë ¬ ì²˜ë¦¬ì™€ ìºì‹±ì„ ì§€ì›í•˜ëŠ” ìµœì í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.

#### ì£¼ìš” ê¸°ëŠ¥
- **ë³‘ë ¬ ê²€ìƒ‰**: ì •í™• ê²€ìƒ‰ê³¼ ì˜ë¯¸ ê²€ìƒ‰ì„ ë™ì‹œ ì‹¤í–‰
- **ê²°ê³¼ ìºì‹±**: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìºì‹œí•˜ì—¬ ì¬ì‚¬ìš©
- **íƒ€ì„ì•„ì›ƒ ì ìš©**: ê²€ìƒ‰ ì‘ì—…ì— íƒ€ì„ì•„ì›ƒ ì„¤ì •
- **ì ìˆ˜ ì„ê³„ê°’**: ìµœì†Œ ì ìˆ˜ ì´í•˜ ê²°ê³¼ í•„í„°ë§

#### ì‚¬ìš©ë²•
```python
from source.services.optimized_hybrid_search_engine import OptimizedHybridSearchEngine, OptimizedSearchConfig

# ê²€ìƒ‰ ì„¤ì •
config = OptimizedSearchConfig(
    max_results_per_type=5,
    parallel_search=True,
    cache_enabled=True,
    timeout_seconds=3.0,
    min_score_threshold=0.3
)

# ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
search_engine = OptimizedHybridSearchEngine(config)

# ê²€ìƒ‰ ì‹¤í–‰
results = await search_engine.search_with_question_type(
    query="ê³„ì•½ì„œ ê²€í†  ìš”ì²­",
    question_type=QuestionType.CONTRACT_REVIEW,
    max_results=20
)

# ì„±ëŠ¥ í†µê³„ í™•ì¸
stats = search_engine.get_stats()
print(f"ì´ ê²€ìƒ‰ íšŸìˆ˜: {stats['total_searches']}")
print(f"ìºì‹œ íˆíŠ¸ìœ¨: {stats['cache_hit_rate']:.1%}")
```

#### ì„¤ì • ì˜µì…˜
```python
# ê²€ìƒ‰ ì„¤ì • ì¡°ì •
config = OptimizedSearchConfig(
    max_results_per_type=10,      # ê° ê²€ìƒ‰ íƒ€ì…ë³„ ìµœëŒ€ ê²°ê³¼ ìˆ˜
    parallel_search=True,         # ë³‘ë ¬ ê²€ìƒ‰ ì‚¬ìš©
    cache_enabled=True,          # ìºì‹± ì‚¬ìš©
    timeout_seconds=5.0,         # ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ
    min_score_threshold=0.5      # ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’
)
```

### 3. IntegratedCacheSystem

#### ê°œìš”
ì§ˆë¬¸ ë¶„ë¥˜, ê²€ìƒ‰ ê²°ê³¼, ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

#### ì£¼ìš” ê¸°ëŠ¥
- **ë‹¤ì¸µ ìºì‹±**: ê° ë‹¨ê³„ë³„ ìºì‹±
- **LRU ìºì‹œ**: ìµœê·¼ ì‚¬ìš©ëœ í•­ëª© ìš°ì„  ë³´ê´€
- **ì˜êµ¬ ìºì‹œ**: ë””ìŠ¤í¬ ê¸°ë°˜ ì¥ê¸° ìºì‹±
- **TTL ê´€ë¦¬**: ìºì‹œ ìœ íš¨ ì‹œê°„ ìë™ ê´€ë¦¬

#### ì‚¬ìš©ë²•
```python
from source.services.integrated_cache_system import cache_system

# ì§ˆë¬¸ ë¶„ë¥˜ ê²°ê³¼ ìºì‹±
classification = cache_system.get_question_classification("ê³„ì•½ì„œ ê²€í†  ìš”ì²­")
if not classification:
    classification = classify_question("ê³„ì•½ì„œ ê²€í†  ìš”ì²­")
    cache_system.put_question_classification("ê³„ì•½ì„œ ê²€í†  ìš”ì²­", classification)

# ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
search_results = cache_system.get_search_results("ê³„ì•½ì„œ ê²€í† ", "contract", 20)
if not search_results:
    search_results = perform_search("ê³„ì•½ì„œ ê²€í† ", 20)
    cache_system.put_search_results("ê³„ì•½ì„œ ê²€í† ", "contract", 20, search_results)

# ë‹µë³€ ìºì‹±
answer = cache_system.get_answer("ê³„ì•½ì„œ ê²€í† ", "contract", "context_hash")
if not answer:
    answer = generate_answer("ê³„ì•½ì„œ ê²€í† ", context)
    cache_system.put_answer("ê³„ì•½ì„œ ê²€í† ", "contract", "context_hash", answer)

# ì„±ëŠ¥ í†µê³„ í™•ì¸
stats = cache_system.get_stats()
print(f"ì§ˆë¬¸ ë¶„ë¥˜ ìºì‹œ íˆíŠ¸ìœ¨: {stats['classification_cache']['hit_rate']:.1%}")
print(f"ê²€ìƒ‰ ìºì‹œ íˆíŠ¸ìœ¨: {stats['search_cache']['hit_rate']:.1%}")
```

#### ìºì‹œ ì„¤ì •
```python
# ìºì‹œ í¬ê¸° ì¡°ì •
cache_system.question_classification_cache.max_size = 1000
cache_system.search_results_cache.max_size = 2000
cache_system.answer_generation_cache.max_size = 500

# ìºì‹œ ì •ë¦¬
cache_system.clear_all()
```

### 4. OptimizedChatService

#### ê°œìš”
ëª¨ë“  ìµœì í™” ê¸°ìˆ ì„ í†µí•©í•œ ìµœì í™”ëœ ì±„íŒ… ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

#### ì£¼ìš” ê¸°ëŠ¥
- **í†µí•© ìµœì í™”**: ëª¨ë“  ìµœì í™” ê¸°ìˆ  í†µí•©
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘
- **ë™ì  ì„¤ì •**: ëŸ°íƒ€ì„ì— ì„±ëŠ¥ ì„¤ì • ì¡°ì •
- **ì—ëŸ¬ ì²˜ë¦¬**: ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬

#### ì‚¬ìš©ë²•
```python
from source.services.optimized_chat_service import OptimizedChatService
from source.utils.config import Config

# ì„¤ì • ì´ˆê¸°í™”
config = Config()

# ìµœì í™”ëœ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
service = OptimizedChatService(config)

# ì§ˆë¬¸ ì²˜ë¦¬
result = await service.process_message("ê³„ì•½ì„œ ê²€í†  ìš”ì²­")
print(f"ì‘ë‹µ: {result['response']}")
print(f"ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
print(f"ìºì‹œ ì‚¬ìš©: {result['cached']}")
```

#### ì„±ëŠ¥ ì„¤ì • ì¡°ì •
```python
# ì„±ëŠ¥ ì„¤ì • ì¡°ì •
performance_config = {
    'enable_caching': True,
    'enable_parallel_search': True,
    'enable_model_preloading': True,
    'max_concurrent_requests': 10,
    'response_timeout': 15.0
}

service.optimize_performance(performance_config)
```

#### ìƒíƒœ í™•ì¸
```python
# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
status = service.get_service_status()
print(f"ì „ì²´ ìƒíƒœ: {status['status']}")
print(f"ì»´í¬ë„ŒíŠ¸ ìƒíƒœ: {status['components']}")

# ì„±ëŠ¥ í†µê³„ í™•ì¸
stats = service.get_performance_stats()
print(f"ì´ ìš”ì²­ ìˆ˜: {stats['total_requests']}")
print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {stats['avg_response_time']:.2f}ì´ˆ")
print(f"ìºì‹œ íˆíŠ¸ìœ¨: {stats['cache_hit_rate']:.1%}")
print(f"ì—ëŸ¬ìœ¨: {stats['error_rate']:.1%}")
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

#### ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘
```python
# ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘
stats = service.get_performance_stats()

# ì£¼ìš” ì§€í‘œ
print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {stats['avg_response_time']:.2f}ì´ˆ")
print(f"ì´ ìš”ì²­ ìˆ˜: {stats['total_requests']}")
print(f"ìºì‹œ íˆíŠ¸ìœ¨: {stats['cache_hit_rate']:.1%}")
print(f"ì—ëŸ¬ìœ¨: {stats['error_rate']:.1%}")

# ìºì‹œ í†µê³„
cache_stats = stats['cache_stats']
print(f"ì§ˆë¬¸ ë¶„ë¥˜ ìºì‹œ: {cache_stats['classification_cache']['hit_rate']:.1%}")
print(f"ê²€ìƒ‰ ìºì‹œ: {cache_stats['search_cache']['hit_rate']:.1%}")
print(f"ë‹µë³€ ìºì‹œ: {cache_stats['answer_cache']['hit_rate']:.1%}")

# ëª¨ë¸ í†µê³„
model_stats = stats['model_stats']
print(f"ëª¨ë¸ ë¡œë”© íšŸìˆ˜: {model_stats['model_loads']}")
print(f"í‰ê·  ë¡œë”© ì‹œê°„: {model_stats['avg_load_time']:.2f}ì´ˆ")

# ê²€ìƒ‰ í†µê³„
search_stats = stats['search_stats']
print(f"ì´ ê²€ìƒ‰ íšŸìˆ˜: {search_stats['total_searches']}")
print(f"ë³‘ë ¬ ê²€ìƒ‰ ë¹„ìœ¨: {search_stats['parallel_search_rate']:.1%}")
```

#### ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
```python
# ì£¼ê¸°ì  ëª¨ë‹ˆí„°ë§
import time
import asyncio

async def monitor_performance():
    while True:
        stats = service.get_performance_stats()
        
        print(f"[{time.strftime('%H:%M:%S')}] "
              f"ì‘ë‹µì‹œê°„: {stats['avg_response_time']:.2f}ì´ˆ, "
              f"ìºì‹œíˆíŠ¸ìœ¨: {stats['cache_hit_rate']:.1%}, "
              f"ì—ëŸ¬ìœ¨: {stats['error_rate']:.1%}")
        
        await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§

# ëª¨ë‹ˆí„°ë§ ì‹œì‘
asyncio.create_task(monitor_performance())
```

### ì„±ëŠ¥ ë¶„ì„

#### ì‘ë‹µ ì‹œê°„ ë¶„ì„
```python
# ì‘ë‹µ ì‹œê°„ ë¶„í¬ ë¶„ì„
response_times = []
for i in range(100):
    start_time = time.time()
    result = await service.process_message(f"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ {i}")
    response_time = time.time() - start_time
    response_times.append(response_time)

# í†µê³„ ê³„ì‚°
import statistics
print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {statistics.mean(response_times):.2f}ì´ˆ")
print(f"ì¤‘ê°„ê°’: {statistics.median(response_times):.2f}ì´ˆ")
print(f"í‘œì¤€í¸ì°¨: {statistics.stdev(response_times):.2f}ì´ˆ")
print(f"ìµœëŒ€ê°’: {max(response_times):.2f}ì´ˆ")
print(f"ìµœì†Œê°’: {min(response_times):.2f}ì´ˆ")
```

#### ìºì‹œ íš¨ê³¼ ë¶„ì„
```python
# ìºì‹œ íš¨ê³¼ ì¸¡ì •
test_query = "ê³„ì•½ì„œ ê²€í†  ìš”ì²­"

# ì²« ë²ˆì§¸ ì§ˆë¬¸ (ìºì‹œ ì—†ìŒ)
start_time = time.time()
result1 = await service.process_message(test_query)
first_time = time.time() - start_time

# ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ìºì‹œ ìˆìŒ)
start_time = time.time()
result2 = await service.process_message(test_query)
second_time = time.time() - start_time

print(f"ì²« ë²ˆì§¸ ì§ˆë¬¸: {first_time:.2f}ì´ˆ")
print(f"ë‘ ë²ˆì§¸ ì§ˆë¬¸: {second_time:.2f}ì´ˆ")
print(f"ìºì‹œ íš¨ê³¼: {first_time / second_time:.1f}ë°° ë¹ ë¦„")
```

## ğŸ”§ ì„±ëŠ¥ íŠœë‹

### ë©”ëª¨ë¦¬ ìµœì í™”

#### ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
# ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
import psutil
import os

def monitor_memory():
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_info.rss / 1024 / 1024:.1f}MB")

# ëª¨ë¸ ë¡œë”© ì „í›„ ë©”ëª¨ë¦¬ ë¹„êµ
monitor_memory()  # ë¡œë”© ì „
model = model_manager.get_sentence_transformer("jhgan/ko-sroberta-multitask")
monitor_memory()  # ë¡œë”© í›„
```

#### ìºì‹œ ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
# ìºì‹œ í¬ê¸° ì¡°ì •
cache_system.question_classification_cache.max_size = 500
cache_system.search_results_cache.max_size = 1000
cache_system.answer_generation_cache.max_size = 200

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¥¸ ìºì‹œ ì •ë¦¬
def cleanup_cache_if_needed():
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    
    if memory_info.rss > 1024 * 1024 * 1024:  # 1GB ì´ìƒ
        cache_system.clear_all()
        print("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì•„ ìºì‹œë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
```

### ê²€ìƒ‰ ì„±ëŠ¥ íŠœë‹

#### ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì¡°ì •
```python
# ê²€ìƒ‰ ì„±ëŠ¥ íŠœë‹
config = OptimizedSearchConfig(
    max_results_per_type=3,       # ê²°ê³¼ ìˆ˜ ê°ì†Œë¡œ ì†ë„ í–¥ìƒ
    parallel_search=True,        # ë³‘ë ¬ ê²€ìƒ‰ í™œì„±í™”
    cache_enabled=True,         # ìºì‹± í™œì„±í™”
    timeout_seconds=2.0,        # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
    min_score_threshold=0.4     # ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
)
```

#### ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í–¥ìƒ
```python
# ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í–¥ìƒ
def improve_search_quality(query, results):
    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    results.sort(key=lambda x: x.score, reverse=True)
    
    # ì¤‘ë³µ ì œê±°
    seen = set()
    unique_results = []
    for result in results:
        if result.content not in seen:
            seen.add(result.content)
            unique_results.append(result)
    
    return unique_results[:10]  # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
```

### ìºì‹œ ìµœì í™”

#### ìºì‹œ ì „ëµ ì¡°ì •
```python
# ìºì‹œ TTL ì¡°ì •
cache_system.put_question_classification(
    query, classification, ttl=3600  # 1ì‹œê°„
)
cache_system.put_search_results(
    query, question_type, max_results, results, ttl=1800  # 30ë¶„
)
cache_system.put_answer(
    query, question_type, context_hash, answer, ttl=7200  # 2ì‹œê°„
)
```

#### ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ
```python
# ì§ˆë¬¸ ì •ê·œí™”ë¡œ ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ
def normalize_query(query):
    # ì†Œë¬¸ì ë³€í™˜
    normalized = query.lower().strip()
    
    # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    normalized = re.sub(r'[^\w\s]', '', normalized)
    
    # ê³µë°± ì •ê·œí™”
    normalized = re.sub(r'\s+', ' ', normalized)
    
    return normalized

# ì •ê·œí™”ëœ ì§ˆë¬¸ìœ¼ë¡œ ìºì‹œ í™•ì¸
normalized_query = normalize_query(query)
cached_result = cache_system.get_question_classification(normalized_query)
```

## ğŸ§ª ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```python
import asyncio
import time
from source.services.optimized_chat_service import OptimizedChatService
from source.utils.config import Config

async def performance_test():
    config = Config()
    service = OptimizedChatService(config)
    
    test_queries = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ê³„ì•½ì„œ ê²€í†  ìš”ì²­",
        "ë¯¼ë²• ì œ750ì¡°ì˜ ë‚´ìš©ì´ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì†í•´ë°°ìƒ ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
        "ì´í˜¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ì§„í–‰í•˜ë‚˜ìš”?"
    ]
    
    results = []
    
    for i, query in enumerate(test_queries):
        print(f"[{i+1}/{len(test_queries)}] Processing: {query[:50]}...")
        start_time = time.time()
        result = await service.process_message(query)
        processing_time = time.time() - start_time
        
        results.append({
            "query": query,
            "processing_time": processing_time,
            "success": bool(result.get("response")),
            "cached": result.get("cached", False)
        })
        
        print(f"ì™„ë£Œ: {processing_time:.2f}ì´ˆ (ìºì‹œ: {result.get('cached', False)})")
    
    # ê²°ê³¼ ë¶„ì„
    avg_time = sum(r["processing_time"] for r in results) / len(results)
    success_rate = sum(1 for r in results if r["success"]) / len(results)
    cache_hit_rate = sum(1 for r in results if r["cached"]) / len(results)
    
    print(f"\n=== ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ")
    print(f"ì„±ê³µë¥ : {success_rate:.1%}")
    print(f"ìºì‹œ íˆíŠ¸ìœ¨: {cache_hit_rate:.1%}")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
asyncio.run(performance_test())
```

### ë¶€í•˜ í…ŒìŠ¤íŠ¸

#### ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸
```python
async def load_test():
    config = Config()
    service = OptimizedChatService(config)
    
    test_query = "ê³„ì•½ì„œ ê²€í†  ìš”ì²­"
    concurrent_requests = 10
    
    # ë™ì‹œ ìš”ì²­ ìƒì„±
    tasks = []
    for i in range(concurrent_requests):
        task = service.process_message(f"{test_query} {i}")
        tasks.append(task)
    
    # ë™ì‹œ ì‹¤í–‰
    start_time = time.time()
    results = await asyncio.gather(*tasks, return_exceptions=True)
    total_time = time.time() - start_time
    
    # ê²°ê³¼ ë¶„ì„
    successful_results = [r for r in results if isinstance(r, dict)]
    failed_count = len(results) - len(successful_results)
    
    print(f"=== ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"ë™ì‹œ ìš”ì²­ ìˆ˜: {concurrent_requests}")
    print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"ì„±ê³µí•œ ìš”ì²­: {len(successful_results)}")
    print(f"ì‹¤íŒ¨í•œ ìš”ì²­: {failed_count}")
    print(f"ì²˜ë¦¬ëŸ‰: {len(successful_results) / total_time:.1f} ìš”ì²­/ì´ˆ")

# ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
asyncio.run(load_test())
```

### ìºì‹œ íš¨ê³¼ í…ŒìŠ¤íŠ¸

#### ìºì‹œ ì„±ëŠ¥ ì¸¡ì •
```python
async def cache_performance_test():
    config = Config()
    service = OptimizedChatService(config)
    
    test_query = "ê³„ì•½ì„œ ê²€í†  ìš”ì²­"
    iterations = 5
    
    first_run_time = None
    cached_times = []
    
    for i in range(iterations):
        start_time = time.time()
        result = await service.process_message(test_query)
        processing_time = time.time() - start_time
        
        if i == 0:
            first_run_time = processing_time
        else:
            cached_times.append(processing_time)
        
        print(f"Iteration {i+1}: {processing_time:.2f}ì´ˆ (cached: {result.get('cached', False)})")
    
    # ìºì‹œ íš¨ê³¼ ê³„ì‚°
    avg_cached_time = sum(cached_times) / len(cached_times)
    speedup_factor = first_run_time / avg_cached_time
    
    print(f"\n=== ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"ì²« ë²ˆì§¸ ì‹¤í–‰: {first_run_time:.2f}ì´ˆ")
    print(f"í‰ê·  ìºì‹œ ì‹œê°„: {avg_cached_time:.2f}ì´ˆ")
    print(f"ì†ë„ í–¥ìƒ: {speedup_factor:.1f}ë°°")

# ìºì‹œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
asyncio.run(cache_performance_test())
```

## ğŸš¨ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ í•´ê²° ë°©ë²•
def handle_memory_shortage():
    # ìºì‹œ ì •ë¦¬
    cache_system.clear_all()
    
    # ëª¨ë¸ ìºì‹œ ì •ë¦¬
    model_manager.clear_cache()
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    import gc
    gc.collect()
    
    print("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
```

#### ì‘ë‹µ ì‹œê°„ ì§€ì—°
```python
# ì‘ë‹µ ì‹œê°„ ì§€ì—° ì‹œ í•´ê²° ë°©ë²•
def handle_slow_response():
    # ìºì‹œ ì„¤ì • í™•ì¸
    if not cache_system.question_classification_cache:
        print("ìºì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    # ë³‘ë ¬ ê²€ìƒ‰ í™•ì¸
    if not search_engine.config.parallel_search:
        print("ë³‘ë ¬ ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    # ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸
    stats = model_manager.get_stats()
    if stats['model_loads'] > 1:
        print("ëª¨ë¸ì´ ì—¬ëŸ¬ ë²ˆ ë¡œë”©ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
```

#### ìºì‹œ íˆíŠ¸ìœ¨ ë‚®ìŒ
```python
# ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ ë°©ë²•
def improve_cache_hit_rate():
    # ì§ˆë¬¸ ì •ê·œí™” ê°•í™”
    def normalize_query_advanced(query):
        # ë” ì •êµí•œ ì •ê·œí™”
        normalized = query.lower().strip()
        normalized = re.sub(r'[^\w\s]', '', normalized)
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # ë²•ë¥  ìš©ì–´ í‘œì¤€í™”
        legal_terms = {
            'ê³„ì•½ì„œ': 'ê³„ì•½',
            'ê³„ì•½': 'ê³„ì•½ì„œ',
            'ê³„ì•½ í•´ì§€': 'ê³„ì•½í•´ì§€',
            'ê³„ì•½í•´ì§€': 'ê³„ì•½ í•´ì§€'
        }
        
        for term, standard in legal_terms.items():
            normalized = normalized.replace(term, standard)
        
        return normalized
    
    # ìºì‹œ TTL ì¡°ì •
    cache_system.put_question_classification(
        query, classification, ttl=7200  # 2ì‹œê°„ìœ¼ë¡œ ì—°ì¥
    )
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```python
# ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
async def real_time_monitoring():
    while True:
        stats = service.get_performance_stats()
        
        # ì„ê³„ê°’ í™•ì¸
        if stats['avg_response_time'] > 5.0:
            print("âš ï¸ ì‘ë‹µ ì‹œê°„ì´ 5ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
        
        if stats['cache_hit_rate'] < 0.3:
            print("âš ï¸ ìºì‹œ íˆíŠ¸ìœ¨ì´ 30% ë¯¸ë§Œì…ë‹ˆë‹¤.")
        
        if stats['error_rate'] > 0.1:
            print("âš ï¸ ì—ëŸ¬ìœ¨ì´ 10%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
        
        await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ í™•ì¸
```

## ğŸ“š ì¶”ê°€ ìë£Œ

### ê´€ë ¨ ë¬¸ì„œ
- [ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ ë³´ê³ ì„œ](performance_optimization_report.md)
- [API ë¬¸ì„œ](../08_api_documentation/)
- [ê°œë°œ ê°€ì´ë“œ](../01_project_overview/development_rules.md)

### ì™¸ë¶€ ìë£Œ
- [Python asyncio ê³µì‹ ë¬¸ì„œ](https://docs.python.org/3/library/asyncio.html)
- [FAISS ê³µì‹ ë¬¸ì„œ](https://faiss.ai/)
- [Sentence-BERT GitHub](https://github.com/UKPLab/sentence-transformers)

---

**ì‘ì„±ì¼**: 2025ë…„ 1ì›” 10ì¼  
**ì‘ì„±ì**: LawFirmAI ê°œë°œíŒ€  
**ë¬¸ì„œ ë²„ì „**: 1.0
