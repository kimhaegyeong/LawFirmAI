# LawFirmAI 기술 스택 벤치마킹 분석 결과

## 📊 벤치마킹 개요

**실행 일시**: 2025-09-24 12:27  
**테스트 환경**: Windows 10, Python 3.13, CPU (16 cores, 31.4GB RAM)  
**테스트 데이터**: 법률 도메인 질문 5개, 판례 데이터 1,000건

---

## 🤖 AI 모델 성능 비교

### KoBART vs KoGPT-2

| 지표 | KoBART | KoGPT-2 | 승자 |
|------|--------|---------|------|
| **모델 크기** | 472.5 MB | 477.5 MB | KoBART (1% 작음) |
| **메모리 사용량** | 400.8 MB | 748.3 MB | **KoBART (47% 적음)** |
| **로딩 시간** | 23.2초 | 14.6초 | **KoGPT-2 (37% 빠름)** |
| **평균 추론 시간** | 13.18초 | 8.34초 | **KoGPT-2 (37% 빠름)** |
| **응답 품질** | 낮음 (반복, 무의미) | 보통 (일관성 있음) | **KoGPT-2** |

### 상세 분석

#### KoBART의 문제점
- **응답 품질 저하**: 반복적인 텍스트 생성, 무의미한 내용
- **긴 추론 시간**: 평균 13.18초로 사용자 경험 저하
- **메모리 효율성**: 상대적으로 좋지만 품질 문제로 인한 실용성 저하

#### KoGPT-2의 장점
- **빠른 추론**: 평균 8.34초로 37% 빠름
- **일관된 응답**: 논리적이고 일관된 답변 생성
- **빠른 로딩**: 37% 빠른 모델 로딩 시간

---

## 🗄️ 벡터 스토어 성능 비교

### FAISS vs ChromaDB

| 지표 | FAISS | ChromaDB | 승자 |
|------|-------|----------|------|
| **구축 시간** | 오류 발생 | 40.5초 | ChromaDB |
| **검색 속도** | 측정 불가 | 0.17초 | ChromaDB |
| **QPS** | 측정 불가 | 5.82 | ChromaDB |
| **메모리 사용량** | 측정 불가 | 920 MB | - |
| **안정성** | 오류 발생 | 정상 동작 | **ChromaDB** |

### 상세 분석

#### FAISS 문제점
- **설치/설정 오류**: Windows 환경에서 정상 동작하지 않음
- **의존성 문제**: 복잡한 설치 과정과 환경 설정 필요
- **디버깅 어려움**: 오류 원인 파악 및 해결 어려움

#### ChromaDB 장점
- **안정적 동작**: Windows 환경에서 문제없이 동작
- **자동 임베딩**: Sentence-BERT 모델 자동 사용
- **간편한 설정**: 최소한의 설정으로 사용 가능
- **적절한 성능**: 5.82 QPS로 실용적 수준

---

## 🎯 최종 기술 스택 결정

### 1. AI 모델: **KoGPT-2** 선택

**선택 이유:**
- ✅ **빠른 추론 속도**: 37% 빠른 응답 생성
- ✅ **일관된 품질**: 논리적이고 일관된 답변
- ✅ **빠른 로딩**: 37% 빠른 모델 로딩
- ✅ **HuggingFace Spaces 적합**: 메모리 사용량이 허용 범위 내

**단점:**
- ❌ 메모리 사용량이 KoBART보다 87% 많음 (748MB vs 400MB)
- ❌ 모델 크기가 약간 큼 (477MB vs 472MB)

### 2. 벡터 스토어: **ChromaDB** 선택

**선택 이유:**
- ✅ **안정적 동작**: Windows 환경에서 문제없이 동작
- ✅ **간편한 설정**: 최소한의 설정으로 사용 가능
- ✅ **자동 임베딩**: Sentence-BERT 모델 자동 사용
- ✅ **적절한 성능**: 5.82 QPS로 실용적 수준
- ✅ **HuggingFace Spaces 호환**: 클라우드 환경에서 안정적 동작

**단점:**
- ❌ FAISS 대비 성능 차이 (FAISS가 더 빠를 것으로 예상)
- ❌ 메모리 사용량이 상대적으로 높음 (920MB)

---

## 📈 성능 최적화 전략

### 1. 모델 최적화
- **양자화 적용**: INT8 양자화로 메모리 사용량 50% 감소 예상
- **ONNX 변환**: 추론 속도 20-30% 향상 예상
- **지연 로딩**: 필요 시에만 모델 로딩으로 초기 시작 시간 단축

### 2. 벡터 스토어 최적화
- **배치 처리**: 대량 문서 처리 시 배치 크기 최적화
- **인덱스 압축**: HNSW 인덱스 압축으로 메모리 사용량 감소
- **캐싱 전략**: 자주 검색되는 쿼리 결과 캐싱

### 3. 시스템 최적화
- **비동기 처리**: FastAPI + asyncio로 동시 요청 처리
- **메모리 관리**: 불필요한 객체 즉시 해제
- **로깅 최적화**: 프로덕션 환경에서 로그 레벨 조정

---

## ⚠️ 위험 요소 및 대응 방안

### 1. 메모리 사용량 위험
- **위험**: KoGPT-2 + ChromaDB = 약 1.7GB 메모리 사용
- **대응**: 모델 양자화, 지연 로딩, 메모리 모니터링

### 2. 성능 저하 위험
- **위험**: HuggingFace Spaces CPU 환경에서 느린 응답
- **대응**: ONNX 변환, 캐싱 전략, 비동기 처리

### 3. 안정성 위험
- **위험**: ChromaDB의 장기적 안정성 미검증
- **대응**: 백업 벡터 스토어 준비, 모니터링 강화

---

## 🚀 구현 우선순위

### Phase 1: 기본 구현 (Week 1-2)
1. KoGPT-2 모델 통합
2. ChromaDB 벡터 스토어 설정
3. 기본 RAG 파이프라인 구축

### Phase 2: 최적화 (Week 3-4)
1. 모델 양자화 적용
2. 성능 모니터링 구현
3. 캐싱 시스템 구축

### Phase 3: 고도화 (Week 5-6)
1. ONNX 변환 적용
2. 고급 검색 기능 구현
3. 사용자 피드백 시스템 구축

---

## 📋 다음 단계

1. **데이터베이스 스키마 설계** (TASK 1.3)
2. **개발 환경 구성** (TASK 1.4)
3. **모델 통합 구현** (TASK 2.1)
4. **벡터 스토어 구현** (TASK 2.2)

---

*이 분석 결과는 LawFirmAI 프로젝트의 기술 스택 선택을 위한 근거 자료로 활용됩니다.*
