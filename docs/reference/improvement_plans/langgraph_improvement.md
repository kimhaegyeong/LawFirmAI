# LangGraph ì›Œí¬í”Œë¡œìš° ê°œì„  ê³„íšì„œ

## ğŸ“‹ ê°œìš”

í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ë³´ê³ ì„œì—ì„œ ë°œê²¬ëœ ë¬¸ì œì ì„ í•´ê²°í•˜ê³ , ë°ì´í„°ë² ì´ìŠ¤/ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ ìƒì„± ë³´ì¥ì„ ìœ„í•œ ê°œì„  ê³„íšì…ë‹ˆë‹¤.

---

## ğŸ” ë°œê²¬ëœ ë¬¸ì œì  ìš”ì•½

### 1. ì¤‘ê°„ ìƒì„± í…ìŠ¤íŠ¸ í¬í•¨ ë¬¸ì œ
- **ì¦ìƒ**: "STEP 0: ì›ë³¸ í’ˆì§ˆ í‰ê°€", "ì§ˆë¬¸ ì •ë³´", "ì›ë³¸ ë‹µë³€" ë“± ì¤‘ê°„ ìƒì„± í…ìŠ¤íŠ¸ê°€ ìµœì¢… ë‹µë³€ì— í¬í•¨ë¨
- **ì˜í–¥**: ë‹µë³€ ê°€ë…ì„± ì €í•˜, ì „ë¬¸ì„± ì €í•˜
- **ë°œìƒ ìœ„ì¹˜**: `lawfirm_langgraph/source/services/answer_formatter.py` (ë˜ëŠ” í•´ë‹¹ ì„œë¹„ìŠ¤)

### 2. ê²€ìƒ‰ë˜ì§€ ì•Šì€ ë‚´ìš©ì´ ë‹µë³€ì— í¬í•¨ë˜ëŠ” ë¬¸ì œ (Hallucination)
- **ì¦ìƒ**: ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ì •ë³´ê°€ ë‹µë³€ì— í¬í•¨ë  ê°€ëŠ¥ì„±
- **ì˜í–¥**: ì˜ëª»ëœ ë²•ë¥  ì •ë³´ ì œê³µ, ì‹ ë¢°ë„ ì €í•˜
- **ë°œìƒ ìœ„ì¹˜**: `core/services/generation/answer_generator.py`, ë‹µë³€ ìƒì„± ê³¼ì •

### 3. ë‹µë³€ ê¸¸ì´ í¸ì°¨
- **ì¦ìƒ**: 800ì ~ 3,781ìê¹Œì§€ í° í¸ì°¨
- **ì˜í–¥**: ì‚¬ìš©ì ê²½í—˜ ì¼ê´€ì„± ë¶€ì¡±

### 4. ì‹ ë¢°ë„ ì ìˆ˜ í¸ì°¨
- **ì¦ìƒ**: 83.15% ~ 95.00%ê¹Œì§€ í¸ì°¨
- **ì˜í–¥**: ì¼ê´€ì„± ë¶€ì¡±

---

## ğŸ¯ ê°œì„  ëª©í‘œ

1. **ì¤‘ê°„ ìƒì„± í…ìŠ¤íŠ¸ ì™„ì „ ì œê±°**: ìµœì¢… ë‹µë³€ì—ì„œ 100% ì œê±°
2. **ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ê²€ì¦ ê°•í™”**: ê²€ìƒ‰ë˜ì§€ ì•Šì€ ë‚´ìš©ì´ ë‹µë³€ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ ë³´ì¥
3. **ë‹µë³€ ê¸¸ì´ ì¼ê´€ì„± í–¥ìƒ**: ì§ˆì˜ ìœ í˜•ë³„ ì ì ˆí•œ ê¸¸ì´ ìœ ì§€
4. **ì‹ ë¢°ë„ ê³„ì‚° ì¼ê´€ì„± í–¥ìƒ**: ì§ˆì˜ ìœ í˜•ë³„ ì¼ê´€ëœ ê¸°ì¤€ ì ìš©

---

## ğŸ“ ìƒì„¸ ê°œì„  ë°©ì•ˆ

## 1. ì¤‘ê°„ ìƒì„± í…ìŠ¤íŠ¸ í•„í„°ë§ ê°•í™”

### 1.1 êµ¬í˜„ ìœ„ì¹˜
- **íŒŒì¼**: `lawfirm_langgraph/source/services/answer_formatter.py` (ë˜ëŠ” í•´ë‹¹ ì„œë¹„ìŠ¤)
- **í•¨ìˆ˜**: `_remove_intermediate_text()` ì¶”ê°€, `_validate_final_answer()` ìˆ˜ì •

> âš ï¸ **ì°¸ê³ **: ì‹¤ì œ íŒŒì¼ ê²½ë¡œëŠ” í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `lawfirm_langgraph/source/services/` ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.

### 1.2 ì œê±°í•  íŒ¨í„´
```python
# ì œê±°í•  íŒ¨í„´ ëª©ë¡
INTERMEDIATE_TEXT_PATTERNS = [
    r'^##\s*STEP\s*0.*?\n(?:.*\n)*?',  # STEP 0 ì„¹ì…˜ ì „ì²´
    r'^##\s*ì›ë³¸\s*í’ˆì§ˆ\s*í‰ê°€.*?\n(?:.*\n)*?',
    r'^##\s*ì§ˆë¬¸\s*ì •ë³´.*?\n(?:.*\n)*?',
    r'^##\s*ì›ë³¸\s*ë‹µë³€.*?\n(?:.*\n)*?',
    r'^\*\*ì§ˆë¬¸\*\*:.*?\n',
    r'^\*\*ì§ˆë¬¸\s*ìœ í˜•\*\*:.*?\n',
    r'^í‰ê°€\s*ê²°\s*ê³¼\s*ì—\s*ë”°ë¥¸\s*ì‘ì—…:.*?\n',
    r'^\s*â€¢\s*\[.*?\].*?ê°œì„ .*?\n',  # ì²´í¬ë¦¬ìŠ¤íŠ¸ íŒ¨í„´
    r'^\[.*?\].*?ì¶©ë¶„í•˜ê³ .*?\n',
    r'^ì›ë³¸\s*ì—\s*ê°œì„ ì´\s*í•„ìš”í•˜ë©´.*?\n',
]
```

### 1.3 êµ¬í˜„ ì½”ë“œ
```python
def _remove_intermediate_text(self, answer_text: str) -> str:
    """
    ì¤‘ê°„ ìƒì„± í…ìŠ¤íŠ¸ ì œê±°
    
    Args:
        answer_text: ì›ë³¸ ë‹µë³€ í…ìŠ¤íŠ¸
        
    Returns:
        ì¤‘ê°„ í…ìŠ¤íŠ¸ê°€ ì œê±°ëœ ë‹µë³€
    """
    import re
    
    if not answer_text or not isinstance(answer_text, str):
        return answer_text
    
    lines = answer_text.split('\n')
    cleaned_lines = []
    skip_section = False
    skip_patterns = [
        r'^##\s*STEP\s*0',
        r'^##\s*ì›ë³¸\s*í’ˆì§ˆ\s*í‰ê°€',
        r'^##\s*ì§ˆë¬¸\s*ì •ë³´',
        r'^##\s*ì›ë³¸\s*ë‹µë³€',
        r'^\*\*ì§ˆë¬¸\*\*:',
        r'^\*\*ì§ˆë¬¸\s*ìœ í˜•\*\*:',
        r'^í‰ê°€\s*ê²°ê³¼',
        r'ì›ë³¸\s*ì—\s*ê°œì„ ì´\s*í•„ìš”í•˜ë©´',
    ]
    
    for i, line in enumerate(lines):
        # ì„¹ì…˜ ì‹œì‘ íŒ¨í„´ í™•ì¸
        is_section_start = False
        for pattern in skip_patterns:
            if re.match(pattern, line, re.IGNORECASE):
                skip_section = True
                is_section_start = True
                break
        
        if is_section_start:
            continue
        
        # ì„¹ì…˜ ì¢…ë£Œ í™•ì¸ (ë‹¤ìŒ ## í—¤ë” ë˜ëŠ” ì‹¤ì œ ë‹µë³€ ì‹œì‘)
        if skip_section:
            # ë‹¤ìŒ ## í—¤ë”ê°€ ë‚˜ì˜¤ê±°ë‚˜, ì‹¤ì œ ë‹µë³€ ì‹œì‘ íŒ¨í„´ í™•ì¸
            if re.match(r'^##\s+[ê°€-í£]', line):  # ì‹¤ì œ ë‹µë³€ ì„¹ì…˜ ì‹œì‘
                skip_section = False
                # ì´ ì¤„ì€ í¬í•¨
                if not any(re.match(p, line, re.IGNORECASE) for p in skip_patterns):
                    cleaned_lines.append(line)
            # ì²´í¬ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
            elif re.match(r'^\s*â€¢\s*\[.*?\].*?', line):
                continue
            else:
                continue
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ê°€ (ë‹¤ì‹œ ì²´í¬ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ í•„í„°ë§)
            if re.match(r'^\s*â€¢\s*\[.*?\].*?', line):
                continue
            cleaned_lines.append(line)
    
    cleaned_text = '\n'.join(cleaned_lines)
    
    # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
    
    return cleaned_text.strip()
```

### 1.4 ì ìš© ìœ„ì¹˜
- `format_and_prepare_final()` ë©”ì„œë“œì—ì„œ `_remove_metadata_sections()` í˜¸ì¶œ ì´í›„ ì¶”ê°€ ì ìš©

---

## 2. ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ê²€ì¦ ê°•í™” (Hallucination ë°©ì§€)

### 2.1 êµ¬í˜„ ìœ„ì¹˜
- **íŒŒì¼**: `lawfirm_langgraph/source/services/quality_validators.py` (ë˜ëŠ” í•´ë‹¹ ì„œë¹„ìŠ¤), `lawfirm_langgraph/source/services/answer_formatter.py` (ë˜ëŠ” í•´ë‹¹ ì„œë¹„ìŠ¤)
- **í•¨ìˆ˜**: `validate_answer_source_verification()`, `_validate_final_answer()` ì¶”ê°€/ìˆ˜ì •

### 2.2 ê²€ì¦ ë¡œì§
```python
def validate_answer_source_verification(
    answer: str,
    retrieved_docs: List[Dict[str, Any]],
    query: str
) -> Dict[str, Any]:
    """
    ë‹µë³€ì˜ ë‚´ìš©ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ê¸°ë°˜í•˜ëŠ”ì§€ ê²€ì¦
    
    Args:
        answer: ê²€ì¦í•  ë‹µë³€ í…ìŠ¤íŠ¸
        retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡
        query: ì›ë³¸ ì§ˆì˜
        
    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        {
            "is_grounded": bool,
            "grounding_score": float,
            "unverified_sections": List[str],
            "source_coverage": float,
            "needs_review": bool
        }
    """
    import re
    from difflib import SequenceMatcher
    
    if not answer or not retrieved_docs:
        return {
            "is_grounded": False,
            "grounding_score": 0.0,
            "unverified_sections": [answer] if answer else [],
            "source_coverage": 0.0,
            "needs_review": True,
            "error": "ë‹µë³€ ë˜ëŠ” ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        }
    
    # 1. ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
    source_texts = []
    for doc in retrieved_docs:
        if isinstance(doc, dict):
            content = (
                doc.get("content") or
                doc.get("text") or
                doc.get("content_text") or
                ""
            )
            if content and len(content.strip()) > 50:
                source_texts.append(content.lower())
    
    if not source_texts:
        return {
            "is_grounded": False,
            "grounding_score": 0.0,
            "unverified_sections": [],
            "source_coverage": 0.0,
            "needs_review": True,
            "error": "ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        }
    
    # 2. ë‹µë³€ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    answer_sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', answer)
    answer_sentences = [s.strip() for s in answer_sentences if len(s.strip()) > 20]
    
    # 3. ê° ë¬¸ì¥ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ê¸°ë°˜í•˜ëŠ”ì§€ ê²€ì¦
    verified_sentences = []
    unverified_sentences = []
    
    for sentence in answer_sentences:
        sentence_lower = sentence.lower()
        
        # ë¬¸ì¥ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶ˆìš©ì–´ ì œê±°)
        stopwords = {'ëŠ”', 'ì€', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€'}
        sentence_words = [w for w in re.findall(r'[ê°€-í£]+', sentence_lower) if len(w) > 1 and w not in stopwords]
        
        if not sentence_words:
            continue
        
        # ê° ì†ŒìŠ¤ í…ìŠ¤íŠ¸ì™€ ìœ ì‚¬ë„ ê³„ì‚°
        max_similarity = 0.0
        best_match_source = None
        
        for source_text in source_texts:
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            matched_keywords = sum(1 for word in sentence_words if word in source_text)
            keyword_score = matched_keywords / len(sentence_words) if sentence_words else 0.0
            
            # ë¬¸ì¥ ìœ ì‚¬ë„ (SequenceMatcher ì‚¬ìš©)
            similarity = SequenceMatcher(None, sentence_lower[:100], source_text[:1000]).ratio()
            
            # ì¢…í•© ì ìˆ˜ (í‚¤ì›Œë“œ ë§¤ì¹­ + ìœ ì‚¬ë„)
            combined_score = (keyword_score * 0.6) + (similarity * 0.4)
            
            if combined_score > max_similarity:
                max_similarity = combined_score
                best_match_source = source_text[:100]  # ë””ë²„ê¹…ìš©
        
        # ê²€ì¦ ê¸°ì¤€: 30% ì´ìƒ ìœ ì‚¬í•˜ê±°ë‚˜ í•µì‹¬ í‚¤ì›Œë“œ 50% ì´ìƒ ë§¤ì¹­
        if max_similarity >= 0.3 or (matched_keywords / len(sentence_words) if sentence_words else 0) >= 0.5:
            verified_sentences.append({
                "sentence": sentence,
                "similarity": max_similarity,
                "source_preview": best_match_source
            })
        else:
            # ë²•ë ¹ ì¸ìš©ì´ë‚˜ ì¼ë°˜ì ì¸ ë©´ì±… ì¡°í•­ì€ ì œì™¸
            if not (re.search(r'\[ë²•ë ¹:\s*[^\]]+\]', sentence) or 
                   re.search(r'ë³¸\s*ë‹µë³€ì€\s*ì¼ë°˜ì ì¸', sentence) or
                   re.search(r'ë³€í˜¸ì‚¬ì™€\s*ì§ì ‘\s*ìƒë‹´', sentence)):
                unverified_sentences.append({
                    "sentence": sentence[:100],
                    "similarity": max_similarity,
                    "keywords": sentence_words[:5]
                })
    
    # 4. ì¢…í•© ê²€ì¦ ì ìˆ˜ ê³„ì‚°
    total_sentences = len(answer_sentences)
    verified_count = len(verified_sentences)
    
    grounding_score = verified_count / total_sentences if total_sentences > 0 else 0.0
    source_coverage = len(set([s["source_preview"] for s in verified_sentences if s.get("source_preview")])) / len(source_texts) if source_texts else 0.0
    
    # 5. ê²€ì¦ í†µê³¼ ê¸°ì¤€: 80% ì´ìƒ ë¬¸ì¥ì´ ê²€ì¦ë¨
    is_grounded = grounding_score >= 0.8
    
    # 6. ì‹ ë¢°ë„ ì¡°ì • (ê²€ì¦ë˜ì§€ ì•Šì€ ë¬¸ì¥ì´ ë§ìœ¼ë©´ ì‹ ë¢°ë„ ê°ì†Œ)
    confidence_penalty = len(unverified_sentences) * 0.05  # ë¬¸ì¥ë‹¹ 5% ê°ì†Œ
    
    return {
        "is_grounded": is_grounded,
        "grounding_score": grounding_score,
        "verified_sentences": verified_sentences[:5],  # ìƒ˜í”Œ
        "unverified_sentences": unverified_sentences,
        "unverified_count": len(unverified_sentences),
        "source_coverage": source_coverage,
        "needs_review": not is_grounded or len(unverified_sentences) > 3,
        "confidence_penalty": min(confidence_penalty, 0.3),  # ìµœëŒ€ 30% ê°ì†Œ
        "total_sentences": total_sentences,
        "verified_count": verified_count
    }
```

### 2.3 ì ìš© ìœ„ì¹˜
- `format_and_prepare_final()` ë©”ì„œë“œì—ì„œ ë‹µë³€ ê²€ì¦ ë‹¨ê³„ì— ì¶”ê°€
- `_validate_final_answer()` ë©”ì„œë“œì— í†µí•©

### 2.4 ë‹µë³€ ì¬ìƒì„± ë¡œì§
```python
def _regenerate_answer_if_needed(
    self,
    state: LegalWorkflowState,
    verification_result: Dict[str, Any]
) -> LegalWorkflowState:
    """
    ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ë‹µë³€ ì¬ìƒì„±
    
    Args:
        state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ
        verification_result: ê²€ì¦ ê²°ê³¼
        
    Returns:
        ìˆ˜ì •ëœ ìƒíƒœ
    """
    if verification_result.get("needs_review", False):
        self.logger.warning(
            f"ë‹µë³€ ê²€ì¦ ì‹¤íŒ¨: grounding_score={verification_result.get('grounding_score', 0):.2f}, "
            f"unverified_count={verification_result.get('unverified_count', 0)}"
        )
        
        # ì‹ ë¢°ë„ ì¡°ì •
        current_confidence = state.get("confidence", 0.8)
        penalty = verification_result.get("confidence_penalty", 0.0)
        adjusted_confidence = max(0.0, current_confidence - penalty)
        state["confidence"] = adjusted_confidence
        
        # ê²€ì¦ë˜ì§€ ì•Šì€ ì„¹ì…˜ì„ ë¡œê·¸ì— ê¸°ë¡
        unverified = verification_result.get("unverified_sentences", [])
        if unverified:
            self.logger.warning(
                f"ê²€ì¦ë˜ì§€ ì•Šì€ ë¬¸ì¥ {len(unverified)}ê°œ ë°œê²¬. "
                f"ìƒ˜í”Œ: {unverified[0].get('sentence', '')[:50]}..."
            )
    
    return state
```

---

## 3. ë‹µë³€ ê¸¸ì´ ì¼ê´€ì„± ê°œì„ 

### 3.1 ì§ˆì˜ ìœ í˜•ë³„ ëª©í‘œ ê¸¸ì´
```python
ANSWER_LENGTH_TARGETS = {
    "simple_question": (500, 1000),      # ê°„ë‹¨í•œ ì§ˆì˜: 500-1000ì
    "term_explanation": (800, 1500),     # ìš©ì–´ ì„¤ëª…: 800-1500ì
    "legal_analysis": (1500, 2500),      # ë²•ë¥  ë¶„ì„: 1500-2500ì
    "complex_question": (2000, 3500),    # ë³µì¡í•œ ì§ˆì˜: 2000-3500ì
    "default": (800, 2000)               # ê¸°ë³¸ê°’: 800-2000ì
}
```

### 3.2 ë‹µë³€ ê¸¸ì´ ì¡°ì ˆ ë¡œì§
```python
def _adjust_answer_length(
    self,
    answer: str,
    query_type: str,
    query_complexity: str
) -> str:
    """
    ë‹µë³€ ê¸¸ì´ë¥¼ ì§ˆì˜ ìœ í˜•ì— ë§ê²Œ ì¡°ì ˆ
    
    Args:
        answer: ì›ë³¸ ë‹µë³€
        query_type: ì§ˆì˜ ìœ í˜•
        query_complexity: ì§ˆì˜ ë³µì¡ë„
        
    Returns:
        ì¡°ì ˆëœ ë‹µë³€
    """
    import re
    
    if not answer:
        return answer
    
    current_length = len(answer)
    
    # ëª©í‘œ ê¸¸ì´ ê²°ì •
    if query_complexity == "simple":
        min_len, max_len = ANSWER_LENGTH_TARGETS.get("simple_question", (500, 1000))
    elif query_complexity == "complex":
        min_len, max_len = ANSWER_LENGTH_TARGETS.get("complex_question", (2000, 3500))
    else:
        targets = ANSWER_LENGTH_TARGETS.get(query_type, ANSWER_LENGTH_TARGETS["default"])
        min_len, max_len = targets
    
    # ê¸¸ì´ê°€ ì ì ˆí•œ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
    if min_len <= current_length <= max_len:
        return answer
    
    # ë„ˆë¬´ ê¸´ ê²½ìš°: í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œ
    if current_length > max_len:
        # ì„¹ì…˜ë³„ë¡œ ë¶„ë¦¬
        sections = re.split(r'\n\n+', answer)
        
        # ê° ì„¹ì…˜ì˜ ì¤‘ìš”ë„ í‰ê°€ (ë²•ë ¹ ì¸ìš©, íŒë¡€ ë“± í¬í•¨ ì—¬ë¶€)
        important_sections = []
        other_sections = []
        
        for section in sections:
            if (re.search(r'\[ë²•ë ¹:', section) or 
                re.search(r'ëŒ€ë²•ì›', section) or
                re.search(r'ì œ\s*\d+\s*ì¡°', section)):
                important_sections.append(section)
            else:
                other_sections.append(section)
        
        # ì¤‘ìš” ì„¹ì…˜ ìš°ì„  í¬í•¨
        result = []
        current_len = 0
        
        for section in important_sections:
            if current_len + len(section) <= max_len:
                result.append(section)
                current_len += len(section)
            else:
                # ì„¹ì…˜ ì¼ë¶€ë§Œ í¬í•¨
                remaining = max_len - current_len
                result.append(section[:remaining] + "...")
                break
        
        # ì—¬ìœ ê°€ ìˆìœ¼ë©´ ë‹¤ë¥¸ ì„¹ì…˜ë„ í¬í•¨
        for section in other_sections:
            if current_len + len(section) <= max_len:
                result.append(section)
                current_len += len(section)
            else:
                break
        
        return '\n\n'.join(result)
    
    # ë„ˆë¬´ ì§§ì€ ê²½ìš°: ì´ë¯¸ ìµœì†Œ ê¸¸ì´ë¡œ ìƒì„±ëœ ê²ƒì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
    # (ì¶”ê°€ ìƒì„±ì€ LLM í˜¸ì¶œì´ í•„ìš”í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” í•˜ì§€ ì•ŠìŒ)
    return answer
```

### 3.3 ì ìš© ìœ„ì¹˜
- `format_and_prepare_final()` ë©”ì„œë“œì—ì„œ ë‹µë³€ ê¸¸ì´ ì¡°ì ˆ

---

## 4. ì‹ ë¢°ë„ ê³„ì‚° ì¼ê´€ì„± ê°œì„ 

### 4.1 ì§ˆì˜ ìœ í˜•ë³„ ì‹ ë¢°ë„ ê¸°ì¤€
```python
def _calculate_consistent_confidence(
    self,
    base_confidence: float,
    query_type: str,
    query_complexity: str,
    grounding_score: float,
    source_coverage: float
) -> float:
    """
    ì¼ê´€ëœ ì‹ ë¢°ë„ ê³„ì‚°
    
    Args:
        base_confidence: ê¸°ë³¸ ì‹ ë¢°ë„
        query_type: ì§ˆì˜ ìœ í˜•
        query_complexity: ì§ˆì˜ ë³µì¡ë„
        grounding_score: ê²€ì¦ ì ìˆ˜
        source_coverage: ì†ŒìŠ¤ ì»¤ë²„ë¦¬ì§€
        
    Returns:
        ì¡°ì •ëœ ì‹ ë¢°ë„
    """
    # 1. ê¸°ë³¸ ì‹ ë¢°ë„ ì¡°ì •
    confidence = base_confidence
    
    # 2. ì§ˆì˜ ë³µì¡ë„ì— ë”°ë¥¸ ì¡°ì •
    complexity_adjustments = {
        "simple": 0.05,      # ê°„ë‹¨í•œ ì§ˆì˜: +5%
        "moderate": 0.0,      # ë³´í†µ: ë³€í™” ì—†ìŒ
        "complex": -0.05      # ë³µì¡í•œ ì§ˆì˜: -5%
    }
    confidence += complexity_adjustments.get(query_complexity, 0.0)
    
    # 3. ê²€ì¦ ì ìˆ˜ì— ë”°ë¥¸ ì¡°ì •
    if grounding_score < 0.8:
        confidence -= (0.8 - grounding_score) * 0.3  # ìµœëŒ€ 30% ê°ì†Œ
    
    # 4. ì†ŒìŠ¤ ì»¤ë²„ë¦¬ì§€ì— ë”°ë¥¸ ì¡°ì •
    if source_coverage < 0.5:
        confidence -= (0.5 - source_coverage) * 0.2  # ìµœëŒ€ 20% ê°ì†Œ
    
    # 5. ë²”ìœ„ ì œí•œ (0.0 ~ 1.0)
    confidence = max(0.0, min(1.0, confidence))
    
    # 6. ì§ˆì˜ ìœ í˜•ë³„ ìµœì†Œ ì‹ ë¢°ë„ ì„¤ì •
    min_confidence_by_type = {
        "simple_question": 0.75,
        "term_explanation": 0.80,
        "legal_analysis": 0.75,
        "complex_question": 0.70
    }
    min_confidence = min_confidence_by_type.get(query_type, 0.70)
    
    # ìµœì†Œ ì‹ ë¢°ë„ë³´ë‹¤ ë‚®ìœ¼ë©´ ê²½ê³ 
    if confidence < min_confidence:
        self.logger.warning(
            f"ì‹ ë¢°ë„ê°€ ìµœì†Œ ê¸°ì¤€({min_confidence:.2%})ë³´ë‹¤ ë‚®ìŒ: {confidence:.2%}"
        )
    
    return confidence
```

### 4.2 ì ìš© ìœ„ì¹˜
- `prepare_final_response_part()` ë©”ì„œë“œì—ì„œ ì‹ ë¢°ë„ ê³„ì‚° ì‹œ ì ìš©

---

## ğŸ”§ êµ¬í˜„ ìˆœì„œ

### Phase 1: ì¤‘ê°„ í…ìŠ¤íŠ¸ í•„í„°ë§ (ìš°ì„ ìˆœìœ„: ë†’ìŒ)
1. `_remove_intermediate_text()` í•¨ìˆ˜ ì¶”ê°€
2. `format_and_prepare_final()` ë©”ì„œë“œì— í†µí•©
3. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### Phase 2: ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ê²€ì¦ (ìš°ì„ ìˆœìœ„: ë†’ìŒ)
1. `validate_answer_source_verification()` í•¨ìˆ˜ ì¶”ê°€ (`quality_validators.py`)
2. `_validate_final_answer()` ë©”ì„œë“œì— í†µí•©
3. `_regenerate_answer_if_needed()` ë¡œì§ ì¶”ê°€
4. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### Phase 3: ë‹µë³€ ê¸¸ì´ ì¡°ì ˆ (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)
1. `_adjust_answer_length()` í•¨ìˆ˜ ì¶”ê°€
2. `format_and_prepare_final()` ë©”ì„œë“œì— í†µí•©
3. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### Phase 4: ì‹ ë¢°ë„ ê³„ì‚° ì¼ê´€ì„± (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)
1. `_calculate_consistent_confidence()` í•¨ìˆ˜ ì¶”ê°€
2. `prepare_final_response_part()` ë©”ì„œë“œì— í†µí•©
3. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

---

## ğŸ“Š ì„±ê³µ ê¸°ì¤€

### ì¤‘ê°„ í…ìŠ¤íŠ¸ í•„í„°ë§
- âœ… "STEP 0", "ì›ë³¸ ë‹µë³€", "ì§ˆë¬¸ ì •ë³´" ë“± íŒ¨í„´ì´ 100% ì œê±°ë¨
- âœ… í…ŒìŠ¤íŠ¸ ì§ˆì˜ì—ì„œ ì¤‘ê°„ í…ìŠ¤íŠ¸ê°€ í¬í•¨ë˜ì§€ ì•ŠìŒ

### ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ê²€ì¦
- âœ… ë‹µë³€ì˜ 80% ì´ìƒì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ê¸°ë°˜í•¨
- âœ… ê²€ì¦ë˜ì§€ ì•Šì€ ë¬¸ì¥ì´ 3ê°œ ì´í•˜
- âœ… ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì‹ ë¢°ë„ ìë™ ì¡°ì •

### ë‹µë³€ ê¸¸ì´ ì¼ê´€ì„±
- âœ… ì§ˆì˜ ìœ í˜•ë³„ ëª©í‘œ ê¸¸ì´ ë²”ìœ„ ë‚´ ìœ ì§€ìœ¨ 90% ì´ìƒ
- âœ… ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ ìë™ ì¶•ì•½

### ì‹ ë¢°ë„ ì¼ê´€ì„±
- âœ… ì§ˆì˜ ìœ í˜•ë³„ ì‹ ë¢°ë„ í¸ì°¨ 10% ì´í•˜
- âœ… ê²€ì¦ ì ìˆ˜ ë°˜ì˜ëœ ì‹ ë¢°ë„ ì¡°ì •

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
1. `_remove_intermediate_text()` í…ŒìŠ¤íŠ¸
2. `validate_answer_source_verification()` í…ŒìŠ¤íŠ¸
3. `_adjust_answer_length()` í…ŒìŠ¤íŠ¸
4. `_calculate_consistent_confidence()` í…ŒìŠ¤íŠ¸

### í†µí•© í…ŒìŠ¤íŠ¸
1. ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ (5ê°€ì§€ ì§ˆì˜)
2. ê²€ì¦ ì ìˆ˜ í™•ì¸
3. ì‹ ë¢°ë„ ì¡°ì • í™•ì¸
4. ë‹µë³€ í’ˆì§ˆ ê²€ì¦

---

## ğŸ“ íŒŒì¼ ìˆ˜ì • ëª©ë¡

### ìˆ˜ì •í•  íŒŒì¼
1. `lawfirm_langgraph/source/services/answer_formatter.py` (ë˜ëŠ” í•´ë‹¹ ì„œë¹„ìŠ¤)
   - `_remove_intermediate_text()` ì¶”ê°€
   - `_adjust_answer_length()` ì¶”ê°€
   - `_calculate_consistent_confidence()` ì¶”ê°€
   - `format_and_prepare_final()` ìˆ˜ì •
   - `_validate_final_answer()` ìˆ˜ì •

2. `lawfirm_langgraph/source/services/quality_validators.py` (ë˜ëŠ” í•´ë‹¹ ì„œë¹„ìŠ¤)
   - `validate_answer_source_verification()` ì¶”ê°€
   - `AnswerValidator` í´ë˜ìŠ¤ í™•ì¥

### ì¶”ê°€í•  í…ŒìŠ¤íŠ¸ íŒŒì¼
1. `tests/unit/test_answer_formatter_improvements.py`
2. `tests/integration/test_source_verification.py`

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì„±ëŠ¥ ì˜í–¥**: ê²€ì¦ ë¡œì§ ì¶”ê°€ë¡œ ì²˜ë¦¬ ì‹œê°„ì´ ì•½ê°„ ì¦ê°€í•  ìˆ˜ ìˆìŒ (ì˜ˆìƒ: +2~3ì´ˆ)
2. **ë‹µë³€ í’ˆì§ˆ**: ê²€ì¦ì´ ë„ˆë¬´ ì—„ê²©í•˜ë©´ ì¼ë¶€ ìœ íš¨í•œ ë‹µë³€ì´ ì œê±°ë  ìˆ˜ ìˆìŒ â†’ ì„ê³„ê°’ ì¡°ì • í•„ìš”
3. **ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡±**: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ê¸°ë³¸ ë‹µë³€ì€ ì œê³µí•´ì•¼ í•¨ â†’ í´ë°± ë¡œì§ í•„ìš”

---

## ğŸ“… ì˜ˆìƒ ì¼ì •

- **Phase 1**: 1ì¼
- **Phase 2**: 2ì¼
- **Phase 3**: 1ì¼
- **Phase 4**: 1ì¼
- **í†µí•© í…ŒìŠ¤íŠ¸**: 1ì¼

**ì´ ì˜ˆìƒ ì¼ì •**: ì•½ 6ì¼

---

## ğŸ”„ ë¡¤ë°± ê³„íš

ê° PhaseëŠ” ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„ë˜ë¯€ë¡œ, ë¬¸ì œ ë°œìƒ ì‹œ í•´ë‹¹ Phaseë§Œ ë¡¤ë°± ê°€ëŠ¥:
1. Git ë¸Œëœì¹˜ì—ì„œ ê°œë³„ Phase ì»¤ë°‹ ë¡¤ë°±
2. ê¸°ëŠ¥ í”Œë˜ê·¸ë¥¼ í†µí•œ ê°œë³„ ê¸°ëŠ¥ ë¹„í™œì„±í™”
