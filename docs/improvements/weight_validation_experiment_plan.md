# 가중치 검증 실험 계획서

## 1. 실험 개요

### 1.1 실험 목적
법률 AI 시스템의 하이브리드 검색 가중치(semantic vs keyword) 최적화를 통해 다음을 달성:
- 검색 품질 향상
- 답변 정확도 및 완전성 개선
- 문서 활용률 최적화
- 질문 유형별 맞춤형 가중치 설정

### 1.2 연구 가설

**주 가설 (H1)**:
- 질문 유형별로 최적의 semantic/keyword 가중치 비율이 다르다
- 법령 조회: keyword 가중치가 높을수록 정확도 향상
- 판례 검색: semantic 가중치가 높을수록 관련성 향상
- 일반 질문: 균형잡힌 가중치가 최적

**대립 가설 (H0)**:
- 질문 유형과 관계없이 동일한 가중치가 최적이다

### 1.3 실험 범위
- **대상**: 하이브리드 검색 가중치 (semantic vs keyword)
- **질문 유형**: 법령 조회, 판례 검색, 일반 질문
- **가중치 범위**: semantic 0.2 ~ 0.8 (0.05 간격)
- **테스트 쿼리**: 각 질문 유형별 20-30개
- **데이터 범위**: 민사법(민법) 데이터만 사용 (현재 데이터베이스 상태 반영)

---

## 2. 실험 설계

### 2.1 실험 변수

#### 독립 변수 (Independent Variables)
1. **법령 조회 가중치** (`hybrid_law`)
   - semantic: 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6
   - keyword: 1.0 - semantic

2. **판례 검색 가중치** (`hybrid_case`)
   - semantic: 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8
   - keyword: 1.0 - semantic

3. **일반 질문 가중치** (`hybrid_general`)
   - semantic: 0.4, 0.45, 0.5, 0.55, 0.6
   - keyword: 1.0 - semantic

#### 종속 변수 (Dependent Variables)
1. **답변 품질 메트릭**
   - 답변 품질 점수 (0-100)
   - 답변 길이 (자)
   - 소스 포함 여부
   - 소스 개수

2. **검색 품질 메트릭**
   - 평균 관련성 점수
   - 최소/최대 관련성 점수
   - 키워드 커버리지

3. **문서 활용 메트릭**
   - 검색된 문서 수
   - 실제 사용된 문서 수
   - 문서 활용률 (used/retrieved)

4. **소스 관련성 메트릭**
   - 소스 관련성 평균
   - 소스 커버리지 (답변이 소스에 기반하는 정도)

5. **성능 메트릭**
   - 총 소요 시간
   - 검색 시간
   - 생성 시간

6. **종합 점수**
   - 가중 평균 점수 (답변 품질 30% + 문서 활용률 25% + 소스 관련성 20% + 검색 점수 15% + 성능 10%)

#### 통제 변수 (Control Variables)
- 동일한 테스트 쿼리 세트
- 동일한 검색 엔진 설정
- 동일한 LLM 모델 및 파라미터
- 동일한 데이터베이스 상태
- 동일한 환경 변수 설정

### 2.2 실험 그룹

#### 대조군 (Control Group)
- **현재 프로덕션 가중치**:
  - `hybrid_law`: {"semantic": 0.45, "keyword": 0.55}
  - `hybrid_case`: {"semantic": 0.65, "keyword": 0.35}
  - `hybrid_general`: {"semantic": 0.5, "keyword": 0.5}

#### 실험군 (Experimental Groups)
- 법령 조회: 9개 조합 (semantic: 0.2 ~ 0.6)
- 판례 검색: 7개 조합 (semantic: 0.5 ~ 0.8)
- 일반 질문: 5개 조합 (semantic: 0.4 ~ 0.6)
- **총 21개 가중치 조합** (대조군 포함)

### 2.3 실험 설계 유형
- **요인 설계 (Factorial Design)**: 각 질문 유형별로 독립적으로 가중치 조합 테스트
- **반복 측정 (Repeated Measures)**: 각 가중치 조합에 대해 동일한 쿼리 세트 반복 테스트
- **무작위화 (Randomization)**: 쿼리 순서 무작위화 (순서 효과 제거)

---

## 3. 테스트 쿼리 세트

### 3.1 법령 조회 (law_inquiry) - 25개

**주의**: 현재 민사법(민법) 데이터만 있으므로 모든 쿼리는 민사법 관련으로 구성됩니다.

#### 민법 조문 조회 (10개)
1. "민법 제750조 손해배상에 대해 설명해주세요"
2. "계약 위약금에 대해 설명해주세요"
3. "민법 제103조 불공정한 법률행위에 대해 설명해주세요"
4. "민법 제563조 매매계약의 해제에 대해 설명해주세요"
5. "민법 제105조 사기·강박에 의한 의사표시에 대해 알려주세요"
6. "민법 제110조 대리권의 범위에 대해 설명해주세요"
7. "민법 제213조 소유권의 내용에 대해 알려주세요"
8. "민법 제618조 임대차의 의의에 대해 설명해주세요"
9. "민법 제543조 계약의 해제에 대해 알려주세요"
10. "민법 제390조 채무불이행에 대해 설명해주세요"

#### 민사법 개념 조회 (8개)
11. "손해배상의 범위는 어떻게 결정되나요?"
12. "계약 해지 사유에는 어떤 것들이 있나요?"
13. "불법행위가 성립하려면 어떤 요건이 필요한가요?"
14. "명예훼손이 성립하려면 어떤 조건이 필요한가요?"
15. "임대차 계약의 효력은 무엇인가요?"
16. "계약 위약금의 법적 효력은 무엇인가요?"
17. "소유권 이전의 요건은 어떻게 되나요?"
18. "채권 양도의 제한사항을 알려주세요"

#### 민사법 절차 조회 (7개)
19. "손해배상 청구 절차는 어떻게 되나요?"
20. "계약 해지 절차를 설명해주세요"
21. "임대차 계약 해지 절차는 무엇인가요?"
22. "명예훼손 고소 절차를 알려주세요"
23. "소유권 이전 등기 절차는 어떻게 되나요?"
24. "계약 위약금 청구 절차를 설명해주세요"
25. "채권 추심 절차는 무엇인가요?"

### 3.2 판례 검색 (precedent_search) - 25개

**주의**: 민사법 관련 판례만 검색합니다.

#### 특정 사건 검색 (10개)
1. "계약 해지 관련 판례를 찾아주세요"
2. "손해배상 청구 사례를 알려주세요"
3. "임대차 계약 해지 판례를 알려주세요"
4. "계약 위약금 관련 판례를 찾아주세요"
5. "명예훼손 판례를 찾아주세요"
6. "계약 해석 관련 판례를 찾아주세요"
7. "소유권 이전 판례를 알려주세요"
8. "채권 양도 무효 판례를 찾아주세요"
9. "불법행위 손해배상 판례를 알려주세요"
10. "계약 체결 무효 판례를 찾아주세요"

#### 유사 사례 검색 (8개)
11. "계약 해지 사유가 불명확한 경우 판례를 찾아주세요"
12. "손해배상 범위 산정 관련 판례를 알려주세요"
13. "임대차 계약 해지 사유 판례를 알려주세요"
14. "계약 위약금 과다 감액 판례를 찾아주세요"
15. "명예훼손 공연성 요건 판례를 알려주세요"
16. "소유권 이전 등기 관련 판례를 찾아주세요"
17. "채권 추심 관련 판례를 알려주세요"
18. "불법행위 인과관계 판례를 찾아주세요"

#### 법원별 판례 검색 (7개)
19. "대법원 계약 해지 판례를 찾아주세요"
20. "고등법원 손해배상 판례를 알려주세요"
21. "지방법원 임대차 판례를 찾아주세요"
22. "대법원 임대차 판례를 알려주세요"
23. "고등법원 계약 위약금 판례를 찾아주세요"
24. "대법원 명예훼손 판례를 알려주세요"
25. "지방법원 소유권 이전 판례를 찾아주세요"

### 3.3 일반 질문 (general) - 7개

**주의**: 민사법 관련 일반 질문으로 구성됩니다. 정보 조회 및 교육 관련 질문은 제외합니다.

#### 민사법 자문 (7개)
1. "민사법 자문이 필요합니다"
2. "계약서 작성 시 주의사항을 알려주세요"
3. "민사법 용어를 설명해주세요"
4. "민사 소송 절차에 대해 안내해주세요"
5. "민사법 상담이 필요합니다"
6. "계약 분쟁 해결 방법을 알려주세요"
7. "손해배상 청구 방법을 설명해주세요"

**총 테스트 쿼리: 57개** (법령 조회 25개 + 판례 검색 25개 + 일반 질문 7개)

---

## 4. 평가 지표 및 기준

### 4.1 주요 평가 지표

#### 1. 종합 점수 (Overall Score) - 최우선 지표
- **계산식**: 
  ```
  overall_score = 0.30 × answer_quality + 
                  0.25 × document_utilization + 
                  0.20 × source_relevance + 
                  0.15 × search_score + 
                  0.10 × performance_score
  ```
- **목표**: 최대화
- **기준**: 0.7 이상 (우수), 0.5-0.7 (양호), 0.5 미만 (개선 필요)

#### 2. 답변 품질 점수 (Answer Quality Score)
- **범위**: 0-100
- **평가 항목**:
  - 답변 존재 여부 (25점)
  - 최소 길이 충족 (25점)
  - 오류 메시지 없음 (25점)
  - 참고자료 존재 (25점)
- **목표**: 75점 이상

#### 3. 문서 활용률 (Document Utilization Rate)
- **계산식**: `used_docs_count / retrieved_docs_count`
- **목표**: 0.3 이상 (검색된 문서의 30% 이상 사용)
- **이상적**: 0.5-0.7 (너무 높으면 과도한 필터링, 너무 낮으면 문서 손실)

#### 4. 평균 관련성 점수 (Average Relevance Score)
- **범위**: 0-1
- **목표**: 0.6 이상
- **이상적**: 0.7-0.9

#### 5. 소스 커버리지 (Source Coverage)
- **계산식**: 답변이 소스에 기반하는 정도
- **목표**: 0.7 이상

### 4.2 보조 평가 지표

#### 성능 지표
- 총 소요 시간: 10초 이하 (목표)
- 검색 시간: 3초 이하 (목표)
- 생성 시간: 5초 이하 (목표)

#### 안정성 지표
- 테스트 성공률: 95% 이상
- 오류 발생률: 5% 이하

---

## 5. 실험 절차

### 5.1 사전 준비

#### 1단계: 환경 설정
- [ ] MLflow 실험 환경 설정
- [ ] 테스트 데이터베이스 확인
- [ ] 검색 인덱스 상태 확인
- [ ] 환경 변수 설정 확인

#### 2단계: 베이스라인 측정
- [ ] 현재 프로덕션 가중치로 전체 테스트 쿼리 실행
- [ ] 베이스라인 메트릭 수집 및 기록
- [ ] MLflow에 베이스라인 run 생성

#### 3단계: 테스트 쿼리 검증
- [ ] 각 테스트 쿼리가 정상 작동하는지 확인
- [ ] 쿼리별 예상 결과 검토
- [ ] 쿼리 세트 최종 확정

### 5.2 실험 실행

#### Phase 1: 법령 조회 가중치 최적화 (예상 시간: 2-3시간)
1. 법령 조회 가중치 조합 생성 (9개)
2. 각 조합별로 법령 조회 쿼리 25개 실행
3. 메트릭 수집 및 MLflow 로깅
4. 중간 결과 분석

#### Phase 2: 판례 검색 가중치 최적화 (예상 시간: 2-3시간)
1. 판례 검색 가중치 조합 생성 (7개)
2. 각 조합별로 판례 검색 쿼리 25개 실행
3. 메트릭 수집 및 MLflow 로깅
4. 중간 결과 분석

#### Phase 3: 일반 질문 가중치 최적화 (예상 시간: 30분-1시간)
1. 일반 질문 가중치 조합 생성 (5개)
2. 각 조합별로 일반 질문 쿼리 7개 실행
3. 메트릭 수집 및 MLflow 로깅
4. 중간 결과 분석

#### Phase 4: 통합 분석 (예상 시간: 1시간)
1. 전체 결과 통합 분석
2. 질문 유형별 최적 가중치 도출
3. 통계적 유의성 검정
4. 최종 리포트 생성

### 5.3 실험 실행 순서

**권장 실행 순서**:
1. 빠른 테스트 모드로 전체 조합 스캔 (`--quick`)
2. 상위 5개 조합 선정
3. 전체 테스트 모드로 상세 검증
4. 최종 최적 가중치 선정

---

## 6. 데이터 수집 계획

### 6.1 수집 데이터

#### 자동 수집 데이터
- 모든 평가 메트릭 (자동)
- 성능 메트릭 (자동)
- MLflow에 자동 로깅

#### 수동 수집 데이터 (선택적)
- 답변 품질 수동 평가 (샘플링)
- 사용자 만족도 (가능한 경우)
- 특이 케이스 분석

### 6.2 데이터 저장

#### MLflow
- 모든 파라미터 및 메트릭
- 실험 메타데이터
- 아티팩트 (결과 리포트)

#### 로컬 파일
- JSON 리포트: `logs/test/weight_validation/weight_validation_YYYYMMDD_HHMMSS.json`
- 텍스트 리포트: `logs/test/weight_validation/weight_validation_YYYYMMDD_HHMMSS.txt`

---

## 7. 분석 방법

### 7.1 기술 통계
- 평균, 중앙값, 표준편차
- 최소값, 최대값
- 사분위수 (Q1, Q2, Q3)

### 7.2 비교 분석
- 베이스라인 vs 실험군 비교
- 질문 유형별 비교
- 가중치 조합별 비교

### 7.3 통계적 검정 (선택적)
- t-검정: 베이스라인 vs 최적 가중치
- ANOVA: 질문 유형별 차이 검정
- 상관관계 분석: 가중치 비율과 성능 지표

### 7.4 시각화
- 가중치 조합별 성능 비교 그래프
- 질문 유형별 최적 가중치 분포
- 메트릭 간 상관관계 히트맵
- MLflow UI 활용

---

## 8. 예상 결과 및 해석

### 8.1 예상 결과

#### 법령 조회
- **예상 최적 가중치**: semantic 0.4-0.5, keyword 0.5-0.6
- **이유**: 정확한 조문 번호 매칭이 중요하지만, 의미적 검색도 필요

#### 판례 검색
- **예상 최적 가중치**: semantic 0.6-0.7, keyword 0.3-0.4
- **이유**: 유사 사례 찾기가 중요하므로 의미적 검색 강조

#### 일반 질문
- **예상 최적 가중치**: semantic 0.5, keyword 0.5
- **이유**: 균형이 중요

### 8.2 결과 해석 기준

#### 최적 가중치 선정 기준
1. **종합 점수가 가장 높은 조합**
2. **안정성**: 표준편차가 낮은 조합 (일관성)
3. **균형**: 모든 메트릭이 균형잡힌 조합
4. **실용성**: 성능 저하가 없는 조합

#### 개선 효과 평가
- **우수**: 베이스라인 대비 10% 이상 개선
- **양호**: 베이스라인 대비 5-10% 개선
- **보통**: 베이스라인 대비 0-5% 개선
- **불량**: 베이스라인보다 낮음

---

## 9. 리스크 및 대응 방안

### 9.1 기술적 리스크

#### 리스크 1: 테스트 시간 과다
- **원인**: 많은 조합 × 많은 쿼리
- **대응**: 
  - 빠른 테스트 모드로 1차 스크린링
  - 병렬 처리 (향후 개선)
  - 샘플링 (대표 쿼리만 사용)

#### 리스크 2: 메트릭 추출 실패
- **원인**: 출력 형식 변경
- **대응**: 
  - 정규식 패턴 유연화
  - 폴백 메커니즘 구현
  - 수동 검증 절차

#### 리스크 3: 환경 불일치
- **원인**: 데이터베이스 상태, 인덱스 버전 차이
- **대응**: 
  - 실험 전 환경 일관성 확인
  - 동일한 MLflow run ID 사용
  - 환경 변수 고정

### 9.2 데이터 품질 리스크

#### 리스크 4: 테스트 쿼리 부적절
- **원인**: 쿼리가 실제 사용 패턴과 다름
- **대응**: 
  - 실제 사용자 쿼리 분석 반영
  - 도메인 전문가 검토
  - 지속적 업데이트

#### 리스크 5: 평가 기준 주관성
- **원인**: 자동 평가의 한계
- **대응**: 
  - 수동 평가 샘플링
  - 여러 평가 지표 조합
  - 전문가 검토

---

## 10. 실험 일정

### 10.1 전체 일정 (예상)

| 단계 | 작업 | 예상 시간 | 담당 |
|------|------|----------|------|
| 사전 준비 | 환경 설정, 베이스라인 측정 | 1일 | 개발팀 |
| Phase 1 | 법령 조회 가중치 최적화 | 2-3시간 | 자동 |
| Phase 2 | 판례 검색 가중치 최적화 | 2-3시간 | 자동 |
| Phase 3 | 일반 질문 가중치 최적화 | 30분-1시간 | 자동 |
| Phase 4 | 통합 분석 및 리포트 | 1시간 | 개발팀 |
| 검토 및 적용 | 결과 검토, 가중치 적용 | 1일 | 개발팀 |

**총 예상 시간**: 2-3일

### 10.2 빠른 테스트 일정

| 단계 | 작업 | 예상 시간 |
|------|------|----------|
| 빠른 스캔 | 전체 조합 빠른 테스트 | 30분-1시간 |
| 상위 조합 선정 | 결과 분석 및 선정 | 30분 |
| 상세 검증 | 상위 5개 조합 상세 테스트 | 1.5-2시간 |
| 최종 분석 | 결과 분석 및 적용 | 1시간 |

**총 예상 시간**: 4-6시간

---

## 11. 성공 기준

### 11.1 실험 성공 기준

#### 필수 조건
- [ ] 모든 테스트 쿼리 실행 완료 (성공률 95% 이상)
- [ ] 모든 메트릭 수집 완료
- [ ] MLflow에 모든 데이터 기록 완료
- [ ] 최적 가중치 도출 완료

#### 품질 기준
- [ ] 최적 가중치의 종합 점수가 베이스라인 대비 5% 이상 개선
- [ ] 최적 가중치의 안정성 (표준편차)이 베이스라인 이하
- [ ] 질문 유형별로 최적 가중치가 명확히 구분됨

### 11.2 적용 기준

#### 프로덕션 적용 조건
- 최적 가중치의 종합 점수가 베이스라인 대비 10% 이상 개선
- 모든 질문 유형에서 성능 저하 없음
- 안정성 검증 완료 (표준편차 낮음)

#### 부분 적용 조건
- 특정 질문 유형에서만 10% 이상 개선
- 해당 질문 유형에만 적용

---

## 12. 후속 작업

### 12.1 실험 후 작업

1. **결과 검토**
   - MLflow UI에서 결과 확인
   - 통계 분석 수행
   - 특이 케이스 분석

2. **최적 가중치 적용**
   - 코드에 최적 가중치 반영
   - 코드 리뷰 및 승인
   - 배포 준비

3. **검증 테스트**
   - 프로덕션 환경에서 샘플 테스트
   - A/B 테스트 (가능한 경우)
   - 모니터링 설정

4. **문서화**
   - 실험 결과 문서화
   - 가중치 설정 근거 문서화
   - 개선 사항 기록

### 12.2 지속적 개선

1. **정기적 재검증**
   - 분기별 가중치 재검증
   - 새로운 데이터 반영
   - 사용자 피드백 반영

2. **적응형 가중치**
   - 질문 유형 자동 감지
   - 동적 가중치 조정
   - 머신러닝 기반 최적화 (장기)

---

## 13. 실행 체크리스트

### 실험 전 체크리스트
- [ ] 환경 변수 설정 확인
- [ ] MLflow 환경 설정 확인
- [ ] 데이터베이스 연결 확인
- [ ] 검색 인덱스 상태 확인
- [ ] 테스트 쿼리 세트 최종 확인
- [ ] 베이스라인 측정 완료
- [ ] 백업 완료

### 실험 중 체크리스트
- [ ] 각 Phase별 진행 상황 모니터링
- [ ] 오류 발생 시 즉시 대응
- [ ] 중간 결과 확인
- [ ] MLflow 로깅 정상 작동 확인

### 실험 후 체크리스트
- [ ] 모든 데이터 수집 완료
- [ ] MLflow에 모든 데이터 기록 완료
- [ ] 결과 리포트 생성 완료
- [ ] 통계 분석 완료
- [ ] 최적 가중치 선정 완료
- [ ] 결과 검토 완료
- [ ] 문서화 완료

---

## 14. 참고 자료

- 가중치 검증 가이드: `docs/weight_validation_guide.md`
- 가중치 설정 분석: `docs/weight_configuration_analysis.md`
- 가중치 근거 분석: `docs/weight_rationale_analysis.md`
- MLflow 가이드: `docs/03_rag_system/mlflow_guide.md`

---

## 부록: 실행 명령어

### 빠른 테스트
```bash
# 전체 질문 유형 빠른 테스트
python lawfirm_langgraph/tests/scripts/validate_weight_configurations.py --quick

# 특정 질문 유형만 빠른 테스트
python lawfirm_langgraph/tests/scripts/validate_weight_configurations.py --quick --query-type law_inquiry
```

### 전체 테스트
```bash
# 전체 질문 유형 전체 테스트
python lawfirm_langgraph/tests/scripts/validate_weight_configurations.py

# 특정 질문 유형만 전체 테스트
python lawfirm_langgraph/tests/scripts/validate_weight_configurations.py --query-type precedent_search
```

### MLflow UI 실행
```bash
mlflow ui --backend-store-uri file:///D:/project/LawFirmAI/mlflow/mlruns
```

