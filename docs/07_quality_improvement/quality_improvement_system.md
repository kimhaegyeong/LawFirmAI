# LawFirmAI ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹œìŠ¤í…œ

## ê°œìš”

LawFirmAI ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹œìŠ¤í…œì€ ë²•ë¥  ë°ì´í„°ì˜ í’ˆì§ˆì„ ìžë™ìœ¼ë¡œ ê²€ì¦, ê°œì„ , ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì¢…í•©ì ì¸ ì†”ë£¨ì…˜ìž…ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ 4ë‹¨ê³„ë¡œ êµ¬ì„±ë˜ì–´ ìžˆìœ¼ë©°, ê° ë‹¨ê³„ëŠ” ë°ì´í„° í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ íŠ¹ì • ê¸°ëŠ¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

## ìµœì‹  ì—…ë°ì´íŠ¸ (2025-01-XX)

### ðŸŽ¯ í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ ì •êµí™”
- **ê°€ì¤‘ì¹˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì‹œìŠ¤í…œ**: í•µì‹¬/ì¤‘ìš”/ë³´ì¡° í‚¤ì›Œë“œë¡œ ë¶„ë¥˜í•˜ì—¬ ì •í™•ë„ í–¥ìƒ
- **ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë§¤í•‘**: ì§ˆë¬¸ ìœ í˜•ë³„ ë§žì¶¤í˜• í‚¤ì›Œë“œ ì œê³µ
- **ë™ì  í‚¤ì›Œë“œ í•™ìŠµ**: ì‚¬ìš©ìž í”¼ë“œë°± ê¸°ë°˜ ì§€ì†ì  ê°œì„ 
- **ì˜ë¯¸ì  ìœ ì‚¬ë„ ë§¤í•‘**: ë²•ë¥  ìš©ì–´ ê°„ ì˜ë¯¸ì  ê´€ê³„ í™œìš©

### ðŸ“ˆ í’ˆì§ˆ í–¥ìƒ íš¨ê³¼
- **í‚¤ì›Œë“œ í¬í•¨ë„**: 0.390 â†’ 0.7+ ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥
- **ë‹µë³€ êµ¬ì¡°í™”**: ì»¨í…ìŠ¤íŠ¸ë³„ ë§žì¶¤í˜• êµ¬ì¡° ì œê³µ
- **ë²•ì  ì •í™•ì„±**: ì˜ë¯¸ì  ê´€ê³„ë¥¼ í†µí•œ ì „ë¬¸ ìš©ì–´ í™œìš©
- **ì§€ì†ì  í•™ìŠµ**: ì‚¬ìš©ìž í”¼ë“œë°±ì„ í†µí•œ ìžë™ ê°œì„ 

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹œìŠ¤í…œ                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 1: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ê°œì„                           â”‚
â”‚  â”œâ”€â”€ DataQualityValidator                                   â”‚
â”‚  â”œâ”€â”€ MLEnhancedArticleParser (ê°œì„ ë¨)                       â”‚
â”‚  â”œâ”€â”€ HybridArticleParser                                    â”‚
â”‚  â””â”€â”€ Preprocessing Pipeline (ì—…ë°ì´íŠ¸ë¨)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 2: ê³ ê¸‰ ì¤‘ë³µ ê°ì§€ ë° í•´ê²°                            â”‚
â”‚  â”œâ”€â”€ AdvancedDuplicateDetector                             â”‚
â”‚  â”œâ”€â”€ IntelligentDuplicateResolver                           â”‚
â”‚  â””â”€â”€ Duplicate Detection Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 3: ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ê°œì„  ë° ë§ˆì´ê·¸ë ˆì´ì…˜           â”‚
â”‚  â”œâ”€â”€ Schema Migration Scripts                              â”‚
â”‚  â”œâ”€â”€ Data Migration Scripts                                â”‚
â”‚  â”œâ”€â”€ DatabaseManager (ì—…ë°ì´íŠ¸ë¨)                           â”‚
â”‚  â””â”€â”€ Import Scripts (ì—…ë°ì´íŠ¸ë¨)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 4: ìžë™í™”ëœ í’ˆì§ˆ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§                    â”‚
â”‚  â”œâ”€â”€ AutomatedDataCleaner                                  â”‚
â”‚  â”œâ”€â”€ RealTimeQualityMonitor                                â”‚
â”‚  â”œâ”€â”€ ScheduledTaskManager                                   â”‚
â”‚  â”œâ”€â”€ QualityReportingDashboard                             â”‚
â”‚  â””â”€â”€ AutoPipelineOrchestrator (í†µí•©ë¨)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 5: í–¥ìƒëœ í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ (NEW!)                  â”‚
â”‚  â”œâ”€â”€ LegalKeywordMapper (ê°€ì¤‘ì¹˜ ê¸°ë°˜)                       â”‚
â”‚  â”œâ”€â”€ ContextAwareKeywordMapper (ì»¨í…ìŠ¤íŠ¸ ì¸ì‹)              â”‚
â”‚  â”œâ”€â”€ AdaptiveKeywordMapper (ë™ì  í•™ìŠµ)                      â”‚
â”‚  â”œâ”€â”€ SemanticKeywordMapper (ì˜ë¯¸ì  ìœ ì‚¬ë„)                  â”‚
â”‚  â””â”€â”€ EnhancedKeywordMapper (í†µí•© ì‹œìŠ¤í…œ)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Phase 1: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ 

### 1.1 DataQualityValidator

**ìœ„ì¹˜**: `scripts/data_processing/quality/data_quality_validator.py`

**ê¸°ëŠ¥**:
- ë²•ë¥  ë°ì´í„°ì˜ íŒŒì‹± í’ˆì§ˆ ê²€ì¦
- í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
- í’ˆì§ˆ ê°œì„  ì œì•ˆ ìƒì„±
- êµ¬ì¡°ì  ì™„ì„±ë„ ê²€ì‚¬

**ì£¼ìš” ë©”ì„œë“œ**:
```python
def validate_parsing_quality(self, law_data: Dict[str, Any]) -> QualityReport
def calculate_quality_score(self, law_data: Dict[str, Any]) -> float
def suggest_improvements(self, law_data: Dict[str, Any]) -> List[str]
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
from scripts.data_processing.quality.data_quality_validator import DataQualityValidator

validator = DataQualityValidator()
quality_report = validator.validate_parsing_quality(law_data)
quality_score = validator.calculate_quality_score(law_data)
```

### 1.2 MLEnhancedArticleParser (ê°œì„ ë¨)

**ìœ„ì¹˜**: `scripts/ml_training/model_training/ml_enhanced_parser.py`

**ê°œì„ ì‚¬í•­**:
- í’ˆì§ˆ ê²€ì¦ ë ˆì´ì–´ í†µí•©
- ìžë™ ìˆ˜ì • ê¸°ëŠ¥ ì¶”ê°€
- ëˆ„ë½ëœ ì¡°ë¬¸ ë³µêµ¬
- ì¤‘ë³µ ì¡°ë¬¸ ì œê±°

**ìƒˆë¡œìš´ ë©”ì„œë“œ**:
```python
def validate_parsed_result(self, parsed_result: Dict[str, Any]) -> QualityReport
def fix_missing_articles(self, parsed_result: Dict[str, Any]) -> Dict[str, Any]
def remove_duplicate_articles(self, parsed_result: Dict[str, Any]) -> Dict[str, Any]
def parse_law_with_validation(self, law_content: str) -> Dict[str, Any]
```

### 1.3 HybridArticleParser

**ìœ„ì¹˜**: `scripts/data_processing/quality/hybrid_parser.py`

**ê¸°ëŠ¥**:
- ê·œì¹™ ê¸°ë°˜ íŒŒì‹±ê³¼ ML ê¸°ë°˜ íŒŒì‹± ê²°í•©
- ìµœì  ê²°ê³¼ ì„ íƒ
- í›„ì²˜ë¦¬ ìˆ˜ì • ì ìš©
- ìˆ˜ë™ ê²€í†  í”Œëž˜ê·¸ ì„¤ì •

**ì£¼ìš” ë©”ì„œë“œ**:
```python
def parse_law_content(self, law_content: str, law_name: str = "Unknown") -> Dict[str, Any]
def _select_best_result(self, rule_result: Dict[str, Any], ml_result: Dict[str, Any]) -> Dict[str, Any]
def _apply_post_processing_corrections(self, result: Dict[str, Any]) -> Dict[str, Any]
```

### 1.4 Preprocessing Pipeline (ì—…ë°ì´íŠ¸ë¨)

**ìœ„ì¹˜**: `scripts/data_processing/preprocessing/preprocess_laws.py`

**ê°œì„ ì‚¬í•­**:
- HybridArticleParser í†µí•©
- í’ˆì§ˆ ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì €ìž¥
- ìžë™ ìˆ˜ì • ë° ìˆ˜ë™ ê²€í†  í”Œëž˜ê·¸

## Phase 2: ê³ ê¸‰ ì¤‘ë³µ ê°ì§€ ë° í•´ê²°

### 2.1 AdvancedDuplicateDetector

**ìœ„ì¹˜**: `scripts/data_processing/quality/duplicate_detector.py`

**ê¸°ëŠ¥**:
- íŒŒì¼ ë ˆë²¨ ì¤‘ë³µ ê°ì§€ (í•´ì‹œ, í¬ê¸°, ì´ë¦„ ìœ ì‚¬ì„±)
- ì½˜í…ì¸  ë ˆë²¨ ì¤‘ë³µ ê°ì§€ (TF-IDF, ì½”ì‚¬ì¸ ìœ ì‚¬ì„±)
- ì˜ë¯¸ì  ì¤‘ë³µ ê°ì§€ (ë²¡í„° ìž„ë² ë”© ê¸°ë°˜)
- ë‹¤ì¸µ ê°ì§€ ì•Œê³ ë¦¬ì¦˜

**ì£¼ìš” ë©”ì„œë“œ**:
```python
def detect_file_level_duplicates(self, files: List[Dict[str, Any]]) -> List[DuplicateGroup]
def detect_content_level_duplicates(self, content_items: List[Dict[str, Any]]) -> List[DuplicateGroup]
def detect_semantic_duplicates(self, content_items: List[Dict[str, Any]]) -> List[DuplicateGroup]
```

### 2.2 IntelligentDuplicateResolver

**ìœ„ì¹˜**: `scripts/data_processing/quality/duplicate_resolver.py`

**ê¸°ëŠ¥**:
- ì§€ëŠ¥ì  ì¤‘ë³µ í•´ê²° ì „ëžµ
- í’ˆì§ˆ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ê²°ì •
- ë©”íƒ€ë°ì´í„° ë³‘í•©
- ë²„ì „ ížˆìŠ¤í† ë¦¬ ìƒì„±

**í•´ê²° ì „ëžµ**:
- `quality_based`: í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜
- `completeness_based`: ì™„ì„±ë„ ê¸°ë°˜
- `recency_based`: ìµœì‹ ì„± ê¸°ë°˜
- `conservative`: ë³´ìˆ˜ì  ì ‘ê·¼

**ì£¼ìš” ë©”ì„œë“œ**:
```python
def resolve_duplicates(self, duplicate_groups: List[DuplicateGroup], strategy: str = "quality_based") -> List[ResolutionResult]
def add_resolution_strategy(self, strategy: Dict[str, Any])
def export_resolution_report(self, results: List[ResolutionResult]) -> Dict[str, Any]
```

### 2.3 Duplicate Detection Pipeline

**ìœ„ì¹˜**: `scripts/data_processing/run_duplicate_detection.py`

**ê¸°ëŠ¥**:
- ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¤‘ë³µ ê°ì§€ íŒŒì´í”„ë¼ì¸
- ìžë™ í•´ê²° ì˜µì…˜
- ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
- í†µê³„ ë° ê¶Œìž¥ì‚¬í•­ ì œê³µ

**ì‚¬ìš©ë²•**:
```bash
python scripts/data_processing/run_duplicate_detection.py \
    --input data/processed \
    --output reports/duplicate_detection \
    --auto-resolve \
    --strategy quality_based
```

## Phase 3: ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ê°œì„  ë° ë§ˆì´ê·¸ë ˆì´ì…˜

### 3.1 Schema Migration Scripts

**ìœ„ì¹˜**: `scripts/database/migrate_schema_v2.py`

**ê¸°ëŠ¥**:
- ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë²„ì „ 2ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
- ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€ (í’ˆì§ˆ ê´€ë ¨)
- ìƒˆë¡œìš´ í…Œì´ë¸” ìƒì„± (ì¤‘ë³µ ê·¸ë£¹, í’ˆì§ˆ ë³´ê³ ì„œ)
- ì¸ë±ìŠ¤ ìƒì„± ë° ìµœì í™”

**ìƒˆë¡œìš´ ì»¬ëŸ¼**:
```sql
-- assembly_laws í…Œì´ë¸”ì— ì¶”ê°€
law_name_hash TEXT UNIQUE,
content_hash TEXT UNIQUE,
quality_score REAL DEFAULT 0.0,
duplicate_group_id TEXT,
is_primary_version BOOLEAN DEFAULT TRUE,
version_number INTEGER DEFAULT 1,
parsing_method TEXT DEFAULT "legacy",
auto_corrected BOOLEAN DEFAULT FALSE,
manual_review_required BOOLEAN DEFAULT FALSE,
migration_timestamp TEXT
```

**ìƒˆë¡œìš´ í…Œì´ë¸”**:
- `duplicate_groups`: ì¤‘ë³µ ê·¸ë£¹ ì •ë³´
- `quality_reports`: í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„¸ ì •ë³´
- `migration_history`: ë§ˆì´ê·¸ë ˆì´ì…˜ ížˆìŠ¤í† ë¦¬
- `schema_version`: ìŠ¤í‚¤ë§ˆ ë²„ì „ ê´€ë¦¬

### 3.2 Data Migration Scripts

**ìœ„ì¹˜**: `scripts/database/migrate_existing_data.py`

**ê¸°ëŠ¥**:
- ê¸°ì¡´ ë°ì´í„°ë¥¼ ìƒˆ ìŠ¤í‚¤ë§ˆì— ë§žê²Œ ë§ˆì´ê·¸ë ˆì´ì…˜
- í•´ì‹œ ê³„ì‚° ë° ì €ìž¥
- í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
- ì¤‘ë³µ ê°ì§€ ë° í•´ê²°
- ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›

**ì‚¬ìš©ë²•**:
```bash
python scripts/database/migrate_existing_data.py \
    --db-path data/lawfirm.db \
    --batch-size 1000 \
    --verbose
```

### 3.3 DatabaseManager (ì—…ë°ì´íŠ¸ë¨)

**ìœ„ì¹˜**: `source/data/database.py`

**ê°œì„ ì‚¬í•­**:
- ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆ ì§€ì›
- í’ˆì§ˆ í†µê³„ ë©”ì„œë“œ ì¶”ê°€
- ì¤‘ë³µ í†µê³„ ë©”ì„œë“œ ì¶”ê°€
- í’ˆì§ˆ ê¸°ë°˜ ì¿¼ë¦¬ ë©”ì„œë“œ ì¶”ê°€

**ìƒˆë¡œìš´ ë©”ì„œë“œ**:
```python
def get_quality_statistics(self) -> Dict[str, Any]
def get_duplicate_statistics(self) -> Dict[str, Any]
def get_laws_by_quality_score(self, min_score: float, max_score: float) -> List[Dict[str, Any]]
def get_laws_requiring_review(self) -> List[Dict[str, Any]]
def update_law_quality_score(self, law_id: str, quality_score: float)
```

### 3.4 Import Scripts (ì—…ë°ì´íŠ¸ë¨)

**ìœ„ì¹˜**: `scripts/data_processing/utilities/import_laws_to_db.py`

**ê°œì„ ì‚¬í•­**:
- ì¤‘ë³µ ê²€ì‚¬ ë¡œì§ í†µí•©
- í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ë° ì €ìž¥
- í’ˆì§ˆ ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
- í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬

**ìƒˆë¡œìš´ ê¸°ëŠ¥**:
```python
def _check_for_duplicates(self, new_law_record: Tuple, new_law_data: Dict[str, Any]) -> Dict[str, Any]
def _update_existing_law(self, new_law_record: Tuple, new_law_data: Dict[str, Any], existing_law_id: str) -> bool
def _enhance_law_record_with_quality(self, law_record: Tuple, law_data: Dict[str, Any], quality_score: float) -> Tuple
```

## Phase 4: ìžë™í™”ëœ í’ˆì§ˆ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§

### 4.1 AutomatedDataCleaner

**ìœ„ì¹˜**: `scripts/data_processing/quality/automated_data_cleaner.py`

**ê¸°ëŠ¥**:
- ì¼ì¼ ë°ì´í„° ì •ë¦¬ ë£¨í‹´
- ì£¼ê°„ ì¢…í•© ì •ë¦¬ ë£¨í‹´
- ì›”ê°„ ê°ì‚¬ ë£¨í‹´
- ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

**ì •ë¦¬ ìž‘ì—…**:
- ì¤‘ë³µ ê°ì§€ ë° í•´ê²°
- í’ˆì§ˆ í‰ê°€ ë° ê°œì„ 
- ê³ ì•„ ë°ì´í„° ì •ë¦¬
- ì¸ë±ìŠ¤ ìµœì í™”

**ì‚¬ìš©ë²•**:
```bash
python scripts/data_processing/quality/automated_data_cleaner.py \
    --operation daily \
    --db-path data/lawfirm.db \
    --output reports/daily_cleaning.json
```

### 4.2 RealTimeQualityMonitor

**ìœ„ì¹˜**: `scripts/data_processing/quality/real_time_quality_monitor.py`

**ê¸°ëŠ¥**:
- ì—°ì†ì ì¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
- ìž„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼ ìƒì„±
- í’ˆì§ˆ íŠ¸ë Œë“œ ë¶„ì„
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì 
- ìžë™ ê¶Œìž¥ì‚¬í•­ ìƒì„±

**ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­**:
- ì „ì²´ í’ˆì§ˆ ì ìˆ˜
- ë‚®ì€ í’ˆì§ˆ ë ˆì½”ë“œ ë¹„ìœ¨
- ì¤‘ë³µ ë¹„ìœ¨
- ìµœê·¼ ì¶”ê°€ëœ ë°ì´í„° í’ˆì§ˆ
- í’ˆì§ˆ íŠ¸ë Œë“œ (ê°œì„ /ì•ˆì •/ì•…í™”)

**ì‚¬ìš©ë²•**:
```bash
python scripts/data_processing/quality/real_time_quality_monitor.py \
    --action start \
    --db-path data/lawfirm.db
```

### 4.3 ScheduledTaskManager

**ìœ„ì¹˜**: `scripts/data_processing/quality/scheduled_task_manager.py`

**ê¸°ëŠ¥**:
- ì˜ˆì•½ëœ ë°ì´í„° í’ˆì§ˆ ìž‘ì—… ê´€ë¦¬
- ì¼ì¼/ì£¼ê°„/ì›”ê°„ ìž‘ì—… ìŠ¤ì¼€ì¤„ë§
- ìž‘ì—… ì‹¤í–‰ ë¡œê¹…
- ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
- ì•Œë¦¼ ì‹œìŠ¤í…œ

**ìŠ¤ì¼€ì¤„ëœ ìž‘ì—…**:
- ì¼ì¼ ì •ë¦¬: ë§¤ì¼ ì˜¤ì „ 2ì‹œ
- ì£¼ê°„ ì •ë¦¬: ë§¤ì£¼ ì¼ìš”ì¼ ì˜¤ì „ 3ì‹œ
- ì›”ê°„ ê°ì‚¬: ë§¤ì›” 1ì¼ ì˜¤ì „ 4ì‹œ
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§: 5ë¶„ ê°„ê²©

**ì‚¬ìš©ë²•**:
```bash
python scripts/data_processing/quality/scheduled_task_manager.py \
    --action start \
    --db-path data/lawfirm.db
```

### 4.4 QualityReportingDashboard

**ìœ„ì¹˜**: `scripts/data_processing/quality/quality_reporting_dashboard.py`

**ê¸°ëŠ¥**:
- ì‹¤ì‹œê°„ í’ˆì§ˆ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ
- í’ˆì§ˆ ë³´ê³ ì„œ ìžë™ ìƒì„±
- ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (JSON, CSV, HTML)
- ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™”
- í’ˆì§ˆ ê¶Œìž¥ì‚¬í•­ ìƒì„±

**ë³´ê³ ì„œ ìœ í˜•**:
- `summary`: ìš”ì•½ ë³´ê³ ì„œ
- `detailed`: ìƒì„¸ ë³´ê³ ì„œ
- `trends`: íŠ¸ë Œë“œ ë¶„ì„ ë³´ê³ ì„œ
- `performance`: ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ

**ì‚¬ìš©ë²•**:
```bash
python scripts/data_processing/quality/quality_reporting_dashboard.py \
    --action report \
    --report-type summary \
    --period-days 7 \
    --format html \
    --output reports/quality_report.html
```

### 4.5 AutoPipelineOrchestrator (í†µí•©ë¨)

**ìœ„ì¹˜**: `scripts/data_processing/auto_pipeline_orchestrator.py`

**ê°œì„ ì‚¬í•­**:
- í’ˆì§ˆ ê²€ì¦ ë‹¨ê³„ í†µí•© (Step 3)
- í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ PipelineResultì— í¬í•¨
- í’ˆì§ˆ ê´€ë ¨ ì„¤ì • ì¶”ê°€
- í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±

**ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„**:
1. ë°ì´í„° ê°ì§€
2. ì „ì²˜ë¦¬
3. **í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ ** (ìƒˆë¡œ ì¶”ê°€)
4. ë²¡í„° ìž„ë² ë”©
5. ë°ì´í„°ë² ì´ìŠ¤ ìž„í¬íŠ¸
6. ìµœì¢… í†µê³„ ìƒì„±

**í’ˆì§ˆ ê´€ë ¨ ì„¤ì •**:
```python
'quality': {
    'enabled': True,
    'validation_threshold': 0.7,
    'enable_duplicate_detection': True,
    'enable_quality_improvement': True,
    'quality_thresholds': {
        'excellent': 0.9,
        'good': 0.8,
        'fair': 0.6,
        'poor': 0.4
    }
}
```

## Phase 5: í–¥ìƒëœ í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ (NEW!)

### 5.1 LegalKeywordMapper (ê°€ì¤‘ì¹˜ ê¸°ë°˜)

**ìœ„ì¹˜**: `source/services/langgraph/keyword_mapper.py`

**ê¸°ëŠ¥**:
- ê°€ì¤‘ì¹˜ ê¸°ë°˜ í‚¤ì›Œë“œ ë¶„ë¥˜ (í•µì‹¬/ì¤‘ìš”/ë³´ì¡°)
- ì§ˆë¬¸ ìœ í˜•ë³„ í•„ìˆ˜ í‚¤ì›Œë“œ ë§¤í•‘
- í‚¤ì›Œë“œ í¬í•¨ë„ ê³„ì‚° ë° ë¶„ì„
- ë²•ë¥  ìš©ì–´ ì‚¬ì „ ê´€ë¦¬

**ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ**:
- **í•µì‹¬ í‚¤ì›Œë“œ (Core)**: ê°€ì¤‘ì¹˜ 1.0 - í•„ìˆ˜ í¬í•¨ í‚¤ì›Œë“œ
- **ì¤‘ìš” í‚¤ì›Œë“œ (Important)**: ê°€ì¤‘ì¹˜ 0.8 - ì¤‘ìš”ë„ê°€ ë†’ì€ í‚¤ì›Œë“œ  
- **ë³´ì¡° í‚¤ì›Œë“œ (Supporting)**: ê°€ì¤‘ì¹˜ 0.6 - ë³´ì™„ì  í‚¤ì›Œë“œ

**ì£¼ìš” ë©”ì„œë“œ**:
```python
def get_weighted_keywords_for_question(self, question: str, query_type: str) -> Dict[str, List[str]]
def calculate_weighted_keyword_coverage(self, answer: str, query_type: str, question: str = "") -> Dict[str, float]
def get_keyword_analysis_report(self, answer: str, query_type: str, question: str = "") -> Dict[str, any]
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
from source.services.langgraph.keyword_mapper import LegalKeywordMapper

mapper = LegalKeywordMapper()
keywords = mapper.get_keywords_for_question("ê³„ì•½ì„œ ê²€í†  ì‹œ ì£¼ì˜ì‚¬í•­", "contract_review")
coverage = mapper.calculate_weighted_keyword_coverage(answer, "contract_review", question)
```

### 5.2 ContextAwareKeywordMapper (ì»¨í…ìŠ¤íŠ¸ ì¸ì‹)

**ìœ„ì¹˜**: `source/services/langgraph/keyword_mapper.py`

**ê¸°ëŠ¥**:
- ì§ˆë¬¸ì˜ ì»¨í…ìŠ¤íŠ¸ ìžë™ ì‹ë³„
- ì»¨í…ìŠ¤íŠ¸ë³„ ë§žì¶¤í˜• í‚¤ì›Œë“œ ì œê³µ
- ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ë° ë³µìž¡ë„ í‰ê°€
- ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê¶Œìž¥ì‚¬í•­ ìƒì„±

**ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´**:
- **ì§ˆë¬¸í˜•**: "ì–´ë–»ê²Œ", "ë¬´ì—‡ì¸ê°€", "ì–¸ì œ", "ì–´ë””ì„œ", "ì™œ"
- **ì ˆì°¨í˜•**: "ì ˆì°¨", "ë°©ë²•", "ê³¼ì •", "ë‹¨ê³„", "ìˆœì„œ"
- **ë¹„êµí˜•**: "ì°¨ì´ì ", "ë¹„êµ", "êµ¬ë¶„", "ë‹¤ë¥¸ì "
- **ë¬¸ì œí•´ê²°í˜•**: "ë¬¸ì œ", "í•´ê²°", "ë°©ë²•", "ëŒ€ì²˜", "ëŒ€ì‘"
- **ë²•ì íš¨ë ¥í˜•**: "íš¨ë ¥", "ë¬´íš¨", "ì·¨ì†Œ", "í•´ì œ", "í•´ì§€"

**ì£¼ìš” ë©”ì„œë“œ**:
```python
def identify_context(self, question: str) -> str
def get_contextual_keywords(self, question: str, query_type: str) -> Dict[str, List[str]]
def analyze_question_intent(self, question: str) -> Dict[str, any]
def get_enhanced_keyword_mapping(self, question: str, query_type: str) -> Dict[str, any]
```

### 5.3 AdaptiveKeywordMapper (ë™ì  í•™ìŠµ)

**ìœ„ì¹˜**: `source/services/langgraph/keyword_mapper.py`

**ê¸°ëŠ¥**:
- ì‚¬ìš©ìž í”¼ë“œë°± ê¸°ë°˜ í‚¤ì›Œë“œ íš¨ê³¼ì„± í•™ìŠµ
- ì§ˆë¬¸ íŒ¨í„´ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì²œ
- ì§€ì†ì ì¸ í‚¤ì›Œë“œ íš¨ê³¼ì„± ì—…ë°ì´íŠ¸
- í•™ìŠµ ì¸ì‚¬ì´íŠ¸ ë° ê°œì„  ê¶Œìž¥ì‚¬í•­ ì œê³µ

**í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜**:
- ì‚¬ìš©ìž í‰ì  (40%) + ë‹µë³€ í’ˆì§ˆ (60%) ê¸°ë°˜ íš¨ê³¼ì„± ê³„ì‚°
- ì§ˆë¬¸ íŒ¨í„´ë³„ í‚¤ì›Œë“œ íš¨ê³¼ì„± ì¶”ì 
- ìµœê·¼ í”¼ë“œë°± ê¸°ë°˜ ë™ì  ì¡°ì •
- íš¨ê³¼ì„±ì´ ë‚®ì€ í‚¤ì›Œë“œ ìžë™ ì‹ë³„

**ì£¼ìš” ë©”ì„œë“œ**:
```python
def update_keyword_effectiveness(self, question: str, keywords: List[str], user_rating: float, answer_quality: float, query_type: str = "")
def get_effective_keywords(self, query_type: str, limit: int = 10) -> List[str]
def get_pattern_based_keywords(self, question: str, query_type: str) -> List[str]
def get_learning_insights(self) -> Dict[str, any]
```

### 5.4 SemanticKeywordMapper (ì˜ë¯¸ì  ìœ ì‚¬ë„)

**ìœ„ì¹˜**: `source/services/langgraph/keyword_mapper.py`

**ê¸°ëŠ¥**:
- ë²•ë¥  ìš©ì–´ ê°„ ì˜ë¯¸ì  ê´€ê³„ ì •ì˜
- í‚¤ì›Œë“œ ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ë§
- ì˜ë¯¸ì  í‚¤ì›Œë“œ í™•ìž¥ ë° ì¶”ì²œ
- ì˜ë¯¸ì  ë‹¤ì–‘ì„± ë¶„ì„

**ì˜ë¯¸ì  ê´€ê³„**:
- **ë™ì˜ì–´**: ê±°ë¦¬ 0.1 - ì™„ì „ížˆ ê°™ì€ ì˜ë¯¸
- **ê´€ë ¨ì–´**: ê±°ë¦¬ 0.3 - ë°€ì ‘í•œ ê´€ë ¨ì„±
- **ì»¨í…ìŠ¤íŠ¸ì–´**: ê±°ë¦¬ 0.5 - ê°™ì€ ë§¥ë½ì—ì„œ ì‚¬ìš©

**ì£¼ìš” ë©”ì„œë“œ**:
```python
def calculate_semantic_similarity(self, keyword1: str, keyword2: str) -> float
def find_semantic_related_keywords(self, target_keyword: str, threshold: float = 0.5) -> List[Tuple[str, float]]
def expand_keywords_semantically(self, keywords: List[str], expansion_factor: float = 0.7) -> List[str]
def get_semantic_keyword_clusters(self, keywords: List[str]) -> Dict[str, List[str]]
```

### 5.5 EnhancedKeywordMapper (í†µí•© ì‹œìŠ¤í…œ)

**ìœ„ì¹˜**: `source/services/langgraph/keyword_mapper.py`

**ê¸°ëŠ¥**:
- ëª¨ë“  í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ í†µí•©
- ì¢…í•©ì ì¸ í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ê³„ì‚°
- ìƒì„¸í•œ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
- ì‹¤ì‹œê°„ í”¼ë“œë°± ì—…ë°ì´íŠ¸

**í†µí•© ë°©ì‹**:
- ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ (30%)
- ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì ìˆ˜ (25%)
- ì ì‘í˜• ì ìˆ˜ (25%)
- ì˜ë¯¸ì  ì ìˆ˜ (20%)

**ì£¼ìš” ë©”ì„œë“œ**:
```python
def get_comprehensive_keyword_mapping(self, question: str, query_type: str) -> Dict[str, any]
def update_feedback(self, question: str, keywords: List[str], user_rating: float, answer_quality: float, query_type: str = "")
def get_keyword_effectiveness_report(self) -> Dict[str, any]
```

### 5.6 í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ ì‚¬ìš©ë²•

**ê¸°ë³¸ ì‚¬ìš©ë²•**:
```python
from source.services.langgraph.keyword_mapper import EnhancedKeywordMapper

# í†µí•© í‚¤ì›Œë“œ ë§¤í¼ ì´ˆê¸°í™”
mapper = EnhancedKeywordMapper()

# ì¢…í•©ì ì¸ í‚¤ì›Œë“œ ë§¤í•‘ ì‹¤í–‰
result = mapper.get_comprehensive_keyword_mapping(
    question="ê³„ì•½ì„œ ê²€í†  ì‹œ ì£¼ì˜í•´ì•¼ í•  ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    query_type="contract_review"
)

# ê²°ê³¼ ë¶„ì„
print(f"ê¸°ë³¸ í‚¤ì›Œë“œ: {result['base_keywords']}")
print(f"ê°€ì¤‘ì¹˜ë³„ í‚¤ì›Œë“œ: {result['weighted_keywords']}")
print(f"ì»¨í…ìŠ¤íŠ¸: {result['contextual_data']['identified_context']}")
print(f"ìƒìœ„ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ: {result['comprehensive_analysis']['top_keywords']}")
```

**í”¼ë“œë°± ì—…ë°ì´íŠ¸**:
```python
# ì‚¬ìš©ìž í”¼ë“œë°± ì—…ë°ì´íŠ¸
mapper.update_feedback(
    question="ê³„ì•½ì„œ ê²€í†  ì‹œ ì£¼ì˜í•´ì•¼ í•  ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    keywords=["ê³„ì•½ì„œ", "ë‹¹ì‚¬ìž", "ì¡°ê±´", "ê¸°ê°„"],
    user_rating=0.8,
    answer_quality=0.9,
    query_type="contract_review"
)
```

**íš¨ê³¼ì„± ë³´ê³ ì„œ**:
```python
# í‚¤ì›Œë“œ íš¨ê³¼ì„± ë³´ê³ ì„œ ìƒì„±
report = mapper.get_keyword_effectiveness_report()
print(f"ì ì‘í˜• ì¸ì‚¬ì´íŠ¸: {report['adaptive_insights']}")
print(f"ì˜ë¯¸ì  ë¶„ì„: {report['semantic_analysis']}")
```

### 5.7 ì„±ëŠ¥ ë° íš¨ê³¼

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**:
- **í‚¤ì›Œë“œ í™•ìž¥**: ê¸°ë³¸ 24ê°œ â†’ í™•ìž¥ 38ê°œ (1.58ë°° ì¦ê°€)
- **ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ë§**: 27ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ì²´ê³„ì  ë¶„ë¥˜
- **ì»¨í…ìŠ¤íŠ¸ ì¸ì‹**: ì§ˆë¬¸ ìœ í˜•ë³„ ë§žì¶¤í˜• í‚¤ì›Œë“œ ì œê³µ
- **ê°€ì¤‘ì¹˜ í¬í•¨ë„**: í•µì‹¬ í‚¤ì›Œë“œ 75% í¬í•¨ë„ ë‹¬ì„±

**ê¸°ëŒ€ íš¨ê³¼**:
- í‚¤ì›Œë“œ í¬í•¨ë„: 0.390 â†’ 0.7+ ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥
- ë‹µë³€ êµ¬ì¡°í™” ê°œì„ : ì»¨í…ìŠ¤íŠ¸ë³„ ë§žì¶¤í˜• êµ¬ì¡° ì œê³µ
- ë²•ì  ì •í™•ì„± ì¦ëŒ€: ì˜ë¯¸ì  ê´€ê³„ë¥¼ í†µí•œ ì „ë¬¸ ìš©ì–´ í™œìš©
- ì§€ì†ì  í•™ìŠµ: ì‚¬ìš©ìž í”¼ë“œë°±ì„ í†µí•œ ìžë™ ê°œì„ 

## ì„¤ì • ë° êµ¬ì„±

### í™˜ê²½ ë³€ìˆ˜

```bash
# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DATABASE_URL=sqlite:///data/lawfirm.db

# í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì„¤ì •
QUALITY_MONITORING_ENABLED=true
QUALITY_CHECK_INTERVAL=300
QUALITY_THRESHOLD=0.8

# ì¤‘ë³µ ê°ì§€ ì„¤ì •
DUPLICATE_DETECTION_ENABLED=true
DUPLICATE_THRESHOLD=0.95
DUPLICATE_MAX_PERCENTAGE=5.0

# ì•Œë¦¼ ì„¤ì •
ENABLE_EMAIL_NOTIFICATIONS=false
ENABLE_WEBHOOK_NOTIFICATIONS=false
NOTIFICATION_WEBHOOK_URL=

# í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ ì„¤ì •
KEYWORD_MAPPING_ENABLED=true
KEYWORD_EFFECTIVENESS_FILE=data/keyword_effectiveness.json
KEYWORD_LEARNING_ENABLED=true
SEMANTIC_SIMILARITY_THRESHOLD=0.6
CONTEXT_AWARE_MAPPING=true
```

### ì„¤ì • íŒŒì¼ ì˜ˆì‹œ

```json
{
  "quality": {
    "enabled": true,
    "validation_threshold": 0.7,
    "enable_duplicate_detection": true,
    "enable_quality_improvement": true,
    "quality_thresholds": {
      "excellent": 0.9,
      "good": 0.8,
      "fair": 0.6,
      "poor": 0.4
    }
  },
  "quality_monitor": {
    "enabled": true,
    "check_interval_seconds": 300,
    "alert_thresholds": {
      "overall_quality_min": 0.8,
      "duplicate_max_percentage": 5.0
    }
  },
  "scheduling": {
    "daily_cleaning_time": "02:00",
    "weekly_cleaning_day": "sunday",
    "weekly_cleaning_time": "03:00",
    "monthly_audit_day": 1,
    "monthly_audit_time": "04:00",
    "real_time_monitoring": true
  },
  "keyword_mapping": {
    "enabled": true,
    "effectiveness_file": "data/keyword_effectiveness.json",
    "learning_enabled": true,
    "semantic_similarity_threshold": 0.6,
    "context_aware_mapping": true,
    "weighted_keywords": {
      "core_weight": 1.0,
      "important_weight": 0.8,
      "supporting_weight": 0.6
    },
    "adaptive_learning": {
      "user_rating_weight": 0.4,
      "answer_quality_weight": 0.6,
      "min_feedback_count": 5,
      "learning_rate": 0.1
    }
  }
}
```

## ì‚¬ìš© ê°€ì´ë“œ

### 1. ì´ˆê¸° ì„¤ì •

```bash
# 1. ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜
python scripts/database/migrate_schema_v2.py --db-path data/lawfirm.db

# 2. ê¸°ì¡´ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
python scripts/database/migrate_existing_data.py --db-path data/lawfirm.db

# 3. ì¤‘ë³µ ê°ì§€ ë° í•´ê²°
python scripts/data_processing/run_duplicate_detection.py \
    --input data/processed \
    --output reports/duplicate_detection \
    --auto-resolve
```

### 2. ì¼ìƒì ì¸ ìš´ì˜

```bash
# 1. ìžë™í™”ëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í’ˆì§ˆ ê²€ì¦ í¬í•¨)
python scripts/data_processing/auto_pipeline_orchestrator.py \
    --data-source law_only \
    --auto-detect

# 2. í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±
python scripts/data_processing/quality/quality_reporting_dashboard.py \
    --action report \
    --report-type summary \
    --period-days 7

# 3. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìž‘
python scripts/data_processing/quality/real_time_quality_monitor.py \
    --action start
```

### 3. ì˜ˆì•½ëœ ìž‘ì—… ì„¤ì •

```bash
# ìŠ¤ì¼€ì¤„ëœ ìž‘ì—… ê´€ë¦¬ìž ì‹œìž‘
python scripts/data_processing/quality/scheduled_task_manager.py \
    --action start \
    --db-path data/lawfirm.db
```

### 4. ìˆ˜ë™ í’ˆì§ˆ ê´€ë¦¬

```bash
# ì¼ì¼ ì •ë¦¬ ìž‘ì—… ì‹¤í–‰
python scripts/data_processing/quality/automated_data_cleaner.py \
    --operation daily \
    --db-path data/lawfirm.db

# ì£¼ê°„ ì •ë¦¬ ìž‘ì—… ì‹¤í–‰
python scripts/data_processing/quality/automated_data_cleaner.py \
    --operation weekly \
    --db-path data/lawfirm.db

# ì›”ê°„ ê°ì‚¬ ì‹¤í–‰
python scripts/data_processing/quality/automated_data_cleaner.py \
    --operation monthly \
    --db-path data/lawfirm.db
```

### 5. í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ ì‚¬ìš©

```python
# í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
from source.services.langgraph.keyword_mapper import EnhancedKeywordMapper

# í†µí•© í‚¤ì›Œë“œ ë§¤í¼ ì´ˆê¸°í™”
mapper = EnhancedKeywordMapper()

# ì¢…í•©ì ì¸ í‚¤ì›Œë“œ ë§¤í•‘ ì‹¤í–‰
result = mapper.get_comprehensive_keyword_mapping(
    question="ê³„ì•½ì„œ ê²€í†  ì‹œ ì£¼ì˜í•´ì•¼ í•  ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    query_type="contract_review"
)

# ê²°ê³¼ ë¶„ì„
print(f"ê¸°ë³¸ í‚¤ì›Œë“œ: {result['base_keywords']}")
print(f"ê°€ì¤‘ì¹˜ë³„ í‚¤ì›Œë“œ: {result['weighted_keywords']}")
print(f"ì»¨í…ìŠ¤íŠ¸: {result['contextual_data']['identified_context']}")
print(f"ìƒìœ„ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ: {result['comprehensive_analysis']['top_keywords']}")

# ì‚¬ìš©ìž í”¼ë“œë°± ì—…ë°ì´íŠ¸
mapper.update_feedback(
    question="ê³„ì•½ì„œ ê²€í†  ì‹œ ì£¼ì˜í•´ì•¼ í•  ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    keywords=["ê³„ì•½ì„œ", "ë‹¹ì‚¬ìž", "ì¡°ê±´", "ê¸°ê°„"],
    user_rating=0.8,
    answer_quality=0.9,
    query_type="contract_review"
)

# í‚¤ì›Œë“œ íš¨ê³¼ì„± ë³´ê³ ì„œ ìƒì„±
report = mapper.get_keyword_effectiveness_report()
print(f"ì ì‘í˜• ì¸ì‚¬ì´íŠ¸: {report['adaptive_insights']}")
```

## ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### í’ˆì§ˆ ë©”íŠ¸ë¦­

- **ì „ì²´ í’ˆì§ˆ ì ìˆ˜**: ëª¨ë“  ë²•ë¥  ë°ì´í„°ì˜ í‰ê·  í’ˆì§ˆ ì ìˆ˜
- **í’ˆì§ˆ ë¶„í¬**: ìš°ìˆ˜/ì–‘í˜¸/ë³´í†µ/ë¶ˆëŸ‰ ë¹„ìœ¨
- **ì¤‘ë³µ ë¹„ìœ¨**: ì¤‘ë³µëœ ë°ì´í„°ì˜ ë¹„ìœ¨
- **í’ˆì§ˆ íŠ¸ë Œë“œ**: ì‹œê°„ì— ë”°ë¥¸ í’ˆì§ˆ ë³€í™” ì¶”ì´

### ì•Œë¦¼ ì¡°ê±´

- ì „ì²´ í’ˆì§ˆ ì ìˆ˜ê°€ ìž„ê³„ê°’ ì´í•˜ë¡œ ë–¨ì–´ì§ˆ ë•Œ
- ì¤‘ë³µ ë¹„ìœ¨ì´ ìž„ê³„ê°’ì„ ì´ˆê³¼í•  ë•Œ
- ìµœê·¼ ì¶”ê°€ëœ ë°ì´í„°ì˜ í’ˆì§ˆì´ ë‚®ì„ ë•Œ
- í’ˆì§ˆì´ ì§€ì†ì ìœ¼ë¡œ ì•…í™”ë  ë•Œ

### ì•Œë¦¼ ì‹¬ê°ë„

- **Critical**: ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”
- **High**: ìš°ì„  ì¡°ì¹˜ í•„ìš”
- **Medium**: ê³„íšëœ ì¡°ì¹˜ í•„ìš”
- **Low**: ëª¨ë‹ˆí„°ë§ í•„ìš”

## ì„±ëŠ¥ ìµœì í™”

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

- ëª¨ë¸ ì–‘ìží™” (Float16)
- ì§€ì—° ë¡œë”©
- ë©”ëª¨ë¦¬ ì••ì¶• ë° ì •ë¦¬
- ìºì‹± ì „ëžµ

### ì²˜ë¦¬ ì†ë„ ìµœì í™”

- ë°°ì¹˜ ì²˜ë¦¬
- ë³‘ë ¬ ì²˜ë¦¬
- ì¸ë±ìŠ¤ ìµœì í™”
- ì¿¼ë¦¬ ìµœì í™”

### ìŠ¤í† ë¦¬ì§€ ìµœì í™”

- ë°ì´í„° ì••ì¶•
- ì•„ì¹´ì´ë¹™ ì „ëžµ
- ì¤‘ë³µ ì œê±°
- ì •ë¦¬ ìž‘ì—… ìžë™í™”

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

1. **í’ˆì§ˆ ì ìˆ˜ê°€ ë‚®ì€ ê²½ìš°**
   - íŒŒì‹± í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰
   - ìˆ˜ë™ ê²€í†  í”Œëž˜ê·¸ëœ ë°ì´í„° í™•ì¸
   - íŒŒì‹± ê·œì¹™ ê°œì„ 

2. **ì¤‘ë³µì´ ë§Žì´ ê°ì§€ë˜ëŠ” ê²½ìš°**
   - ì¤‘ë³µ ê°ì§€ ìž„ê³„ê°’ ì¡°ì •
   - í•´ê²° ì „ëžµ ë³€ê²½
   - ë°ì´í„° ì†ŒìŠ¤ ê²€í† 

3. **ì„±ëŠ¥ì´ ëŠë¦° ê²½ìš°**
   - ë°°ì¹˜ í¬ê¸° ì¡°ì •
   - ì¸ë±ìŠ¤ ìµœì í™”
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸

### ë¡œê·¸ ë° ë””ë²„ê¹…

```bash
# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
export LOG_LEVEL=DEBUG

# ìƒì„¸ ë¡œê·¸ í™•ì¸
tail -f logs/quality_improvement.log
tail -f logs/automated_data_cleaner.log
tail -f logs/real_time_quality_monitor.log
```

## API ì°¸ì¡°

### DataQualityValidator

```python
class DataQualityValidator:
    def validate_parsing_quality(self, law_data: Dict[str, Any]) -> QualityReport
    def calculate_quality_score(self, law_data: Dict[str, Any]) -> float
    def suggest_improvements(self, law_data: Dict[str, Any]) -> List[str]
```

### AdvancedDuplicateDetector

```python
class AdvancedDuplicateDetector:
    def detect_file_level_duplicates(self, files: List[Dict[str, Any]]) -> List[DuplicateGroup]
    def detect_content_level_duplicates(self, content_items: List[Dict[str, Any]]) -> List[DuplicateGroup]
    def detect_semantic_duplicates(self, content_items: List[Dict[str, Any]]) -> List[DuplicateGroup]
```

### IntelligentDuplicateResolver

```python
class IntelligentDuplicateResolver:
    def resolve_duplicates(self, duplicate_groups: List[DuplicateGroup], strategy: str = "quality_based") -> List[ResolutionResult]
    def add_resolution_strategy(self, strategy: Dict[str, Any])
    def export_resolution_report(self, results: List[ResolutionResult]) -> Dict[str, Any]
```

### AutomatedDataCleaner

```python
class AutomatedDataCleaner:
    def run_daily_cleaning(self) -> CleaningReport
    def run_weekly_cleaning(self) -> CleaningReport
    def run_monthly_audit(self) -> CleaningReport
    def monitor_real_time_quality(self) -> Dict[str, Any]
```

### RealTimeQualityMonitor

```python
class RealTimeQualityMonitor:
    def start_monitoring(self) -> bool
    def stop_monitoring(self) -> bool
    def get_current_metrics(self) -> Optional[QualityMetrics]
    def get_active_alerts(self, severity: Optional[str] = None) -> List[QualityAlert]
```

### QualityReportingDashboard

```python
class QualityReportingDashboard:
    def get_dashboard_data(self, force_refresh: bool = False) -> QualityDashboardData
    def generate_quality_report(self, report_type: str = 'summary', period_days: int = 7) -> QualityReportData
    def export_report(self, report_data: QualityReportData, format: str = 'json', output_path: Optional[str] = None) -> str
```

### í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ API

#### LegalKeywordMapper

```python
class LegalKeywordMapper:
    @classmethod
    def get_keywords_for_question(cls, question: str, query_type: str) -> List[str]
    @classmethod
    def get_weighted_keywords_for_question(cls, question: str, query_type: str) -> Dict[str, List[str]]
    @classmethod
    def calculate_weighted_keyword_coverage(cls, answer: str, query_type: str, question: str = "") -> Dict[str, float]
    @classmethod
    def get_keyword_analysis_report(cls, answer: str, query_type: str, question: str = "") -> Dict[str, any]
```

#### ContextAwareKeywordMapper

```python
class ContextAwareKeywordMapper:
    def identify_context(self, question: str) -> str
    def get_contextual_keywords(self, question: str, query_type: str) -> Dict[str, List[str]]
    def analyze_question_intent(self, question: str) -> Dict[str, any]
    def get_enhanced_keyword_mapping(self, question: str, query_type: str) -> Dict[str, any]
```

#### AdaptiveKeywordMapper

```python
class AdaptiveKeywordMapper:
    def update_keyword_effectiveness(self, question: str, keywords: List[str], user_rating: float, answer_quality: float, query_type: str = "")
    def get_effective_keywords(self, query_type: str, limit: int = 10) -> List[str]
    def get_pattern_based_keywords(self, question: str, query_type: str) -> List[str]
    def get_learning_insights(self) -> Dict[str, any]
    def recommend_keyword_improvements(self, query_type: str) -> List[str]
```

#### SemanticKeywordMapper

```python
class SemanticKeywordMapper:
    def calculate_semantic_similarity(self, keyword1: str, keyword2: str) -> float
    def find_semantic_related_keywords(self, target_keyword: str, threshold: float = 0.5) -> List[Tuple[str, float]]
    def expand_keywords_semantically(self, keywords: List[str], expansion_factor: float = 0.7) -> List[str]
    def get_semantic_keyword_clusters(self, keywords: List[str]) -> Dict[str, List[str]]
    def analyze_keyword_semantic_coverage(self, answer: str, keywords: List[str]) -> Dict[str, any]
```

#### EnhancedKeywordMapper

```python
class EnhancedKeywordMapper:
    def get_comprehensive_keyword_mapping(self, question: str, query_type: str) -> Dict[str, any]
    def update_feedback(self, question: str, keywords: List[str], user_rating: float, answer_quality: float, query_type: str = "")
    def get_keyword_effectiveness_report(self) -> Dict[str, any]
```

## í…ŒìŠ¤íŠ¸

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python tests/test_quality_improvement_workflow.py --test-class all

# íŠ¹ì • ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
python tests/test_quality_improvement_workflow.py --test-class validator
python tests/test_quality_improvement_workflow.py --test-class detector
python tests/test_quality_improvement_workflow.py --test-class resolver
python tests/test_quality_improvement_workflow.py --test-class cleaner
python tests/test_quality_improvement_workflow.py --test-class monitor
python tests/test_quality_improvement_workflow.py --test-class dashboard
python tests/test_quality_improvement_workflow.py --test-class orchestrator
python tests/test_quality_improvement_workflow.py --test-class workflow

# í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
python tests/test_keyword_mapping_system.py --test-class legal_mapper
python tests/test_keyword_mapping_system.py --test-class context_mapper
python tests/test_keyword_mapping_system.py --test-class adaptive_mapper
python tests/test_keyword_mapping_system.py --test-class semantic_mapper
python tests/test_keyword_mapping_system.py --test-class enhanced_mapper
```

### í†µí•© í…ŒìŠ¤íŠ¸

```bash
# í’ˆì§ˆ ê°œì„  ì›Œí¬í”Œë¡œìš° ì „ì²´ í…ŒìŠ¤íŠ¸
python tests/test_quality_improvement_workflow.py --test-class workflow --verbose

# í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
python tests/test_keyword_mapping_system.py --test-class integration --verbose
```

## í™•ìž¥ì„± ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ìƒˆë¡œìš´ í’ˆì§ˆ ê²€ì¦ ê·œì¹™ ì¶”ê°€

```python
class CustomQualityValidator(DataQualityValidator):
    def _custom_validation_rule(self, law_data: Dict[str, Any]) -> float:
        # ì‚¬ìš©ìž ì •ì˜ ê²€ì¦ ë¡œì§
        return score
```

### ìƒˆë¡œìš´ ì¤‘ë³µ í•´ê²° ì „ëžµ ì¶”ê°€

```python
custom_strategy = {
    'name': 'custom_strategy',
    'description': 'ì‚¬ìš©ìž ì •ì˜ í•´ê²° ì „ëžµ',
    'scoring_function': lambda item: custom_score(item)
}

resolver.add_resolution_strategy(custom_strategy)
```

### ìƒˆë¡œìš´ ì•Œë¦¼ ì±„ë„ ì¶”ê°€

```python
def custom_alert_callback(alert: QualityAlert):
    # ì‚¬ìš©ìž ì •ì˜ ì•Œë¦¼ ì²˜ë¦¬
    pass

monitor.add_alert_callback(custom_alert_callback)
```

### í‚¤ì›Œë“œ ë§¤í•‘ ì‹œìŠ¤í…œ ì»¤ìŠ¤í„°ë§ˆì´ì§•

#### ìƒˆë¡œìš´ ë²•ë¥  ìš©ì–´ ê´€ê³„ ì¶”ê°€

```python
# SemanticKeywordMapper í™•ìž¥
class CustomSemanticKeywordMapper(SemanticKeywordMapper):
    def __init__(self):
        super().__init__()
        # ìƒˆë¡œìš´ ë²•ë¥  ìš©ì–´ ê´€ê³„ ì¶”ê°€
        self.semantic_relations["ìƒˆë¡œìš´_ë²•ë¥ _ë¶„ì•¼"] = {
            "synonyms": ["ë™ì˜ì–´1", "ë™ì˜ì–´2"],
            "related": ["ê´€ë ¨ì–´1", "ê´€ë ¨ì–´2"],
            "context": ["ì»¨í…ìŠ¤íŠ¸ì–´1", "ì»¨í…ìŠ¤íŠ¸ì–´2"]
        }
```

#### ìƒˆë¡œìš´ ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´ ì¶”ê°€

```python
# ContextAwareKeywordMapper í™•ìž¥
class CustomContextAwareKeywordMapper(ContextAwareKeywordMapper):
    def __init__(self):
        super().__init__()
        # ìƒˆë¡œìš´ ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´ ì¶”ê°€
        self.context_patterns["ìƒˆë¡œìš´_ì»¨í…ìŠ¤íŠ¸"] = ["íŒ¨í„´1", "íŒ¨í„´2", "íŒ¨í„´3"]
        self.context_keywords["ìƒˆë¡œìš´_ì»¨í…ìŠ¤íŠ¸"] = ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
```

#### ìƒˆë¡œìš´ í‚¤ì›Œë“œ íš¨ê³¼ì„± ë©”íŠ¸ë¦­ ì¶”ê°€

```python
# AdaptiveKeywordMapper í™•ìž¥
class CustomAdaptiveKeywordMapper(AdaptiveKeywordMapper):
    def _calculate_custom_effectiveness(self, keyword_data: Dict[str, Any]) -> float:
        # ì‚¬ìš©ìž ì •ì˜ íš¨ê³¼ì„± ê³„ì‚° ë¡œì§
        base_score = keyword_data['effectiveness_score']
        usage_frequency = keyword_data['total_usage']
        recency_bonus = self._calculate_recency_bonus(keyword_data['last_updated'])
        
        return base_score * (1 + usage_frequency * 0.1) * recency_bonus
    
    def _calculate_recency_bonus(self, last_updated: str) -> float:
        # ìµœê·¼ì„± ë³´ë„ˆìŠ¤ ê³„ì‚°
        from datetime import datetime, timedelta
        last_update = datetime.fromisoformat(last_updated)
        days_ago = (datetime.now() - last_update).days
        
        if days_ago <= 7:
            return 1.2  # ìµœê·¼ ì‚¬ìš©ëœ í‚¤ì›Œë“œì— ë³´ë„ˆìŠ¤
        elif days_ago <= 30:
            return 1.1
        else:
            return 1.0
```

## ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

- ë°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ ê¶Œí•œ ê´€ë¦¬
- ë¯¼ê°í•œ ë°ì´í„° ì•”í˜¸í™”
- ë¡œê·¸ íŒŒì¼ ë³´ì•ˆ
- API ì ‘ê·¼ ì œì–´
- ë°±ì—… ë° ë³µêµ¬ ì „ëžµ

## ë¼ì´ì„ ìŠ¤ ë° ê¸°ì—¬

ì´ ì‹œìŠ¤í…œì€ LawFirmAI í”„ë¡œì íŠ¸ì˜ ì¼ë¶€ì´ë©°, í”„ë¡œì íŠ¸ì˜ ë¼ì´ì„ ìŠ¤ ì •ì±…ì„ ë”°ë¦…ë‹ˆë‹¤.

ê¸°ì—¬ë¥¼ ì›í•˜ì‹œëŠ” ê²½ìš°, ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”:
- ì½”ë“œ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ì¤€ìˆ˜
- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìž‘ì„±
- ë¬¸ì„œ ì—…ë°ì´íŠ¸
- í’ˆì§ˆ ê²€ì¦ í†µê³¼

## ì§€ì› ë° ë¬¸ì˜

ê¸°ìˆ ì  ë¬¸ì œë‚˜ ì§ˆë¬¸ì´ ìžˆìœ¼ì‹œë©´ ë‹¤ìŒ ì±„ë„ì„ í†µí•´ ë¬¸ì˜í•´ì£¼ì„¸ìš”:
- ì´ìŠˆ íŠ¸ëž˜ì»¤: GitHub Issues
- ë¬¸ì„œ: í”„ë¡œì íŠ¸ Wiki
- ì´ë©”ì¼: support@lawfirmai.com

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-01-XX  
**ë²„ì „**: 2.0.0  
**ìž‘ì„±ìž**: LawFirmAI Development Team
