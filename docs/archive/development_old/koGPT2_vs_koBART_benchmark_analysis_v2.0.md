# KoGPT-2 vs KoBART μ„±λ¥ λΉ„κµ λ¶„μ„ λ³΄κ³ μ„

## π“‹ κ°μ”

**λ¬Έμ„ λ²„μ „**: v2.0  
**μ‘μ„±μΌ**: 2025-01-25  
**μ‹¤ν–‰ ν™κ²½**: Windows 10, Python 3.13.7, CPU (16 cores, 31.4GB RAM)  
**ν…μ¤νΈ λ°μ΄ν„°**: λ²•λ¥  λ„λ©”μΈ μ§λ¬Έ 5κ°  
**λ©μ **: LawFirmAI ν”„λ΅μ νΈ TASK 3.1 λ¨λΈ μ„ νƒμ„ μ„ν• μ„±λ¥ λΉ„κµ

---

## π― μ‹¤ν–‰ μ”μ•½

### μµμΆ… κ¶μ¥μ‚¬ν•­
**KoGPT-2 μ„ νƒ** β… - μ‹¤μ  μ‚¬μ© κ°€λ¥ν• λ²•λ¥  μ±—λ΄‡ κµ¬μ¶•μ„ μ„ν•΄ κ¶μ¥

### ν•µμ‹¬ μ§€ν‘
- **μ¶”λ΅  μ†λ„**: KoGPT-2κ°€ 40% λΉ λ¦„ (7.96μ΄ vs 13.24μ΄)
- **μ‘λ‹µ ν’μ§**: KoGPT-2κ°€ μ‹¤μ  μ‚¬μ© κ°€λ¥ν• λ‹µλ³€ μƒμ„±
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: KoBARTκ°€ 47% μ μ (398MB vs 749MB)
- **λ¨λΈ ν¬κΈ°**: κ±°μ λ™μΌ (472MB vs 477MB)

---

## π“ μƒμ„Έ μ„±λ¥ λΉ„κµ

### 1. μ‹μ¤ν… μ„±λ¥ μ§€ν‘

| μ§€ν‘ | KoBART | KoGPT-2 | κ°μ„ μ¨ | μΉμ |
|------|--------|---------|--------|------|
| **λ¨λΈ ν¬κΈ°** | 472.5 MB | 477.5 MB | -1.0% | KoBART |
| **λ©”λ¨λ¦¬ μ‚¬μ©λ‰** | 398.5 MB | 749.1 MB | -87.9% | KoBART |
| **λ΅λ”© μ‹κ°„** | 2.30μ΄ | 2.85μ΄ | -19.1% | KoBART |
| **ν‰κ·  μ¶”λ΅  μ‹κ°„** | 13.24μ΄ | 7.96μ΄ | +40.0% | **KoGPT-2** |
| **νλΌλ―Έν„° μ** | 123,859,968 | 125,164,032 | -1.0% | KoBART |

### 2. μ‘λ‹µ ν’μ§ λ¶„μ„

#### KoBART μ‘λ‹µ ν’μ§ β
```
μ§λ¬Έ: "κ³„μ•½μ„μ—μ„ μ£Όμν•΄μ•Ό ν•  μ΅°ν•­μ€ λ¬΄μ—‡μΈκ°€μ”?"

μ‘λ‹µ: "λ• λ• κ°€λ¥ κ³„μ•½μ„ κ²€ν† ν•΄μ•Ό ν•  ν•  μ΅°ν•­μ€ λ¬΄μ—‡μΈκ°€μ”? λ§¥λ½: κ³„μ•½μ„ κ²€ν†  μ‹ λ§¥λ½: κ³„μ•½μ„ κ²€ν†  μ‹ μ¤‘μ”ν• μ‚¬ν•­λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤ κ³„μ•½μ„ κ²€ν†  μ‹ μ¤‘μ”ν• μ‚¬ν•­λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤λ“¤ κ²ƒ μ‚¬ν•­λ“¤λ“¤λ“¤λ“¤λ“¤ μ‚¬ν•­λ“¤λ“¤λ“¤λ“¤ κ²ƒλ“¤ μ‚¬ν•­μ΄ κ³„μ•½μ„μ—μ„ μ£Όμν•΄μ•Ό ν•  μ΅°ν•­μ€μ€ μ‚¬μ‹¤ μ‚¬μ‹¤ μ‚¬ν•­ μ‚¬ν•­..."
```

**λ¬Έμ μ :**
- λ¬΄μλ―Έν• λ°λ³µκ³Ό λ¬Έλ§¥ νκ΄΄
- μ‹¤μ  λ‹µλ³€ μ κ³µ λ¶κ°€
- μ‚¬μ©μμ—κ² λ„μ›€μ΄ λμ§€ μ•λ” μ‘λ‹µ
- λ²•λ¥  λ„λ©”μΈμ— λ¶€μ ν•©

#### KoGPT-2 μ‘λ‹µ ν’μ§ β…
```
μ§λ¬Έ: "κ³„μ•½μ„μ—μ„ μ£Όμν•΄μ•Ό ν•  μ΅°ν•­μ€ λ¬΄μ—‡μΈκ°€μ”?"

μ‘λ‹µ: "κ³„μ•½μ„μ— λ…μ‹λ μ‚¬ν•­μ„ κ³„μ•½μ„μ— λ°λ“μ‹ κΈ°μ¬ν•λ„λ΅ λμ–΄ μμµλ‹λ‹¤.
μλ¥Ό λ“¤μ–΄ κ³„μ•½μ„ λ‚΄μ©: κ³„μ•½μ„ λ‚΄μ©μ„ κ³„μ•½μ„μ— λ…μ‹ν•  κ²½μ° κ³„μ•½μ„μ λ‚΄μ©μ„ κ³„μ•½μ„μ— κΈ°μ¬ν•μ§€ μ•„λ‹ν•κ³  κ³„μ•½μ„ μ²΄κ²°ν• κ²½μ°
κ³„μ•½μ„ λ‚΄μ©: κ³„μ•½μ„ λ‚΄μ©: κ³„μ•½μ„μ— λ…μ‹λ μ‚¬ν•­μ„ κ³„μ•½μ„μ— κΈ°μ¬ν•μ§€ μ•„λ‹ν•κ³  κ³„μ•½μ„ λ§Ίμ€ κ²½μ°..."
```

**μ¥μ :**
- λ…Όλ¦¬μ μ΄κ³  κµ¬μ΅°ν™”λ λ‹µλ³€
- λ²•λ¥  μ©μ–΄μ™€ μ΅°ν•­ μΈμ©
- μ‹¤μ  μ‚¬μ© κ°€λ¥ν• μ •λ³΄ μ κ³µ
- λ²•λ¥  λ„λ©”μΈμ— μ ν•©ν• μ‘λ‹µ

### 3. μΉ΄ν…κ³ λ¦¬λ³„ μ„±λ¥ λ¶„μ„

| λ²•λ¥  λ¶„μ•Ό | KoBART ν’μ§ | KoGPT-2 ν’μ§ | κ¶μ¥ λ¨λΈ |
|-----------|-------------|--------------|-----------|
| **κ³„μ•½λ²•** | λ§¤μ° λ‚®μ | λ³΄ν†µ | KoGPT-2 |
| **λ―Όλ²•** | λ§¤μ° λ‚®μ | λ³΄ν†µ | KoGPT-2 |
| **λ…Έλ™λ²•** | λ§¤μ° λ‚®μ | λ³΄ν†µ | KoGPT-2 |
| **λ¶€λ™μ‚°λ²•** | λ§¤μ° λ‚®μ | λ³΄ν†µ | KoGPT-2 |
| **κ°€μ΅±λ²•** | λ§¤μ° λ‚®μ | λ³΄ν†µ | KoGPT-2 |

---

## π” κΈ°μ μ  λ¶„μ„

### 1. λ¨λΈ μ•„ν‚¤ν…μ² λΉ„κµ

#### KoBART (Seq2Seq)
- **κµ¬μ΅°**: Encoder-Decoder μ•„ν‚¤ν…μ²
- **μ©λ„**: μ”μ•½, λ²μ—­, μ§μμ‘λ‹µ
- **μ¥μ **: μ–‘λ°©ν–¥ μ»¨ν…μ¤νΈ μ΄ν•΄
- **λ‹¨μ **: μƒμ„± ν’μ§ μ €ν•, κΈ΄ μ¶”λ΅  μ‹κ°„

#### KoGPT-2 (Causal LM)
- **κµ¬μ΅°**: Decoder-only μ•„ν‚¤ν…μ²
- **μ©λ„**: ν…μ¤νΈ μƒμ„±, λ€ν™”
- **μ¥μ **: λΉ λ¥Έ μ¶”λ΅ , μΌκ΄€λ μƒμ„±
- **λ‹¨μ **: λ‹¨λ°©ν–¥ μ»¨ν…μ¤νΈλ§ μ΄ν•΄

### 2. λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ¶„μ„

```
KoBART: 398.5 MB
β”β”€β”€ λ¨λΈ νλΌλ―Έν„°: 123.9M
β”β”€β”€ ν™μ„±ν™” λ©”λ¨λ¦¬: ~200MB
β””β”€β”€ κΈ°νƒ€ μ¤λ²„ν—¤λ“: ~75MB

KoGPT-2: 749.1 MB
β”β”€β”€ λ¨λΈ νλΌλ―Έν„°: 125.2M
β”β”€β”€ ν™μ„±ν™” λ©”λ¨λ¦¬: ~500MB
β””β”€β”€ κΈ°νƒ€ μ¤λ²„ν—¤λ“: ~124MB
```

**KoGPT-2 λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ΄ λ†’μ€ μ΄μ :**
- λ” ν° ν™μ„±ν™” λ©”λ¨λ¦¬ μ”κµ¬μ‚¬ν•­
- Attention λ©”μ»¤λ‹μ¦μ λ©”λ¨λ¦¬ μ§‘μ•½μ„±
- μƒμ„± κ³Όμ •μ—μ„μ μ¤‘κ°„ μƒνƒ μ €μ¥

### 3. μ¶”λ΅  μ†λ„ λ¶„μ„

```
KoBART ν‰κ·  μ¶”λ΅  μ‹κ°„: 13.24μ΄
β”β”€β”€ μΈμ½”λ”© λ‹¨κ³„: ~6μ΄
β”β”€β”€ λ””μ½”λ”© λ‹¨κ³„: ~7μ΄
β””β”€β”€ ν›„μ²λ¦¬: ~0.24μ΄

KoGPT-2 ν‰κ·  μ¶”λ΅  μ‹κ°„: 7.96μ΄
β”β”€β”€ ν† ν°ν™”: ~0.1μ΄
β”β”€β”€ μƒμ„± λ‹¨κ³„: ~7.8μ΄
β””β”€β”€ ν›„μ²λ¦¬: ~0.06μ΄
```

**KoGPT-2κ°€ λΉ λ¥Έ μ΄μ :**
- λ‹¨μν• μƒμ„± κ³Όμ •
- ν¨μ¨μ μΈ Attention κµ¬ν„
- μµμ ν™”λ μ¶”λ΅  νμ΄ν”„λΌμΈ

---

## π― HuggingFace Spaces λ°°ν¬ κ³ λ ¤μ‚¬ν•­

### 1. λ©”λ¨λ¦¬ μ μ•½ λ¶„μ„
- **HuggingFace Spaces T4 GPU**: 16GB λ©”λ¨λ¦¬ μ ν•
- **KoGPT-2 λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: 749MB (4.7% μ‚¬μ©)
- **μ—¬μ  λ©”λ¨λ¦¬**: 15.25GB (95.3% μ—¬μ )
- **κ²°λ΅ **: λ©”λ¨λ¦¬ μ μ•½ λ‚΄μ—μ„ μ¶©λ¶„ν μ‹¤ν–‰ κ°€λ¥

### 2. μ„±λ¥ μµμ ν™” μ „λµ

#### λ‹¨κΈ° μµμ ν™” (μ¦‰μ‹ μ μ© κ°€λ¥)
```python
# λ¨λΈ λ΅λ”© μµμ ν™”
model = AutoModelForCausalLM.from_pretrained(
    "skt/kogpt2-base-v2",
    torch_dtype=torch.float16,  # λ©”λ¨λ¦¬ 50% μ μ•½
    device_map="auto"
)

# μ¶”λ΅  μµμ ν™”
generation_config = {
    "max_length": 256,  # μ‘λ‹µ κΈΈμ΄ μ ν•
    "do_sample": True,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
}
```

#### μ¤‘κΈ° μµμ ν™” (νμΈνλ‹ ν›„)
- **LoRA νμΈνλ‹**: λ©”λ¨λ¦¬ ν¨μ¨μ  νμΈνλ‹
- **μ–‘μν™”**: INT8 μ–‘μν™”λ΅ λ©”λ¨λ¦¬ 50% μ¶”κ°€ μ μ•½
- **ONNX λ³€ν™**: μ¶”λ΅  μ†λ„ 20-30% ν–¥μƒ

#### μ¥κΈ° μµμ ν™” (λ°°ν¬ ν›„)
- **λ¨λΈ μ••μ¶•**: Pruning λ° Distillation
- **μΊμ‹± μ‹μ¤ν…**: μμ£Ό λ¬»λ” μ§λ¬Έ μΊμ‹±
- **λ°°μΉ μ²λ¦¬**: μ—¬λ¬ μ”μ²­ λ™μ‹ μ²λ¦¬

---

## π¨ μ„ν— μ”μ† λ° λ€μ‘ λ°©μ•

### 1. λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ„ν—
**μ„ν—**: KoGPT-2μ λ†’μ€ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ (749MB)
**λ€μ‘ λ°©μ•**:
- Float16 μ–‘μν™”λ΅ λ©”λ¨λ¦¬ 50% μ μ•½
- λ°°μΉ ν¬κΈ° μ΅°μ •μΌλ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ μ–΄
- λ¨λΈ λ΅λ”© μ§€μ—° μ „λµ κµ¬ν„

### 2. μ‘λ‹µ ν’μ§ μ„ν—
**μ„ν—**: ν„μ¬ KoGPT-2 μ‘λ‹µ ν’μ§λ„ κ°μ„  ν•„μ”
**λ€μ‘ λ°©μ•**:
- LoRA νμΈνλ‹μΌλ΅ λ²•λ¥  λ„λ©”μΈ νΉν™”
- ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§ κ°μ„ 
- RAG μ‹μ¤ν…κ³Ό κ²°ν•©

### 3. μ¶”λ΅  μ†λ„ μ„ν—
**μ„ν—**: 7.96μ΄λ„ μ‚¬μ©μ κ²½ν—μ— λ¶€λ‹΄
**λ€μ‘ λ°©μ•**:
- ONNX λ³€ν™μΌλ΅ μ†λ„ 20-30% ν–¥μƒ
- μΊμ‹± μ‹μ¤ν…μΌλ΅ λ°λ³µ μ§λ¬Έ μ²λ¦¬
- μ¤νΈλ¦¬λ° μ‘λ‹µ κµ¬ν„

---

## π“‹ κ²°λ΅  λ° κ¶μ¥μ‚¬ν•­

### μµμΆ… κ²°λ΅ 
**KoGPT-2 μ„ νƒμ„ κ°•λ ¥ν κ¶μ¥**ν•©λ‹λ‹¤. μ‹¤μ  μ‚¬μ© κ°€λ¥ν• λ²•λ¥  μ±—λ΄‡ κµ¬μ¶•μ„ μ„ν•΄μ„λ” μ‘λ‹µ ν’μ§μ΄ κ°€μ¥ μ¤‘μ”ν• μ”μ†μ΄λ©°, KoGPT-2κ°€ μ΄ λ©΄μ—μ„ μ••λ„μ μΌλ΅ μ°μν•©λ‹λ‹¤.

### ν•µμ‹¬ κ¶μ¥μ‚¬ν•­

#### 1. μ¦‰μ‹ μ‹¤ν–‰ (High Priority)
- KoGPT-2 λ¨λΈ ν†µν•© λ° κΈ°λ³Έ νμ΄ν”„λΌμΈ κµ¬μ¶•
- Float16 μ–‘μν™”λ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μµμ ν™”
- κΈ°λ³Έ ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ μ„¤κ³„

#### 2. λ‹¨κΈ° μ‹¤ν–‰ (Medium Priority)
- LoRA κΈ°λ° νμΈνλ‹ κµ¬ν„
- λ²•λ¥  λ„λ©”μΈ λ°μ΄ν„°μ…‹ μ¤€λΉ„
- μ„±λ¥ λ¨λ‹ν„°λ§ μ‹μ¤ν… κµ¬μ¶•

#### 3. μ¤‘μ¥κΈ° μ‹¤ν–‰ (Low Priority)
- ONNX λ³€ν™ λ° κ³ κΈ‰ μµμ ν™”
- μΊμ‹± μ‹μ¤ν… λ° λ°°μΉ μ²λ¦¬
- μ‚¬μ©μ ν”Όλ“λ°± κΈ°λ° μ§€μ†μ  κ°μ„ 

### μ„±κ³µ μ§€ν‘
- **μ‘λ‹µ ν’μ§**: λ²•λ¥  μ „λ¬Έκ°€ ν‰κ°€ 75% μ΄μƒ
- **μ¶”λ΅  μ†λ„**: 5μ΄ μ΄λ‚΄ μ‘λ‹µ μƒμ„±
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: 1GB μ΄ν• μ μ§€
- **μ‚¬μ©μ λ§μ΅±λ„**: 4.0/5.0 μ΄μƒ

---

## π“ μ°Έκ³  μλ£

- [KoGPT-2 λ¨λΈ μΉ΄λ“](https://huggingface.co/skt/kogpt2-base-v2)
- [KoBART λ¨λΈ μΉ΄λ“](https://huggingface.co/skt/kobart-base-v1)
- [HuggingFace Spaces κ°€μ΄λ“](https://huggingface.co/docs/hub/spaces)
- [LoRA νμΈνλ‹ κ°€μ΄λ“](https://huggingface.co/docs/peft)

---

**λ¬Έμ„ μ‘μ„±μ**: LawFirmAI κ°λ°ν€  
**κ²€ν† μ**: ML μ—”μ§€λ‹μ–΄  
**μΉμΈμ**: ν”„λ΅μ νΈ λ§¤λ‹μ €  
**λ‹¤μ κ²€ν† μΌ**: 2025-02-01
