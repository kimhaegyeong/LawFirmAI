# -*- coding: utf-8 -*-
"""
ê°„ë‹¨í•œ LangChain ê¸°ë°˜ Gradio ì• í”Œë¦¬ì¼€ì´ì…˜
LawFirmAI - ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸
"""

import os
import sys
import logging
import time
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# Gradio ë° LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬
import gradio as gr
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from source.data.database import DatabaseManager
from source.data.vector_store import LegalVectorStore
from prompt_manager import prompt_manager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/simple_langchain_gradio.log')
    ]
)
logger = logging.getLogger(__name__)

class LawFirmAIService:
    """LawFirmAI ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.vector_store = None
        self.embeddings = None
        self.llm = None
        self.database_manager = None
        self.initialized = False
    
    def initialize(self) -> bool:
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            logger.info("Initializing LawFirmAI services...")
            
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
            logger.info("Embeddings initialized")
            
            # LLM ì´ˆê¸°í™”
            self._initialize_llm()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
            self.database_manager = DatabaseManager()
            logger.info("Database manager initialized")
            
            # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
            self._initialize_vector_store()
            
            self.initialized = True
            logger.info("All services initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize services: {e}")
            return False
    
    def _initialize_llm(self):
        """LLM ì´ˆê¸°í™”"""
        llm_initialized = False
        
        # OpenAI ì‹œë„
        try:
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                self.llm = ChatOpenAI(
                    model_name="gpt-3.5-turbo",
                    temperature=0.7,
                    max_tokens=1000,
                    api_key=openai_api_key
                )
                logger.info("OpenAI LLM initialized")
                llm_initialized = True
        except Exception as e:
            logger.warning(f"OpenAI initialization failed: {e}")
        
        # Google AI ì‹œë„
        if not llm_initialized:
            try:
                google_api_key = os.getenv("GOOGLE_API_KEY")
                if google_api_key:
                    self.llm = ChatGoogleGenerativeAI(
                        model="gemini-pro",
                        temperature=0.7,
                        max_output_tokens=1000,
                        google_api_key=google_api_key
                    )
                    logger.info("Google AI LLM initialized")
                    llm_initialized = True
            except Exception as e:
                logger.warning(f"Google AI initialization failed: {e}")
        
        if not llm_initialized:
            self.llm = None
            logger.info("Using fallback response system (no external LLM available)")
    
    def _initialize_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”"""
        logger.info("Initializing vector store...")
        vector_store_init_start = time.time()
        
        self.vector_store = LegalVectorStore(model_name="jhgan/ko-sroberta-multitask")
        vector_store_init_time = time.time() - vector_store_init_start
        logger.info(f"Vector store initialized in {vector_store_init_time:.3f}s")
        
        # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹œë„
        project_root = Path(__file__).parent.parent
        vector_store_paths = [
            str(project_root / "data" / "embeddings" / "ml_enhanced_ko_sroberta"),
            str(project_root / "data" / "embeddings" / "ml_enhanced_bge_m3"),
            str(project_root / "data" / "embeddings" / "faiss_index")
        ]
        
        vector_store_loaded = False
        for path in vector_store_paths:
            if os.path.exists(path):
                try:
                    if os.path.isdir(path):
                        files = os.listdir(path)
                        faiss_files = [f for f in files if f.endswith('.faiss')]
                        if faiss_files:
                            faiss_file_path = os.path.join(path, faiss_files[0])
                            success = self.vector_store.load_index(faiss_file_path)
                        else:
                            success = False
                    else:
                        success = self.vector_store.load_index(path)
                    
                    if success:
                        logger.info(f"Vector store loaded successfully from {path}")
                        vector_store_loaded = True
                        break
                except Exception as e:
                    logger.warning(f"Error loading vector store from {path}: {e}")
        
        if not vector_store_loaded:
            logger.warning("No vector store could be loaded, using database search only")
    
    def search_documents(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        results = []
        
        try:
            # ë²¡í„° ì €ì¥ì†Œ ê²€ìƒ‰
            if self.vector_store:
                try:
                    similar_docs = self.vector_store.search(query, top_k)
                    for doc in similar_docs:
                        doc_info = {
                            'content': doc.get('text', '') or doc.get('content', ''),
                            'metadata': doc.get('metadata', {}),
                            'similarity': doc.get('score', 0.0),
                            'source': doc.get('metadata', {}).get('law_name', 'unknown')
                        }
                        results.append(doc_info)
                except Exception as e:
                    logger.error(f"Vector search failed: {e}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ (ë°±ì—…)
            if not results and self.database_manager:
                try:
                    assembly_results = self.database_manager.search_assembly_documents(query, top_k)
                    for result in assembly_results:
                        doc_info = {
                            'content': result.get('content', ''),
                            'metadata': {
                                'law_name': result.get('law_name', ''),
                                'article_number': result.get('article_number', ''),
                                'article_title': result.get('article_title', '')
                            },
                            'similarity': result.get('relevance_score', 0.8),
                            'source': result.get('law_name', 'assembly_database')
                        }
                        results.append(doc_info)
                except Exception as e:
                    logger.warning(f"Database search failed: {e}")
            
            # ìƒ˜í”Œ ë°ì´í„° ì œê³µ
            if not results:
                results = self._get_sample_legal_documents(query)
            
            return results
            
        except Exception as e:
            logger.error(f"Error in search_documents: {e}")
            return self._get_sample_legal_documents(query)
    
    def _get_sample_legal_documents(self, query: str) -> List[Dict[str, Any]]:
        """ìƒ˜í”Œ ë²•ë¥  ë¬¸ì„œ ì œê³µ"""
        sample_docs = [
            {
                'content': 'ë¯¼ë²• ì œ750ì¡°(ë¶ˆë²•í–‰ìœ„ì˜ ë‚´ìš©) íƒ€ì¸ì˜ ê³ ì˜ ë˜ëŠ” ê³¼ì‹¤ë¡œ ì¸í•œ ë¶ˆë²•í–‰ìœ„ë¡œ ì¸í•˜ì—¬ ì†í•´ë¥¼ ë°›ì€ ìëŠ” ê·¸ ì†í•´ë¥¼ ê°€í•œ ìì—ê²Œ ì†í•´ë°°ìƒì„ ì²­êµ¬í•  ìˆ˜ ìˆë‹¤.',
                'metadata': {
                    'law_name': 'ë¯¼ë²•',
                    'article_number': 'ì œ750ì¡°',
                    'article_title': 'ë¶ˆë²•í–‰ìœ„ì˜ ë‚´ìš©'
                },
                'similarity': 0.7,
                'source': 'ë¯¼ë²•'
            },
            {
                'content': 'ë¯¼ë²• ì œ543ì¡°(ê³„ì•½ì˜ ì„±ë¦½) ê³„ì•½ì€ ë‹¹ì‚¬ì ì¼ë°©ì´ ìƒëŒ€ë°©ì—ê²Œ ê³„ì•½ì„ ì²´ê²°í•  ì˜ì‚¬ë¥¼ í‘œì‹œí•˜ê³  ìƒëŒ€ë°©ì´ ì´ë¥¼ ìŠ¹ë‚™í•¨ìœ¼ë¡œì¨ ì„±ë¦½í•œë‹¤.',
                'metadata': {
                    'law_name': 'ë¯¼ë²•',
                    'article_number': 'ì œ543ì¡°',
                    'article_title': 'ê³„ì•½ì˜ ì„±ë¦½'
                },
                'similarity': 0.6,
                'source': 'ë¯¼ë²•'
            }
        ]
        
        # ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œë§Œ í•„í„°ë§
        filtered_docs = []
        query_lower = query.lower()
        
        for doc in sample_docs:
            content_lower = doc['content'].lower()
            metadata_lower = str(doc['metadata']).lower()
            
            if any(keyword in content_lower or keyword in metadata_lower 
                   for keyword in ['ë¯¼ë²•', 'ìƒë²•', 'í˜•ë²•', 'ê³„ì•½', 'ë¶ˆë²•í–‰ìœ„']):
                filtered_docs.append(doc)
        
        return filtered_docs[:3]
    
    def generate_response(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """ì‘ë‹µ ìƒì„±"""
        try:
            if not self.llm:
                return self._generate_fallback_response(query, context_docs)
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n\n".join([
                f"[ë¬¸ì„œ: {doc['source']}]\n{doc['content'][:500]}..."
                for doc in context_docs[:3]
            ])
            
            # ë²•ë¥  ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            legal_prompt = prompt_manager.get_current_prompt()
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
            template = f"""{legal_prompt}

ë¬¸ì„œ ë‚´ìš©:
{{context}}

ì§ˆë¬¸: {{question}}

ìœ„ì˜ ë²•ë¥  ì „ë¬¸ê°€ ì—­í• ì— ë”°ë¼ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""
            
            prompt = PromptTemplate(
                template=template,
                input_variables=["context", "question"]
            )
            
            # LangChain ì²´ì¸ ìƒì„±
            chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=None,
                return_source_documents=False
            )
            
            # ì‘ë‹µ ìƒì„±
            response = chain.run(
                query=prompt.format(context=context, question=query)
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return self._generate_fallback_response(query, context_docs)
    
    def _generate_fallback_response(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        if not context_docs:
            return f"""ì•ˆë…•í•˜ì„¸ìš”! ë§ì”€í•˜ì‹  '{query}'ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ë§ì”€í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ê´€ë ¨ ë²•ë¥  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë ¤ìš´ ìƒí™©ì…ë‹ˆë‹¤.

ğŸ’¡ ì‹¤ë¬´ì  ì¡°ì–¸
ì´ëŸ¬í•œ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•´ë³´ì„¸ìš”
2. ê´€ë ¨ ë²•ë¥  ë¶„ì•¼ë¥¼ ëª…ì‹œí•˜ì—¬ ì§ˆë¬¸í•´ë³´ì„¸ìš”
3. êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ ì‚¬ë¡€ë¥¼ í¬í•¨í•˜ì—¬ ì§ˆë¬¸í•´ë³´ì„¸ìš”

âš ï¸ ì£¼ì˜ì‚¬í•­
- ë²•ë¥ ì€ í•´ì„ì˜ ì—¬ì§€ê°€ ìˆìœ¼ë¯€ë¡œ ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ì¶©ë¶„í•œ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤
- ê°œë³„ ì‚¬ì•ˆì— ëŒ€í•´ì„œëŠ” ë³€í˜¸ì‚¬ì™€ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤

ğŸ“ ì¶”ê°€ ë„ì›€
ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!

ë³¸ ë‹µë³€ì€ ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, ê°œë³„ ì‚¬ì•ˆì— ëŒ€í•œ ë²•ë¥  ìë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë³€í˜¸ì‚¬ì™€ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""
        
        # ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±
        response = f"""ì•ˆë…•í•˜ì„¸ìš”! ë§ì”€í•˜ì‹  '{query}'ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ë§ì”€í•˜ì‹  ì§ˆë¬¸ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œêµ°ìš”.

ğŸ“‹ ê´€ë ¨ ë²•ë¥  ì¡°í•­"""
        
        # ì‹¤ì œ ì¡°ë¬¸ ë‚´ìš© í¬í•¨
        main_doc = context_docs[0] if context_docs else None
        if main_doc and main_doc.get('content'):
            metadata = main_doc.get('metadata', {})
            law_name = metadata.get('law_name', 'ê´€ë ¨ ë²•ë¥ ')
            article_number = metadata.get('article_number', '')
            article_title = metadata.get('article_title', '')
            actual_content = main_doc['content']
            
            response += f"\n\n**{law_name} {article_number}**"
            if article_title:
                response += f" ({article_title})"
            response += f"\n{actual_content}"
        
        response += f"""

ğŸ’¡ ì‰½ê²Œ ì„¤ëª…í•˜ë©´
ì´ ì¡°í•­ì€ ë§ì”€í•˜ì‹  ë‚´ìš©ê³¼ ê´€ë ¨ëœ ë²•ë¥ ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤.

ğŸ” ì‹¤ì œ ì ìš© ì˜ˆì‹œ
ì˜ˆë¥¼ ë“¤ì–´, ì‹¤ì œ ìƒí™©ì—ì„œ ì´ ë²•ë¥ ì´ ì ìš©ë  ë•ŒëŠ” êµ¬ì²´ì ì¸ ì ˆì°¨ì™€ ìš”ê±´ì„ ë”°ë¥´ê²Œ ë©ë‹ˆë‹¤.

âš ï¸ ì£¼ì˜ì‚¬í•­
ì´ëŸ° ê²½ìš°ì—ëŠ” ê´€ë ¨ ë²•ë¥ ì˜ êµ¬ì²´ì ì¸ ìš”ê±´ê³¼ ì ˆì°¨ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì‹œëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

ğŸ“ ì¶”ê°€ ë„ì›€
ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!

ë³¸ ë‹µë³€ì€ ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, ê°œë³„ ì‚¬ì•ˆì— ëŒ€í•œ ë²•ë¥  ìë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë³€í˜¸ì‚¬ì™€ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""
        
        return response
    
    def process_query(self, message: str) -> Tuple[str, List[Dict[str, Any]]]:
        """ì¿¼ë¦¬ ì²˜ë¦¬"""
        if not self.initialized:
            return "ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", []
        
        if not message.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", []
        
        try:
            # ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.search_documents(message, top_k=5)
            
            # ì‘ë‹µ ìƒì„±
            response = self.generate_response(message, search_results)
            
            return response, search_results
            
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", []

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
lawfirm_service = LawFirmAIService()

def process_langchain_query(message: str, history: List) -> Tuple[str, List]:
    """LangChain ê¸°ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬"""
    response, sources = lawfirm_service.process_query(message)
    
    # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": response})
    
    return "", history

def create_simple_langchain_gradio_interface():
    """ê°„ë‹¨í•œ LangChain ê¸°ë°˜ Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    # ì»¤ìŠ¤í…€ CSS ë¡œë“œ
    css_file = Path("gradio/static/custom.css")
    custom_css = ""
    if css_file.exists():
        with open(css_file, 'r', encoding='utf-8') as f:
            custom_css = f.read()
    
    with gr.Blocks(
        title="LawFirmAI - Simple LangChain ê¸°ë°˜ ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸",
        css=custom_css,
        theme=gr.themes.Soft()
    ) as interface:
        
        # í—¤ë”
        gr.Markdown("""
        # ğŸ›ï¸ LawFirmAI - Simple LangChain ê¸°ë°˜ ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸
        
        **LangChainê³¼ RAG ê¸°ìˆ ì„ í™œìš©í•œ ì§€ëŠ¥í˜• ë²•ë¥  ìƒë‹´ ì„œë¹„ìŠ¤**
        
        ---
        """)
        
        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
        chatbot = gr.Chatbot(
            label="ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸",
            height=500,
            show_label=True,
            container=True,
            type="messages"
        )
        
        with gr.Row():
            msg = gr.Textbox(
                placeholder="ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
                label="ì§ˆë¬¸",
                lines=2,
                scale=4
            )
            submit_btn = gr.Button("ì „ì†¡", variant="primary", scale=1)
        
        with gr.Row():
            clear_btn = gr.Button("ëŒ€í™” ì´ˆê¸°í™”", variant="secondary", scale=1)
            natural_prompt_btn = gr.Button("ğŸ˜Š ìì—°ìŠ¤ëŸ¬ìš´ ìƒë‹´", scale=1)
            formal_prompt_btn = gr.Button("âš–ï¸ ì „ë¬¸ê°€ ìƒë‹´", scale=1)
        
        # ì˜ˆì‹œ ì§ˆë¬¸
        with gr.Accordion("ğŸ“ ì˜ˆì‹œ ì§ˆë¬¸", open=False):
            gr.Markdown("""
            **ë¯¼ë²• ê´€ë ¨:**
            - ê³„ì•½ì˜ ì„±ë¦½ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?
            - ë¶ˆë²•í–‰ìœ„ì˜ êµ¬ì„±ìš”ê±´ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”
            - ì†Œìœ ê¶Œ ì´ì „ì˜ ì‹œì ì€ ì–¸ì œì¸ê°€ìš”?
            
            **ìƒë²• ê´€ë ¨:**
            - ì£¼ì‹íšŒì‚¬ì˜ ì„¤ë¦½ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
            - ì´ì‚¬ì˜ ì˜ë¬´ì™€ ì±…ì„ì€ ë¬´ì—‡ì¸ê°€ìš”?
            - ì£¼ì£¼ì´íšŒì˜ ê¶Œí•œì€ ë¬´ì—‡ì¸ê°€ìš”?
            
            **í˜•ë²• ê´€ë ¨:**
            - ì ˆë„ì£„ì˜ êµ¬ì„±ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?
            - ì •ë‹¹ë°©ìœ„ì˜ ìš”ê±´ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”
            - ë¯¸ìˆ˜ë²”ì˜ ì²˜ë²Œ ê¸°ì¤€ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
            """)
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        submit_btn.click(
            process_langchain_query,
            inputs=[msg, chatbot],
            outputs=[msg, chatbot]
        )
        
        msg.submit(
            process_langchain_query,
            inputs=[msg, chatbot],
            outputs=[msg, chatbot]
        )
        
        clear_btn.click(lambda: [], outputs=[chatbot])
        
        # í”„ë¡¬í”„íŠ¸ ì „í™˜ ë²„íŠ¼ ì´ë²¤íŠ¸
        def switch_to_natural_prompt():
            """ìì—°ìŠ¤ëŸ¬ìš´ í”„ë¡¬í”„íŠ¸ë¡œ ì „í™˜"""
            success = prompt_manager.switch_to_version("natural_legal_consultant_v1.0")
            if success:
                return "ğŸ˜Š ìì—°ìŠ¤ëŸ¬ìš´ ìƒë‹´ ëª¨ë“œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤!"
            return "í”„ë¡¬í”„íŠ¸ ì „í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        
        def switch_to_formal_prompt():
            """ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ë¡œ ì „í™˜"""
            success = prompt_manager.switch_to_version("legal_expert_v1.0")
            if success:
                return "âš–ï¸ ì „ë¬¸ê°€ ìƒë‹´ ëª¨ë“œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤!"
            return "í”„ë¡¬í”„íŠ¸ ì „í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        
        natural_prompt_btn.click(switch_to_natural_prompt, outputs=[chatbot])
        formal_prompt_btn.click(switch_to_formal_prompt, outputs=[chatbot])
    
    return interface

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import signal
    import atexit
    
    # í”„ë¡œì„¸ìŠ¤ ID ì €ì¥
    pid = os.getpid()
    logger.info(f"Starting LawFirmAI Simple LangChain Gradio application... (PID: {pid})")
    
    # PID íŒŒì¼ ê²½ë¡œ
    pid_file = Path("gradio_server.pid")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ìˆ˜ì •
    os.chdir(Path(__file__).parent.parent)  # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
    
    def save_pid():
        """PIDë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            pid_data = {
                "pid": pid,
                "start_time": time.time(),
                "status": "running",
                "type": "simple_langchain"
            }
            with open(pid_file, 'w', encoding='utf-8') as f:
                json.dump(pid_data, f, indent=2)
            logger.info(f"PID saved to {pid_file}")
        except Exception as e:
            logger.error(f"Failed to save PID: {e}")
    
    def cleanup():
        """ì •ë¦¬ í•¨ìˆ˜"""
        logger.info("Cleaning up resources...")
        try:
            if pid_file.exists():
                pid_file.unlink()
                logger.info("PID file removed")
        except Exception as e:
            logger.error(f"Failed to remove PID file: {e}")
    
    def signal_handler(signum, frame):
        """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬"""
        logger.info(f"Received signal {signum}, shutting down gracefully...")
        cleanup()
        sys.exit(0)
    
    # PID ì €ì¥
    save_pid()
    
    # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    atexit.register(cleanup)
    
    try:
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        if not lawfirm_service.initialize():
            logger.error("Failed to initialize services")
            sys.exit(1)
        
        # ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
        interface = create_simple_langchain_gradio_interface()
        
        # Launch
        interface.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            debug=os.getenv("DEBUG", "false").lower() == "true",
            show_error=True
        )
        
    except Exception as e:
        logger.error(f"Failed to start Simple LangChain application: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()