#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LawFirmAI - ML Enhanced Gradio Web Interface
HuggingFace Spaces ë°°í¬ìš© ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (ML ê°•í™” ë²„ì „)
"""

import os
import sys
import logging
import json
from pathlib import Path
from typing import Dict, Any, List, Optional

# Add source directory to Python path
sys.path.append(str(Path(__file__).parent.parent / "source"))

import gradio as gr
from services.chat_service import ChatService
from services.rag_service import MLEnhancedRAGService
from services.search_service import MLEnhancedSearchService
from data.database import DatabaseManager
from data.vector_store import LegalVectorStore
from models.model_manager import LegalModelManager
from utils.config import Config
from utils.logger import setup_logging

# Setup logging
setup_logging()
logger = logging.getLogger(__name__)

def create_ml_enhanced_gradio_interface():
    """ML ê°•í™” Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    try:
        # Initialize configuration
        config = Config()
        
        # Initialize ML-enhanced services
        database = DatabaseManager(config.database_path)
        vector_store = LegalVectorStore(
            model_name="BAAI/bge-m3",
            dimension=1024,
            index_type="flat"
        )
        model_manager = LegalModelManager(config)
        
        ml_rag_service = MLEnhancedRAGService(config, model_manager, vector_store, database)
        ml_search_service = MLEnhancedSearchService(config, database, vector_store, model_manager)
        
        # Legacy chat service for compatibility
        chat_service = ChatService(config)
        
        def process_ml_enhanced_chat(message: str, history: List, 
                                   use_ml_enhanced: bool = True,
                                   quality_threshold: float = 0.7,
                                   search_type: str = "hybrid") -> tuple:
            """ML ê°•í™” ì±„íŒ… ì²˜ë¦¬ í•¨ìˆ˜"""
            try:
                if not message.strip():
                    return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", history
                
                if use_ml_enhanced:
                    # ML ê°•í™” RAG ì„œë¹„ìŠ¤ ì‚¬ìš©
                    filters = {"quality_threshold": quality_threshold} if quality_threshold > 0 else None
                    response = ml_rag_service.process_query(
                        query=message,
                        top_k=5,
                        filters=filters
                    )
                    
                    # ì‘ë‹µ í¬ë§·íŒ…
                    formatted_response = format_ml_enhanced_response(response)
                    
                else:
                    # ë ˆê±°ì‹œ ì±„íŒ… ì„œë¹„ìŠ¤ ì‚¬ìš©
                    response = chat_service.process_message(message)
                    formatted_response = response.get("response", "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
                history.append([message, formatted_response])
                
                return "", history
                
            except Exception as e:
                logger.error(f"ML-enhanced chat processing error: {e}")
                error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                history.append([message, error_msg])
                return "", history
        
        def format_ml_enhanced_response(response: Dict[str, Any]) -> str:
            """ML ê°•í™” ì‘ë‹µ í¬ë§·íŒ…"""
            try:
                main_response = response.get("response", "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                sources = response.get("sources", [])
                ml_stats = response.get("ml_stats", {})
                
                formatted = f"**ë‹µë³€:**\n{main_response}\n\n"
                
                if sources:
                    formatted += "**ì°¸ê³  ìë£Œ:**\n"
                    for i, source in enumerate(sources[:3], 1):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                        formatted += f"{i}. {source.get('title', 'Unknown')}"
                        if source.get('article_number'):
                            formatted += f" - {source['article_number']}"
                        if source.get('article_title'):
                            formatted += f"({source['article_title']})"
                        formatted += f" (í’ˆì§ˆ: {source.get('quality_score', 0.0):.2f})\n"
                
                if ml_stats:
                    formatted += f"\n**ML í†µê³„:**\n"
                    formatted += f"- ê²€ìƒ‰ëœ ë¬¸ì„œ: {ml_stats.get('total_documents', 0)}ê°œ\n"
                    formatted += f"- ë³¸ì¹™ ì¡°ë¬¸: {ml_stats.get('main_articles', 0)}ê°œ\n"
                    formatted += f"- ë¶€ì¹™ ì¡°ë¬¸: {ml_stats.get('supplementary_articles', 0)}ê°œ\n"
                    formatted += f"- í‰ê·  í’ˆì§ˆ ì ìˆ˜: {ml_stats.get('avg_quality_score', 0.0):.3f}\n"
                
                return formatted
                
            except Exception as e:
                logger.error(f"Error formatting ML-enhanced response: {e}")
                return response.get("response", "ì‘ë‹µ í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        def search_documents(query: str, search_type: str = "hybrid", 
                           limit: int = 10) -> str:
            """ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜"""
            try:
                if not query.strip():
                    return "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
                
                results = ml_search_service.search_documents(
                    query=query,
                    search_type=search_type,
                    limit=limit
                )
                
                if not results:
                    return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                
                formatted_results = f"**ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):**\n\n"
                
                for i, result in enumerate(results, 1):
                    formatted_results += f"**{i}. {result.get('title', 'Unknown')}**\n"
                    if result.get('article_number'):
                        formatted_results += f"- ì¡°ë¬¸: {result['article_number']}"
                        if result.get('article_title'):
                            formatted_results += f"({result['article_title']})"
                        formatted_results += "\n"
                    
                    formatted_results += f"- ìœ ì‚¬ë„: {result.get('similarity', 0.0):.3f}\n"
                    formatted_results += f"- í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0.0):.3f}\n"
                    formatted_results += f"- ì¡°ë¬¸ ìœ í˜•: {result.get('article_type', 'main')}\n"
                    
                    if result.get('is_supplementary'):
                        formatted_results += "- ë¶€ì¹™ ì¡°ë¬¸\n"
                    
                    content = result.get('content', '')
                    if content:
                        formatted_results += f"- ë‚´ìš©: {content[:200]}{'...' if len(content) > 200 else ''}\n"
                    
                    formatted_results += "\n"
                
                return formatted_results
                
            except Exception as e:
                logger.error(f"Document search error: {e}")
                return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
        def get_quality_stats() -> str:
            """í’ˆì§ˆ í†µê³„ ì¡°íšŒ í•¨ìˆ˜"""
            try:
                # ê°„ë‹¨í•œ í’ˆì§ˆ í†µê³„ ì¿¼ë¦¬
                stats_query = """
                    SELECT 
                        COUNT(*) as total_articles,
                        SUM(CASE WHEN al.ml_enhanced = 1 THEN 1 ELSE 0 END) as ml_enhanced_articles,
                        AVG(al.parsing_quality_score) as avg_quality_score,
                        SUM(CASE WHEN aa.article_type = 'main' THEN 1 ELSE 0 END) as main_articles,
                        SUM(CASE WHEN aa.article_type = 'supplementary' THEN 1 ELSE 0 END) as supplementary_articles
                    FROM assembly_articles aa
                    LEFT JOIN assembly_laws al ON aa.law_id = al.law_id
                """
                
                results = database.execute_query(stats_query)
                stats = results[0] if results else {}
                
                formatted_stats = "**ğŸ“Š í’ˆì§ˆ í†µê³„**\n\n"
                formatted_stats += f"- ì´ ì¡°ë¬¸ ìˆ˜: {stats.get('total_articles', 0):,}ê°œ\n"
                formatted_stats += f"- ML ê°•í™” ì¡°ë¬¸: {stats.get('ml_enhanced_articles', 0):,}ê°œ\n"
                formatted_stats += f"- í‰ê·  í’ˆì§ˆ ì ìˆ˜: {stats.get('avg_quality_score', 0.0):.3f}\n"
                formatted_stats += f"- ë³¸ì¹™ ì¡°ë¬¸: {stats.get('main_articles', 0):,}ê°œ\n"
                formatted_stats += f"- ë¶€ì¹™ ì¡°ë¬¸: {stats.get('supplementary_articles', 0):,}ê°œ\n"
                
                ml_rate = (stats.get('ml_enhanced_articles', 0) / max(stats.get('total_articles', 1), 1)) * 100
                formatted_stats += f"- ML ê°•í™” ë¹„ìœ¨: {ml_rate:.1f}%\n"
                
                return formatted_stats
                
            except Exception as e:
                logger.error(f"Quality stats error: {e}")
                return f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
        def clear_history():
            """ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
            return [], []
        
        # Create ML-enhanced Gradio interface
        with gr.Blocks(
            title="ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸ (ML ê°•í™”)",
            theme=gr.themes.Soft(),
            css="""
            .gradio-container {
                max-width: 1400px !important;
                margin: auto !important;
            }
            .ml-stats {
                background-color: #f0f8ff;
                padding: 10px;
                border-radius: 5px;
                margin: 10px 0;
            }
            """
        ) as interface:
            
            gr.Markdown(
                """
                # âš–ï¸ ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸ (ML ê°•í™” ë²„ì „)
                
                ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ë²•ë¥  ë¬¸ì„œ íŒŒì‹±ê³¼ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ í™œìš©í•œ ê³ ê¸‰ ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                """
            )
            
            with gr.Tabs():
                # ì±„íŒ… íƒ­
                with gr.Tab("ğŸ’¬ AI ì±„íŒ…"):
                    with gr.Row():
                        with gr.Column(scale=3):
                            chatbot = gr.Chatbot(
                                label="ML ê°•í™” ëŒ€í™”",
                                height=500,
                                show_label=True,
                                container=True,
                                bubble_full_width=False
                            )
                            
                            with gr.Row():
                                msg = gr.Textbox(
                                    label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
                                    placeholder="ì˜ˆ: ê³„ì•½ì„œì—ì„œ ì£¼ì˜í•´ì•¼ í•  ì¡°í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                                    lines=2,
                                    scale=4
                                )
                                submit_btn = gr.Button("ì „ì†¡", variant="primary", scale=1)
                            
                            with gr.Row():
                                clear_btn = gr.Button("ëŒ€í™” ì´ˆê¸°í™”", variant="secondary")
                                stats_btn = gr.Button("í’ˆì§ˆ í†µê³„", variant="secondary")
                        
                        with gr.Column(scale=1):
                            gr.Markdown("### âš™ï¸ ML ì„¤ì •")
                            
                            use_ml_enhanced = gr.Checkbox(
                                label="ML ê°•í™” ëª¨ë“œ",
                                value=True,
                                info="ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš©"
                            )
                            
                            quality_threshold = gr.Slider(
                                minimum=0.0,
                                maximum=1.0,
                                value=0.7,
                                step=0.1,
                                label="í’ˆì§ˆ ì„ê³„ê°’",
                                info="ê²€ìƒ‰ í’ˆì§ˆ ê¸°ì¤€"
                            )
                            
                            search_type = gr.Dropdown(
                                choices=["hybrid", "semantic", "keyword", "supplementary", "high_quality"],
                                value="hybrid",
                                label="ê²€ìƒ‰ ìœ í˜•",
                                info="ê²€ìƒ‰ ë°©ë²• ì„ íƒ"
                            )
                            
                            gr.Markdown(
                                """
                                ### ğŸ“š ì£¼ìš” ê¸°ëŠ¥
                                
                                - **ML ê°•í™” íŒŒì‹±**: ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì¡°ë¬¸ ê²½ê³„ ê°ì§€
                                - **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: ì˜ë¯¸ì  + í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©
                                - **í’ˆì§ˆ í•„í„°ë§**: íŒŒì‹± í’ˆì§ˆ ê¸°ë°˜ ê²°ê³¼ í•„í„°ë§
                                - **ë¶€ì¹™ íŒŒì‹±**: ë³¸ì¹™ê³¼ ë¶€ì¹™ ë¶„ë¦¬ íŒŒì‹±
                                - **ì‹¤ì‹œê°„ í†µê³„**: ML ê°•í™” í†µê³„ ë° í’ˆì§ˆ ì§€í‘œ
                                """
                            )
                
                # ê²€ìƒ‰ íƒ­
                with gr.Tab("ğŸ” ë¬¸ì„œ ê²€ìƒ‰"):
                    with gr.Row():
                        with gr.Column(scale=2):
                            search_query = gr.Textbox(
                                label="ê²€ìƒ‰ì–´",
                                placeholder="ì˜ˆ: ê³„ì•½ì„œ, ì†í•´ë°°ìƒ, ë¶€ë™ì‚°",
                                lines=1
                            )
                            
                            with gr.Row():
                                search_type_dropdown = gr.Dropdown(
                                    choices=["hybrid", "semantic", "keyword", "supplementary", "high_quality"],
                                    value="hybrid",
                                    label="ê²€ìƒ‰ ìœ í˜•"
                                )
                                search_limit = gr.Slider(
                                    minimum=5,
                                    maximum=20,
                                    value=10,
                                    step=1,
                                    label="ê²°ê³¼ ê°œìˆ˜"
                                )
                                search_btn = gr.Button("ê²€ìƒ‰", variant="primary")
                        
                        with gr.Column(scale=3):
                            search_results = gr.Markdown(
                                label="ê²€ìƒ‰ ê²°ê³¼",
                                value="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ê³  ê²€ìƒ‰ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
                            )
                
                # í†µê³„ íƒ­
                with gr.Tab("ğŸ“Š í’ˆì§ˆ í†µê³„"):
                    with gr.Row():
                        with gr.Column():
                            quality_stats_display = gr.Markdown(
                                label="í’ˆì§ˆ í†µê³„",
                                value="í’ˆì§ˆ í†µê³„ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìµœì‹  í†µê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”."
                            )
                            
                            refresh_stats_btn = gr.Button("í†µê³„ ìƒˆë¡œê³ ì¹¨", variant="primary")
            
            # Event handlers
            msg.submit(
                process_ml_enhanced_chat, 
                [msg, chatbot, use_ml_enhanced, quality_threshold, search_type], 
                [msg, chatbot]
            )
            submit_btn.click(
                process_ml_enhanced_chat, 
                [msg, chatbot, use_ml_enhanced, quality_threshold, search_type], 
                [msg, chatbot]
            )
            clear_btn.click(clear_history, outputs=[chatbot, msg])
            stats_btn.click(get_quality_stats, outputs=quality_stats_display)
            
            search_btn.click(
                search_documents,
                [search_query, search_type_dropdown, search_limit],
                outputs=search_results
            )
            
            refresh_stats_btn.click(get_quality_stats, outputs=quality_stats_display)
        
        return interface
        
    except Exception as e:
        logger.error(f"Failed to create ML-enhanced Gradio interface: {e}")
        raise

def create_gradio_interface():
    """ë ˆê±°ì‹œ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ Gradio ì¸í„°í˜ì´ìŠ¤"""
    try:
        # Initialize configuration
        config = Config()
        
        # Initialize chat service
        chat_service = ChatService(config)
        
        def process_chat(message, history):
            """ì±„íŒ… ì²˜ë¦¬ í•¨ìˆ˜"""
            try:
                if not message.strip():
                    return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
                
                # Process message through chat service
                response = chat_service.process_message(message)
                
                return response.get("response", "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
            except Exception as e:
                logger.error(f"Chat processing error: {e}")
                return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
        def clear_history():
            """ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
            return [], []
        
        # Create Gradio interface
        with gr.Blocks(
            title="ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸",
            theme=gr.themes.Soft(),
            css="""
            .gradio-container {
                max-width: 1200px !important;
                margin: auto !important;
            }
            """
        ) as interface:
            
            gr.Markdown(
                """
                # âš–ï¸ ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸
                
                ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ë“œë¦¬ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                íŒë¡€, ë²•ë ¹, Q&A ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
                """
            )
            
            with gr.Row():
                with gr.Column(scale=3):
                    chatbot = gr.Chatbot(
                        label="ëŒ€í™”",
                        height=500,
                        show_label=True,
                        container=True,
                        bubble_full_width=False
                    )
                    
                    with gr.Row():
                        msg = gr.Textbox(
                            label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
                            placeholder="ì˜ˆ: ê³„ì•½ì„œì—ì„œ ì£¼ì˜í•´ì•¼ í•  ì¡°í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                            lines=2,
                            scale=4
                        )
                        submit_btn = gr.Button("ì „ì†¡", variant="primary", scale=1)
                    
                    with gr.Row():
                        clear_btn = gr.Button("ëŒ€í™” ì´ˆê¸°í™”", variant="secondary")
                
                with gr.Column(scale=1):
                    gr.Markdown(
                        """
                        ## ğŸ“š ì£¼ìš” ê¸°ëŠ¥
                        
                        - **íŒë¡€ ê²€ìƒ‰**: ë²•ì› íŒë¡€ ê²€ìƒ‰ ë° ë¶„ì„
                        - **ë²•ë ¹ í•´ì„¤**: ë²•ë ¹ ì¡°ë¬¸ í•´ì„ ë° ì„¤ëª…
                        - **ê³„ì•½ì„œ ë¶„ì„**: ê³„ì•½ì„œ ê²€í†  ë° ìœ„í—˜ ìš”ì†Œ ë¶„ì„
                        - **Q&A**: ìì£¼ ë¬»ëŠ” ë²•ë¥  ì§ˆë¬¸ ë‹µë³€
                        
                        ## ğŸ’¡ ì‚¬ìš© íŒ
                        
                        - êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”
                        - ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
                        - ë³µì¡í•œ ì§ˆë¬¸ì€ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ì§ˆë¬¸í•˜ì„¸ìš”
                        """
                    )
            
            # Event handlers
            msg.submit(process_chat, [msg, chatbot], [chatbot, msg])
            submit_btn.click(process_chat, [msg, chatbot], [chatbot, msg])
            clear_btn.click(clear_history, outputs=[chatbot, msg])
        
        return interface
        
    except Exception as e:
        logger.error(f"Failed to create Gradio interface: {e}")
        raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("Starting LawFirmAI ML-Enhanced Gradio application...")
        
        # Create and launch ML-enhanced interface
        interface = create_ml_enhanced_gradio_interface()
        
        # Launch with HuggingFace Spaces configuration
        interface.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            debug=os.getenv("DEBUG", "false").lower() == "true",
            show_error=True
        )
        
    except Exception as e:
        logger.error(f"Failed to start ML-enhanced application: {e}")
        # Fallback to legacy interface
        try:
            logger.info("Falling back to legacy interface...")
            interface = create_gradio_interface()
            interface.launch(
                server_name="0.0.0.0",
                server_port=7860,
                share=False,
                debug=os.getenv("DEBUG", "false").lower() == "true",
                show_error=True
            )
        except Exception as fallback_error:
            logger.error(f"Failed to start legacy application: {fallback_error}")
            sys.exit(1)

if __name__ == "__main__":
    main()
