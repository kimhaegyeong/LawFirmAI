# -*- coding: utf-8 -*-
"""
LawFirmAI - HuggingFace Spaces ì „ìš© Gradio ì• í”Œë¦¬ì¼€ì´ì…˜
Phase 2ì˜ ëª¨ë“  ê°œì„ ì‚¬í•­ì„ í†µí•©í•œ ìµœì í™”ëœ ë²„ì „
"""

import os
import sys
import logging
import time
import json
import gc
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from contextlib import contextmanager

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# LangGraph í™œì„±í™” ì„¤ì •
os.environ["USE_LANGGRAPH"] = os.getenv("USE_LANGGRAPH", "true")

# Gradio ë° ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import gradio as gr
import torch
import psutil

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from source.data.database import DatabaseManager
from source.data.vector_store import LegalVectorStore
from source.services.question_classifier import QuestionClassifier, QuestionType
from source.services.hybrid_search_engine import HybridSearchEngine
from source.services.optimized_search_engine import OptimizedSearchEngine
from source.services.prompt_templates import PromptTemplateManager
from source.services.unified_prompt_manager import UnifiedPromptManager, LegalDomain, ModelType
from source.services.dynamic_prompt_updater import create_dynamic_prompt_updater
from source.services.prompt_optimizer import create_prompt_optimizer
from source.services.confidence_calculator import ConfidenceCalculator
from source.services.legal_term_expander import LegalTermExpander
from source.services.gemini_client import GeminiClient
from source.services.improved_answer_generator import ImprovedAnswerGenerator
from source.services.answer_formatter import AnswerFormatter
from source.services.context_builder import ContextBuilder
from source.services.chat_service import ChatService
from source.services.performance_monitor import PerformanceMonitor as SourcePerformanceMonitor, PerformanceContext

# Phase 1: ëŒ€í™” ë§¥ë½ ê°•í™” ëª¨ë“ˆ
from source.services.integrated_session_manager import IntegratedSessionManager

# AKLS ëª¨ë“ˆ (ì„ì‹œ ë¹„í™œì„±í™”)
# from gradio.components.akls_search_interface import create_akls_interface
from source.services.multi_turn_handler import MultiTurnQuestionHandler
from source.services.context_compressor import ContextCompressor

# Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„ ëª¨ë“ˆ
from source.services.user_profile_manager import UserProfileManager, ExpertiseLevel, DetailLevel
from source.services.emotion_intent_analyzer import EmotionIntentAnalyzer, EmotionType, IntentType, UrgencyLevel
from source.services.conversation_flow_tracker import ConversationFlowTracker

from source.utils.config import Config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/huggingface_spaces_app.log')
    ]
)
logger = logging.getLogger(__name__)

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, max_memory_percent: float = 85.0):
        self.max_memory_percent = max_memory_percent
        self.logger = logging.getLogger(__name__)
    
    @contextmanager
    def memory_efficient_inference(self):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì¶”ë¡  ì»¨í…ìŠ¤íŠ¸"""
        # ì¶”ë¡  ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
        self._cleanup_memory()
        
        try:
            yield
        finally:
            # ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            self._cleanup_memory()
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def monitor_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        memory_percent = psutil.virtual_memory().percent
        if memory_percent > self.max_memory_percent:
            self.logger.warning(f"High memory usage: {memory_percent:.1f}%")
            self._cleanup_memory()
        return memory_percent

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.response_times = []
        self.error_count = 0
        self.total_requests = 0
    
    def log_request(self, response_time: float, success: bool = True, operation: str = None):
        """ìš”ì²­ ë¡œê¹…"""
        self.total_requests += 1
        self.response_times.append(response_time)
        
        if not success:
            self.error_count += 1
        
        # ìµœê·¼ 100ê°œ ìš”ì²­ë§Œ ìœ ì§€
        if len(self.response_times) > 100:
            self.response_times = self.response_times[-100:]
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.response_times:
            return {
                "avg_response_time": 0,
                "error_rate": 0,
                "total_requests": 0
            }
        
        return {
            "avg_response_time": sum(self.response_times) / len(self.response_times),
            "error_rate": self.error_count / self.total_requests if self.total_requests > 0 else 0,
            "total_requests": self.total_requests
        }

class HuggingFaceSpacesApp:
    """HuggingFace Spaces ì „ìš© LawFirmAI ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.memory_optimizer = MemoryOptimizer()
        self.performance_monitor = PerformanceMonitor()
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ì˜µì…˜)
        if hasattr(self.performance_monitor, 'start_monitoring'):
            self.performance_monitor.start_monitoring()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.db_manager = None
        self.vector_store = None
        self.question_classifier = None
        self.hybrid_search_engine = None
        self.prompt_template_manager = None
        self.unified_prompt_manager = None
        self.dynamic_prompt_updater = None
        self.prompt_optimizer = None
        self.confidence_calculator = None
        self.legal_term_expander = None
        self.gemini_client = None
        self.improved_answer_generator = None
        self.answer_formatter = None
        self.context_builder = None
        
        # ChatService ì´ˆê¸°í™” (LangGraph í†µí•©)
        self.chat_service = None
        
        # Phase 1: ëŒ€í™” ë§¥ë½ ê°•í™” ì»´í¬ë„ŒíŠ¸
        self.session_manager = None
        self.multi_turn_handler = None
        self.context_compressor = None
        
        # Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„ ì»´í¬ë„ŒíŠ¸
        self.user_profile_manager = None
        self.emotion_intent_analyzer = None
        self.conversation_flow_tracker = None
        
        # ì„¸ì…˜ ê´€ë¦¬
        self.current_session_id = None
        self.current_user_id = None
        
        # ì´ˆê¸°í™” ìƒíƒœ
        self.is_initialized = False
        self.initialization_error = None
        
        # HuggingFace Spaces í™˜ê²½ ì„¤ì •
        self._setup_huggingface_spaces_env()
    
    def _setup_huggingface_spaces_env(self):
        """HuggingFace Spaces í™˜ê²½ ì„¤ì •"""
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ.setdefault('GRADIO_SERVER_NAME', '0.0.0.0')
        os.environ.setdefault('GRADIO_SERVER_PORT', '7860')
        os.environ.setdefault('HF_HUB_DISABLE_SYMLINKS_WARNING', '1')
        
        # ë¡œê¹… ë ˆë²¨ ì„¤ì •
        if os.getenv('HUGGINGFACE_SPACES', '').lower() == 'true':
            logging.getLogger().setLevel(logging.WARNING)
            self.logger.setLevel(logging.WARNING)
    
    def initialize_components(self) -> bool:
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            self.logger.info("Initializing LawFirmAI components...")
            start_time = time.time()
            
            with self.memory_optimizer.memory_efficient_inference():
                # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì ì´ˆê¸°í™”
                self.db_manager = DatabaseManager("data/lawfirm.db")
                self.logger.info("Database manager initialized")
                
                # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” (íŒë¡€ ë°ì´í„°ìš©)
                self.vector_store = LegalVectorStore(
                    model_name='jhgan/ko-sroberta-multitask',
                    dimension=768,
                    index_type='flat'
                )
                # íŒë¡€ ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ
                if not self.vector_store.load_index('data/embeddings/ml_enhanced_ko_sroberta_precedents'):
                    self.logger.warning("Failed to load precedent vector index, using law index")
                    self.vector_store.load_index('data/embeddings/ml_enhanced_ko_sroberta')
                self.logger.info("Vector store initialized")
                
                # ì§ˆë¬¸ ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
                self.question_classifier = QuestionClassifier()
                self.logger.info("Question classifier initialized")
                
                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
                self.hybrid_search_engine = HybridSearchEngine()
                self.logger.info("Hybrid search engine initialized")
                
                # ìµœì í™”ëœ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
                self.optimized_search_engine = OptimizedSearchEngine(
                    vector_store=self.vector_store,
                    hybrid_engine=self.hybrid_search_engine,
                    cache_size=1000,
                    cache_ttl=3600
                )
                self.logger.info("Optimized search engine initialized")
                
                # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬ì ì´ˆê¸°í™”
                self.prompt_template_manager = PromptTemplateManager()
                self.logger.info("Prompt template manager initialized")
                
                # í†µí•© í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì
                self.unified_prompt_manager = UnifiedPromptManager()
                self.logger.info("Unified prompt manager initialized")
                
                # ë™ì  í”„ë¡¬í”„íŠ¸ ì—…ë°ì´í„°
                self.dynamic_prompt_updater = create_dynamic_prompt_updater(self.unified_prompt_manager)
                self.logger.info("Dynamic prompt updater initialized")
                
                # í”„ë¡¬í”„íŠ¸ ìµœì í™”ê¸°
                self.prompt_optimizer = create_prompt_optimizer(self.unified_prompt_manager)
                self.logger.info("Prompt optimizer initialized")
                
                # ì‹ ë¢°ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”
                self.confidence_calculator = ConfidenceCalculator()
                self.logger.info("Confidence calculator initialized")
                
                # ë²•ë¥  ìš©ì–´ í™•ì¥ê¸° ì´ˆê¸°í™”
                self.legal_term_expander = LegalTermExpander()
                self.logger.info("Legal term expander initialized")
                
                # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                if os.getenv('GEMINI_ENABLED', 'true').lower() == 'true':
                    self.gemini_client = GeminiClient()
                    self.logger.info("Gemini client initialized")
                
                # ë‹µë³€ í¬ë§·í„° ì´ˆê¸°í™”
                self.answer_formatter = AnswerFormatter()
                self.logger.info("Answer formatter initialized")
                
                # ì»¨í…ìŠ¤íŠ¸ ë¹Œë” ì´ˆê¸°í™”
                self.context_builder = ContextBuilder()
                self.logger.info("Context builder initialized")
                
                # ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸° ì´ˆê¸°í™”
                self.improved_answer_generator = ImprovedAnswerGenerator(
                    gemini_client=self.gemini_client,
                    prompt_template_manager=self.prompt_template_manager,
                    confidence_calculator=self.confidence_calculator,
                    answer_formatter=self.answer_formatter,
                    context_builder=self.context_builder
                )
                self.logger.info("Improved answer generator initialized")
                
                # ChatService ì´ˆê¸°í™” (LangGraph í†µí•©)
                config = Config()
                self.chat_service = ChatService(config)
                self.logger.info("ChatService initialized with LangGraph integration")
                
                # Phase 1: ëŒ€í™” ë§¥ë½ ê°•í™” ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
                self.session_manager = IntegratedSessionManager("data/conversations.db")
                self.logger.info("Integrated session manager initialized")
                
                self.multi_turn_handler = MultiTurnQuestionHandler()
                self.logger.info("Multi-turn question handler initialized")
                
                self.context_compressor = ContextCompressor(max_tokens=2000)
                self.logger.info("Context compressor initialized")
                
                # Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
                self.user_profile_manager = UserProfileManager(self.session_manager.conversation_store)
                self.logger.info("User profile manager initialized")
                
                self.emotion_intent_analyzer = EmotionIntentAnalyzer()
                self.logger.info("Emotion intent analyzer initialized")
                
                self.conversation_flow_tracker = ConversationFlowTracker()
                self.logger.info("Conversation flow tracker initialized")
                
                # ì„¸ì…˜ ID ìƒì„±
                self.current_session_id = f"gradio_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                self.current_user_id = "gradio_user"  # ê¸°ë³¸ ì‚¬ìš©ì ID
                self.logger.info(f"Session initialized: {self.current_session_id}")
            
            initialization_time = time.time() - start_time
            self.logger.info(f"All components initialized successfully in {initialization_time:.2f} seconds")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            self.initialization_error = str(e)
            self.logger.error(f"Failed to initialize components: {e}")
            return False
    
    def process_query(self, query: str, session_id: str = None, user_id: str = None) -> Dict[str, Any]:
        """ì§ˆì˜ ì²˜ë¦¬ (Phase 1 ëŒ€í™” ë§¥ë½ ê°•í™” ì ìš©)"""
        if not self.is_initialized:
            return {
                "answer": "ì‹œìŠ¤í…œì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "error": "System not initialized",
                "confidence": {"confidence": 0, "reliability_level": "VERY_LOW"}
            }
        
        # ì„¸ì…˜ ID ì„¤ì •
        if not session_id:
            session_id = self.current_session_id
        if not user_id:
            user_id = self.current_user_id
        
        start_time = time.time()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (PerformanceContext ëŒ€ì‹  ì§ì ‘ ì²˜ë¦¬)
        try:
                with self.memory_optimizer.memory_efficient_inference():
                    # Phase 1: ëŒ€í™” ë§¥ë½ ì²˜ë¦¬
                    context = None
                    resolved_query = query
                    multi_turn_info = {}
                    
                    if self.session_manager:
                        # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
                        context = self.session_manager.get_or_create_session(session_id, user_id)
                        
                        # ë‹¤ì¤‘ í„´ ì§ˆë¬¸ ì²˜ë¦¬
                        if self.multi_turn_handler and context:
                            is_multi_turn = self.multi_turn_handler.detect_multi_turn_question(query, context)
                            if is_multi_turn:
                                multi_turn_result = self.multi_turn_handler.build_complete_query(query, context)
                                resolved_query = multi_turn_result["resolved_query"]
                                multi_turn_info = {
                                    "is_multi_turn": True,
                                    "original_query": query,
                                    "resolved_query": resolved_query,
                                    "confidence": multi_turn_result["confidence"],
                                    "reasoning": multi_turn_result["reasoning"]
                                }
                                self.logger.info(f"Multi-turn question resolved: '{query}' -> '{resolved_query}'")
                    
                    # Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„
                    personalized_context = {}
                    emotion_intent_info = {}
                    flow_tracking_info = {}
                    
                    if self.user_profile_manager:
                        # ê°œì¸í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                        personalized_context = self.user_profile_manager.get_personalized_context(user_id, resolved_query)
                        self.logger.info(f"Personalized context created for user {user_id}")
                    
                    if self.emotion_intent_analyzer:
                        # ê°ì • ë° ì˜ë„ ë¶„ì„
                        emotion_result = self.emotion_intent_analyzer.analyze_emotion(resolved_query)
                        intent_result = self.emotion_intent_analyzer.analyze_intent(resolved_query, context)
                        
                        # ì‘ë‹µ í†¤ ê²°ì •
                        response_tone = self.emotion_intent_analyzer.get_contextual_response_tone(
                            emotion_result, intent_result, personalized_context
                        )
                        
                        emotion_intent_info = {
                            "emotion": {
                                "primary_emotion": emotion_result.primary_emotion.value,
                                "confidence": emotion_result.confidence,
                                "intensity": emotion_result.intensity,
                                "reasoning": emotion_result.reasoning
                            },
                            "intent": {
                                "primary_intent": intent_result.primary_intent.value,
                                "confidence": intent_result.confidence,
                                "urgency_level": intent_result.urgency_level.value,
                                "reasoning": intent_result.reasoning
                            },
                            "response_tone": {
                                "tone_type": response_tone.tone_type,
                                "empathy_level": response_tone.empathy_level,
                                "formality_level": response_tone.formality_level,
                                "urgency_response": response_tone.urgency_response,
                                "explanation_depth": response_tone.explanation_depth
                            }
                        }
                        self.logger.info(f"Emotion: {emotion_result.primary_emotion.value}, Intent: {intent_result.primary_intent.value}")
                    
                    if self.conversation_flow_tracker and context:
                        # ëŒ€í™” íë¦„ ì¶”ì 
                        from source.services.conversation_manager import ConversationTurn
                        self.conversation_flow_tracker.track_conversation_flow(session_id, 
                            ConversationTurn(
                                user_query=query,
                                bot_response="",  # ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ
                                timestamp=datetime.now(),
                                question_type="general_question"
                            )
                        )
                        
                        # ë‹¤ìŒ ì˜ë„ ì˜ˆì¸¡ ë° í›„ì† ì§ˆë¬¸ ì œì•ˆ
                        predicted_intents = self.conversation_flow_tracker.predict_next_intent(context)
                        suggested_questions = self.conversation_flow_tracker.suggest_follow_up_questions(context)
                        conversation_state = self.conversation_flow_tracker.get_conversation_state(context)
                        
                        flow_tracking_info = {
                            "predicted_intents": predicted_intents,
                            "suggested_questions": suggested_questions,
                            "conversation_state": conversation_state
                        }
                        self.logger.info(f"Flow tracking: state={conversation_state}, suggestions={len(suggested_questions)}")
                    
                    # ChatServiceë¥¼ ì‚¬ìš©í•œ ì²˜ë¦¬ (ì‹¤ì œ RAG ì‹œìŠ¤í…œ)
                    if self.chat_service and hasattr(self.chat_service, 'improved_answer_generator') and self.chat_service.improved_answer_generator:
                        import asyncio
                        result = asyncio.run(self.chat_service.process_message(resolved_query, session_id=session_id))
                        
                        response_time = time.time() - start_time
                        self.performance_monitor.log_request(response_time, success=True, operation="process_query")
                        
                        # Phase 1: ì„¸ì…˜ì— í„´ ì¶”ê°€
                        if self.session_manager and context:
                            self.session_manager.add_turn(
                                session_id,
                                query,  # ì›ë³¸ ì§ˆë¬¸ ì €ì¥
                                result.get("response", ""),
                                result.get("question_type", "general_question"),
                                user_id
                            )
                        
                        # ì»¨í…ìŠ¤íŠ¸ ì••ì¶• í™•ì¸
                        compression_info = {}
                        if self.context_compressor and context:
                            current_tokens = self.context_compressor.calculate_tokens(context)
                            if current_tokens > 1500:  # ì••ì¶• ì„ê³„ê°’
                                compression_result = self.context_compressor.compress_long_conversation(context)
                                compression_info = {
                                    "compressed": True,
                                    "original_tokens": compression_result.original_tokens,
                                    "compressed_tokens": compression_result.compressed_tokens,
                                    "compression_ratio": compression_result.compression_ratio,
                                    "summary": compression_result.summary
                                }
                                self.logger.info(f"Context compressed: {compression_result.compression_ratio:.2f} ratio")
                        
                        return {
                            "answer": result.get("response", ""),
                            "confidence": {
                                "confidence": result.get("confidence", 0.0),
                                "reliability_level": "HIGH" if result.get("confidence", 0) > 0.7 else "MEDIUM" if result.get("confidence", 0) > 0.4 else "LOW"
                            },
                            "processing_time": response_time,
                            "memory_usage": self.memory_optimizer.monitor_memory_usage(),
                            "question_type": result.get("question_type", "general_question"),
                            "session_id": session_id,
                            "user_id": user_id,
                            "multi_turn_info": multi_turn_info,
                            "compression_info": compression_info,
                            "context_stats": {
                                "total_turns": len(context.turns) if context else 0,
                                "total_entities": sum(len(entities) for entities in context.entities.values()) if context else 0,
                                "topics": list(context.topic_stack) if context else []
                            },
                            # Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„ ì •ë³´
                            "personalized_context": personalized_context,
                            "emotion_intent_info": emotion_intent_info,
                            "flow_tracking_info": flow_tracking_info,
                            "session_id": result.get("session_id"),
                            "query_type": result.get("query_type", ""),
                            "legal_references": result.get("legal_references", []),
                            "processing_steps": result.get("processing_steps", []),
                            "metadata": result.get("metadata", {}),
                            "errors": result.get("errors", [])
                        }
                    else:
                        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
                        return self._process_query_legacy(query, start_time)
                        
        except Exception as e:
                response_time = time.time() - start_time
                self.performance_monitor.log_request(response_time, success=False, operation="process_query_error")
                self.logger.error(f"Error processing query: {e}")
                
                return {
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "error": str(e),
                    "confidence": {"confidence": 0, "reliability_level": "VERY_LOW"},
                    "processing_time": response_time
                }
    
    def _process_query_legacy(self, query: str, start_time: float) -> Dict[str, Any]:
        """ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì§ˆì˜ ì²˜ë¦¬ (í´ë°±)"""
        try:
            with self.memory_optimizer.memory_efficient_inference():
                # ì§ˆë¬¸ ë¶„ë¥˜
                question_classification = self.question_classifier.classify_question(query)
                
                # ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹¤í–‰
                search_results = self.hybrid_search_engine.search_with_question_type(
                    query=query,
                    question_type=question_classification,
                    max_results=10
                )
                
                # ë‹µë³€ ìƒì„±
                answer_result = self.improved_answer_generator.generate_answer(
                    query=query,
                    question_type=question_classification,
                    context="",
                    sources=search_results,
                    conversation_history=None
                )
                
                response_time = time.time() - start_time
                self.performance_monitor.log_request(response_time, success=True, operation="legacy_process_query")
                
                return {
                    "answer": answer_result.answer,
                    "question_type": question_classification.question_type.value,
                        "confidence": {
                            "confidence": answer_result.confidence.confidence,
                            "reliability_level": answer_result.confidence.level.value
                        },
                    "processing_time": response_time,
                    "memory_usage": self.memory_optimizer.monitor_memory_usage()
                }
                
        except Exception as e:
            response_time = time.time() - start_time
            self.performance_monitor.log_request(response_time, success=False, operation="legacy_process_query_error")
            self.logger.error(f"Error in legacy processing: {e}")
            
            return {
                "answer": "ê¸°ì¡´ ì²˜ë¦¬ ë°©ì‹ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "error": str(e),
                "confidence": {"confidence": 0, "reliability_level": "VERY_LOW"},
                "processing_time": response_time
            }
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        stats = self.performance_monitor.get_stats()
        memory_usage = self.memory_optimizer.monitor_memory_usage()
        
        status = {
            "initialized": self.is_initialized,
            "initialization_error": self.initialization_error,
            "memory_usage_percent": memory_usage,
            "performance_stats": stats,
            "timestamp": datetime.now().isoformat(),
            "prompt_system": {
                "unified_manager_available": self.unified_prompt_manager is not None,
                "dynamic_updater_available": self.dynamic_prompt_updater is not None,
                "prompt_optimizer_available": self.prompt_optimizer is not None,
                "prompt_analytics": self.unified_prompt_manager.get_prompt_analytics() if self.unified_prompt_manager else {},
                "optimization_recommendations": self.prompt_optimizer.get_optimization_recommendations() if self.prompt_optimizer else []
            }
        }
        
        # ChatService ìƒíƒœ ì¶”ê°€
        if self.chat_service:
            try:
                chat_status = self.chat_service.get_service_status()
                status["chat_service"] = chat_status
            except Exception as e:
                status["chat_service_error"] = str(e)
        
        # Phase 1: ëŒ€í™” ë§¥ë½ ê°•í™” ìƒíƒœ ì¶”ê°€
        try:
            phase1_status = {
                "session_manager_available": self.session_manager is not None,
                "multi_turn_handler_available": self.multi_turn_handler is not None,
                "context_compressor_available": self.context_compressor is not None,
                "current_session_id": self.current_session_id,
                "current_user_id": self.current_user_id
            }
            
            # ì„¸ì…˜ í†µê³„ ì¶”ê°€
            if self.session_manager:
                session_stats = self.session_manager.get_session_stats()
                phase1_status["session_stats"] = session_stats
            
            status["phase1_context_enhancement"] = phase1_status
            
        except Exception as e:
            status["phase1_error"] = str(e)
        
        # Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„ ìƒíƒœ ì¶”ê°€
        try:
            phase2_status = {
                "user_profile_manager_available": self.user_profile_manager is not None,
                "emotion_intent_analyzer_available": self.emotion_intent_analyzer is not None,
                "conversation_flow_tracker_available": self.conversation_flow_tracker is not None,
                "current_user_id": self.current_user_id
            }
            
            # ì‚¬ìš©ì í”„ë¡œí•„ í†µê³„ ì¶”ê°€
            if self.user_profile_manager:
                try:
                    user_stats = self.user_profile_manager.get_user_statistics(self.current_user_id)
                    phase2_status["user_profile_stats"] = user_stats
                except Exception as e:
                    phase2_status["user_profile_error"] = str(e)
            
            status["phase2_personalization_analysis"] = phase2_status
            
        except Exception as e:
            status["phase2_error"] = str(e)
        
        # Phase 3: ì¥ê¸° ê¸°ì–µ ë° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ìƒíƒœ ì¶”ê°€
        try:
            phase3_status = {
                "contextual_memory_manager_available": self.contextual_memory_manager is not None,
                "quality_monitor_available": self.quality_monitor is not None,
                "current_user_id": self.current_user_id
            }
            
            # ë©”ëª¨ë¦¬ í†µê³„ ì¶”ê°€
            if self.contextual_memory_manager:
                try:
                    memory_stats = self.contextual_memory_manager.get_memory_statistics(self.current_user_id)
                    phase3_status["memory_stats"] = memory_stats
                except Exception as e:
                    phase3_status["memory_error"] = str(e)
            
            # í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¶”ê°€
            if self.quality_monitor:
                try:
                    quality_dashboard = self.quality_monitor.get_quality_dashboard_data(self.current_user_id)
                    phase3_status["quality_dashboard"] = quality_dashboard
                except Exception as e:
                    phase3_status["quality_error"] = str(e)
            
            status["phase3_memory_quality"] = phase3_status
            
        except Exception as e:
            status["phase3_error"] = str(e)
        
        return status

# ì „ì—­ ì•± ì¸ìŠ¤í„´ìŠ¤
app_instance = HuggingFaceSpacesApp()

def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    if not app_instance.initialize_components():
        logger.error("Failed to initialize components")
    
    # ì»¤ìŠ¤í…€ CSS
    css = """
    .gradio-container {
        max-width: 1200px !important;
        margin: auto !important;
    }
    .chat-message {
        padding: 10px;
        margin: 5px 0;
        border-radius: 10px;
    }
    .user-message {
        background-color: #e3f2fd;
        text-align: right;
    }
    .bot-message {
        background-color: #f5f5f5;
        text-align: left;
    }
    .confidence-high {
        color: #4caf50;
        font-weight: bold;
    }
    .confidence-medium {
        color: #ff9800;
        font-weight: bold;
    }
    .confidence-low {
        color: #f44336;
        font-weight: bold;
    }
    
    /* ìŠ¤íŠ¸ë¦¼ ëª¨ë“œìš© CSS */
    .stream-toggle {
        background: linear-gradient(45deg, #667eea, #764ba2) !important;
        color: white !important;
        border-radius: 8px !important;
        padding: 8px 16px !important;
        font-weight: 500 !important;
    }
    
    .stream-toggle:hover {
        background: linear-gradient(45deg, #5a6fd8, #6a4190) !important;
        transform: translateY(-1px) !important;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3) !important;
    }
    
    /* ìŠ¤íŠ¸ë¦¼ ì• ë‹ˆë©”ì´ì…˜ */
    @keyframes stream-pulse {
        0% { opacity: 1; }
        50% { opacity: 0.7; }
        100% { opacity: 1; }
    }
    
    .streaming {
        animation: stream-pulse 1.5s infinite !important;
    }
    
    /* íƒ€ì´í•‘ íš¨ê³¼ */
    @keyframes typing {
        from { width: 0; }
        to { width: 100%; }
    }
    
    .typing-effect {
        overflow: hidden;
        white-space: nowrap;
        animation: typing 2s steps(40, end);
    }
    """
    
    # HTML í—¤ë“œì— ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë° ë©”íƒ€ íƒœê·¸ ì¶”ê°€
    head_html = """
    <link rel="manifest" href="/static/manifest.json">
    <meta name="theme-color" content="#000000">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="default">
    <meta name="apple-mobile-web-app-title" content="LawFirmAI">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    """
    
    with gr.Blocks(
        css=css, 
        title="LawFirmAI - ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸",
        head=head_html
    ) as interface:
        gr.Markdown("""
        # âš–ï¸ LawFirmAI - ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸
        
        **Phase 1 ì™„ë£Œ**: ëŒ€í™” ë§¥ë½ ê°•í™”, ë‹¤ì¤‘ í„´ ì§ˆë¬¸ ì²˜ë¦¬, ì»¨í…ìŠ¤íŠ¸ ì••ì¶•, ì˜êµ¬ì  ì„¸ì…˜ ì €ì¥
        **Phase 2 ì™„ë£Œ**: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„, ê°ì •/ì˜ë„ ë¶„ì„, ëŒ€í™” íë¦„ ì¶”ì , ì‚¬ìš©ì í”„ë¡œí•„ ê´€ë¦¬
        
        ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
        """)
        
        with gr.Row():
            with gr.Column(scale=3):
                # íƒ­ êµ¬ì¡° ì¶”ê°€
                with gr.Tabs():
                    with gr.Tab("ğŸ’¬ ì±„íŒ…"):
                        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
                        chatbot = gr.Chatbot(
                            label="ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸",
                            height=500,
                            show_label=True,
                            type="messages"  # ìµœì‹  Gradio í˜•ì‹ ì‚¬ìš©
                        )
                        
                        with gr.Row():
                            msg = gr.Textbox(
                                placeholder="ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
                                label="ì§ˆë¬¸",
                                scale=4
                            )
                            submit_btn = gr.Button("ì „ì†¡", scale=1, variant="primary")
                        
                        # ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ í† ê¸€
                        with gr.Row():
                            stream_mode = gr.Checkbox(label="ğŸ”„ ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ (ì‹¤ì‹œê°„ ë‹µë³€)", value=True, elem_classes=["stream-toggle"])
                        
                        # ì˜ˆì‹œ ì§ˆë¬¸
                        gr.Examples(
                            examples=[
                                "ê³„ì•½ í•´ì œ ì¡°ê±´ì´ ë¬´ì—‡ì¸ê°€ìš”?",
                                "ì†í•´ë°°ìƒ ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                                "ë¶ˆë²•í–‰ìœ„ì˜ ë²•ì  ê·¼ê±°ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                                "ì´í˜¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ì§„í–‰í•˜ë‚˜ìš”?",
                                "ë¯¼ë²• ì œ750ì¡°ì˜ ë‚´ìš©ì´ ë¬´ì—‡ì¸ê°€ìš”?"
                            ],
                            inputs=msg
                        )
                    
                    with gr.Tab("ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„"):
                        with gr.Row():
                            with gr.Column():
                                user_id_input = gr.Textbox(
                                    label="ì‚¬ìš©ì ID",
                                    value="gradio_user",
                                    placeholder="ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•˜ì„¸ìš”"
                                )
                                expertise_level = gr.Radio(
                                    choices=["beginner", "intermediate", "advanced", "expert"],
                                    value="beginner",
                                    label="ì „ë¬¸ì„± ìˆ˜ì¤€"
                                )
                                detail_level = gr.Radio(
                                    choices=["simple", "medium", "detailed"],
                                    value="medium",
                                    label="ë‹µë³€ ìƒì„¸ë„"
                                )
                                interest_areas = gr.CheckboxGroup(
                                    choices=["ë¯¼ë²•", "í˜•ë²•", "ìƒë²•", "ê·¼ë¡œê¸°ì¤€ë²•", "ë¶€ë™ì‚°", "ê¸ˆìœµ", "ì§€ì ì¬ì‚°ê¶Œ", "ì„¸ë²•", "í™˜ê²½ë²•", "ì˜ë£Œë²•"],
                                    label="ê´€ì‹¬ ë¶„ì•¼",
                                    value=[]
                                )
                                save_profile_btn = gr.Button("í”„ë¡œí•„ ì €ì¥", variant="primary")
                            
                            with gr.Column():
                                profile_status = gr.JSON(label="í”„ë¡œí•„ ìƒíƒœ")
                                user_statistics = gr.JSON(label="ì‚¬ìš©ì í†µê³„")
                    
                    with gr.Tab("ğŸ§  ì§€ëŠ¥í˜• ë¶„ì„"):
                        with gr.Row():
                            with gr.Column():
                                emotion_analysis = gr.JSON(label="ê°ì • ë¶„ì„")
                                intent_analysis = gr.JSON(label="ì˜ë„ ë¶„ì„")
                                response_tone = gr.JSON(label="ì‘ë‹µ í†¤")
                            
                            with gr.Column():
                                flow_tracking = gr.JSON(label="ëŒ€í™” íë¦„ ì¶”ì ")
                                suggested_questions = gr.JSON(label="ì œì•ˆëœ í›„ì† ì§ˆë¬¸")
                                conversation_state = gr.Textbox(label="ëŒ€í™” ìƒíƒœ", interactive=False)
                    
                    with gr.Tab("ğŸ“Š ëŒ€í™” ì´ë ¥"):
                        session_list = gr.Dataframe(
                            label="ì„¸ì…˜ ëª©ë¡",
                            headers=["ì„¸ì…˜ ID", "ìƒì„±ì¼", "ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸", "í„´ ìˆ˜", "ì£¼ì œ"],
                            interactive=False
                        )
                        load_session_btn = gr.Button("ì„¸ì…˜ ë¶ˆëŸ¬ì˜¤ê¸°")
                        session_details = gr.JSON(label="ì„¸ì…˜ ìƒì„¸ ì •ë³´")
                    
                        with gr.Tab("ğŸ§  ì¥ê¸° ê¸°ì–µ"):
                            with gr.Row():
                                with gr.Column():
                                    memory_search_query = gr.Textbox(
                                        label="ë©”ëª¨ë¦¬ ê²€ìƒ‰",
                                        placeholder="ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”"
                                    )
                                    search_memory_btn = gr.Button("ë©”ëª¨ë¦¬ ê²€ìƒ‰", variant="primary")
                                    memory_search_results = gr.JSON(label="ê²€ìƒ‰ ê²°ê³¼")
                                
                                with gr.Column():
                                    memory_statistics = gr.JSON(label="ë©”ëª¨ë¦¬ í†µê³„")
                                    consolidate_memory_btn = gr.Button("ë©”ëª¨ë¦¬ í†µí•©", variant="secondary")
                                    cleanup_memory_btn = gr.Button("ì˜¤ë˜ëœ ë©”ëª¨ë¦¬ ì •ë¦¬", variant="secondary")
                        
                        with gr.Tab("ğŸ“Š í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"):
                            with gr.Row():
                                with gr.Column():
                                    quality_assessment = gr.JSON(label="ëŒ€í™” í’ˆì§ˆ í‰ê°€")
                                    quality_trends = gr.JSON(label="í’ˆì§ˆ íŠ¸ë Œë“œ")
                                    quality_issues = gr.JSON(label="ê°ì§€ëœ ë¬¸ì œì ")
                                
                                with gr.Column():
                                    improvement_suggestions = gr.JSON(label="ê°œì„  ì œì•ˆ")
                                    quality_dashboard = gr.JSON(label="í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ")
                                    refresh_quality_btn = gr.Button("í’ˆì§ˆ ë¶„ì„ ìƒˆë¡œê³ ì¹¨", variant="secondary")
                        
                        with gr.Tab("âš™ï¸ ê³ ê¸‰ ì„¤ì •"):
                            with gr.Row():
                                with gr.Column():
                                    show_follow_ups = gr.Checkbox(label="í›„ì† ì§ˆë¬¸ ì œì•ˆ í‘œì‹œ", value=True)
                                    enable_emotion_analysis = gr.Checkbox(label="ê°ì • ë¶„ì„ í™œì„±í™”", value=True)
                                    enable_flow_tracking = gr.Checkbox(label="ëŒ€í™” íë¦„ ì¶”ì  í™œì„±í™”", value=True)
                                    enable_memory_management = gr.Checkbox(label="ì¥ê¸° ê¸°ì–µ ê´€ë¦¬ í™œì„±í™”", value=True)
                                    enable_quality_monitoring = gr.Checkbox(label="í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ í™œì„±í™”", value=True)
                                    max_context_turns = gr.Slider(
                                        minimum=5, maximum=20, value=10, step=1,
                                        label="ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ í„´ ìˆ˜"
                                    )
                                
                                with gr.Column():
                                    compression_threshold = gr.Slider(
                                        minimum=1000, maximum=5000, value=2000, step=100,
                                        label="ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì„ê³„ê°’ (í† í°)"
                                    )
                                    session_timeout = gr.Slider(
                                        minimum=1, maximum=24, value=24, step=1,
                                        label="ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ (ì‹œê°„)"
                                    )
                                    memory_retention_days = gr.Slider(
                                        minimum=7, maximum=90, value=30, step=1,
                                        label="ë©”ëª¨ë¦¬ ë³´ê´€ ê¸°ê°„ (ì¼)"
                                    )
                                    auto_backup = gr.Checkbox(label="ìë™ ë°±ì—… í™œì„±í™”", value=True)
                    
                    # AKLS í‘œì¤€íŒë¡€ ê²€ìƒ‰ íƒ­ ì¶”ê°€ (ì„ì‹œ ë¹„í™œì„±í™”)
                    # akls_tab = create_akls_interface()
            
            with gr.Column(scale=1):
                # ì‹œìŠ¤í…œ ìƒíƒœ
                status_output = gr.JSON(
                    label="ì‹œìŠ¤í…œ ìƒíƒœ",
                    value=app_instance.get_system_status()
                )
                
                # ì‹ ë¢°ë„ ì •ë³´
                confidence_output = gr.JSON(
                    label="ì‹ ë¢°ë„ ì •ë³´"
                )
                
                # ì„±ëŠ¥ í†µê³„
                performance_output = gr.JSON(
                    label="ì„±ëŠ¥ í†µê³„"
                )
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        def respond(message, history):
            """ì‘ë‹µ ìƒì„± (Phase 1 ëŒ€í™” ë§¥ë½ ê°•í™” ì ìš©)"""
            if not message.strip():
                return history, "", {}
            
            # ì§ˆì˜ ì²˜ë¦¬ (Phase 1 ê¸°ëŠ¥ í¬í•¨)
            result = app_instance.process_query(message)
            
            # ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜ (type="messages"ì— ë§ê²Œ)
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": result["answer"]})
            
            # ì‹ ë¢°ë„ ì •ë³´ (Phase 1 ë° Phase 2 ì •ë³´ í¬í•¨)
            confidence_info = {
                "ì‹ ë¢°ë„": f"{result['confidence']['confidence']:.1%}",
                "ìˆ˜ì¤€": result['confidence']['reliability_level'],
                "ì²˜ë¦¬ ì‹œê°„": f"{result.get('processing_time', 0):.2f}ì´ˆ",
                "ì§ˆë¬¸ ìœ í˜•": result.get('question_type', 'Unknown'),
                "ì„¸ì…˜ ID": result.get('session_id', 'Unknown'),
                "ì‚¬ìš©ì ID": result.get('user_id', 'Unknown')
            }
        
        def respond_stream(message, history):
            """ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ ìƒì„±"""
            if not message.strip():
                return history, "", {}
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            history.append({"role": "user", "content": message})
            
            # ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜
            import time
            
            # ì´ˆê¸° ìƒíƒœ ë©”ì‹œì§€
            history.append({"role": "assistant", "content": "ğŸ”„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."})
            yield history, "", {}
            time.sleep(0.5)
            
            # ê²€ìƒ‰ ìƒíƒœ ë©”ì‹œì§€
            history[-1] = {"role": "assistant", "content": "ğŸ” ê´€ë ¨ ë²•ë ¹ê³¼ íŒë¡€ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."}
            yield history, "", {}
            time.sleep(0.5)
            
            # ë‹µë³€ ìƒì„± ìƒíƒœ ë©”ì‹œì§€
            history[-1] = {"role": "assistant", "content": "ğŸ“ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."}
            yield history, "", {}
            time.sleep(0.5)
            
            # ì‹¤ì œ ë‹µë³€ ìƒì„±
            result = app_instance.process_query(message)
            answer = result.get("answer", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë‹µë³€ì„ ë‹¨ì–´ë³„ë¡œ ìŠ¤íŠ¸ë¦¼
            words = answer.split()
            current_response = ""
            
            for i, word in enumerate(words):
                current_response += word + " "
                history[-1] = {"role": "assistant", "content": current_response.strip()}
                
                # ì‹ ë¢°ë„ ì •ë³´ ì—…ë°ì´íŠ¸
                confidence_info = {
                    "ì‹ ë¢°ë„": f"{result['confidence']['confidence']:.1%}",
                    "ìˆ˜ì¤€": result['confidence']['reliability_level'],
                    "ì²˜ë¦¬ ì‹œê°„": f"{result.get('processing_time', 0):.2f}ì´ˆ",
                    "ì§ˆë¬¸ ìœ í˜•": result.get('question_type', 'Unknown'),
                    "ì„¸ì…˜ ID": result.get('session_id', 'Unknown'),
                    "ì‚¬ìš©ì ID": result.get('user_id', 'Unknown'),
                    "ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ": "í™œì„±í™”",
                    "ì§„í–‰ë¥ ": f"{((i + 1) / len(words) * 100):.0f}%"
                }
                
                yield history, "", confidence_info
                time.sleep(0.1)  # ìŠ¤íŠ¸ë¦¼ íš¨ê³¼ë¥¼ ìœ„í•œ ì§€ì—°
            
            # ë‹¤ì¤‘ í„´ ì§ˆë¬¸ ì •ë³´ ì¶”ê°€
            if result.get('multi_turn_info', {}).get('is_multi_turn'):
                multi_turn = result['multi_turn_info']
                confidence_info["ë‹¤ì¤‘ í„´ ì§ˆë¬¸"] = "ì˜ˆ"
                confidence_info["ì›ë³¸ ì§ˆë¬¸"] = multi_turn.get('original_query', '')
                confidence_info["í•´ê²°ëœ ì§ˆë¬¸"] = multi_turn.get('resolved_query', '')
                confidence_info["í•´ê²° ì‹ ë¢°ë„"] = f"{multi_turn.get('confidence', 0):.1%}"
            else:
                confidence_info["ë‹¤ì¤‘ í„´ ì§ˆë¬¸"] = "ì•„ë‹ˆì˜¤"
            
            # ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì •ë³´ ì¶”ê°€
            if result.get('compression_info', {}).get('compressed'):
                compression = result['compression_info']
                confidence_info["ì»¨í…ìŠ¤íŠ¸ ì••ì¶•"] = "ì˜ˆ"
                confidence_info["ì••ì¶• ë¹„ìœ¨"] = f"{compression.get('compression_ratio', 0):.1%}"
                confidence_info["ì••ì¶• ìš”ì•½"] = compression.get('summary', '')[:100] + "..."
            else:
                confidence_info["ì»¨í…ìŠ¤íŠ¸ ì••ì¶•"] = "ì•„ë‹ˆì˜¤"
            
            # ì»¨í…ìŠ¤íŠ¸ í†µê³„ ì¶”ê°€
            context_stats = result.get('context_stats', {})
            confidence_info["ì´ ëŒ€í™” í„´"] = context_stats.get('total_turns', 0)
            confidence_info["ì´ ì—”í‹°í‹°"] = context_stats.get('total_entities', 0)
            confidence_info["ì£¼ì œ"] = ", ".join(context_stats.get('topics', []))
            
            # Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„ ì •ë³´ ì¶”ê°€
            personalized_context = result.get('personalized_context', {})
            if personalized_context:
                confidence_info["ê°œì¸í™” ì ìˆ˜"] = f"{personalized_context.get('personalization_score', 0):.1%}"
                confidence_info["ì „ë¬¸ì„± ìˆ˜ì¤€"] = personalized_context.get('expertise_level', 'Unknown')
                confidence_info["ê´€ì‹¬ ë¶„ì•¼"] = ", ".join(personalized_context.get('interest_areas', []))
            
            emotion_intent_info = result.get('emotion_intent_info', {})
            if emotion_intent_info:
                emotion = emotion_intent_info.get('emotion', {})
                intent = emotion_intent_info.get('intent', {})
                response_tone = emotion_intent_info.get('response_tone', {})
                
                confidence_info["ê°ì •"] = emotion.get('primary_emotion', 'Unknown')
                confidence_info["ì˜ë„"] = intent.get('primary_intent', 'Unknown')
                confidence_info["ê¸´ê¸‰ë„"] = intent.get('urgency_level', 'Unknown')
                confidence_info["ì‘ë‹µ í†¤"] = response_tone.get('tone_type', 'Unknown')
                confidence_info["ê³µê° ìˆ˜ì¤€"] = f"{response_tone.get('empathy_level', 0):.1%}"
                confidence_info["ê²©ì‹ ìˆ˜ì¤€"] = f"{response_tone.get('formality_level', 0):.1%}"
            
            flow_tracking_info = result.get('flow_tracking_info', {})
            if flow_tracking_info:
                confidence_info["ëŒ€í™” ìƒíƒœ"] = flow_tracking_info.get('conversation_state', 'Unknown')
                suggested_questions = flow_tracking_info.get('suggested_questions', [])
                confidence_info["ì œì•ˆëœ ì§ˆë¬¸ ìˆ˜"] = len(suggested_questions)
                if suggested_questions:
                    confidence_info["ì œì•ˆëœ ì§ˆë¬¸"] = suggested_questions[:3]  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
            
            # Phase 3: ì¥ê¸° ê¸°ì–µ ë° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì •ë³´ ì¶”ê°€
            memory_search_results = result.get('memory_search_results', [])
            if memory_search_results:
                confidence_info["ê´€ë ¨ ë©”ëª¨ë¦¬ ìˆ˜"] = len(memory_search_results)
                confidence_info["ìµœê³  ê´€ë ¨ì„± ì ìˆ˜"] = f"{max([m.get('relevance_score', 0) for m in memory_search_results]):.2f}"
            
            quality_assessment = result.get('quality_assessment', {})
            if quality_assessment:
                confidence_info["í’ˆì§ˆ ì ìˆ˜"] = f"{quality_assessment.get('overall_score', 0):.2f}"
                confidence_info["ì™„ê²°ì„± ì ìˆ˜"] = f"{quality_assessment.get('completeness_score', 0):.2f}"
                confidence_info["ë§Œì¡±ë„ ì ìˆ˜"] = f"{quality_assessment.get('satisfaction_score', 0):.2f}"
                confidence_info["ì •í™•ì„± ì ìˆ˜"] = f"{quality_assessment.get('accuracy_score', 0):.2f}"
                
                issues = quality_assessment.get('issues', [])
                if issues:
                    confidence_info["ê°ì§€ëœ ë¬¸ì œì "] = issues[:3]  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                
                suggestions = quality_assessment.get('suggestions', [])
                if suggestions:
                    confidence_info["ê°œì„  ì œì•ˆ"] = suggestions[:2]  # ìµœëŒ€ 2ê°œë§Œ í‘œì‹œ
            
            # Phaseë³„ ìƒíƒœ ì •ë³´ ì¶”ê°€
            phase_info = result.get('phase_info', {})
            if phase_info:
                phase1_status = phase_info.get('phase1', {})
                phase2_status = phase_info.get('phase2', {})
                phase3_status = phase_info.get('phase3', {})
                
                confidence_info["Phase 1 í™œì„±í™”"] = "ì˜ˆ" if phase1_status.get('enabled') else "ì•„ë‹ˆì˜¤"
                confidence_info["Phase 2 í™œì„±í™”"] = "ì˜ˆ" if phase2_status.get('enabled') else "ì•„ë‹ˆì˜¤"
                confidence_info["Phase 3 í™œì„±í™”"] = "ì˜ˆ" if phase3_status.get('enabled') else "ì•„ë‹ˆì˜¤"
                
                # ì—ëŸ¬ ì •ë³´ ì¶”ê°€
                phase1_errors = phase1_status.get('errors', [])
                phase2_errors = phase2_status.get('errors', [])
                phase3_errors = phase3_status.get('errors', [])
                
                if phase1_errors:
                    confidence_info["Phase 1 ì—ëŸ¬"] = phase1_errors[0]  # ì²« ë²ˆì§¸ ì—ëŸ¬ë§Œ í‘œì‹œ
                if phase2_errors:
                    confidence_info["Phase 2 ì—ëŸ¬"] = phase2_errors[0]
                if phase3_errors:
                    confidence_info["Phase 3 ì—ëŸ¬"] = phase3_errors[0]
            
            return history, "", confidence_info
        
        def update_status():
            """ìƒíƒœ ì—…ë°ì´íŠ¸"""
            return app_instance.get_system_status()
        
        def update_performance():
            """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
            return app_instance.performance_monitor.get_stats()
        
        # Phase 2 ê¸°ëŠ¥ë“¤ì„ ìœ„í•œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë“¤
        def save_user_profile(user_id, expertise_level, detail_level, interest_areas):
            """ì‚¬ìš©ì í”„ë¡œí•„ ì €ì¥"""
            try:
                profile_data = {
                    "expertise_level": expertise_level,
                    "preferred_detail_level": detail_level,
                    "interest_areas": interest_areas
                }
                
                success = app_instance.user_profile_manager.create_profile(user_id, profile_data)
                if success:
                    return {"status": "success", "message": "í”„ë¡œí•„ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}
                else:
                    return {"status": "error", "message": "í”„ë¡œí•„ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
            except Exception as e:
                return {"status": "error", "message": f"ì˜¤ë¥˜: {str(e)}"}
        
        def get_user_statistics(user_id):
            """ì‚¬ìš©ì í†µê³„ ì¡°íšŒ"""
            try:
                stats = app_instance.user_profile_manager.get_user_statistics(user_id)
                return stats
            except Exception as e:
                return {"error": str(e)}
        
        def load_session_history():
            """ì„¸ì…˜ ì´ë ¥ ë¡œë“œ"""
            try:
                sessions = app_instance.session_manager.get_user_sessions(app_instance.current_user_id, limit=10)
                session_data = []
                for session in sessions:
                    session_data.append([
                        session["session_id"],
                        session["created_at"],
                        session["last_updated"],
                        session.get("turn_count", 0),
                        ", ".join(session.get("topics", []))
                    ])
                return session_data
            except Exception as e:
                return []
        
        def get_session_details(session_id):
            """ì„¸ì…˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
            try:
                session_data = app_instance.session_manager.conversation_store.load_session(session_id)
                return session_data
            except Exception as e:
                return {"error": str(e)}
        
        def search_memory(query):
            """ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
            try:
                if app_instance.contextual_memory_manager:
                    results = app_instance.contextual_memory_manager.retrieve_relevant_memory(
                        app_instance.current_session_id, query, app_instance.current_user_id
                    )
                    return [
                        {
                            "memory_id": result.memory.memory_id,
                            "content": result.memory.content,
                            "memory_type": result.memory.memory_type,
                            "importance_score": result.memory.importance_score,
                            "relevance_score": result.relevance_score,
                            "match_reason": result.match_reason
                        } for result in results[:10]  # ìƒìœ„ 10ê°œë§Œ
                    ]
                else:
                    return []
            except Exception as e:
                return {"error": str(e)}
        
        def get_memory_statistics():
            """ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ"""
            try:
                if app_instance.contextual_memory_manager:
                    stats = app_instance.contextual_memory_manager.get_memory_statistics(app_instance.current_user_id)
                    return stats
                else:
                    return {"error": "Memory manager not available"}
            except Exception as e:
                return {"error": str(e)}
        
        def consolidate_memories():
            """ë©”ëª¨ë¦¬ í†µí•©"""
            try:
                if app_instance.contextual_memory_manager:
                    count = app_instance.contextual_memory_manager.consolidate_memories(app_instance.current_user_id)
                    return {"status": "success", "consolidated_count": count}
                else:
                    return {"error": "Memory manager not available"}
            except Exception as e:
                return {"error": str(e)}
        
        def cleanup_old_memories():
            """ì˜¤ë˜ëœ ë©”ëª¨ë¦¬ ì •ë¦¬"""
            try:
                if app_instance.contextual_memory_manager:
                    count = app_instance.contextual_memory_manager.cleanup_old_memories(days=30)
                    return {"status": "success", "cleaned_count": count}
                else:
                    return {"error": "Memory manager not available"}
            except Exception as e:
                return {"error": str(e)}
        
        def get_quality_assessment():
            """í’ˆì§ˆ í‰ê°€ ì¡°íšŒ"""
            try:
                if app_instance.quality_monitor and app_instance.session_manager:
                    context = app_instance.session_manager.get_or_create_session(
                        app_instance.current_session_id, app_instance.current_user_id
                    )
                    assessment = app_instance.quality_monitor.assess_conversation_quality(context)
                    return assessment
                else:
                    return {"error": "Quality monitor not available"}
            except Exception as e:
                return {"error": str(e)}
        
        def get_quality_trends():
            """í’ˆì§ˆ íŠ¸ë Œë“œ ì¡°íšŒ"""
            try:
                if app_instance.quality_monitor:
                    # ìµœê·¼ ì„¸ì…˜ë“¤ ì¡°íšŒ
                    sessions = app_instance.session_manager.get_user_sessions(app_instance.current_user_id, limit=10)
                    session_ids = [session["session_id"] for session in sessions]
                    trends = app_instance.quality_monitor.analyze_quality_trends(session_ids)
                    return trends
                else:
                    return {"error": "Quality monitor not available"}
            except Exception as e:
                return {"error": str(e)}
        
        def get_quality_dashboard():
            """í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ"""
            try:
                if app_instance.quality_monitor:
                    dashboard_data = app_instance.quality_monitor.get_quality_dashboard_data(app_instance.current_user_id)
                    return dashboard_data
                else:
                    return {"error": "Quality monitor not available"}
            except Exception as e:
                return {"error": str(e)}
        
        def detect_quality_issues():
            """í’ˆì§ˆ ë¬¸ì œì  ê°ì§€"""
            try:
                if app_instance.quality_monitor and app_instance.session_manager:
                    context = app_instance.session_manager.get_or_create_session(
                        app_instance.current_session_id, app_instance.current_user_id
                    )
                    issues = app_instance.quality_monitor.detect_conversation_issues(context)
                    suggestions = app_instance.quality_monitor.suggest_improvements(context)
                    return {
                        "issues": issues,
                        "suggestions": suggestions
                    }
                else:
                    return {"error": "Quality monitor not available"}
            except Exception as e:
                return {"error": str(e)}
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        def handle_submit(message, history, use_stream):
            """ì œì¶œ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¼/ì¼ë°˜ ëª¨ë“œ ì„ íƒ)"""
            if use_stream:
                return respond_stream(message, history)
            else:
                return respond(message, history)
        
        submit_btn.click(
            handle_submit,
            inputs=[msg, chatbot, stream_mode],
            outputs=[chatbot, msg, confidence_output]
        )
        
        msg.submit(
            handle_submit,
            inputs=[msg, chatbot, stream_mode],
            outputs=[chatbot, msg, confidence_output]
        )
        
        # Phase 2 ì´ë²¤íŠ¸ ì—°ê²°
        save_profile_btn.click(
            save_user_profile,
            inputs=[user_id_input, expertise_level, detail_level, interest_areas],
            outputs=[profile_status]
        )
        
        user_id_input.change(
            get_user_statistics,
            inputs=[user_id_input],
            outputs=[user_statistics]
        )
        
        load_session_btn.click(
            load_session_history,
            outputs=[session_list]
        )
        
        session_list.select(
            get_session_details,
            inputs=[session_list],
            outputs=[session_details]
        )
        
        # Phase 3 ì´ë²¤íŠ¸ ì—°ê²°
        search_memory_btn.click(
            search_memory,
            inputs=[memory_search_query],
            outputs=[memory_search_results]
        )
        
        consolidate_memory_btn.click(
            consolidate_memories,
            outputs=[memory_statistics]
        )
        
        cleanup_memory_btn.click(
            cleanup_old_memories,
            outputs=[memory_statistics]
        )
        
        refresh_quality_btn.click(
            get_quality_assessment,
            outputs=[quality_assessment]
        )
        
        # ìë™ ë¡œë“œ ì´ë²¤íŠ¸ë“¤
        user_id_input.change(
            get_memory_statistics,
            outputs=[memory_statistics]
        )
        
        user_id_input.change(
            get_quality_dashboard,
            outputs=[quality_dashboard]
        )
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ ë²„íŠ¼ ì¶”ê°€ (ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ ëŒ€ì‹  ìˆ˜ë™ ì—…ë°ì´íŠ¸)
        with gr.Row():
            refresh_status_btn = gr.Button("ìƒíƒœ ìƒˆë¡œê³ ì¹¨", variant="secondary")
            refresh_performance_btn = gr.Button("ì„±ëŠ¥ í†µê³„ ìƒˆë¡œê³ ì¹¨", variant="secondary")
        
        # ìˆ˜ë™ ìƒíƒœ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸
        refresh_status_btn.click(
            update_status,
            outputs=status_output
        )
        
        refresh_performance_btn.click(
            update_performance,
            outputs=performance_output
        )
    
        return interface
    
    def get_performance_dashboard(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"""
        with gr.Blocks(title="ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ") as dashboard:
            gr.Markdown("# LawFirmAI ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("## ì‹œìŠ¤í…œ ìƒíƒœ")
                    system_health = gr.JSON(label="ì‹œìŠ¤í…œ ìƒíƒœ", value=self.performance_monitor.get_system_health())
                    
                    refresh_btn = gr.Button("ìƒˆë¡œê³ ì¹¨", variant="secondary")
                    
                with gr.Column():
                    gr.Markdown("## ì„±ëŠ¥ ìš”ì•½ (ìµœê·¼ 24ì‹œê°„)")
                    performance_summary = gr.JSON(label="ì„±ëŠ¥ ìš”ì•½", value=self.performance_monitor.get_performance_summary())
            
            with gr.Row():
                gr.Markdown("## ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°")
                export_btn = gr.Button("ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°", variant="primary")
                export_status = gr.Textbox(label="ë‚´ë³´ë‚´ê¸° ìƒíƒœ", interactive=False)
            
            def refresh_data():
                return (
                    self.performance_monitor.get_system_health(),
                    self.performance_monitor.get_performance_summary()
                )
            
            def export_metrics():
                try:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filepath = f"performance_metrics_{timestamp}.json"
                    self.performance_monitor.export_metrics(filepath)
                    return f"ë©”íŠ¸ë¦­ì´ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
                except Exception as e:
                    return f"ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}"
            
            refresh_btn.click(
                fn=refresh_data,
                outputs=[system_health, performance_summary]
            )
            
            export_btn.click(
                fn=export_metrics,
                outputs=[export_status]
            )
        
        return dashboard

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("Starting LawFirmAI HuggingFace Spaces application...")
    
    # Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
    interface = create_gradio_interface()
    
    # ì•ˆì •ì ì¸ ì‹¤í–‰ ì„¤ì •
    interface.launch(
        server_name="0.0.0.0",
        server_port=7861,
        share=False,
        show_error=True,
        quiet=False,
        max_threads=40,
        # ê³µìœ  ê¸°ëŠ¥ ì™„ì „ ë¹„í™œì„±í™”
        show_api=False,
        # ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •
        favicon_path="gradio/static/favicon.ico" if os.path.exists("gradio/static/favicon.ico") else None
    )

if __name__ == "__main__":
    main()
