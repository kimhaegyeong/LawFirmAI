"""
ì±„íŒ… ì„œë¹„ìŠ¤ (lawfirm_langgraph ë˜í¼)
"""
import sys
import json
import logging
import asyncio
import uuid
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, AsyncGenerator

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# lawfirm_langgraph ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€ (core ëª¨ë“ˆ importë¥¼ ìœ„í•´)
lawfirm_langgraph_path = project_root / "lawfirm_langgraph"
if lawfirm_langgraph_path.exists():
    sys.path.insert(0, str(lawfirm_langgraph_path))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œë” ì‚¬ìš©)
try:
    from utils.env_loader import ensure_env_loaded
    ensure_env_loaded(project_root)
except ImportError as e:
    logging.warning(f"âš ï¸  Failed to load environment variables: {e}")
    logging.warning("   Make sure utils/env_loader.py exists in the project root")
except Exception as e:
    logging.warning(f"âš ï¸  Failed to load environment variables: {e}")

try:
    from lawfirm_langgraph.core.workflow.workflow_service import LangGraphWorkflowService
    from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
    LANGGRAPH_AVAILABLE = True
except ImportError as e:
    LANGGRAPH_AVAILABLE = False
    logging.warning(f"LangGraph not available: {e}")

logger = logging.getLogger(__name__)

# ë¡œê±° ë ˆë²¨ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (ë£¨íŠ¸ ë¡œê±° ë ˆë²¨ê³¼ ë™ê¸°í™”)
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œê·¸ ë ˆë²¨ ì½ê¸°
import os  # noqa: E402
log_level_str = os.getenv("LOG_LEVEL", "info").upper()
log_level_map = {
    "CRITICAL": logging.CRITICAL,
    "ERROR": logging.ERROR,
    "WARNING": logging.WARNING,
    "INFO": logging.INFO,
    "DEBUG": logging.DEBUG,
}
log_level = log_level_map.get(log_level_str, logging.INFO)
logger.setLevel(log_level)
logger.disabled = False  # ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™”
logger.propagate = True  # ë£¨íŠ¸ ë¡œê±°ë¡œ ì „íŒŒ

# ë¡œê¹…ì´ ë¹„í™œì„±í™”ë˜ì§€ ì•Šë„ë¡ ë³´í˜¸
logging.disable(logging.NOTSET)  # ëª¨ë“  ë¡œê¹… í™œì„±í™”

# ë£¨íŠ¸ ë¡œê±°ì— í•¸ë“¤ëŸ¬ê°€ ì—†ìœ¼ë©´ ì¶”ê°€ (ëª¨ë“ˆ import ì‹œì ì— ë¡œê¹…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ)
root_logger = logging.getLogger()
if not root_logger.handlers:
    handler = logging.StreamHandler()
    handler.setLevel(log_level)
    handler.setFormatter(
        logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    )
    root_logger.addHandler(handler)
    root_logger.setLevel(log_level)
    root_logger.disabled = False


class ChatService:
    """ì±„íŒ… ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.workflow_service: Optional[LangGraphWorkflowService] = None
        self._initialize_workflow()
        
        # ìŠ¤íŠ¸ë¦¬ë° ì„¤ì • ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        from api.services.stream_config import StreamConfig
        from api.services.stream_event_processor import StreamEventProcessor
        from api.services.sources_extractor import SourcesExtractor
        from api.services.session_service import session_service
        from api.services.streaming.stream_handler import StreamHandler
        from api.utils.langgraph_config_helper import create_langgraph_config
        
        self.stream_config = StreamConfig.from_env()
        self.event_processor = StreamEventProcessor(config=self.stream_config)
        self.sources_extractor = SourcesExtractor(
            workflow_service=self.workflow_service,
            session_service=session_service
        )
        self.stream_handler = StreamHandler(
            workflow_service=self.workflow_service,
            sources_extractor=self.sources_extractor,
            extract_related_questions_fn=self._extract_related_questions_from_state
        )
        
        logger.info("âœ… ChatService.__init__() completed")
    
    def _initialize_workflow(self):
        """ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”"""
        if not LANGGRAPH_AVAILABLE:
            logger.warning("LangGraph is not available. Service will continue without LangGraph features.")
            return
        
        try:
            import os
            # í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (ë¯¼ê° ì •ë³´ëŠ” ë¡œê·¸ì— ë…¸ì¶œí•˜ì§€ ì•ŠìŒ)
            google_api_key = os.getenv("GOOGLE_API_KEY", "")
            if not google_api_key:
                logger.warning("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                logger.warning("LangGraphëŠ” Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                logger.info("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            logger.info("Loading LangGraphConfig from environment...")
            
            config = LangGraphConfig.from_env()
            logger.info(f"LangGraph Config loaded: langgraph_enabled={config.langgraph_enabled}, llm_provider={config.llm_provider}")
            
            if not config.langgraph_enabled:
                logger.warning("LangGraph is disabled in configuration")
                return
            
            logger.info("Initializing LangGraphWorkflowService...")
            
            self.workflow_service = LangGraphWorkflowService(config)
            logger.info("âœ… ChatService initialized successfully with LangGraph workflow")
        except ImportError as e:
            logger.error(f"Import error during workflow initialization: {e}", exc_info=True)
            self.workflow_service = None
        except Exception as e:
            logger.error(f"Failed to initialize workflow service: {e}", exc_info=True)
            import traceback
            tb = traceback.format_exc()
            logger.error(f"Traceback:\n{tb}")
            self.workflow_service = None
    
    async def process_message(
        self,
        message: str,
        session_id: Optional[str] = None,
        enable_checkpoint: bool = True
    ) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ ì²˜ë¦¬
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            enable_checkpoint: ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        if not self.workflow_service:
            import os
            error_details = []
            
            # ì›ì¸ ë¶„ì„
            if not LANGGRAPH_AVAILABLE:
                error_details.append("LangGraph ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                google_api_key = os.getenv("GOOGLE_API_KEY", "")
                if not google_api_key:
                    error_details.append("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                else:
                    error_details.append("ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
            error_msg = f"Workflow service unavailable. Details: {', '.join(error_details)}"
            logger.error(error_msg)
            
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\nì›ì¸:\n" + "\n".join(f"- {detail}" for detail in error_details) + "\n\nAPI ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "confidence": 0.0,
                "legal_references": [],
                "processing_steps": ["ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨"],
                "session_id": session_id or "error",
                "processing_time": 0.0,
                "query_type": "error",
                "metadata": {"error_details": error_details},
                "errors": error_details
            }
        
        try:
            result = await self.workflow_service.process_query(
                query=message,
                session_id=session_id,
                enable_checkpoint=enable_checkpoint
            )
            return result
        except asyncio.CancelledError:
            logger.warning(f"âš ï¸ [process_message] ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (CancelledError)")
            import os
            debug_mode = os.getenv("DEBUG", "false").lower() == "true"
            error_detail = "ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤" if not debug_mode else "CancelledError: ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤"
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "confidence": 0.0,
                "legal_references": [],
                "processing_steps": [f"ì˜¤ë¥˜: {error_detail}"],
                "session_id": session_id or "error",
                "processing_time": 0.0,
                "query_type": "error",
                "metadata": {"error": error_detail, "cancelled": True} if debug_mode else {"error": True, "cancelled": True},
                "errors": [error_detail]
            }
        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)
            import os
            debug_mode = os.getenv("DEBUG", "false").lower() == "true"
            error_detail = str(e) if debug_mode else "ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0,
                "legal_references": [],
                "processing_steps": [f"ì˜¤ë¥˜: {error_detail}"],
                "session_id": session_id or "error",
                "processing_time": 0.0,
                "query_type": "error",
                "metadata": {"error": error_detail} if debug_mode else {"error": True},
                "errors": [error_detail]
            }
    
    async def stream_message(
        self,
        message: str,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì²˜ë¦¬ (Server-Sent Events)
        ì‹¤ì œ LLM í† í° ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•©ë‹ˆë‹¤.
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            
        Yields:
            ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬ (í† í° ë‹¨ìœ„)
        """
        # ì´ë²¤íŠ¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.event_processor.reset()
        
        if not self.workflow_service:
            error_event = self._create_error_event("[ì˜¤ë¥˜] ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            yield json.dumps(error_event, ensure_ascii=False) + "\n"
            return
        
        try:
            import uuid
            
            # ì„¸ì…˜ ID ìƒì„±
            if not session_id:
                session_id = str(uuid.uuid4())
            
            # ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            from lawfirm_langgraph.core.workflow.state.state_definitions import create_initial_legal_state
            
            # ë¡œê¹…: message ê°’ í™•ì¸
            logger.info(f"stream_message: ë°›ì€ message='{message[:100] if message else 'EMPTY'}...', length={len(message) if message else 0}")
            
            # messageë¥¼ queryë¡œ ì‚¬ìš© (create_initial_legal_stateì˜ ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ëŠ” query)
            initial_state = create_initial_legal_state(message, session_id)
            
            
            initial_query = self._validate_and_augment_state(initial_state, message, session_id)
            if not initial_query:
                error_event = self._create_error_event("[ì˜¤ë¥˜] ì§ˆë¬¸ì´ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                yield json.dumps(error_event, ensure_ascii=False) + "\n"
                return
            
            # LangGraph config ìƒì„± (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©)
            config = create_langgraph_config(
                session_id=session_id,
                enable_checkpoint=enable_checkpoint
            )
            
            # ë””ë²„ê·¸ ëª¨ë“œ í™•ì¸
            DEBUG_STREAM = self.stream_config.debug_stream
            
            try:
                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
                event_count = 0
                llm_stream_count = 0
                event_types_seen = set()  # ë³¸ ì´ë²¤íŠ¸ íƒ€ì… ì¶”ì  (ë””ë²„ê¹…ìš©, ì œí•œì  ì‚¬ìš©)
                node_names_seen = set()  # ë³¸ ë…¸ë“œ ì´ë¦„ ì¶”ì  (ë””ë²„ê¹…ìš©, ì œí•œì  ì‚¬ìš©)
                
                # ê´€ë ¨ ì´ë²¤íŠ¸ íƒ€ì… ì§‘í•© (ì„±ëŠ¥ ìµœì í™”)
                RELEVANT_EVENT_TYPES = self.stream_config.relevant_event_types
                
                # ë©”ëª¨ë¦¬ ìµœì í™”: ì´ë²¤íŠ¸ íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
                MAX_EVENT_HISTORY = self.stream_config.max_event_history
                
                try:
                    async for event in self._get_stream_events(initial_state, config):
                        event_count += 1
                        # ì´ë²¤íŠ¸ íƒ€ì… í™•ì¸
                        event_type = event.get("event", "")
                        event_name = event.get("name", "")
                        
                        # ì´ë²¤íŠ¸ íƒ€ì… ì¶”ì  (ë””ë²„ê¹…ìš©)
                        event_types_seen.add(event_type)
                        if event_name:
                            node_names_seen.add(event_name)
                        
                        # ê´€ë ¨ ì—†ëŠ” ì´ë²¤íŠ¸ëŠ” ë¡œê¹…ë§Œ í•˜ê³  ê±´ë„ˆë›°ê¸° (ì„±ëŠ¥ ìµœì í™” - ì¡°ê¸° ì¢…ë£Œ)
                        if event_type not in RELEVANT_EVENT_TYPES:
                            if DEBUG_STREAM and event_count <= 20:
                                logger.debug(f"ê±´ë„ˆë›´ ì´ë²¤íŠ¸ #{event_count}: type={event_type}, name={event_name} (ê´€ë ¨ ì´ë²¤íŠ¸ íƒ€ì… ì•„ë‹˜)")
                            continue
                        
                        # ë””ë²„ê¹… ëª¨ë“œì—ì„œë§Œ ì´ë²¤íŠ¸ ì¶”ì  (ë©”ëª¨ë¦¬ ìµœì í™”: ì œí•œì  ì¶”ì )
                        if DEBUG_STREAM and event_count <= MAX_EVENT_HISTORY:
                            if event_count <= 20:
                                logger.debug(f"ì²˜ë¦¬í•  ì´ë²¤íŠ¸ #{event_count}: type={event_type}, name={event_name}")
                        
                        # StreamEventProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë²¤íŠ¸ ì²˜ë¦¬
                        try:
                            stream_event = self.event_processor.process_stream_event(event)
                            if stream_event:
                                yield json.dumps(stream_event, ensure_ascii=False) + "\n"
                                if stream_event.get("type") == "stream":
                                    llm_stream_count += 1
                                    if DEBUG_STREAM and llm_stream_count <= 10:
                                        logger.debug(f"âœ… Stream ì´ë²¤íŠ¸ ìƒì„±: content_length={len(stream_event.get('content', ''))}")
                            elif DEBUG_STREAM and event_type in ["on_llm_stream", "on_chat_model_stream"]:
                                # on_llm_stream ì´ë²¤íŠ¸ê°€ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²½ìš° ë¡œê¹…
                                logger.warning(
                                    f"âš ï¸ on_llm_stream ì´ë²¤íŠ¸ê°€ stream_eventë¡œ ë³€í™˜ë˜ì§€ ì•ŠìŒ: "
                                    f"event_type={event_type}, name={event_name}, "
                                    f"data_keys={list(event.get('data', {}).keys()) if isinstance(event.get('data'), dict) else 'N/A'}"
                                )
                        except Exception as process_error:
                            logger.error(
                                f"ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: event_type={event_type}, name={event_name}, "
                                f"error={process_error}",
                                exc_info=True
                            )
                except asyncio.CancelledError:
                    logger.warning("âš ï¸ [stream_message] ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë°ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (CancelledError)")
                    # ì·¨ì†Œëœ ê²½ìš° ì—ëŸ¬ ì´ë²¤íŠ¸ ì „ì†¡
                    error_event = self._create_error_event(
                        "[ì˜¤ë¥˜] ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                        error_type="cancelled"
                    )
                    yield json.dumps(error_event, ensure_ascii=False) + "\n"
                    return
                
                # event_processorì—ì„œ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                full_answer = self.event_processor.full_answer
                answer_found = self.event_processor.answer_found
                tokens_received = self.event_processor.tokens_received
                
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ìµœì¢… í™•ì¸ (DEBUG_STREAMì´ trueì¼ ë•Œë§Œ)
                if DEBUG_STREAM:
                    logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: ì´ {event_count}ê°œ ì´ë²¤íŠ¸, LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ {llm_stream_count}ê°œ, í† í° ìˆ˜ì‹  {tokens_received}ê°œ")
                
                # LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ê°€ ì—†ì„ ë•Œë§Œ ê²½ê³  (í”„ë¡œë•ì…˜ì—ì„œë„)
                if llm_stream_count == 0:
                    if DEBUG_STREAM:
                        logger.warning("âš ï¸ LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        logger.debug(f"ë°œìƒí•œ ëª¨ë“  ì´ë²¤íŠ¸ íƒ€ì…: {sorted(event_types_seen)}")
                        logger.debug(f"ë°œìƒí•œ ëª¨ë“  ë…¸ë“œ ì´ë¦„: {sorted(node_names_seen)}")
                
                if not answer_found:
                    missing_event = await self._handle_missing_answer(message, session_id, full_answer)
                    if missing_event:
                        yield json.dumps(missing_event, ensure_ascii=False) + "\n"
                        if missing_event.get("type") == "stream":
                            self.event_processor.answer_found = True
            
            except Exception as stream_error:
                # astream_events ì‹¤íŒ¨ ì‹œ astreamìœ¼ë¡œ í´ë°±
                if DEBUG_STREAM:
                    logger.warning(f"astream_events ì‹¤íŒ¨, astreamìœ¼ë¡œ í´ë°±: {stream_error}")
                # stream_mode="updates" ì‚¬ìš© ì‹œ ë³€ê²½ëœ í•„ë“œë§Œ í¬í•¨ë˜ë¯€ë¡œ ì§ì ‘ í™•ì¸ ê°€ëŠ¥
                async for event in self.workflow_service.app.astream(initial_state, config, stream_mode="updates"):
                    for node_name, node_state in event.items():
                        if isinstance(node_state, dict):
                            answer = None
                            # answer ê·¸ë£¹ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
                            if "answer" in node_state:
                                answer = node_state.get("answer", "")
                            # common ê·¸ë£¹ì—ì„œ answer í™•ì¸ (ë³€ê²½ëœ ê²½ìš°ì—ë§Œ í¬í•¨)
                            elif "common" in node_state and isinstance(node_state["common"], dict):
                                common = node_state["common"]
                                if "answer" in common:
                                    answer = common.get("answer", "")
                            
                            if answer and isinstance(answer, str):
                                current_full_answer = self.event_processor.full_answer
                                
                                # ğŸ”¥ [END] í‚¤ì›Œë“œê°€ ì´ë¯¸ ë°œê²¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                # ì´ë¯¸ [END] ì´í›„ë¼ë©´ ë” ì´ìƒ ì „ì†¡í•˜ì§€ ì•ŠìŒ
                                if '[END]' in current_full_answer.upper():
                                    # [END] í‚¤ì›Œë“œê°€ ì´ë¯¸ ë°œê²¬ë˜ì—ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ë°ì´í„° ì „ì†¡ ì•ˆ í•¨
                                    continue
                                
                                if len(answer) > len(current_full_answer):
                                    new_part = answer[len(current_full_answer):]
                                    if new_part:
                                        # ğŸ”¥ [END] í‚¤ì›Œë“œ ì´í›„ ë‚´ìš© í•„í„°ë§
                                        # í˜„ì¬ê¹Œì§€ì˜ ì „ì²´ ë‹µë³€ì—ì„œ [END] ìœ„ì¹˜ í™•ì¸ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
                                        full_answer_with_new = self.event_processor.full_answer + new_part
                                        end_keyword_pos = -1
                                        
                                        # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ [END] í‚¤ì›Œë“œ ì°¾ê¸°
                                        full_answer_upper = full_answer_with_new.upper()
                                        for keyword in ['[END]', '[END', 'END]']:
                                            pos = full_answer_upper.find(keyword.upper())
                                            if pos != -1:
                                                end_keyword_pos = pos
                                                break
                                        
                                        if end_keyword_pos != -1:
                                            # [END] í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ë©´ ê·¸ ì´í›„ ë‚´ìš©ì€ ì œì™¸
                                            # [END] í‚¤ì›Œë“œê¹Œì§€ì˜ ë‚´ìš©ë§Œ ì „ì†¡
                                            answer_until_end = full_answer_with_new[:end_keyword_pos].rstrip()
                                            # ì´ë¯¸ ì „ì†¡í•œ ë¶€ë¶„ ì´í›„ì˜ ìƒˆë¡œìš´ ë¶€ë¶„ë§Œ ê³„ì‚°
                                            already_sent = len(self.event_processor.full_answer)
                                            if len(answer_until_end) > already_sent:
                                                new_part = answer_until_end[already_sent:]
                                                self.event_processor.full_answer = answer_until_end
                                            else:
                                                # [END] ì´ì „ ë‚´ìš©ì´ ì´ë¯¸ ëª¨ë‘ ì „ì†¡ë¨
                                                new_part = ""
                                        else:
                                            # [END] í‚¤ì›Œë“œê°€ ì•„ì§ ì—†ìœ¼ë©´ ì •ìƒì ìœ¼ë¡œ ì „ì†¡
                                            self.event_processor.full_answer = full_answer_with_new
                                        
                                        if new_part:
                                            # ìŠ¤íŠ¸ë¦¼ ì²­í¬ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ì „ì†¡
                                            stream_event = {
                                                "type": "stream",
                                                "content": new_part,
                                                "timestamp": datetime.now().isoformat()
                                            }
                                            yield json.dumps(stream_event, ensure_ascii=False) + "\n"
                                            self.event_processor.answer_found = True
            
            sources_data = await self._extract_sources_from_state(session_id) if session_id else {}
            final_sources = sources_data.get("sources", [])
            final_legal_references = sources_data.get("legal_references", [])
            final_sources_detail = sources_data.get("sources_detail", [])
            final_related_questions = sources_data.get("related_questions", [])
            
            # event_processorì—ì„œ full_answer ê°€ì ¸ì˜¤ê¸°
            full_answer = self.event_processor.full_answer
            answer_found = self.event_processor.answer_found
            tokens_received = self.event_processor.tokens_received
            
            if full_answer:
                
                # í† í° ì œí•œ í™•ì¸
                MAX_OUTPUT_TOKENS = self.stream_config.max_output_tokens
                should_split = tokens_received >= MAX_OUTPUT_TOKENS
                
                import uuid
                
                # ë©”ì‹œì§€ ID ìƒì„± (chat.pyì—ì„œ ì €ì¥ ì‹œ ì‚¬ìš©)
                message_id = str(uuid.uuid4())
                
                if not final_sources and not final_legal_references and not final_sources_detail:
                    re_extracted = await self._re_extract_sources_before_final(session_id, {})
                    if re_extracted.get("sources"):
                        final_sources = re_extracted["sources"]
                    if re_extracted.get("legal_references"):
                        final_legal_references = re_extracted["legal_references"]
                    if re_extracted.get("sources_detail"):
                        final_sources_detail = re_extracted["sources_detail"]
                    if re_extracted.get("related_questions"):
                        final_related_questions = re_extracted["related_questions"]
                
                if should_split:
                    from api.services.answer_splitter import AnswerSplitter
                    splitter = AnswerSplitter(chunk_size=self.stream_config.chunk_size)
                    chunks = splitter.split_answer(full_answer)
                    
                    content = chunks[0].content if chunks else full_answer
                    final_event = self._create_final_event(
                        content, tokens_received, answer_found,
                        final_sources, final_legal_references,
                        final_sources_detail, final_related_questions,
                        message_id, needs_continuation=bool(chunks)
                    )
                    yield json.dumps(final_event, ensure_ascii=False) + "\n"
                else:
                    final_event = self._create_final_event(
                        full_answer, tokens_received, answer_found,
                        final_sources, final_legal_references,
                        final_sources_detail, final_related_questions,
                        message_id
                    )
                    yield json.dumps(final_event, ensure_ascii=False) + "\n"
            else:
                if DEBUG_STREAM:
                    logger.warning("ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                if not answer_found:
                    error_event = self._create_error_event("[ì˜¤ë¥˜] ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    error_event["metadata"]["tokens_received"] = tokens_received
                    yield json.dumps(error_event, ensure_ascii=False) + "\n"
            
        except Exception as e:
            logger.error(f"Error in stream_message: {e}", exc_info=True)
            try:
                error_event = self._create_error_event(
                    f"[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                    error_type=type(e).__name__
                )
                yield json.dumps(error_event, ensure_ascii=False) + "\n"
            except Exception as yield_error:
                logger.error(f"Error yielding error message: {yield_error}")
                try:
                    fallback_event = self._create_error_event("[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    yield json.dumps(fallback_event, ensure_ascii=False) + "\n"
                except Exception:
                    pass
        # finally ë¸”ë¡ ì œê±°: finallyì—ì„œ yieldë¥¼ í•˜ë©´ ì œë„ˆë ˆì´í„°ê°€ ì œëŒ€ë¡œ ì¢…ë£Œë˜ì§€ ì•Šì•„
        # ERR_INCOMPLETE_CHUNKED_ENCODING ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
        # ìŠ¤íŠ¸ë¦¼ ì¢…ë£ŒëŠ” FastAPI StreamingResponseê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
    
    async def get_sources_from_session(
        self,
        session_id: str,
        message_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ì„¸ì…˜ì˜ ìµœì¢… stateì—ì„œ sources ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            session_id: ì„¸ì…˜ ID
            message_id: ë©”ì‹œì§€ ID (ì„ íƒì‚¬í•­, í•´ë‹¹ ë©”ì‹œì§€ì˜ metadataì—ì„œ sources ê°€ì ¸ì˜¤ê¸°)
        
        Returns:
            sources, legal_references, sources_detail ë”•ì…”ë„ˆë¦¬
        """
        # ë¨¼ì € ë©”ì‹œì§€ì˜ metadataì—ì„œ sourcesë¥¼ ê°€ì ¸ì˜¤ê¸° ì‹œë„
        result = await self.sources_extractor.extract_from_message_metadata(session_id, message_id)
        
        # ì—†ìœ¼ë©´ stateì—ì„œ ê°€ì ¸ì˜¤ê¸°
        if not any(result.values()):
            result = await self.sources_extractor.extract_from_state(session_id)
        
        return result
    
    async def stream_final_answer(
        self,
        message: str,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        process_query(use_astream_events=True)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
        
        workflow_service.process_query()ì˜ ë‚´ë¶€ ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ì„ í™œìš©í•˜ì—¬
        run_query_test.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        if not self.workflow_service:
            error_event = self._create_error_event(
                "[ì˜¤ë¥˜] ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                error_type="initialization_error"
            )
            error_chunk = f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"
            yield error_chunk
            return
        
        try:
            if not session_id:
                session_id = str(uuid.uuid4())
            
            # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í ìƒì„±
            from api.services.streaming.event_builder import StreamEventBuilder
            from api.utils.sse_formatter import format_sse_event
            from datetime import datetime
            
            event_builder = StreamEventBuilder()
            stream_queue = asyncio.Queue()
            process_completed = asyncio.Event()
            process_error = None
            
            # [END] í‚¤ì›Œë“œ í•„í„°ë§ì„ ìœ„í•œ ë³€ìˆ˜
            end_keyword_found = False
            current_full_answer = ""
            
            async def process_query_task():
                """ë°±ê·¸ë¼ìš´ë“œì—ì„œ process_query ì‹¤í–‰"""
                nonlocal process_error
                try:
                    # process_query ì‹¤í–‰ (use_astream_events=True, stream_queue ì „ë‹¬)
                    result = await self.workflow_service.process_query(
                        query=message,
                        session_id=session_id,
                        enable_checkpoint=False,
                        use_astream_events=True,
                        stream_queue=stream_queue
                    )
                    
                    # ê²°ê³¼ë¥¼ íì— ë„£ê¸°
                    await stream_queue.put({
                        "type": "result",
                        "data": result
                    })
                except Exception as e:
                    process_error = e
                    await stream_queue.put({
                        "type": "error",
                        "data": str(e)
                    })
                finally:
                    process_completed.set()
            
            # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘
            process_task = asyncio.create_task(process_query_task())
            
            # ì§„í–‰ ì´ë²¤íŠ¸ ì „ì†¡
            progress_event = event_builder.create_progress_event("ë‹µë³€ ìƒì„± ì¤‘...")
            yield format_sse_event(progress_event)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
            done_event_sent = False
            try:
                # íê°€ ë¹„ì–´ìˆì§€ ì•Šê±°ë‚˜ í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê³„ì† ì²˜ë¦¬
                max_empty_iterations = 300  # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸° (0.1ì´ˆ * 300)
                empty_iterations = 0
                
                while True:
                    try:
                        # íì—ì„œ ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                        try:
                            item = await asyncio.wait_for(
                                stream_queue.get(),
                                timeout=0.1
                            )
                            empty_iterations = 0  # ì´ë²¤íŠ¸ë¥¼ ë°›ì•˜ìœ¼ë©´ ì¹´ìš´í„° ë¦¬ì…‹
                        except asyncio.TimeoutError:
                            # íƒ€ì„ì•„ì›ƒì€ ì •ìƒ (íê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ)
                            empty_iterations += 1
                            
                            # í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë˜ì—ˆê³  íê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¢…ë£Œ
                            if process_completed.is_set():
                                # ğŸ”¥ ê°œì„ : í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë˜ì—ˆì–´ë„ íì— ì´ë²¤íŠ¸ê°€ ìˆìœ¼ë©´ ê³„ì† ì²˜ë¦¬
                                # íê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ë” ê¸°ë‹¤ë¦¼ (ì´ë²¤íŠ¸ê°€ íì— ë“¤ì–´ì˜¤ëŠ” ì¤‘ì¼ ìˆ˜ ìˆìŒ)
                                if not stream_queue.empty():
                                    # íì— ì´ë²¤íŠ¸ê°€ ìˆìœ¼ë©´ empty_iterations ë¦¬ì…‹í•˜ê³  ê³„ì† ì²˜ë¦¬
                                    empty_iterations = 0
                                    continue
                                
                                # íê°€ ë¹„ì–´ìˆê³  í”„ë¡œì„¸ìŠ¤ë„ ì™„ë£Œë˜ì—ˆìœ¼ë©´ ì¢…ë£Œ
                                if stream_queue.empty():
                                    logger.debug("[stream_final_answer] í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ë° í ë¹„ì–´ìˆìŒ, ì¢…ë£Œ")
                                    break
                                
                                # íê°€ ë¹„ì–´ìˆì§€ ì•Šì€ë° ê³„ì† íƒ€ì„ì•„ì›ƒì´ ë°œìƒí•˜ë©´ ì¢…ë£Œ
                                if empty_iterations >= max_empty_iterations:
                                    logger.warning("[stream_final_answer] í ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼, ì¢…ë£Œ")
                                    break
                            else:
                                # í”„ë¡œì„¸ìŠ¤ê°€ ì•„ì§ ì‹¤í–‰ ì¤‘ì´ë©´ ê³„ì† ëŒ€ê¸°
                                # ğŸ”¥ ê°œì„ : í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¼ ë•ŒëŠ” ë” ì˜¤ë˜ ê¸°ë‹¤ë¦¼ (ì›Œí¬í”Œë¡œìš°ê°€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
                                if empty_iterations < max_empty_iterations * 3:  # 90ì´ˆë¡œ ì¦ê°€ (30ì´ˆ * 3)
                                    continue
                                else:
                                    logger.warning("[stream_final_answer] í”„ë¡œì„¸ìŠ¤ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼")
                                    break
                        
                        if item["type"] == "stream":
                            # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
                            chunk_content = item.get("content", "")
                            
                            if chunk_content and not end_keyword_found:
                                # [END] í‚¤ì›Œë“œ í™•ì¸
                                current_full_answer += chunk_content
                                
                                # [END] í‚¤ì›Œë“œ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                                end_pos = -1
                                for keyword in ["[END]", "[end]", "[End]"]:
                                    pos = current_full_answer.find(keyword)
                                    if pos != -1:
                                        end_pos = pos
                                        break
                                
                                if end_pos != -1:
                                    end_keyword_found = True
                                    # [END] ì´ì „ ë‚´ìš©ë§Œ ì „ì†¡
                                    content_to_send = current_full_answer[:end_pos].rstrip()
                                    if content_to_send:
                                        stream_event = event_builder.create_stream_event(
                                            content_to_send,
                                            source="process_query"
                                        )
                                        yield format_sse_event(stream_event)
                                    logger.debug(f"âœ… [STREAM] [END] í‚¤ì›Œë“œ ì´í›„ ë‚´ìš© ì œê±°ë¨ (ìœ„ì¹˜: {end_pos})")
                                else:
                                    # [END] í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì²­í¬ ì „ì†¡
                                    stream_event = event_builder.create_stream_event(
                                        chunk_content,
                                        source="process_query"
                                    )
                                    yield format_sse_event(stream_event)
                        
                        elif item["type"] == "result":
                            # ê²°ê³¼ ì²˜ë¦¬
                            result = item["data"]
                            answer = result.get("answer", "")
                            
                            # [END] í‚¤ì›Œë“œ í•„í„°ë§
                            if not end_keyword_found:
                                # [END] í‚¤ì›Œë“œ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                                end_pos = -1
                                for keyword in ["[END]", "[end]", "[End]"]:
                                    pos = answer.find(keyword)
                                    if pos != -1:
                                        end_pos = pos
                                        break
                                
                                if end_pos != -1:
                                    end_keyword_found = True
                                    answer = answer[:end_pos].rstrip()
                                    logger.debug(f"âœ… [STREAM] [END] í‚¤ì›Œë“œ ì´í›„ ë‚´ìš© ì œê±°ë¨ (ìœ„ì¹˜: {end_pos})")
                            
                            # ìµœì¢… ì´ë²¤íŠ¸ ì „ì†¡
                            final_event = event_builder.create_final_event(
                                content=answer,
                                metadata=result.get("metadata", {})
                            )
                            yield format_sse_event(final_event)
                            
                        elif item["type"] == "error":
                            # ì—ëŸ¬ ì´ë²¤íŠ¸ ì „ì†¡
                            error_event = event_builder.create_error_event(
                                f"[ì˜¤ë¥˜] {item['data']}"
                            )
                            yield format_sse_event(error_event)
                            
                    except Exception as e:
                        logger.error(f"[stream_final_answer] í ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ done ì´ë²¤íŠ¸ ì „ì†¡ ë³´ì¥
                        break
                
                # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                try:
                    await asyncio.wait_for(process_task, timeout=300.0)  # ìµœëŒ€ 5ë¶„ ëŒ€ê¸°
                except asyncio.TimeoutError:
                    logger.warning("[stream_final_answer] process_query íƒ€ì„ì•„ì›ƒ")
                    process_task.cancel()
                    try:
                        await process_task
                    except asyncio.CancelledError:
                        pass
                
                # ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš° ì²˜ë¦¬
                if process_error:
                    error_event = event_builder.create_error_event(
                        f"[ì˜¤ë¥˜] {str(process_error)}"
                    )
                    yield format_sse_event(error_event)
                
            except asyncio.CancelledError:
                # íƒœìŠ¤í¬ ì·¨ì†Œ
                logger.debug("[stream_final_answer] Stream cancelled (client disconnected)")
                process_task.cancel()
                try:
                    await process_task
                except asyncio.CancelledError:
                    pass
                # GeneratorExitê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ done ì´ë²¤íŠ¸ ì „ì†¡ ì‹œë„
                # í•˜ì§€ë§Œ CancelledErrorëŠ” í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš°ì´ë¯€ë¡œ yield ì‹œë„í•˜ì§€ ì•ŠìŒ
                raise
            except GeneratorExit:
                # GeneratorExitëŠ” ì œë„ˆë ˆì´í„°ê°€ ì´ë¯¸ ì¢…ë£Œëœ ìƒíƒœì´ë¯€ë¡œ yieldë¥¼ ì‹œë„í•˜ë©´ ì•ˆ ë¨
                logger.debug("[stream_final_answer] Generator exit (client disconnected)")
                # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì •ë¦¬
                if not process_task.done():
                    process_task.cancel()
                    try:
                        await process_task
                    except (asyncio.CancelledError, GeneratorExit):
                        pass
                # GeneratorExitëŠ” ë°”ë¡œ raise (yield ì‹œë„í•˜ì§€ ì•ŠìŒ)
                raise
            except GeneratorExit:
                # GeneratorExitëŠ” ì œë„ˆë ˆì´í„°ê°€ ì´ë¯¸ ì¢…ë£Œëœ ìƒíƒœì´ë¯€ë¡œ yieldë¥¼ ì‹œë„í•˜ë©´ ì•ˆ ë¨
                logger.debug("[stream_final_answer] Generator exit during stream processing")
                # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì •ë¦¬
                if not process_task.done():
                    process_task.cancel()
                    try:
                        await process_task
                    except (asyncio.CancelledError, GeneratorExit):
                        pass
                raise
            except Exception as e:
                logger.error(f"[stream_final_answer] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                # ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€: done ì´ë²¤íŠ¸ ì „ì†¡ ì‹œë„
                try:
                    error_event = event_builder.create_error_event(str(e))
                    yield format_sse_event(error_event)
                    done_event = {"type": "done", "timestamp": datetime.now().isoformat(), "error": str(e)}
                    yield format_sse_event(done_event)
                    done_event_sent = True
                except GeneratorExit:
                    # GeneratorExitëŠ” ì œë„ˆë ˆì´í„°ê°€ ì´ë¯¸ ì¢…ë£Œëœ ìƒíƒœì´ë¯€ë¡œ ë°”ë¡œ raise
                    raise
                except Exception:
                    pass  # ì´ë¯¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ë¬´ì‹œ
                raise
            finally:
                # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì •ë¦¬
                if 'process_task' in locals() and not process_task.done():
                    process_task.cancel()
                    try:
                        await process_task
                    except (asyncio.CancelledError, GeneratorExit):
                        pass
                    except Exception as cleanup_error:
                        logger.debug(f"[stream_final_answer] Error cleaning up process_task: {cleanup_error}")
                
                # GeneratorExitê°€ ë°œìƒí•œ ê²½ìš° yieldë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŒ
                # GeneratorExitëŠ” ì œë„ˆë ˆì´í„°ê°€ ì´ë¯¸ ì¢…ë£Œëœ ìƒíƒœì´ë¯€ë¡œ yieldë¥¼ ì‹œë„í•˜ë©´ RuntimeError ë°œìƒ
                try:
                    # ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€: done ì´ë²¤íŠ¸ê°€ ì•„ì§ ì „ì†¡ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì „ì†¡
                    if not done_event_sent:
                        done_event = {"type": "done", "timestamp": datetime.now().isoformat()}
                        yield format_sse_event(done_event)
                        done_event_sent = True
                except GeneratorExit:
                    # GeneratorExitëŠ” ì œë„ˆë ˆì´í„°ê°€ ì´ë¯¸ ì¢…ë£Œëœ ìƒíƒœì´ë¯€ë¡œ ë°”ë¡œ raise
                    raise
                except Exception:
                    # ë‹¤ë¥¸ ì˜ˆì™¸ëŠ” ë¬´ì‹œ (ì´ë¯¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ë“±)
                    pass
            
        except asyncio.CancelledError:
            logger.warning("âš ï¸ [stream_final_answer] ìŠ¤íŠ¸ë¦¬ë°ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (CancelledError)")
            # CancelledErrorëŠ” í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš°ì´ë¯€ë¡œ yield ì‹œë„í•˜ì§€ ì•ŠìŒ
            raise  # ìƒìœ„ë¡œ ì „íŒŒ
        except GeneratorExit:
            # GeneratorExitëŠ” ì œë„ˆë ˆì´í„°ê°€ ì´ë¯¸ ì¢…ë£Œëœ ìƒíƒœì´ë¯€ë¡œ yieldë¥¼ ì‹œë„í•˜ë©´ ì•ˆ ë¨
            logger.debug("[stream_final_answer] Generator exit at outer level")
            raise
        except Exception as e:
            logger.error(f"[stream_final_answer] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            # ğŸ”¥ ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€: done ì´ë²¤íŠ¸ ì „ì†¡ ì‹œë„
            try:
                error_event = self._create_error_event(
                    f"[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                )
                error_chunk = f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"
                yield error_chunk
                done_event = {"type": "done", "timestamp": datetime.now().isoformat(), "error": str(e)}
                yield format_sse_event(done_event)
            except Exception:
                pass  # ì´ë¯¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ë¬´ì‹œ
    
    
    def _create_error_event(self, content: str, error_type: Optional[str] = None) -> Dict[str, Any]:
        """ì—ëŸ¬ ì´ë²¤íŠ¸ ìƒì„±"""
        from api.services.streaming.event_builder import StreamEventBuilder
        error_event = StreamEventBuilder.create_error_event(content, error_type)
        return {
            "type": "final",
            "content": content,
            "metadata": {**error_event.get("metadata", {}), "error": True},
            "timestamp": error_event.get("timestamp")
        }
    
    def _validate_and_augment_state(self, initial_state: Dict[str, Any], message: str, session_id: str) -> Optional[str]:
        """ìƒíƒœ ê²€ì¦ ë° ë³´ê°•"""
        if "input" not in initial_state:
            initial_state["input"] = {}
        if not initial_state["input"].get("query"):
            initial_state["input"]["query"] = message
        if not initial_state["input"].get("session_id"):
            initial_state["input"]["session_id"] = session_id
        
        if not initial_state.get("query"):
            initial_state["query"] = message
        if not initial_state.get("session_id"):
            initial_state["session_id"] = session_id
        
        initial_query = initial_state.get("input", {}).get("query") or initial_state.get("query")
        if not initial_query or not str(initial_query).strip():
            logger.error(f"Initial state query is empty! Input message was: '{message[:50]}...'")
            return None
        return initial_query
    
    async def _get_stream_events(self, initial_state: Dict[str, Any], config: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ìµœì‹  LangGraph API í˜¸í™˜, í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
        DEBUG_STREAM = self.stream_config.debug_stream
        
        try:
            # ìµœì‹  API ì‹œë„: version íŒŒë¼ë¯¸í„° ì—†ì´, í•„í„°ë§ ì—†ì´ ëª¨ë“  ì´ë²¤íŠ¸ ìˆ˜ì‹ 
            if DEBUG_STREAM:
                logger.info("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: astream_events() ì‚¬ìš© (ìµœì‹  API, í•„í„°ë§ ì—†ìŒ)")
            
            event_count = 0
            event_types_received = set()
            
            try:
                # í•„í„°ë§ ì—†ì´ ëª¨ë“  ì´ë²¤íŠ¸ ìˆ˜ì‹  (ë¬¸ì œ ì§„ë‹¨ì„ ìœ„í•´)
                async for event in self.workflow_service.app.astream_events(
                    initial_state, 
                    config
                ):
                    event_count += 1
                    event_type = event.get("event", "")
                    event_name = event.get("name", "")
                    event_types_received.add(event_type)
                    
                    # ìƒì„¸ ë¡œê¹… (ì²˜ìŒ 50ê°œ ì´ë²¤íŠ¸ë§Œ)
                    if DEBUG_STREAM and event_count <= 50:
                        logger.debug(
                            f"[_get_stream_events] ì´ë²¤íŠ¸ #{event_count}: "
                            f"type={event_type}, name={event_name}, "
                            f"data_keys={list(event.get('data', {}).keys()) if isinstance(event.get('data'), dict) else 'N/A'}"
                        )
                    
                    # on_llm_stream ì´ë²¤íŠ¸ ìƒì„¸ ë¡œê¹…
                    if event_type == "on_llm_stream" and DEBUG_STREAM:
                        event_data = event.get("data", {})
                        chunk = event_data.get("chunk") if isinstance(event_data, dict) else None
                        logger.info(
                            f"[_get_stream_events] âœ… on_llm_stream ì´ë²¤íŠ¸ ìˆ˜ì‹ : "
                            f"name={event_name}, "
                            f"chunk_type={type(chunk).__name__ if chunk else 'None'}, "
                            f"has_chunk={chunk is not None}"
                        )
                    
                    yield event
                
                # ì´ë²¤íŠ¸ ìˆ˜ì‹  ì™„ë£Œ í›„ í†µê³„ ë¡œê¹…
                if DEBUG_STREAM:
                    logger.info(
                        f"[_get_stream_events] ì´ë²¤íŠ¸ ìˆ˜ì‹  ì™„ë£Œ: "
                        f"ì´ {event_count}ê°œ, "
                        f"ì´ë²¤íŠ¸ íƒ€ì…={sorted(event_types_received)}, "
                        f"on_llm_stream í¬í•¨={'on_llm_stream' in event_types_received}"
                    )
                    
            except (TypeError, AttributeError) as ve:
                # êµ¬ë²„ì „ API í´ë°±: version="v2" íŒŒë¼ë¯¸í„° ì‚¬ìš©
                logger.warning(f"ìµœì‹  API ì‹¤íŒ¨, êµ¬ë²„ì „ API ì‹œë„: {ve}")
                if DEBUG_STREAM:
                    logger.info("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: astream_events(version='v2') ì‚¬ìš© (êµ¬ë²„ì „ API)")
                try:
                    async for event in self.workflow_service.app.astream_events(
                        initial_state, 
                        config,
                        version="v2"
                    ):
                        event_type = event.get("event", "")
                        if DEBUG_STREAM and event_type == "on_llm_stream":
                            logger.info(f"[_get_stream_events] âœ… on_llm_stream ì´ë²¤íŠ¸ ìˆ˜ì‹  (êµ¬ë²„ì „ API)")
                        yield event
                except (TypeError, AttributeError) as ve2:
                    # version íŒŒë¼ë¯¸í„°ë„ ë¯¸ì§€ì› ì‹œ ê¸°ë³¸ í˜¸ì¶œ
                    logger.warning(f"version íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›, ê¸°ë³¸ í˜¸ì¶œ: {ve2}")
                    if DEBUG_STREAM:
                        logger.info("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: astream_events() ì‚¬ìš© (ê¸°ë³¸ ë²„ì „)")
                    async for event in self.workflow_service.app.astream_events(
                        initial_state, 
                        config
                    ):
                        yield event
        except asyncio.CancelledError:
            logger.warning("âš ï¸ [_get_stream_events] astream_eventsê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (CancelledError)")
            raise  # ìƒìœ„ë¡œ ì „íŒŒ
        except Exception as e:
            logger.error(f"âš ï¸ [_get_stream_events] astream_events ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            raise
    
    async def _extract_sources_from_state(self, session_id: str, timeout: float = 2.0) -> Dict[str, Any]:
        """Stateì—ì„œ sources ì¶”ì¶œ"""
        result = {
            "sources": [],
            "legal_references": [],
            "sources_detail": [],
            "related_questions": []
        }
        
        try:
            sources_data = await asyncio.wait_for(
                self.sources_extractor.extract_from_state(session_id),
                timeout=timeout
            )
            result["sources"] = sources_data.get("sources", [])
            result["legal_references"] = sources_data.get("legal_references", [])
            result["sources_detail"] = sources_data.get("sources_detail", [])
            result["related_questions"] = sources_data.get("related_questions", [])
        except asyncio.TimeoutError:
            logger.warning(f"Timeout getting sources from LangGraph state for session {session_id}")
        except Exception as e:
            logger.warning(f"Failed to get sources from LangGraph state: {e}")
        
        return result
    
    async def _re_extract_sources_before_final(self, session_id: str, state_values: Dict[str, Any]) -> Dict[str, Any]:
        """ìµœì¢… ì´ë²¤íŠ¸ ì „ sources ì¬ì¶”ì¶œ"""
        result = {
            "sources": [],
            "legal_references": [],
            "sources_detail": [],
            "related_questions": []
        }
        
        if not session_id or not self.workflow_service or not self.workflow_service.app:
            return result
        
        try:
            # LangGraph config ìƒì„± (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©)
            config = create_langgraph_config(session_id=session_id)
            final_state = await asyncio.wait_for(
                self.workflow_service.app.aget_state(config),
                timeout=2.0
            )
            
            if final_state and final_state.values:
                state_values = final_state.values
                # retrieved_docsì—ì„œ ì§ì ‘ sources_by_type ìƒì„±
                retrieved_docs = state_values.get("retrieved_docs", [])
                related_questions_data = self.sources_extractor._extract_related_questions(state_values)
                
                if retrieved_docs and isinstance(retrieved_docs, list):
                    sources_by_type = self.sources_extractor._generate_sources_by_type_from_retrieved_docs(retrieved_docs)
                    result["sources_by_type"] = sources_by_type
                if related_questions_data:
                    result["related_questions"] = related_questions_data
                
                logger.info(f"Re-extracted sources before final event: sources_by_type={bool(result.get('sources_by_type'))}, related_questions={len(result.get('related_questions', []))}")
        except asyncio.TimeoutError:
            logger.warning("Timeout re-getting sources before final event")
        except Exception as e:
            logger.warning(f"Failed to re-get sources before final event: {e}")
        
        return result
    
    def _create_final_event(
        self,
        content: str,
        tokens_received: int,
        answer_found: bool,
        sources: list,
        legal_references: list,
        sources_detail: list,
        related_questions: list,
        message_id: str,
        needs_continuation: bool = False
    ) -> Dict[str, Any]:
        """ìµœì¢… ì´ë²¤íŠ¸ ìƒì„±"""
        metadata = {
            "tokens_received": tokens_received,
            "length": len(content),
            "answer_found": answer_found,
            "sources": sources,
            "legal_references": legal_references,
            "sources_detail": sources_detail,
            "related_questions": related_questions,
            "message_id": message_id
        }
        if needs_continuation:
            metadata["needs_continuation"] = True
        
        return {
            "type": "final",
            "content": content,
            "metadata": metadata,
            "timestamp": datetime.now().isoformat()
        }
    
    async def _handle_missing_answer(self, message: str, session_id: str, full_answer: str) -> Optional[Dict[str, Any]]:
        """ë‹µë³€ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²˜ë¦¬"""
        DEBUG_STREAM = self.stream_config.debug_stream
        
        if DEBUG_STREAM:
            logger.warning("LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            logger.info("ìµœì¢… ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        
        try:
            result = await self.process_message(message, session_id)
            final_answer = result.get("answer", "")
            if final_answer and len(final_answer) > len(full_answer):
                missing_part = final_answer[len(full_answer):]
                if missing_part:
                    self.event_processor.full_answer = final_answer
                    return {
                        "type": "stream",
                        "content": missing_part,
                        "timestamp": datetime.now().isoformat()
                    }
            elif final_answer:
                self.event_processor.full_answer = final_answer
                return {
                    "type": "stream",
                    "content": final_answer,
                    "timestamp": datetime.now().isoformat()
                }
        except Exception as e:
            if DEBUG_STREAM:
                logger.error(f"ìµœì¢… ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}", exc_info=True)
            if not self.event_processor.answer_found:
                return self._create_error_event(f"[ì˜¤ë¥˜] ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
        
        return None
    
    def _extract_token_from_event(self, event_data: Dict[str, Any]) -> Optional[str]:
        """ì´ë²¤íŠ¸ì—ì„œ í† í° ì¶”ì¶œ"""
        from api.services.streaming.token_extractor import TokenExtractor
        return TokenExtractor.extract_from_event(event_data)
    
    def _is_target_node(self, event_name: str, event_parent: Any, last_node_name: Optional[str]) -> bool:
        """íƒ€ê²Ÿ ë…¸ë“œì¸ì§€ í™•ì¸"""
        from api.services.streaming.node_filter import NodeFilter
        node_filter = NodeFilter()
        return node_filter.is_target_node(event_name, event_parent, last_node_name)
    
    async def _extract_related_questions_from_state(
        self,
        state_values: Dict[str, Any],
        initial_state: Dict[str, Any],
        message: str,
        full_answer: str,
        session_id: str
    ) -> list:
        """Stateì—ì„œ related_questions ì¶”ì¶œ ë° ìƒì„±"""
        related_questions = state_values.get("metadata", {}).get("related_questions", [])
        
        if related_questions:
            return related_questions
        
        sources = state_values.get("sources", [])
        sources_detail = state_values.get("sources_detail", [])
        
        # retrieved_docsì—ì„œ sources_by_type ìƒì„±
        retrieved_docs = state_values.get("retrieved_docs", [])
        sources_by_type = state_values.get("sources_by_type")
        if not sources_by_type and retrieved_docs and isinstance(retrieved_docs, list) and hasattr(self, 'sources_extractor') and self.sources_extractor:
            try:
                sources_by_type = self.sources_extractor._generate_sources_by_type_from_retrieved_docs(retrieved_docs)
                state_values["sources_by_type"] = sources_by_type
                
                related_questions_data = self.sources_extractor._extract_related_questions(state_values)
                if related_questions_data:
                    related_questions = related_questions_data
                    # state_valuesì˜ metadataì— ì €ì¥
                    if isinstance(state_values, dict):
                        if "metadata" not in state_values:
                            state_values["metadata"] = {}
                        if isinstance(state_values["metadata"], dict):
                            state_values["metadata"]["related_questions"] = related_questions_data
                            logger.info(f"[chat_service] Saved {len(related_questions_data)} related_questions to state metadata")
            except Exception as e:
                logger.warning(f"[chat_service] Failed to generate sources_by_type from retrieved_docs: {e}", exc_info=True)
        
        if not related_questions and hasattr(self, 'sources_extractor') and self.sources_extractor:
            try:
                if sources_detail and "sources_detail" not in state_values:
                    state_values["sources_detail"] = sources_detail
                if sources and "sources" not in state_values:
                    state_values["sources"] = sources
                
                related_questions_data = self.sources_extractor._extract_related_questions(state_values)
                if related_questions_data:
                    related_questions = related_questions_data
                    # state_valuesì˜ metadataì— ì €ì¥
                    if isinstance(state_values, dict):
                        if "metadata" not in state_values:
                            state_values["metadata"] = {}
                        if isinstance(state_values["metadata"], dict):
                            state_values["metadata"]["related_questions"] = related_questions_data
                            logger.info(f"[chat_service] Saved {len(related_questions_data)} related_questions to state metadata")
            except Exception as e:
                logger.warning(f"[stream_final_answer] Failed to extract related_questions from state: {e}", exc_info=True)
        
        if not related_questions and self.workflow_service and hasattr(self.workflow_service, 'conversation_flow_tracker') and self.workflow_service.conversation_flow_tracker:
            try:
                from lawfirm_langgraph.core.services.conversation_manager import ConversationContext, ConversationTurn
                
                query = state_values.get("query", "") or initial_state.get("query", "") or message
                answer = state_values.get("answer", "") or full_answer or ""
                query_type = state_values.get("metadata", {}).get("query_type", "general_question")
                
                if query and answer and len(answer.strip()) >= 10:
                    turn = ConversationTurn(
                        user_query=query,
                        bot_response=answer,
                        timestamp=datetime.now(),
                        question_type=query_type
                    )
                    
                    context = ConversationContext(
                        session_id=session_id or "default",
                        turns=[turn],
                        entities={},
                        topic_stack=[],
                        created_at=datetime.now(),
                        last_updated=datetime.now()
                    )
                    
                    suggested_questions = self.workflow_service.conversation_flow_tracker.suggest_follow_up_questions(context)
                    
                    if suggested_questions and len(suggested_questions) > 0:
                        related_questions = [str(q).strip() for q in suggested_questions if q and str(q).strip()]
            except Exception as e:
                logger.warning(f"[stream_final_answer] Failed to generate related_questions using ConversationFlowTracker: {e}", exc_info=True)
        
        return related_questions
    
    def is_available(self) -> bool:
        """ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return self.workflow_service is not None


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì§€ì—° ì´ˆê¸°í™”)
chat_service: Optional[ChatService] = None

def get_chat_service() -> ChatService:
    """ChatService ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global chat_service
    if chat_service is None:
        try:
            chat_service = ChatService()
        except Exception as e:
            logger.error(f"Failed to initialize ChatService: {e}", exc_info=True)
            import traceback
            tb = traceback.format_exc()
            logger.error(f"Traceback:\n{tb}")
            # ì‹¤íŒ¨í•´ë„ ChatService ì¸ìŠ¤í„´ìŠ¤ëŠ” ìƒì„± (workflow_serviceê°€ Noneì¼ ìˆ˜ ìˆìŒ)
            chat_service = ChatService()
    return chat_service

# ëª¨ë“ˆ import ì‹œì ì—ëŠ” ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ (ì§€ì—° ì´ˆê¸°í™”)
# ì²« ìš”ì²­ ì‹œ get_chat_service()ë¥¼ í†µí•´ ì´ˆê¸°í™”
# ì´ë ‡ê²Œ í•˜ë©´ api/main.pyì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ë¡œë“œí•œ í›„ ì´ˆê¸°í™” ê°€ëŠ¥
logger.info("ChatService module loaded. Will initialize on first request via get_chat_service().")

