"""
ì±„íŒ… ì„œë¹„ìŠ¤ (lawfirm_langgraph ë˜í¼)
"""
import sys
import json
import logging
import asyncio
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, AsyncGenerator

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# lawfirm_langgraph ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€ (core ëª¨ë“ˆ importë¥¼ ìœ„í•´)
lawfirm_langgraph_path = project_root / "lawfirm_langgraph"
if lawfirm_langgraph_path.exists():
    sys.path.insert(0, str(lawfirm_langgraph_path))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œë” ì‚¬ìš©)
try:
    from utils.env_loader import ensure_env_loaded
    ensure_env_loaded(project_root)
except ImportError as e:
    logging.warning(f"âš ï¸  Failed to load environment variables: {e}")
    logging.warning("   Make sure utils/env_loader.py exists in the project root")
except Exception as e:
    logging.warning(f"âš ï¸  Failed to load environment variables: {e}")

try:
    from lawfirm_langgraph.core.workflow.workflow_service import LangGraphWorkflowService
    from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
    LANGGRAPH_AVAILABLE = True
except ImportError as e:
    LANGGRAPH_AVAILABLE = False
    logging.warning(f"LangGraph not available: {e}")

logger = logging.getLogger(__name__)

# ë¡œê±° ë ˆë²¨ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (ë£¨íŠ¸ ë¡œê±° ë ˆë²¨ê³¼ ë™ê¸°í™”)
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œê·¸ ë ˆë²¨ ì½ê¸°
import os  # noqa: E402
log_level_str = os.getenv("LOG_LEVEL", "info").upper()
log_level_map = {
    "CRITICAL": logging.CRITICAL,
    "ERROR": logging.ERROR,
    "WARNING": logging.WARNING,
    "INFO": logging.INFO,
    "DEBUG": logging.DEBUG,
}
log_level = log_level_map.get(log_level_str, logging.INFO)
logger.setLevel(log_level)
logger.disabled = False  # ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™”
logger.propagate = True  # ë£¨íŠ¸ ë¡œê±°ë¡œ ì „íŒŒ

# ë¡œê¹…ì´ ë¹„í™œì„±í™”ë˜ì§€ ì•Šë„ë¡ ë³´í˜¸
logging.disable(logging.NOTSET)  # ëª¨ë“  ë¡œê¹… í™œì„±í™”

# ë£¨íŠ¸ ë¡œê±°ì— í•¸ë“¤ëŸ¬ê°€ ì—†ìœ¼ë©´ ì¶”ê°€ (ëª¨ë“ˆ import ì‹œì ì— ë¡œê¹…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ)
root_logger = logging.getLogger()
if not root_logger.handlers:
    handler = logging.StreamHandler()
    handler.setLevel(log_level)
    handler.setFormatter(
        logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    )
    root_logger.addHandler(handler)
    root_logger.setLevel(log_level)
    root_logger.disabled = False

# í…ŒìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥ (ëª¨ë“ˆ import ì‹œì )
logger.info("âœ… ChatService logger initialized and enabled")
logger.debug("âœ… ChatService logger debug level enabled")


class ChatService:
    """ì±„íŒ… ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("ğŸš€ ChatService.__init__() called - Initializing ChatService...")
        self.workflow_service: Optional[LangGraphWorkflowService] = None
        self._initialize_workflow()
        
        # ìŠ¤íŠ¸ë¦¬ë° ì„¤ì • ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        from api.services.stream_config import StreamConfig
        from api.services.stream_event_processor import StreamEventProcessor
        from api.services.sources_extractor import SourcesExtractor
        from api.services.session_service import session_service
        from api.services.streaming.stream_handler import StreamHandler
        
        self.stream_config = StreamConfig.from_env()
        self.event_processor = StreamEventProcessor(config=self.stream_config)
        self.sources_extractor = SourcesExtractor(
            workflow_service=self.workflow_service,
            session_service=session_service
        )
        self.stream_handler = StreamHandler(
            workflow_service=self.workflow_service,
            sources_extractor=self.sources_extractor,
            extract_related_questions_fn=self._extract_related_questions_from_state
        )
        
        logger.info("âœ… ChatService.__init__() completed")
    
    def _initialize_workflow(self):
        """ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”"""
        if not LANGGRAPH_AVAILABLE:
            logger.warning("LangGraph is not available. Service will continue without LangGraph features.")
            return
        
        try:
            import os
            # í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (ë¯¼ê° ì •ë³´ëŠ” ë¡œê·¸ì— ë…¸ì¶œí•˜ì§€ ì•ŠìŒ)
            google_api_key = os.getenv("GOOGLE_API_KEY", "")
            if not google_api_key:
                logger.warning("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                logger.warning("LangGraphëŠ” Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                logger.info("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            logger.info("Loading LangGraphConfig from environment...")
            
            config = LangGraphConfig.from_env()
            logger.info(f"LangGraph Config loaded: langgraph_enabled={config.langgraph_enabled}, llm_provider={config.llm_provider}")
            
            if not config.langgraph_enabled:
                logger.warning("LangGraph is disabled in configuration")
                return
            
            logger.info("Initializing LangGraphWorkflowService...")
            
            self.workflow_service = LangGraphWorkflowService(config)
            logger.info("âœ… ChatService initialized successfully with LangGraph workflow")
        except ImportError as e:
            logger.error(f"Import error during workflow initialization: {e}", exc_info=True)
            self.workflow_service = None
        except Exception as e:
            logger.error(f"Failed to initialize workflow service: {e}", exc_info=True)
            import traceback
            tb = traceback.format_exc()
            logger.error(f"Traceback:\n{tb}")
            self.workflow_service = None
    
    async def process_message(
        self,
        message: str,
        session_id: Optional[str] = None,
        enable_checkpoint: bool = True
    ) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ ì²˜ë¦¬
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            enable_checkpoint: ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        if not self.workflow_service:
            import os
            error_details = []
            
            # ì›ì¸ ë¶„ì„
            if not LANGGRAPH_AVAILABLE:
                error_details.append("LangGraph ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                google_api_key = os.getenv("GOOGLE_API_KEY", "")
                if not google_api_key:
                    error_details.append("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                else:
                    error_details.append("ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
            error_msg = f"Workflow service unavailable. Details: {', '.join(error_details)}"
            logger.error(error_msg)
            
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\nì›ì¸:\n" + "\n".join(f"- {detail}" for detail in error_details) + "\n\nAPI ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "confidence": 0.0,
                "legal_references": [],
                "processing_steps": ["ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨"],
                "session_id": session_id or "error",
                "processing_time": 0.0,
                "query_type": "error",
                "metadata": {"error_details": error_details},
                "errors": error_details
            }
        
        try:
            result = await self.workflow_service.process_query(
                query=message,
                session_id=session_id,
                enable_checkpoint=enable_checkpoint
            )
            return result
        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)
            import os
            debug_mode = os.getenv("DEBUG", "false").lower() == "true"
            error_detail = str(e) if debug_mode else "ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0,
                "legal_references": [],
                "processing_steps": [f"ì˜¤ë¥˜: {error_detail}"],
                "session_id": session_id or "error",
                "processing_time": 0.0,
                "query_type": "error",
                "metadata": {"error": error_detail} if debug_mode else {"error": True},
                "errors": [error_detail]
            }
    
    async def stream_message(
        self,
        message: str,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì²˜ë¦¬ (Server-Sent Events)
        ì‹¤ì œ LLM í† í° ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•©ë‹ˆë‹¤.
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            
        Yields:
            ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬ (í† í° ë‹¨ìœ„)
        """
        # ì´ë²¤íŠ¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.event_processor.reset()
        
        has_yielded = False  # ìµœì†Œí•œ í•˜ë‚˜ì˜ yieldê°€ ìˆì—ˆëŠ”ì§€ ì¶”ì 
        
        if not self.workflow_service:
            error_event = self._create_error_event("[ì˜¤ë¥˜] ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            yield json.dumps(error_event, ensure_ascii=False) + "\n"
            has_yielded = True
            return
        
        try:
            import uuid
            
            # ì„¸ì…˜ ID ìƒì„±
            if not session_id:
                session_id = str(uuid.uuid4())
            
            # ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            from lawfirm_langgraph.core.workflow.state.state_definitions import create_initial_legal_state
            
            # ë¡œê¹…: message ê°’ í™•ì¸
            logger.info(f"stream_message: ë°›ì€ message='{message[:100] if message else 'EMPTY'}...', length={len(message) if message else 0}")
            
            # messageë¥¼ queryë¡œ ì‚¬ìš© (create_initial_legal_stateì˜ ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ëŠ” query)
            initial_state = create_initial_legal_state(message, session_id)
            
            
            initial_query = self._validate_and_augment_state(initial_state, message, session_id)
            if not initial_query:
                error_event = self._create_error_event("[ì˜¤ë¥˜] ì§ˆë¬¸ì´ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                yield json.dumps(error_event, ensure_ascii=False) + "\n"
                return
            
            config = {"configurable": {"thread_id": session_id}}
            
            # ë””ë²„ê·¸ ëª¨ë“œ í™•ì¸
            DEBUG_STREAM = self.stream_config.debug_stream
            
            try:
                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
                event_count = 0
                llm_stream_count = 0
                event_types_seen = set()  # ë³¸ ì´ë²¤íŠ¸ íƒ€ì… ì¶”ì  (ë””ë²„ê¹…ìš©, ì œí•œì  ì‚¬ìš©)
                node_names_seen = set()  # ë³¸ ë…¸ë“œ ì´ë¦„ ì¶”ì  (ë””ë²„ê¹…ìš©, ì œí•œì  ì‚¬ìš©)
                
                # ê´€ë ¨ ì´ë²¤íŠ¸ íƒ€ì… ì§‘í•© (ì„±ëŠ¥ ìµœì í™”)
                RELEVANT_EVENT_TYPES = self.stream_config.relevant_event_types
                
                # ë©”ëª¨ë¦¬ ìµœì í™”: ì´ë²¤íŠ¸ íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
                MAX_EVENT_HISTORY = self.stream_config.max_event_history
                
                async for event in self._get_stream_events(initial_state, config):
                    event_count += 1
                    # ì´ë²¤íŠ¸ íƒ€ì… í™•ì¸
                    event_type = event.get("event", "")
                    event_name = event.get("name", "")
                    
                    # ê´€ë ¨ ì—†ëŠ” ì´ë²¤íŠ¸ëŠ” ì¦‰ì‹œ ê±´ë„ˆë›°ê¸° (ì„±ëŠ¥ ìµœì í™” - ì¡°ê¸° ì¢…ë£Œ)
                    if event_type not in RELEVANT_EVENT_TYPES:
                        continue
                    
                    # ë””ë²„ê¹… ëª¨ë“œì—ì„œë§Œ ì´ë²¤íŠ¸ ì¶”ì  (ë©”ëª¨ë¦¬ ìµœì í™”: ì œí•œì  ì¶”ì )
                    if DEBUG_STREAM and event_count <= MAX_EVENT_HISTORY:
                        event_types_seen.add(event_type)
                        if event_name:
                            node_names_seen.add(event_name)
                        if event_count <= 20:
                            logger.debug(f"ì²˜ë¦¬í•  ì´ë²¤íŠ¸ #{event_count}: type={event_type}, name={event_name}")
                    
                    # StreamEventProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë²¤íŠ¸ ì²˜ë¦¬
                    stream_event = self.event_processor.process_stream_event(event)
                    if stream_event:
                        yield json.dumps(stream_event, ensure_ascii=False) + "\n"
                        has_yielded = True
                        if stream_event.get("type") == "stream":
                            llm_stream_count += 1
                
                # event_processorì—ì„œ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                full_answer = self.event_processor.full_answer
                answer_found = self.event_processor.answer_found
                tokens_received = self.event_processor.tokens_received
                
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ìµœì¢… í™•ì¸ (DEBUG_STREAMì´ trueì¼ ë•Œë§Œ)
                if DEBUG_STREAM:
                    logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: ì´ {event_count}ê°œ ì´ë²¤íŠ¸, LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ {llm_stream_count}ê°œ, í† í° ìˆ˜ì‹  {tokens_received}ê°œ")
                
                # LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ê°€ ì—†ì„ ë•Œë§Œ ê²½ê³  (í”„ë¡œë•ì…˜ì—ì„œë„)
                if llm_stream_count == 0:
                    if DEBUG_STREAM:
                        logger.warning("âš ï¸ LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        logger.debug(f"ë°œìƒí•œ ëª¨ë“  ì´ë²¤íŠ¸ íƒ€ì…: {sorted(event_types_seen)}")
                        logger.debug(f"ë°œìƒí•œ ëª¨ë“  ë…¸ë“œ ì´ë¦„: {sorted(node_names_seen)}")
                
                if not answer_found:
                    missing_event = await self._handle_missing_answer(message, session_id, full_answer)
                    if missing_event:
                        yield json.dumps(missing_event, ensure_ascii=False) + "\n"
                        has_yielded = True
                        if missing_event.get("type") == "stream":
                            self.event_processor.answer_found = True
            
            except Exception as stream_error:
                # astream_events ì‹¤íŒ¨ ì‹œ astreamìœ¼ë¡œ í´ë°±
                if DEBUG_STREAM:
                    logger.warning(f"astream_events ì‹¤íŒ¨, astreamìœ¼ë¡œ í´ë°±: {stream_error}")
                # stream_mode="updates" ì‚¬ìš© ì‹œ ë³€ê²½ëœ í•„ë“œë§Œ í¬í•¨ë˜ë¯€ë¡œ ì§ì ‘ í™•ì¸ ê°€ëŠ¥
                async for event in self.workflow_service.app.astream(initial_state, config, stream_mode="updates"):
                    for node_name, node_state in event.items():
                        if isinstance(node_state, dict):
                            answer = None
                            # answer ê·¸ë£¹ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
                            if "answer" in node_state:
                                answer = node_state.get("answer", "")
                            # common ê·¸ë£¹ì—ì„œ answer í™•ì¸ (ë³€ê²½ëœ ê²½ìš°ì—ë§Œ í¬í•¨)
                            elif "common" in node_state and isinstance(node_state["common"], dict):
                                common = node_state["common"]
                                if "answer" in common:
                                    answer = common.get("answer", "")
                            
                            if answer and isinstance(answer, str):
                                current_full_answer = self.event_processor.full_answer
                                if len(answer) > len(current_full_answer):
                                    new_part = answer[len(current_full_answer):]
                                    if new_part:
                                        self.event_processor.full_answer = answer
                                        # ìŠ¤íŠ¸ë¦¼ ì²­í¬ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ì „ì†¡
                                        stream_event = {
                                            "type": "stream",
                                            "content": new_part,
                                            "timestamp": datetime.now().isoformat()
                                        }
                                        yield json.dumps(stream_event, ensure_ascii=False) + "\n"
                                        self.event_processor.answer_found = True
            
            sources_data = await self._extract_sources_from_state(session_id) if session_id else {}
            final_sources = sources_data.get("sources", [])
            final_legal_references = sources_data.get("legal_references", [])
            final_sources_detail = sources_data.get("sources_detail", [])
            final_related_questions = sources_data.get("related_questions", [])
            
            # event_processorì—ì„œ full_answer ê°€ì ¸ì˜¤ê¸°
            full_answer = self.event_processor.full_answer
            answer_found = self.event_processor.answer_found
            tokens_received = self.event_processor.tokens_received
            
            if full_answer:
                has_yielded = True
                
                # í† í° ì œí•œ í™•ì¸
                MAX_OUTPUT_TOKENS = self.stream_config.max_output_tokens
                should_split = tokens_received >= MAX_OUTPUT_TOKENS
                
                import uuid
                
                # ë©”ì‹œì§€ ID ìƒì„± (chat.pyì—ì„œ ì €ì¥ ì‹œ ì‚¬ìš©)
                message_id = str(uuid.uuid4())
                
                if not final_sources and not final_legal_references and not final_sources_detail:
                    re_extracted = await self._re_extract_sources_before_final(session_id, {})
                    if re_extracted.get("sources"):
                        final_sources = re_extracted["sources"]
                    if re_extracted.get("legal_references"):
                        final_legal_references = re_extracted["legal_references"]
                    if re_extracted.get("sources_detail"):
                        final_sources_detail = re_extracted["sources_detail"]
                    if re_extracted.get("related_questions"):
                        final_related_questions = re_extracted["related_questions"]
                
                if should_split:
                    from api.services.answer_splitter import AnswerSplitter
                    splitter = AnswerSplitter(chunk_size=self.stream_config.chunk_size)
                    chunks = splitter.split_answer(full_answer)
                    
                    content = chunks[0].content if chunks else full_answer
                    final_event = self._create_final_event(
                        content, tokens_received, answer_found,
                        final_sources, final_legal_references,
                        final_sources_detail, final_related_questions,
                        message_id, needs_continuation=bool(chunks)
                    )
                    yield json.dumps(final_event, ensure_ascii=False) + "\n"
                else:
                    final_event = self._create_final_event(
                        full_answer, tokens_received, answer_found,
                        final_sources, final_legal_references,
                        final_sources_detail, final_related_questions,
                        message_id
                    )
                    yield json.dumps(final_event, ensure_ascii=False) + "\n"
            else:
                if DEBUG_STREAM:
                    logger.warning("ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                if not answer_found:
                    error_event = self._create_error_event("[ì˜¤ë¥˜] ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    error_event["metadata"]["tokens_received"] = tokens_received
                    yield json.dumps(error_event, ensure_ascii=False) + "\n"
                    has_yielded = True
            
        except Exception as e:
            logger.error(f"Error in stream_message: {e}", exc_info=True)
            try:
                error_event = self._create_error_event(
                    f"[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                    error_type=type(e).__name__
                )
                yield json.dumps(error_event, ensure_ascii=False) + "\n"
                has_yielded = True
            except Exception as yield_error:
                logger.error(f"Error yielding error message: {yield_error}")
                try:
                    fallback_event = self._create_error_event("[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    yield json.dumps(fallback_event, ensure_ascii=False) + "\n"
                    has_yielded = True
                except Exception:
                    pass
        # finally ë¸”ë¡ ì œê±°: finallyì—ì„œ yieldë¥¼ í•˜ë©´ ì œë„ˆë ˆì´í„°ê°€ ì œëŒ€ë¡œ ì¢…ë£Œë˜ì§€ ì•Šì•„
        # ERR_INCOMPLETE_CHUNKED_ENCODING ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
        # ìŠ¤íŠ¸ë¦¼ ì¢…ë£ŒëŠ” FastAPI StreamingResponseê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
    
    async def get_sources_from_session(
        self,
        session_id: str,
        message_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ì„¸ì…˜ì˜ ìµœì¢… stateì—ì„œ sources ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            session_id: ì„¸ì…˜ ID
            message_id: ë©”ì‹œì§€ ID (ì„ íƒì‚¬í•­, í•´ë‹¹ ë©”ì‹œì§€ì˜ metadataì—ì„œ sources ê°€ì ¸ì˜¤ê¸°)
        
        Returns:
            sources, legal_references, sources_detail ë”•ì…”ë„ˆë¦¬
        """
        # ë¨¼ì € ë©”ì‹œì§€ì˜ metadataì—ì„œ sourcesë¥¼ ê°€ì ¸ì˜¤ê¸° ì‹œë„
        result = await self.sources_extractor.extract_from_message_metadata(session_id, message_id)
        
        # ì—†ìœ¼ë©´ stateì—ì„œ ê°€ì ¸ì˜¤ê¸°
        if not any(result.values()):
            result = await self.sources_extractor.extract_from_state(session_id)
        
        return result
    
    async def stream_final_answer(
        self,
        message: str,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        LangGraphì˜ astream_events()ë¥¼ ì‚¬ìš©í•˜ì—¬ 
        generate_and_validate_answer ë…¸ë“œì˜ LLM ì‘ë‹µë§Œ ìŠ¤íŠ¸ë¦¼ í˜•íƒœë¡œ ì „ë‹¬
        
        ì˜ˆì œ ì½”ë“œ ì°¸ê³ :
        async for event in compiled_graph.astream_events({"topic": "AI"}):
            if event["event"] == "on_llm_stream" and event["name"] == "generate_response":
                yield f"data: {json.dumps({'token': data})}\n\n"
        """
        if not self.workflow_service:
            error_event = {
                "type": "error",
                "content": "[ì˜¤ë¥˜] ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "timestamp": datetime.now().isoformat()
            }
            yield f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"
            return
        
        try:
            import uuid
            
            # ì„¸ì…˜ ID ìƒì„±
            if not session_id:
                session_id = str(uuid.uuid4())
            
            from lawfirm_langgraph.core.workflow.state.state_definitions import create_initial_legal_state
            initial_state = create_initial_legal_state(message, session_id)
            
            initial_query = self._validate_and_augment_state(initial_state, message, session_id)
            if not initial_query:
                error_event = {
                    "type": "error",
                    "content": "[ì˜¤ë¥˜] ì§ˆë¬¸ì´ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    "timestamp": datetime.now().isoformat()
                }
                yield f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"
                return
            
            # ì½œë°± í•¸ë“¤ëŸ¬ ë° í ìƒì„±
            callback_queue = asyncio.Queue()
            callback_handler = None
            if self.workflow_service and hasattr(self.workflow_service, 'create_streaming_callback_handler'):
                callback_handler = self.workflow_service.create_streaming_callback_handler(queue=callback_queue)
                if callback_handler:
                    logger.info("[stream_final_answer] âœ… StreamingCallbackHandler created and ready")
                else:
                    logger.warning("[stream_final_answer] âš ï¸ Failed to create StreamingCallbackHandler")
            
            # configì— ì½œë°± í¬í•¨
            config = {"configurable": {"thread_id": session_id}}
            if callback_handler:
                config = self.workflow_service.get_config_with_callbacks(
                    session_id=session_id,
                    callbacks=[callback_handler]
                )
                logger.info(f"[stream_final_answer] âœ… Callbacks added to config: {len(config.get('callbacks', []))} callback(s)")
            else:
                logger.warning("[stream_final_answer] âš ï¸ No callback handler, streaming may not work optimally")
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_event = {
                "type": "progress",
                "content": "ë‹µë³€ ìƒì„± ì¤‘...",
                "timestamp": datetime.now().isoformat()
            }
            yield f"data: {json.dumps(progress_event, ensure_ascii=False)}\n\n"
            
            # LangGraphì˜ astream_events() ì‚¬ìš©
            # generate_and_validate_answer ë…¸ë“œì˜ LLM ìŠ¤íŠ¸ë¦¼ë§Œ í•„í„°ë§
            # StreamEventProcessorì˜ ë¡œì§ì„ ì°¸ê³ í•˜ì—¬ êµ¬í˜„
            answer_generation_started = False
            last_node_name = None
            event_count = 0
            stream_event_count = 0
            on_llm_stream_count = 0
            on_chat_model_stream_count = 0
            on_chain_stream_count = 0
            full_answer = ""  # ìŠ¤íŠ¸ë¦¬ë°ëœ ë‹µë³€ ì¶”ì 
            callback_chunks_received = 0  # ì½œë°±ì—ì„œ ë°›ì€ ì²­í¬ ìˆ˜
            processed_callback_chunks = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ ì½œë°± ì²­í¬ ì¶”ì 
            
            # ì½œë°± íì˜ ì²­í¬ë¥¼ ì €ì¥í•  ì¶œë ¥ í ìƒì„±
            chunk_output_queue = asyncio.Queue() if callback_queue else None
            
            # ì½œë°± í ëª¨ë‹ˆí„°ë§ì„ ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
            callback_monitoring_active = True
            
            async def monitor_callback_queue():
                """ì½œë°± íë¥¼ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ì²­í¬ë¥¼ ì¶œë ¥ íì— ë„£ê¸°"""
                nonlocal callback_monitoring_active
                while callback_monitoring_active:
                    try:
                        if callback_queue and chunk_output_queue:
                            try:
                                # íì—ì„œ ì²­í¬ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                                chunk_data = await asyncio.wait_for(
                                    callback_queue.get(),
                                    timeout=0.1
                                )
                                if chunk_data and chunk_data.get("type") == "chunk":
                                    # ì¶œë ¥ íì— ë„£ê¸°
                                    await chunk_output_queue.put(chunk_data)
                            except asyncio.TimeoutError:
                                # íƒ€ì„ì•„ì›ƒì€ ì •ìƒ (íê°€ ë¹„ì–´ìˆìŒ)
                                await asyncio.sleep(0.01)  # ì§§ì€ ëŒ€ê¸°
                                continue
                            except asyncio.QueueEmpty:
                                await asyncio.sleep(0.01)  # ì§§ì€ ëŒ€ê¸°
                                continue
                        else:
                            await asyncio.sleep(0.1)
                    except Exception as e:
                        logger.debug(f"[stream_final_answer] Error in callback queue monitoring: {e}")
                        await asyncio.sleep(0.1)
            
            # ì½œë°± í ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì‹œì‘
            callback_task = None
            if callback_queue and chunk_output_queue:
                callback_task = asyncio.create_task(monitor_callback_queue())
                logger.info("[stream_final_answer] âœ… Callback queue monitoring task started")
            
            # astream_eventsì™€ ì½œë°± í ëª¨ë‹ˆí„°ë§ ë³‘í–‰
            try:
                async for event in self.workflow_service.app.astream_events(
                    initial_state,
                    config,
                    version="v2"
                ):
                    # ì½œë°± ì¶œë ¥ íì—ì„œ ì²­í¬ í™•ì¸ (non-blocking) - answer_generation_startedì™€ ë¬´ê´€í•˜ê²Œ ì²˜ë¦¬
                    if chunk_output_queue:
                        try:
                            while True:
                                try:
                                    chunk_data = chunk_output_queue.get_nowait()
                                    if chunk_data and chunk_data.get("type") == "chunk":
                                        content = chunk_data.get("content", "")
                                        chunk_index = chunk_data.get("chunk_index", 0)
                                        chunk_key = f"{chunk_index}_{content[:10]}"
                                        
                                        # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
                                        if chunk_key not in processed_callback_chunks and content:
                                            processed_callback_chunks.add(chunk_key)
                                            callback_chunks_received += 1
                                            full_answer += content
                                            
                                            # ì½œë°±ì—ì„œ ë°›ì€ ì²­í¬ëŠ” ì¦‰ì‹œ ì „ë‹¬ (í”Œë˜ê·¸ì™€ ë¬´ê´€)
                                            stream_event = {
                                                "type": "stream",
                                                "content": content,
                                                "source": "callback",
                                                "timestamp": datetime.now().isoformat()
                                            }
                                            yield f"data: {json.dumps(stream_event, ensure_ascii=False)}\n\n"
                                            
                                            # ì²˜ìŒ 10ê°œ ì²­í¬ë§Œ ìƒì„¸ ë¡œê¹…
                                            if callback_chunks_received <= 10:
                                                logger.info(
                                                    f"[stream_final_answer] âœ… Callback chunk #{callback_chunks_received}: "
                                                    f"length={len(content)}, content={content[:50]}..."
                                                )
                                except asyncio.QueueEmpty:
                                    break
                        except Exception as e:
                            logger.debug(f"[stream_final_answer] Error checking callback output queue: {e}")
                    
                    event_count += 1
                    event_type = event.get("event", "")
                    event_name = event.get("name", "")
                    event_parent = event.get("parent", {})
                    
                    # on_llm_stream ì´ë²¤íŠ¸ ë°œìƒ ì¶”ì 
                    if event_type == "on_llm_stream":
                        on_llm_stream_count += 1
                        if on_llm_stream_count <= 10:  # ì²˜ìŒ 10ê°œë§Œ ìƒì„¸ ë¡œê¹…
                            logger.debug(
                                f"[stream_final_answer] on_llm_stream ì´ë²¤íŠ¸ #{on_llm_stream_count}: "
                                f"name={event_name}, "
                                f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}"
                            )
                    elif event_type == "on_chat_model_stream":
                        on_chat_model_stream_count += 1
                        if on_chat_model_stream_count <= 10:
                            logger.debug(
                                f"[stream_final_answer] on_chat_model_stream ì´ë²¤íŠ¸ #{on_chat_model_stream_count}: "
                                f"name={event_name}, "
                                f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}"
                            )
                        
                        # ì œê±°: on_chat_model_stream ì´ë²¤íŠ¸ë¥¼ ì½œë°± íì— ì¶”ê°€í•˜ëŠ” ë¡œì§
                        # astream_events()ì—ì„œ ì´ë¯¸ ì§ì ‘ ì²˜ë¦¬í•˜ë¯€ë¡œ(line 712-745) ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì œê±°
                        # ì½œë°± í•¸ë“¤ëŸ¬ëŠ” LLMì˜ stream() í˜¸ì¶œ ì‹œ ìë™ìœ¼ë¡œ on_llm_streamì„ í˜¸ì¶œí•˜ë¯€ë¡œ
                        # ë³„ë„ë¡œ ì½œë°± íì— ì¶”ê°€í•  í•„ìš” ì—†ìŒ
                    elif event_type == "on_chain_stream":
                        on_chain_stream_count += 1
                        if on_chain_stream_count <= 5:
                            logger.debug(
                                f"[stream_final_answer] on_chain_stream ì´ë²¤íŠ¸ #{on_chain_stream_count}: "
                                f"name={event_name}, "
                                f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}"
                            )
                    
                    # on_chain_start: ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹œì‘ ê°ì§€
                    if event_type == "on_chain_start":
                        node_name = event_name
                        if node_name in ["generate_answer_stream", "generate_answer_enhanced", "generate_and_validate_answer"]:
                            answer_generation_started = True
                            last_node_name = node_name
                            logger.debug(f"[stream_final_answer] ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹œì‘: {node_name}")
                    
                    # on_chain_end: ë‹µë³€ ìƒì„± ë…¸ë“œ ì™„ë£Œ
                    elif event_type == "on_chain_end":
                        node_name = event_name
                        if node_name in ["generate_answer_stream", "generate_and_validate_answer"]:
                            answer_generation_started = False
                            logger.debug(f"[stream_final_answer] ë‹µë³€ ìƒì„± ë…¸ë“œ ì™„ë£Œ: {node_name}")
                    
                    # LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ (on_llm_stream, on_chat_model_streamë§Œ ì²˜ë¦¬)
                    # on_chain_streamì€ ì œì™¸ (ì „ì²´ ë‹µë³€ì„ í•œ ë²ˆì— ì „ë‹¬í•˜ë¯€ë¡œ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ë¶ˆê°€)
                    elif event_type in ["on_llm_stream", "on_chat_model_stream"]:
                        # on_llm_stream ì´ë²¤íŠ¸ ë°œìƒ ë¡œê¹…
                        logger.debug(
                            f"[stream_final_answer] on_llm_stream ì´ë²¤íŠ¸ ë°œìƒ: "
                            f"type={event_type}, name={event_name}, "
                            f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}, "
                            f"last_node={last_node_name}, started={answer_generation_started}"
                        )
                        
                        # ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                        if not answer_generation_started:
                            logger.debug(f"[stream_final_answer] ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì‹œì‘ë˜ì§€ ì•Šì•„ ê±´ë„ˆëœ€: {event_name}")
                            continue
                        
                        if self._is_target_node(event_name, event_parent, last_node_name):
                            logger.debug(f"[stream_final_answer] íƒ€ê²Ÿ ë…¸ë“œ í™•ì¸ë¨: {event_name}, í† í° ì¶”ì¶œ ì‹œì‘")
                            event_data = event.get("data", {})
                            token = self._extract_token_from_event(event_data)
                            
                            if token:
                                stream_event_count += 1
                                full_answer += token  # full_answer ì¶”ì 
                                logger.debug(
                                    f"[stream_final_answer] í† í° ì „ì†¡: "
                                    f"token_length={len(token)}, "
                                    f"token_preview={token[:50]}..., "
                                    f"stream_event_count={stream_event_count}"
                                )
                                stream_event = {
                                    "type": "stream",
                                    "content": token,
                                    "timestamp": datetime.now().isoformat()
                                }
                                yield f"data: {json.dumps(stream_event, ensure_ascii=False)}\n\n"
                            else:
                                logger.debug(
                                    f"[stream_final_answer] í† í° ì¶”ì¶œ ì‹¤íŒ¨: "
                                    f"token={token}, "
                                    f"event_data_keys={list(event_data.keys()) if isinstance(event_data, dict) else []}"
                                )
                        else:
                            # ë””ë²„ê¹…: í•„í„°ë§ë˜ì§€ ì•Šì€ ì´ë²¤íŠ¸ ë¡œê¹…
                            logger.debug(
                                f"[stream_final_answer] íƒ€ê²Ÿ ë…¸ë“œê°€ ì•„ë‹˜ (í•„í„°ë§ë¨): "
                                f"type={event_type}, name={event_name}, "
                                f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}, "
                                f"last_node={last_node_name}, started={answer_generation_started}"
                            )
                    
                    # ë‹µë³€ ìƒì„± ë…¸ë“œ ì™„ë£Œ ì‹œì  (generate_answer_stream ë˜ëŠ” generate_and_validate_answer)
                    if event_type == "on_chain_end" and event_name in ["generate_answer_stream", "generate_and_validate_answer"]:
                        # ì½œë°± íì— ë‚¨ì€ ì²­í¬ ì²˜ë¦¬
                        if chunk_output_queue:
                            try:
                                # íì— ë‚¨ì€ ëª¨ë“  ì²­í¬ ì²˜ë¦¬
                                while True:
                                    try:
                                        chunk_data = chunk_output_queue.get_nowait()
                                        if chunk_data and chunk_data.get("type") == "chunk":
                                            content = chunk_data.get("content", "")
                                            chunk_index = chunk_data.get("chunk_index", 0)
                                            chunk_key = f"{chunk_index}_{content[:10]}"
                                            
                                            if chunk_key not in processed_callback_chunks and content:
                                                processed_callback_chunks.add(chunk_key)
                                                callback_chunks_received += 1
                                                full_answer += content
                                                
                                                stream_event = {
                                                    "type": "stream",
                                                    "content": content,
                                                    "source": "callback",
                                                    "timestamp": datetime.now().isoformat()
                                                }
                                                yield f"data: {json.dumps(stream_event, ensure_ascii=False)}\n\n"
                                    except asyncio.QueueEmpty:
                                        break
                            except Exception as e:
                                logger.debug(f"[stream_final_answer] Error processing remaining callback chunks: {e}")
                        
                        logger.info(
                            f"[stream_final_answer] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: "
                            f"ì´ {event_count}ê°œ ì´ë²¤íŠ¸, "
                            f"ìŠ¤íŠ¸ë¦¼ ì´ë²¤íŠ¸ {stream_event_count}ê°œ, "
                            f"ì½œë°± ì²­í¬ {callback_chunks_received}ê°œ, "
                            f"on_llm_stream={on_llm_stream_count}ê°œ, "
                            f"on_chat_model_stream={on_chat_model_stream_count}ê°œ, "
                            f"on_chain_stream={on_chain_stream_count}ê°œ, "
                            f"full_answer_length={len(full_answer)}"
                        )
                        
                        # Stateê°€ ì™„ì „íˆ ì €ì¥ë  ë•Œê¹Œì§€ ì•½ê°„ì˜ ì§€ì—° (íƒ€ì´ë° ë¬¸ì œ í•´ê²°)
                        await asyncio.sleep(0.2)
                        
                        # ìµœì¢… ì™„ë£Œ ì´ë²¤íŠ¸ (metadata í¬í•¨)
                        try:
                            # State ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ ìµœì†Œí™”í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ ë¸”ë¡œí‚¹ ë°©ì§€)
                            final_state = None
                            state_values = None
                            
                            try:
                                # ì¦‰ì‹œ ì‹œë„ (íƒ€ì„ì•„ì›ƒ 2ì´ˆ)
                                final_state = await asyncio.wait_for(
                                    self.workflow_service.app.aget_state(config),
                                    timeout=2.0
                                )
                                if final_state and final_state.values:
                                    state_values = final_state.values
                                    logger.debug(f"[stream_final_answer] State retrieved: answer_length={len(state_values.get('answer', ''))}, full_answer_length={len(full_answer)}")
                            except asyncio.TimeoutError:
                                logger.warning(f"[stream_final_answer] Timeout getting state, using empty metadata")
                            except Exception as e:
                                logger.warning(f"[stream_final_answer] Error getting state: {e}")
                            
                            if final_state and state_values:
                                sources = state_values.get("sources", [])
                                legal_references = state_values.get("legal_references", [])
                                sources_detail = state_values.get("sources_detail", [])
                                
                                if not sources and not sources_detail:
                                    retrieved_docs = state_values.get("retrieved_docs", [])
                                    if retrieved_docs and hasattr(self, 'sources_extractor') and self.sources_extractor:
                                        try:
                                            sources_data = self.sources_extractor._extract_sources(state_values)
                                            legal_references_data = self.sources_extractor._extract_legal_references(state_values)
                                            sources_detail_data = self.sources_extractor._extract_sources_detail(state_values)
                                            
                                            if sources_detail_data:
                                                sources_detail = sources_detail_data
                                                state_values["sources_detail"] = sources_detail_data
                                            if sources_data:
                                                sources = sources_data
                                                state_values["sources"] = sources_data
                                            if legal_references_data:
                                                legal_references = legal_references_data
                                        except Exception as e:
                                            logger.warning(f"[stream_final_answer] Failed to extract sources from retrieved_docs: {e}", exc_info=True)
                                
                                related_questions = await self._extract_related_questions_from_state(
                                    state_values, initial_state, message, full_answer, session_id
                                )
                                
                                llm_validation_result = state_values.get("metadata", {}).get("llm_validation_result", {})
                                
                                final_metadata = {
                                    "sources": sources,
                                    "legal_references": legal_references,
                                    "sources_detail": sources_detail,
                                    "related_questions": related_questions,
                                    "llm_validation": llm_validation_result if llm_validation_result else None
                                }
                            else:
                                final_metadata = {}
                        except Exception as e:
                            logger.error(f"Error getting final state: {e}", exc_info=True)
                            final_metadata = {}
                        
                        # í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê²€ì¦ ì´ë²¤íŠ¸ ì „ì†¡
                        if final_metadata.get("llm_validation"):
                            validation_result = final_metadata["llm_validation"]
                            validation_event = {
                                "type": "validation",
                                "content": "ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ",
                                "metadata": {
                                    "quality_score": validation_result.get("quality_score", 0.0),
                                    "is_valid": validation_result.get("is_valid", False),
                                    "needs_regeneration": validation_result.get("needs_regeneration", False),
                                    "regeneration_reason": validation_result.get("regeneration_reason"),
                                    "issues": validation_result.get("issues", []),
                                    "strengths": validation_result.get("strengths", [])
                                },
                                "timestamp": datetime.now().isoformat()
                            }
                            yield f"data: {json.dumps(validation_event, ensure_ascii=False)}\n\n"
                        
                        # ìµœì¢… ë‹µë³€ ì´ë²¤íŠ¸
                        final_event = {
                            "type": "final",
                            "content": "",  # ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì´ë¯¸ ì „ì†¡ë¨
                            "metadata": final_metadata or {},
                            "timestamp": datetime.now().isoformat()
                        }
                        yield f"data: {json.dumps(final_event, ensure_ascii=False)}\n\n"
                        
                        # ì™„ë£Œ ì´ë²¤íŠ¸ (ìµœì¢… ë‹µë³€ê³¼ ë©”íƒ€ë°ì´í„° í¬í•¨)
                        done_event = {
                            "type": "done",
                            "content": full_answer,  # ìµœì¢… ë‹µë³€ í¬í•¨
                            "metadata": final_metadata or {},
                            "timestamp": datetime.now().isoformat()
                        }
                        yield f"data: {json.dumps(done_event, ensure_ascii=False)}\n\n"
                        break  # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ
                
            finally:
                # ì½œë°± ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì¢…ë£Œ
                callback_monitoring_active = False
                if callback_task:
                    callback_task.cancel()
                    try:
                        await callback_task
                    except asyncio.CancelledError:
                        pass
                
                # ì½œë°± í•¸ë“¤ëŸ¬ í†µê³„ ë¡œê¹…
                if callback_handler and hasattr(callback_handler, 'get_stats'):
                    stats = callback_handler.get_stats()
                    logger.info(
                        f"[stream_final_answer] Callback stats: "
                        f"chunks={callback_chunks_received}, "
                        f"total_chunks={stats.get('total_chunks', 0)}, "
                        f"streaming_active={stats.get('streaming_active', False)}"
                    )
        
        except GeneratorExit:
            # í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš° ì •ìƒ ì¢…ë£Œ
            logger.debug("[stream_final_answer] Client disconnected, closing stream")
            return
        except Exception as e:
            logger.error(f"Stream error: {e}", exc_info=True)
            try:
                error_event = {
                    "type": "error",
                    "content": f"[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }
                yield f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"
            except GeneratorExit:
                # yield ì¤‘ í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš°
                logger.debug("[stream_final_answer] Client disconnected during error handling")
                return
            except Exception as yield_error:
                # yield ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš° (ìŠ¤íŠ¸ë¦¼ì´ ì´ë¯¸ ë‹«í˜)
                logger.error(f"Failed to yield error event: {yield_error}")
                return
    
    
    def _create_error_event(self, content: str, error_type: Optional[str] = None) -> Dict[str, Any]:
        """ì—ëŸ¬ ì´ë²¤íŠ¸ ìƒì„±"""
        event = {
            "type": "final",
            "content": content,
            "metadata": {"error": True},
            "timestamp": datetime.now().isoformat()
        }
        if error_type:
            event["metadata"]["error_type"] = error_type
        return event
    
    def _validate_and_augment_state(self, initial_state: Dict[str, Any], message: str, session_id: str) -> Optional[str]:
        """ìƒíƒœ ê²€ì¦ ë° ë³´ê°•"""
        if "input" not in initial_state:
            initial_state["input"] = {}
        if not initial_state["input"].get("query"):
            initial_state["input"]["query"] = message
        if not initial_state["input"].get("session_id"):
            initial_state["input"]["session_id"] = session_id
        
        if not initial_state.get("query"):
            initial_state["query"] = message
        if not initial_state.get("session_id"):
            initial_state["session_id"] = session_id
        
        initial_query = initial_state.get("input", {}).get("query") or initial_state.get("query")
        if not initial_query or not str(initial_query).strip():
            logger.error(f"Initial state query is empty! Input message was: '{message[:50]}...'")
            return None
        return initial_query
    
    async def _get_stream_events(self, initial_state: Dict[str, Any], config: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬)"""
        DEBUG_STREAM = self.stream_config.debug_stream
        
        try:
            if DEBUG_STREAM:
                logger.info("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: astream_events(version='v2') ì‚¬ìš©")
            try:
                try:
                    async for event in self.workflow_service.app.astream_events(
                        initial_state, 
                        config,
                        version="v2",
                        include_names=["generate_answer_stream", "generate_answer_enhanced", "generate_and_validate_answer"]
                    ):
                        yield event
                except (TypeError, AttributeError):
                    try:
                        async for event in self.workflow_service.app.astream_events(
                            initial_state, 
                            config,
                            version="v2",
                            exclude_names=["classify_query_and_complexity", "classification_parallel", 
                                          "expand_keywords", "validate_answer_quality", "prepare_search_query",
                                          "execute_searches_parallel", "process_search_results_combined",
                                          "prepare_documents_and_terms", "prepare_final_response"]
                        ):
                            yield event
                    except (TypeError, AttributeError):
                        async for event in self.workflow_service.app.astream_events(
                            initial_state, 
                            config,
                            version="v2"
                        ):
                            yield event
            except (TypeError, AttributeError):
                async for event in self.workflow_service.app.astream_events(
                    initial_state, 
                    config,
                    version="v2"
                ):
                    yield event
        except (TypeError, AttributeError) as ve:
            logger.debug(f"astream_eventsì—ì„œ version íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›: {ve}, ê¸°ë³¸ ë²„ì „ ì‚¬ìš©")
            if DEBUG_STREAM:
                logger.info("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: astream_events() ì‚¬ìš© (ê¸°ë³¸ ë²„ì „)")
            async for event in self.workflow_service.app.astream_events(
                initial_state, 
                config
            ):
                yield event
    
    async def _extract_sources_from_state(self, session_id: str, timeout: float = 2.0) -> Dict[str, Any]:
        """Stateì—ì„œ sources ì¶”ì¶œ"""
        result = {
            "sources": [],
            "legal_references": [],
            "sources_detail": [],
            "related_questions": []
        }
        
        try:
            sources_data = await asyncio.wait_for(
                self.sources_extractor.extract_from_state(session_id),
                timeout=timeout
            )
            result["sources"] = sources_data.get("sources", [])
            result["legal_references"] = sources_data.get("legal_references", [])
            result["sources_detail"] = sources_data.get("sources_detail", [])
            result["related_questions"] = sources_data.get("related_questions", [])
        except asyncio.TimeoutError:
            logger.warning(f"Timeout getting sources from LangGraph state for session {session_id}")
        except Exception as e:
            logger.warning(f"Failed to get sources from LangGraph state: {e}")
        
        return result
    
    async def _re_extract_sources_before_final(self, session_id: str, state_values: Dict[str, Any]) -> Dict[str, Any]:
        """ìµœì¢… ì´ë²¤íŠ¸ ì „ sources ì¬ì¶”ì¶œ"""
        result = {
            "sources": [],
            "legal_references": [],
            "sources_detail": [],
            "related_questions": []
        }
        
        if not session_id or not self.workflow_service or not self.workflow_service.app:
            return result
        
        try:
            config = {"configurable": {"thread_id": session_id}}
            final_state = await asyncio.wait_for(
                self.workflow_service.app.aget_state(config),
                timeout=2.0
            )
            
            if final_state and final_state.values:
                state_values = final_state.values
                sources_data = self.sources_extractor._extract_sources(state_values)
                legal_references_data = self.sources_extractor._extract_legal_references(state_values)
                sources_detail_data = self.sources_extractor._extract_sources_detail(state_values)
                related_questions_data = self.sources_extractor._extract_related_questions(state_values)
                
                if sources_data:
                    result["sources"] = sources_data
                if legal_references_data:
                    result["legal_references"] = legal_references_data
                if sources_detail_data:
                    result["sources_detail"] = sources_detail_data
                if related_questions_data:
                    result["related_questions"] = related_questions_data
                
                logger.info(f"Re-extracted sources before final event: {len(result['sources'])} sources, {len(result['legal_references'])} legal_references, {len(result['sources_detail'])} sources_detail, {len(result['related_questions'])} related_questions")
        except asyncio.TimeoutError:
            logger.warning("Timeout re-getting sources before final event")
        except Exception as e:
            logger.warning(f"Failed to re-get sources before final event: {e}")
        
        return result
    
    def _create_final_event(
        self,
        content: str,
        tokens_received: int,
        answer_found: bool,
        sources: list,
        legal_references: list,
        sources_detail: list,
        related_questions: list,
        message_id: str,
        needs_continuation: bool = False
    ) -> Dict[str, Any]:
        """ìµœì¢… ì´ë²¤íŠ¸ ìƒì„±"""
        metadata = {
            "tokens_received": tokens_received,
            "length": len(content),
            "answer_found": answer_found,
            "sources": sources,
            "legal_references": legal_references,
            "sources_detail": sources_detail,
            "related_questions": related_questions,
            "message_id": message_id
        }
        if needs_continuation:
            metadata["needs_continuation"] = True
        
        return {
            "type": "final",
            "content": content,
            "metadata": metadata,
            "timestamp": datetime.now().isoformat()
        }
    
    async def _handle_missing_answer(self, message: str, session_id: str, full_answer: str) -> Optional[Dict[str, Any]]:
        """ë‹µë³€ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²˜ë¦¬"""
        DEBUG_STREAM = self.stream_config.debug_stream
        
        if DEBUG_STREAM:
            logger.warning("LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            logger.info("ìµœì¢… ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        
        try:
            result = await self.process_message(message, session_id)
            final_answer = result.get("answer", "")
            if final_answer and len(final_answer) > len(full_answer):
                missing_part = final_answer[len(full_answer):]
                if missing_part:
                    self.event_processor.full_answer = final_answer
                    return {
                        "type": "stream",
                        "content": missing_part,
                        "timestamp": datetime.now().isoformat()
                    }
            elif final_answer:
                self.event_processor.full_answer = final_answer
                return {
                    "type": "stream",
                    "content": final_answer,
                    "timestamp": datetime.now().isoformat()
                }
        except Exception as e:
            if DEBUG_STREAM:
                logger.error(f"ìµœì¢… ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}", exc_info=True)
            if not self.event_processor.answer_found:
                return self._create_error_event(f"[ì˜¤ë¥˜] ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
        
        return None
    
    def _extract_token_from_event(self, event_data: Dict[str, Any]) -> Optional[str]:
        """ì´ë²¤íŠ¸ì—ì„œ í† í° ì¶”ì¶œ"""
        chunk_obj = event_data.get("chunk")
        token = None
        
        if chunk_obj:
            if hasattr(chunk_obj, "content"):
                content = chunk_obj.content
                if isinstance(content, str):
                    token = content
                elif isinstance(content, list) and len(content) > 0:
                    token = content[0] if isinstance(content[0], str) else str(content[0])
                else:
                    token = str(content) if content else None
            elif isinstance(chunk_obj, str):
                token = chunk_obj
            elif isinstance(chunk_obj, dict):
                token = chunk_obj.get("content") or chunk_obj.get("text")
            elif hasattr(chunk_obj, "text"):
                token = chunk_obj.text
            elif hasattr(chunk_obj, "__class__") and "AIMessageChunk" in str(type(chunk_obj)):
                try:
                    content = getattr(chunk_obj, "content", None)
                    if isinstance(content, str):
                        token = content
                    elif isinstance(content, list) and len(content) > 0:
                        token = content[0] if isinstance(content[0], str) else str(content[0])
                    elif content is not None:
                        token = str(content)
                except Exception:
                    token = None
            else:
                token = str(chunk_obj) if chunk_obj else None
        
        if not token and "delta" in event_data:
            delta = event_data["delta"]
            if isinstance(delta, dict):
                token = delta.get("content") or delta.get("text")
            elif isinstance(delta, str):
                token = delta
        
        return token if isinstance(token, str) and len(token) > 0 else None
    
    def _is_target_node(self, event_name: str, event_parent: Any, last_node_name: Optional[str]) -> bool:
        """íƒ€ê²Ÿ ë…¸ë“œì¸ì§€ í™•ì¸"""
        target_nodes = ["generate_answer_stream", "generate_answer_enhanced", "generate_and_validate_answer", "direct_answer"]
        
        if "generate_answer" in event_name.lower() or \
           "generate_and_validate" in event_name.lower() or \
           event_name in target_nodes:
            return True
        
        if isinstance(event_parent, dict):
            parent_node_name = event_parent.get("name", "")
            if parent_node_name and (
                "generate_answer" in parent_node_name.lower() or 
                "generate_and_validate" in parent_node_name.lower() or
                parent_node_name in target_nodes
            ):
                return True
        
        if last_node_name in target_nodes:
            if "Chat" in event_name or "LLM" in event_name or "Model" in event_name:
                return True
        
        return False
    
    async def _extract_related_questions_from_state(
        self,
        state_values: Dict[str, Any],
        initial_state: Dict[str, Any],
        message: str,
        full_answer: str,
        session_id: str
    ) -> list:
        """Stateì—ì„œ related_questions ì¶”ì¶œ ë° ìƒì„±"""
        related_questions = state_values.get("metadata", {}).get("related_questions", [])
        
        if related_questions:
            return related_questions
        
        sources = state_values.get("sources", [])
        sources_detail = state_values.get("sources_detail", [])
        
        if not sources and not sources_detail:
            retrieved_docs = state_values.get("retrieved_docs", [])
            if retrieved_docs and hasattr(self, 'sources_extractor') and self.sources_extractor:
                try:
                    sources_data = self.sources_extractor._extract_sources(state_values)
                    sources_detail_data = self.sources_extractor._extract_sources_detail(state_values)
                    
                    if sources_detail_data:
                        sources_detail = sources_detail_data
                        state_values["sources_detail"] = sources_detail_data
                    if sources_data:
                        sources = sources_data
                        state_values["sources"] = sources_data
                    
                    related_questions_data = self.sources_extractor._extract_related_questions(state_values)
                    if related_questions_data:
                        related_questions = related_questions_data
                except Exception as e:
                    logger.warning(f"[stream_final_answer] Failed to extract sources from retrieved_docs: {e}", exc_info=True)
        
        if not related_questions and hasattr(self, 'sources_extractor') and self.sources_extractor:
            try:
                if sources_detail and "sources_detail" not in state_values:
                    state_values["sources_detail"] = sources_detail
                if sources and "sources" not in state_values:
                    state_values["sources"] = sources
                
                related_questions_data = self.sources_extractor._extract_related_questions(state_values)
                if related_questions_data:
                    related_questions = related_questions_data
            except Exception as e:
                logger.warning(f"[stream_final_answer] Failed to extract related_questions from state: {e}", exc_info=True)
        
        if not related_questions and self.workflow_service and hasattr(self.workflow_service, 'conversation_flow_tracker') and self.workflow_service.conversation_flow_tracker:
            try:
                from lawfirm_langgraph.core.services.conversation_manager import ConversationContext, ConversationTurn
                
                query = state_values.get("query", "") or initial_state.get("query", "") or message
                answer = state_values.get("answer", "") or full_answer or ""
                query_type = state_values.get("metadata", {}).get("query_type", "general_question")
                
                if query and answer and len(answer.strip()) >= 10:
                    turn = ConversationTurn(
                        user_query=query,
                        bot_response=answer,
                        timestamp=datetime.now(),
                        question_type=query_type
                    )
                    
                    context = ConversationContext(
                        session_id=session_id or "default",
                        turns=[turn],
                        entities={},
                        topic_stack=[],
                        created_at=datetime.now(),
                        last_updated=datetime.now()
                    )
                    
                    suggested_questions = self.workflow_service.conversation_flow_tracker.suggest_follow_up_questions(context)
                    
                    if suggested_questions and len(suggested_questions) > 0:
                        related_questions = [str(q).strip() for q in suggested_questions if q and str(q).strip()]
            except Exception as e:
                logger.warning(f"[stream_final_answer] Failed to generate related_questions using ConversationFlowTracker: {e}", exc_info=True)
        
        return related_questions
    
    def is_available(self) -> bool:
        """ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return self.workflow_service is not None


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì§€ì—° ì´ˆê¸°í™”)
chat_service: Optional[ChatService] = None

def get_chat_service() -> ChatService:
    """ChatService ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global chat_service
    if chat_service is None:
        try:
            logger.info("Initializing ChatService...")
            chat_service = ChatService()
            if chat_service.is_available():
                logger.info("âœ… ChatService initialized successfully with workflow service")
            else:
                logger.warning("âš ï¸  ChatService initialized but workflow service is not available")
                logger.warning("   Check API server logs for initialization errors")
        except Exception as e:
            logger.error(f"Failed to initialize ChatService: {e}", exc_info=True)
            import traceback
            tb = traceback.format_exc()
            logger.error(f"Traceback:\n{tb}")
            # ì‹¤íŒ¨í•´ë„ ChatService ì¸ìŠ¤í„´ìŠ¤ëŠ” ìƒì„± (workflow_serviceê°€ Noneì¼ ìˆ˜ ìˆìŒ)
            chat_service = ChatService()
    return chat_service

# ëª¨ë“ˆ import ì‹œì ì—ëŠ” ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ (ì§€ì—° ì´ˆê¸°í™”)
# ì²« ìš”ì²­ ì‹œ get_chat_service()ë¥¼ í†µí•´ ì´ˆê¸°í™”
# ì´ë ‡ê²Œ í•˜ë©´ api/main.pyì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ë¡œë“œí•œ í›„ ì´ˆê¸°í™” ê°€ëŠ¥
logger.info("ChatService module loaded. Will initialize on first request via get_chat_service().")

