"""
ì±„íŒ… ì„œë¹„ìŠ¤ (lawfirm_langgraph ë˜í¼)
"""
import sys
import json
import logging
import asyncio
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, AsyncGenerator

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# lawfirm_langgraph ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€ (core ëª¨ë“ˆ importë¥¼ ìœ„í•´)
lawfirm_langgraph_path = project_root / "lawfirm_langgraph"
if lawfirm_langgraph_path.exists():
    sys.path.insert(0, str(lawfirm_langgraph_path))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œë” ì‚¬ìš©)
try:
    from utils.env_loader import ensure_env_loaded
    ensure_env_loaded(project_root)
except ImportError as e:
    logging.warning(f"âš ï¸  Failed to load environment variables: {e}")
    logging.warning("   Make sure utils/env_loader.py exists in the project root")
except Exception as e:
    logging.warning(f"âš ï¸  Failed to load environment variables: {e}")

try:
    from lawfirm_langgraph.core.workflow.workflow_service import LangGraphWorkflowService
    from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
    LANGGRAPH_AVAILABLE = True
except ImportError as e:
    LANGGRAPH_AVAILABLE = False
    logging.warning(f"LangGraph not available: {e}")

logger = logging.getLogger(__name__)

# ë¡œê±° ë ˆë²¨ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (ë£¨íŠ¸ ë¡œê±° ë ˆë²¨ê³¼ ë™ê¸°í™”)
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œê·¸ ë ˆë²¨ ì½ê¸°
import os  # noqa: E402
log_level_str = os.getenv("LOG_LEVEL", "info").upper()
log_level_map = {
    "CRITICAL": logging.CRITICAL,
    "ERROR": logging.ERROR,
    "WARNING": logging.WARNING,
    "INFO": logging.INFO,
    "DEBUG": logging.DEBUG,
}
log_level = log_level_map.get(log_level_str, logging.INFO)
logger.setLevel(log_level)
logger.disabled = False  # ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™”
logger.propagate = True  # ë£¨íŠ¸ ë¡œê±°ë¡œ ì „íŒŒ

# ë¡œê¹…ì´ ë¹„í™œì„±í™”ë˜ì§€ ì•Šë„ë¡ ë³´í˜¸
logging.disable(logging.NOTSET)  # ëª¨ë“  ë¡œê¹… í™œì„±í™”

# ë£¨íŠ¸ ë¡œê±°ì— í•¸ë“¤ëŸ¬ê°€ ì—†ìœ¼ë©´ ì¶”ê°€ (ëª¨ë“ˆ import ì‹œì ì— ë¡œê¹…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ)
root_logger = logging.getLogger()
if not root_logger.handlers:
    handler = logging.StreamHandler()
    handler.setLevel(log_level)
    handler.setFormatter(
        logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    )
    root_logger.addHandler(handler)
    root_logger.setLevel(log_level)
    root_logger.disabled = False

# í…ŒìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥ (ëª¨ë“ˆ import ì‹œì )
logger.info("âœ… ChatService logger initialized and enabled")
logger.debug("âœ… ChatService logger debug level enabled")


class ChatService:
    """ì±„íŒ… ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("ğŸš€ ChatService.__init__() called - Initializing ChatService...")
        self.workflow_service: Optional[LangGraphWorkflowService] = None
        self._initialize_workflow()
        
        # ìŠ¤íŠ¸ë¦¬ë° ì„¤ì • ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        from api.services.stream_config import StreamConfig
        from api.services.stream_event_processor import StreamEventProcessor
        from api.services.sources_extractor import SourcesExtractor
        from api.services.session_service import session_service
        
        self.stream_config = StreamConfig.from_env()
        self.event_processor = StreamEventProcessor(config=self.stream_config)
        self.sources_extractor = SourcesExtractor(
            workflow_service=self.workflow_service,
            session_service=session_service
        )
        
        logger.info("âœ… ChatService.__init__() completed")
    
    def _initialize_workflow(self):
        """ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”"""
        if not LANGGRAPH_AVAILABLE:
            logger.warning("LangGraph is not available. Service will continue without LangGraph features.")
            return
        
        try:
            import os
            # í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (ë¯¼ê° ì •ë³´ëŠ” ë¡œê·¸ì— ë…¸ì¶œí•˜ì§€ ì•ŠìŒ)
            google_api_key = os.getenv("GOOGLE_API_KEY", "")
            if not google_api_key:
                logger.warning("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                logger.warning("LangGraphëŠ” Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                logger.info("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            logger.info("Loading LangGraphConfig from environment...")
            
            config = LangGraphConfig.from_env()
            logger.info(f"LangGraph Config loaded: langgraph_enabled={config.langgraph_enabled}, llm_provider={config.llm_provider}")
            
            if not config.langgraph_enabled:
                logger.warning("LangGraph is disabled in configuration")
                return
            
            logger.info("Initializing LangGraphWorkflowService...")
            
            self.workflow_service = LangGraphWorkflowService(config)
            logger.info("âœ… ChatService initialized successfully with LangGraph workflow")
        except ImportError as e:
            logger.error(f"Import error during workflow initialization: {e}", exc_info=True)
            self.workflow_service = None
        except Exception as e:
            logger.error(f"Failed to initialize workflow service: {e}", exc_info=True)
            import traceback
            tb = traceback.format_exc()
            logger.error(f"Traceback:\n{tb}")
            self.workflow_service = None
    
    async def process_message(
        self,
        message: str,
        session_id: Optional[str] = None,
        enable_checkpoint: bool = True
    ) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ ì²˜ë¦¬
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            enable_checkpoint: ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        if not self.workflow_service:
            import os
            error_details = []
            
            # ì›ì¸ ë¶„ì„
            if not LANGGRAPH_AVAILABLE:
                error_details.append("LangGraph ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                google_api_key = os.getenv("GOOGLE_API_KEY", "")
                if not google_api_key:
                    error_details.append("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                else:
                    error_details.append("ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
            error_msg = f"Workflow service unavailable. Details: {', '.join(error_details)}"
            logger.error(error_msg)
            
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\nì›ì¸:\n" + "\n".join(f"- {detail}" for detail in error_details) + "\n\nAPI ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "confidence": 0.0,
                "legal_references": [],
                "processing_steps": ["ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨"],
                "session_id": session_id or "error",
                "processing_time": 0.0,
                "query_type": "error",
                "metadata": {"error_details": error_details},
                "errors": error_details
            }
        
        try:
            result = await self.workflow_service.process_query(
                query=message,
                session_id=session_id,
                enable_checkpoint=enable_checkpoint
            )
            return result
        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)
            import os
            debug_mode = os.getenv("DEBUG", "false").lower() == "true"
            error_detail = str(e) if debug_mode else "ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0,
                "legal_references": [],
                "processing_steps": [f"ì˜¤ë¥˜: {error_detail}"],
                "session_id": session_id or "error",
                "processing_time": 0.0,
                "query_type": "error",
                "metadata": {"error": error_detail} if debug_mode else {"error": True},
                "errors": [error_detail]
            }
    
    async def stream_message(
        self,
        message: str,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì²˜ë¦¬ (Server-Sent Events)
        ì‹¤ì œ LLM í† í° ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•©ë‹ˆë‹¤.
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            
        Yields:
            ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬ (í† í° ë‹¨ìœ„)
        """
        # ì´ë²¤íŠ¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.event_processor.reset()
        
        has_yielded = False  # ìµœì†Œí•œ í•˜ë‚˜ì˜ yieldê°€ ìˆì—ˆëŠ”ì§€ ì¶”ì 
        
        if not self.workflow_service:
            error_event = {
                "type": "final",
                "content": "[ì˜¤ë¥˜] ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "metadata": {"error": True},
                "timestamp": datetime.now().isoformat()
            }
            yield json.dumps(error_event, ensure_ascii=False) + "\n"
            has_yielded = True
            return
        
        try:
            import uuid
            
            # ì„¸ì…˜ ID ìƒì„±
            if not session_id:
                session_id = str(uuid.uuid4())
            
            # ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            from lawfirm_langgraph.core.workflow.state.state_definitions import create_initial_legal_state
            
            # ë¡œê¹…: message ê°’ í™•ì¸
            logger.info(f"stream_message: ë°›ì€ message='{message[:100] if message else 'EMPTY'}...', length={len(message) if message else 0}")
            
            # messageë¥¼ queryë¡œ ì‚¬ìš© (create_initial_legal_stateì˜ ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ëŠ” query)
            initial_state = create_initial_legal_state(message, session_id)
            
            # ìƒíƒœ ê²€ì¦ ë° ë³´ê°• (í†µí•©ëœ ê²€ì¦ ë¡œì§)
            if "input" not in initial_state:
                initial_state["input"] = {}
            if not initial_state["input"].get("query"):
                initial_state["input"]["query"] = message
            if not initial_state["input"].get("session_id"):
                initial_state["input"]["session_id"] = session_id
            
            if not initial_state.get("query"):
                initial_state["query"] = message
            if not initial_state.get("session_id"):
                initial_state["session_id"] = session_id
            
            # ì´ˆê¸° state ê²€ì¦ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
            initial_query = initial_state.get("input", {}).get("query") or initial_state.get("query")
            if not initial_query or not str(initial_query).strip():
                logger.error(f"Initial state query is empty! Input message was: '{message[:50]}...'")
                error_event = {
                    "type": "final",
                    "content": "[ì˜¤ë¥˜] ì§ˆë¬¸ì´ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "metadata": {"error": True},
                    "timestamp": datetime.now().isoformat()
                }
                yield json.dumps(error_event, ensure_ascii=False) + "\n"
                return
            
            config = {"configurable": {"thread_id": session_id}}
            
            # ë””ë²„ê·¸ ëª¨ë“œ í™•ì¸
            DEBUG_STREAM = self.stream_config.debug_stream
            
            # astream_events()ë¥¼ ì‚¬ìš©í•˜ì—¬ LLM í† í° ìŠ¤íŠ¸ë¦¬ë° ê°ì§€
            # 
            # ìŠ¤íŠ¸ë¦¬ë° íë¦„:
            # 1. LangGraphì˜ astream_events()ê°€ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°
            # 2. LLM í˜¸ì¶œ ì‹œ on_llm_stream ë˜ëŠ” on_chat_model_stream ì´ë²¤íŠ¸ ë°œìƒ
            # 3. LangChainì˜ ChatGoogleGenerativeAI/OllamaëŠ” invoke() í˜¸ì¶œ ì‹œì—ë„
            #    ë‚´ë¶€ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°ì„ ì‚¬ìš©í•˜ë¯€ë¡œ astream_events()ê°€ ì´ë¥¼ ìº¡ì²˜ ê°€ëŠ¥
            # 4. ë‹µë³€ ìƒì„± ë…¸ë“œ(generate_answer_enhanced)ì—ì„œ ë°œìƒí•œ ì´ë²¤íŠ¸ë§Œ í•„í„°ë§
            # 5. ê° í† í°ì„ JSONL í˜•ì‹ìœ¼ë¡œ yieldí•˜ì—¬ HTTP ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ë‹¬
            try:
                # ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
                # LangGraph ë²„ì „ë³„ í˜¸í™˜ì„±: version íŒŒë¼ë¯¸í„°ê°€ ì—†ì„ ìˆ˜ë„ ìˆìŒ
                # wrapper í•¨ìˆ˜ë¡œ ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
                async def get_stream_events():
                    """ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ë˜í¼
                    
                    LangGraphì˜ astream_events()ëŠ” ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼
                    ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤. LLM í˜¸ì¶œ ì‹œ on_llm_stream ë˜ëŠ” 
                    on_chat_model_stream ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ë©°, LangChainì˜ LLMì€ 
                    invoke() í˜¸ì¶œ ì‹œì—ë„ ë‚´ë¶€ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°ì„ ì‚¬ìš©í•˜ë¯€ë¡œ 
                    ì´ ì´ë²¤íŠ¸ë“¤ì„ í†µí•´ ì‹¤ì‹œê°„ í† í° ìŠ¤íŠ¸ë¦¬ë°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
                    """
                    try:
                        # version="v2" ì‹œë„ (LangGraph ìµœì‹  ë²„ì „)
                        # include_namesë¡œ generate_and_validate_answer ë…¸ë“œì˜ ì´ë²¤íŠ¸ë§Œ í•„í„°ë§ ì‹œë„
                        if DEBUG_STREAM:
                            logger.info("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: astream_events(version='v2') ì‚¬ìš©")
                        try:
                            # include_names íŒŒë¼ë¯¸í„°ë¡œ íŠ¹ì • ë…¸ë“œë§Œ í•„í„°ë§ ì‹œë„
                            try:
                                async for event in self.workflow_service.app.astream_events(
                                    initial_state, 
                                    config,
                                    version="v2",
                                    include_names=["generate_and_validate_answer", "generate_answer_enhanced"]
                                ):
                                    yield event
                            except (TypeError, AttributeError):
                                # include_namesê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° exclude_names ì‹œë„
                                try:
                                    async for event in self.workflow_service.app.astream_events(
                                        initial_state, 
                                        config,
                                        version="v2",
                                        exclude_names=["classify_query_and_complexity", "classification_parallel", 
                                                      "expand_keywords", "validate_answer_quality", "prepare_search_query",
                                                      "execute_searches_parallel", "process_search_results_combined",
                                                      "prepare_documents_and_terms", "prepare_final_response"]
                                    ):
                                        yield event
                                except (TypeError, AttributeError):
                                    # exclude_namesë„ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° ëª¨ë“  ì´ë²¤íŠ¸ ì²˜ë¦¬
                                    async for event in self.workflow_service.app.astream_events(
                                        initial_state, 
                                        config,
                                        version="v2"
                                    ):
                                        yield event
                        except (TypeError, AttributeError):
                            # include_namesê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° ëª¨ë“  ì´ë²¤íŠ¸ ì²˜ë¦¬
                            async for event in self.workflow_service.app.astream_events(
                                initial_state, 
                                config,
                                version="v2"
                            ):
                                yield event
                    except (TypeError, AttributeError) as ve:
                        # version íŒŒë¼ë¯¸í„°ê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° (êµ¬ë²„ì „)
                        logger.debug(f"astream_eventsì—ì„œ version íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›: {ve}, ê¸°ë³¸ ë²„ì „ ì‚¬ìš©")
                        if DEBUG_STREAM:
                            logger.info("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: astream_events() ì‚¬ìš© (ê¸°ë³¸ ë²„ì „)")
                        async for event in self.workflow_service.app.astream_events(
                            initial_state, 
                            config
                        ):
                            yield event
                
                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
                event_count = 0
                llm_stream_count = 0
                event_types_seen = set()  # ë³¸ ì´ë²¤íŠ¸ íƒ€ì… ì¶”ì  (ë””ë²„ê¹…ìš©, ì œí•œì  ì‚¬ìš©)
                node_names_seen = set()  # ë³¸ ë…¸ë“œ ì´ë¦„ ì¶”ì  (ë””ë²„ê¹…ìš©, ì œí•œì  ì‚¬ìš©)
                
                # ê´€ë ¨ ì´ë²¤íŠ¸ íƒ€ì… ì§‘í•© (ì„±ëŠ¥ ìµœì í™”)
                RELEVANT_EVENT_TYPES = self.stream_config.relevant_event_types
                
                # ë©”ëª¨ë¦¬ ìµœì í™”: ì´ë²¤íŠ¸ íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
                MAX_EVENT_HISTORY = self.stream_config.max_event_history
                
                async for event in get_stream_events():
                    event_count += 1
                    # ì´ë²¤íŠ¸ íƒ€ì… í™•ì¸
                    event_type = event.get("event", "")
                    event_name = event.get("name", "")
                    
                    # ê´€ë ¨ ì—†ëŠ” ì´ë²¤íŠ¸ëŠ” ì¦‰ì‹œ ê±´ë„ˆë›°ê¸° (ì„±ëŠ¥ ìµœì í™” - ì¡°ê¸° ì¢…ë£Œ)
                    if event_type not in RELEVANT_EVENT_TYPES:
                        continue
                    
                    # ë””ë²„ê¹… ëª¨ë“œì—ì„œë§Œ ì´ë²¤íŠ¸ ì¶”ì  (ë©”ëª¨ë¦¬ ìµœì í™”: ì œí•œì  ì¶”ì )
                    if DEBUG_STREAM and event_count <= MAX_EVENT_HISTORY:
                        event_types_seen.add(event_type)
                        if event_name:
                            node_names_seen.add(event_name)
                        if event_count <= 20:
                            logger.debug(f"ì²˜ë¦¬í•  ì´ë²¤íŠ¸ #{event_count}: type={event_type}, name={event_name}")
                    
                    # StreamEventProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë²¤íŠ¸ ì²˜ë¦¬
                    stream_event = self.event_processor.process_stream_event(event)
                    if stream_event:
                        yield json.dumps(stream_event, ensure_ascii=False) + "\n"
                        has_yielded = True
                        if stream_event.get("type") == "stream":
                            llm_stream_count += 1
                
                # event_processorì—ì„œ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                full_answer = self.event_processor.full_answer
                answer_found = self.event_processor.answer_found
                tokens_received = self.event_processor.tokens_received
                
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ìµœì¢… í™•ì¸ (DEBUG_STREAMì´ trueì¼ ë•Œë§Œ)
                if DEBUG_STREAM:
                    logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: ì´ {event_count}ê°œ ì´ë²¤íŠ¸, LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ {llm_stream_count}ê°œ, í† í° ìˆ˜ì‹  {tokens_received}ê°œ")
                
                # LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ê°€ ì—†ì„ ë•Œë§Œ ê²½ê³  (í”„ë¡œë•ì…˜ì—ì„œë„)
                if llm_stream_count == 0:
                    if DEBUG_STREAM:
                        logger.warning("âš ï¸ LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        logger.debug(f"ë°œìƒí•œ ëª¨ë“  ì´ë²¤íŠ¸ íƒ€ì…: {sorted(event_types_seen)}")
                        logger.debug(f"ë°œìƒí•œ ëª¨ë“  ë…¸ë“œ ì´ë¦„: {sorted(node_names_seen)}")
                
                if not answer_found:
                    # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
                    # process_messageë¥¼ í˜¸ì¶œí•˜ë©´ ì¤‘ë³µ ì‹¤í–‰ì´ë¯€ë¡œ, ìµœì¢… ê²°ê³¼ë§Œ ê°€ì ¸ì˜¤ëŠ” ë°©ë²• ì‚¬ìš©
                    if DEBUG_STREAM:
                        logger.warning(f"LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì´ë²¤íŠ¸ ìˆ˜: {event_count}, LLM ìŠ¤íŠ¸ë¦¬ë°: {llm_stream_count})")
                        logger.info("ìµœì¢… ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤...")
                    # ìµœì¢… ê²°ê³¼ë§Œ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)
                    try:
                        result = await self.process_message(message, session_id)
                        final_answer = result.get("answer", "")
                        if final_answer and len(final_answer) > len(full_answer):
                            # ëˆ„ë½ëœ ë¶€ë¶„ë§Œ ì „ì†¡
                            missing_part = final_answer[len(full_answer):]
                            if missing_part:
                                self.event_processor.full_answer = final_answer
                                # ìŠ¤íŠ¸ë¦¼ ì²­í¬ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ì „ì†¡
                                stream_event = {
                                    "type": "stream",
                                    "content": missing_part,
                                    "timestamp": datetime.now().isoformat()
                                }
                                yield json.dumps(stream_event, ensure_ascii=False) + "\n"
                                self.event_processor.answer_found = True
                                if DEBUG_STREAM:
                                    logger.info(f"ìµœì¢… ë‹µë³€ì—ì„œ ëˆ„ë½ëœ ë¶€ë¶„ ì „ì†¡: {len(missing_part)}ì")
                        elif final_answer:
                            # ì „ì²´ ë‹µë³€ ì „ì†¡ (full_answerê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°)
                            self.event_processor.full_answer = final_answer
                            stream_event = {
                                "type": "stream",
                                "content": final_answer,
                                "timestamp": datetime.now().isoformat()
                            }
                            yield json.dumps(stream_event, ensure_ascii=False) + "\n"
                            self.event_processor.answer_found = True
                            if DEBUG_STREAM:
                                logger.info(f"ì „ì²´ ë‹µë³€ ì „ì†¡: {len(final_answer)}ì")
                    except Exception as e:
                        if DEBUG_STREAM:
                            logger.error(f"ìµœì¢… ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}", exc_info=True)
                        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ìµœì†Œí•œ ì—ëŸ¬ ë©”ì‹œì§€ yield (ìŠ¤íŠ¸ë¦¼ì´ ë¹„ì–´ìˆì§€ ì•Šë„ë¡)
                        if not self.event_processor.answer_found:
                            error_event = {
                                "type": "final",
                                "content": f"[ì˜¤ë¥˜] ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}",
                                "metadata": {"error": True},
                                "timestamp": datetime.now().isoformat()
                            }
                            yield json.dumps(error_event, ensure_ascii=False) + "\n"
                            self.event_processor.answer_found = True
            
            except Exception as stream_error:
                # astream_events ì‹¤íŒ¨ ì‹œ astreamìœ¼ë¡œ í´ë°±
                if DEBUG_STREAM:
                    logger.warning(f"astream_events ì‹¤íŒ¨, astreamìœ¼ë¡œ í´ë°±: {stream_error}")
                # stream_mode="updates" ì‚¬ìš© ì‹œ ë³€ê²½ëœ í•„ë“œë§Œ í¬í•¨ë˜ë¯€ë¡œ ì§ì ‘ í™•ì¸ ê°€ëŠ¥
                async for event in self.workflow_service.app.astream(initial_state, config, stream_mode="updates"):
                    for node_name, node_state in event.items():
                        if isinstance(node_state, dict):
                            answer = None
                            # answer ê·¸ë£¹ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
                            if "answer" in node_state:
                                answer = node_state.get("answer", "")
                            # common ê·¸ë£¹ì—ì„œ answer í™•ì¸ (ë³€ê²½ëœ ê²½ìš°ì—ë§Œ í¬í•¨)
                            elif "common" in node_state and isinstance(node_state["common"], dict):
                                common = node_state["common"]
                                if "answer" in common:
                                    answer = common.get("answer", "")
                            
                            if answer and isinstance(answer, str):
                                current_full_answer = self.event_processor.full_answer
                                if len(answer) > len(current_full_answer):
                                    new_part = answer[len(current_full_answer):]
                                    if new_part:
                                        self.event_processor.full_answer = answer
                                        # ìŠ¤íŠ¸ë¦¼ ì²­í¬ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ì „ì†¡
                                        stream_event = {
                                            "type": "stream",
                                            "content": new_part,
                                            "timestamp": datetime.now().isoformat()
                                        }
                                        yield json.dumps(stream_event, ensure_ascii=False) + "\n"
                                        self.event_processor.answer_found = True
            
            # ì™„ë£Œ ë©”íƒ€ë°ì´í„° (ë‹µë³€ì´ ì—†ì–´ë„ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡)
            # sourcesëŠ” ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì§ì „ì— SourcesExtractorë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì ¸ì˜¤ê¸°
            final_sources = []
            final_legal_references = []
            final_sources_detail = []
            final_related_questions = []
            
            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì§ì „ì— sources ê°€ì ¸ì˜¤ê¸° (ë¹ ë¥´ê²Œ ì²˜ë¦¬)
            if session_id:
                try:
                    try:
                        sources_data = await asyncio.wait_for(
                            self.sources_extractor.extract_from_state(session_id),
                            timeout=2.0  # 2ì´ˆ íƒ€ì„ì•„ì›ƒ
                        )
                        final_sources = sources_data.get("sources", [])
                        final_legal_references = sources_data.get("legal_references", [])
                        final_sources_detail = sources_data.get("sources_detail", [])
                        final_related_questions = sources_data.get("related_questions", [])
                    except asyncio.TimeoutError:
                        logger.warning(f"Timeout getting sources from LangGraph state for session {session_id}")
                    except Exception as e:
                        logger.warning(f"Failed to get sources from LangGraph state: {e}")
                except Exception as e:
                    logger.warning(f"Error getting sources from LangGraph state: {e}")
            
            # event_processorì—ì„œ full_answer ê°€ì ¸ì˜¤ê¸°
            full_answer = self.event_processor.full_answer
            answer_found = self.event_processor.answer_found
            tokens_received = self.event_processor.tokens_received
            
            if full_answer:
                has_yielded = True
                
                # í† í° ì œí•œ í™•ì¸
                MAX_OUTPUT_TOKENS = self.stream_config.max_output_tokens
                should_split = tokens_received >= MAX_OUTPUT_TOKENS
                
                import uuid
                
                # ë©”ì‹œì§€ ID ìƒì„± (chat.pyì—ì„œ ì €ì¥ ì‹œ ì‚¬ìš©)
                message_id = str(uuid.uuid4())
                
                # final ì´ë²¤íŠ¸ ìƒì„± ì§ì „ì— sourcesë¥¼ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸
                # LangGraph stateì—ì„œ sourcesë¥¼ ê°€ì ¸ì˜¤ëŠ” ì‹œì ì´ ë„ˆë¬´ ë¹ ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ
                # final ì´ë²¤íŠ¸ ìƒì„± ì§ì „ì— ë‹¤ì‹œ í™•ì¸ (ì¬ì‹œë„ ìµœì†Œí™”í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ ë¸”ë¡œí‚¹ ë°©ì§€)
                if not final_sources and not final_legal_references and not final_sources_detail:
                    if session_id and self.workflow_service and self.workflow_service.app:
                        try:
                            config = {"configurable": {"thread_id": session_id}}
                            
                            # ì¦‰ì‹œ ì‹œë„ (íƒ€ì„ì•„ì›ƒ 2ì´ˆ, ì¬ì‹œë„ ì—†ìŒ)
                            try:
                                final_state = await asyncio.wait_for(
                                    self.workflow_service.app.aget_state(config),
                                    timeout=2.0
                                )
                                
                                if final_state and final_state.values:
                                    state_values = final_state.values
                                    
                                    # sources ì¶”ì¶œ
                                    sources_data = self.sources_extractor._extract_sources(state_values)
                                    legal_references_data = self.sources_extractor._extract_legal_references(state_values)
                                    sources_detail_data = self.sources_extractor._extract_sources_detail(state_values)
                                    related_questions_data = self.sources_extractor._extract_related_questions(state_values)
                                    
                                    if sources_data or legal_references_data or sources_detail_data or related_questions_data:
                                        if sources_data:
                                            final_sources = sources_data
                                        if legal_references_data:
                                            final_legal_references = legal_references_data
                                        if sources_detail_data:
                                            final_sources_detail = sources_detail_data
                                        if related_questions_data:
                                            final_related_questions = related_questions_data
                                        
                                        logger.info(f"Re-extracted sources before final event: {len(final_sources)} sources, {len(final_legal_references)} legal_references, {len(final_sources_detail)} sources_detail, {len(final_related_questions)} related_questions")
                            except asyncio.TimeoutError:
                                logger.warning(f"Timeout re-getting sources before final event")
                            except Exception as e:
                                logger.warning(f"Failed to re-get sources before final event: {e}")
                        except Exception as e:
                            logger.warning(f"Error re-getting sources before final event: {e}")
                
                if should_split:
                    # í† í° ì œí•œì„ ì´ˆê³¼í–ˆì„ ë•Œë§Œ ë‹µë³€ ë¶„í•  ì²˜ë¦¬
                    from api.services.answer_splitter import AnswerSplitter
                    
                    splitter = AnswerSplitter(chunk_size=self.stream_config.chunk_size)
                    chunks = splitter.split_answer(full_answer)
                    
                    # ë©”ì‹œì§€ ì €ì¥ì€ chat.pyì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
                    # message_idëŠ” ìƒì„±ë§Œ í•˜ê³  final_eventì— í¬í•¨
                    
                    # ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì¦‰ì‹œ ì „ì†¡
                    if chunks:
                        first_chunk = chunks[0]
                        
                        # ìµœì¢… ì™„ë£Œ ì´ë²¤íŠ¸ì— ì²« ë²ˆì§¸ ì²­í¬ë§Œ í¬í•¨
                        final_event = {
                            "type": "final",
                            "content": first_chunk.content,
                            "metadata": {
                                "tokens_received": tokens_received,
                                "length": len(full_answer),
                                "answer_found": answer_found,
                                "sources": final_sources,
                                "legal_references": final_legal_references,
                                "sources_detail": final_sources_detail,
                                "related_questions": final_related_questions,
                                "needs_continuation": True,
                                "message_id": message_id
                            },
                            "timestamp": datetime.now().isoformat()
                        }
                        yield json.dumps(final_event, ensure_ascii=False) + "\n"
                    else:
                        # ì²­í¬ê°€ ì—†ëŠ” ê²½ìš° (ì§§ì€ ë‹µë³€) ê¸°ì¡´ ë°©ì‹ ìœ ì§€
                        final_event = {
                            "type": "final",
                            "content": full_answer,
                            "metadata": {
                                "tokens_received": tokens_received,
                                "length": len(full_answer),
                                "answer_found": answer_found,
                                "sources": final_sources,
                                "legal_references": final_legal_references,
                                "sources_detail": final_sources_detail,
                                "related_questions": final_related_questions,
                                "message_id": message_id
                            },
                            "timestamp": datetime.now().isoformat()
                        }
                        yield json.dumps(final_event, ensure_ascii=False) + "\n"
                else:
                    # ë©”ì‹œì§€ ì €ì¥ì€ chat.pyì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
                    # message_idëŠ” ìƒì„±ë§Œ í•˜ê³  final_eventì— í¬í•¨
                    
                    # ì „ì²´ ë‹µë³€ì„ í•œ ë²ˆì— ì „ì†¡ (needs_continuation ì—†ìŒ)
                    final_event = {
                        "type": "final",
                        "content": full_answer,
                        "metadata": {
                            "tokens_received": tokens_received,
                            "length": len(full_answer),
                            "answer_found": answer_found,
                            "sources": final_sources,
                            "legal_references": final_legal_references,
                            "sources_detail": final_sources_detail,
                            "related_questions": final_related_questions,
                            "message_id": message_id
                        },
                        "timestamp": datetime.now().isoformat()
                    }
                    yield json.dumps(final_event, ensure_ascii=False) + "\n"
            else:
                if DEBUG_STREAM:
                    logger.warning("ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                # ë‹µë³€ì´ ì—†ì–´ë„ ìµœì†Œí•œ ë¹ˆ ì‘ë‹µì„ yieldí•˜ì—¬ ìŠ¤íŠ¸ë¦¼ì´ ë¹„ì–´ìˆì§€ ì•Šë„ë¡ ë³´ì¥
                if not answer_found:
                    error_event = {
                        "type": "final",
                        "content": "[ì˜¤ë¥˜] ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                        "metadata": {
                            "error": True,
                            "tokens_received": tokens_received
                        },
                        "timestamp": datetime.now().isoformat()
                    }
                    yield json.dumps(error_event, ensure_ascii=False) + "\n"
                    has_yielded = True
            
        except Exception as e:
            logger.error(f"Error in stream_message: {e}", exc_info=True)
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ì „ì†¡
            try:
                error_event = {
                    "type": "final",
                    "content": f"[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                    "metadata": {
                        "error": True,
                        "error_type": type(e).__name__
                    },
                    "timestamp": datetime.now().isoformat()
                }
                yield json.dumps(error_event, ensure_ascii=False) + "\n"
                has_yielded = True
            except Exception as yield_error:
                logger.error(f"Error yielding error message: {yield_error}")
                # yield ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ìµœì†Œí•œ ë¹ˆ ë¬¸ìì—´ì´ë¼ë„ yield ì‹œë„
                try:
                    fallback_event = {
                        "type": "final",
                        "content": "[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                        "metadata": {"error": True},
                        "timestamp": datetime.now().isoformat()
                    }
                    yield json.dumps(fallback_event, ensure_ascii=False) + "\n"
                    has_yielded = True
                except Exception:
                    # yield ìì²´ê°€ ì‹¤íŒ¨í•˜ë©´ ìŠ¤íŠ¸ë¦¼ì´ ëŠì–´ì§ˆ ìˆ˜ ìˆìŒ
                    # í•˜ì§€ë§Œ ì´ ê²½ìš°ëŠ” ë§¤ìš° ë“œë¬¼ê³ , FastAPIê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
                    pass
        # finally ë¸”ë¡ ì œê±°: finallyì—ì„œ yieldë¥¼ í•˜ë©´ ì œë„ˆë ˆì´í„°ê°€ ì œëŒ€ë¡œ ì¢…ë£Œë˜ì§€ ì•Šì•„
        # ERR_INCOMPLETE_CHUNKED_ENCODING ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
        # ìŠ¤íŠ¸ë¦¼ ì¢…ë£ŒëŠ” FastAPI StreamingResponseê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
    
    async def get_sources_from_session(
        self,
        session_id: str,
        message_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ì„¸ì…˜ì˜ ìµœì¢… stateì—ì„œ sources ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            session_id: ì„¸ì…˜ ID
            message_id: ë©”ì‹œì§€ ID (ì„ íƒì‚¬í•­, í•´ë‹¹ ë©”ì‹œì§€ì˜ metadataì—ì„œ sources ê°€ì ¸ì˜¤ê¸°)
        
        Returns:
            sources, legal_references, sources_detail ë”•ì…”ë„ˆë¦¬
        """
        # ë¨¼ì € ë©”ì‹œì§€ì˜ metadataì—ì„œ sourcesë¥¼ ê°€ì ¸ì˜¤ê¸° ì‹œë„
        result = await self.sources_extractor.extract_from_message_metadata(session_id, message_id)
        
        # ì—†ìœ¼ë©´ stateì—ì„œ ê°€ì ¸ì˜¤ê¸°
        if not any(result.values()):
            result = await self.sources_extractor.extract_from_state(session_id)
        
        return result
    
    async def stream_final_answer(
        self,
        message: str,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        LangGraphì˜ astream_events()ë¥¼ ì‚¬ìš©í•˜ì—¬ 
        generate_and_validate_answer ë…¸ë“œì˜ LLM ì‘ë‹µë§Œ ìŠ¤íŠ¸ë¦¼ í˜•íƒœë¡œ ì „ë‹¬
        
        ì˜ˆì œ ì½”ë“œ ì°¸ê³ :
        async for event in compiled_graph.astream_events({"topic": "AI"}):
            if event["event"] == "on_llm_stream" and event["name"] == "generate_response":
                yield f"data: {json.dumps({'token': data})}\n\n"
        """
        if not self.workflow_service:
            error_event = {
                "type": "error",
                "content": "[ì˜¤ë¥˜] ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "timestamp": datetime.now().isoformat()
            }
            yield f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"
            return
        
        try:
            import uuid
            
            # ì„¸ì…˜ ID ìƒì„±
            if not session_id:
                session_id = str(uuid.uuid4())
            
            # ì´ˆê¸° state ìƒì„±
            from lawfirm_langgraph.core.workflow.state.state_definitions import create_initial_legal_state
            initial_state = create_initial_legal_state(message, session_id)
            
            # ìƒíƒœ ê²€ì¦ ë° ë³´ê°•
            if "input" not in initial_state:
                initial_state["input"] = {}
            if not initial_state["input"].get("query"):
                initial_state["input"]["query"] = message
            if not initial_state["input"].get("session_id"):
                initial_state["input"]["session_id"] = session_id
            
            if not initial_state.get("query"):
                initial_state["query"] = message
            if not initial_state.get("session_id"):
                initial_state["session_id"] = session_id
            
            # ìƒíƒœ ê²€ì¦
            initial_query = initial_state.get("input", {}).get("query") or initial_state.get("query")
            if not initial_query or not str(initial_query).strip():
                error_event = {
                    "type": "error",
                    "content": "[ì˜¤ë¥˜] ì§ˆë¬¸ì´ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    "timestamp": datetime.now().isoformat()
                }
                yield f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"
                return
            
            config = {"configurable": {"thread_id": session_id}}
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_event = {
                "type": "progress",
                "content": "ë‹µë³€ ìƒì„± ì¤‘...",
                "timestamp": datetime.now().isoformat()
            }
            yield f"data: {json.dumps(progress_event, ensure_ascii=False)}\n\n"
            
            # LangGraphì˜ astream_events() ì‚¬ìš©
            # generate_and_validate_answer ë…¸ë“œì˜ LLM ìŠ¤íŠ¸ë¦¼ë§Œ í•„í„°ë§
            # StreamEventProcessorì˜ ë¡œì§ì„ ì°¸ê³ í•˜ì—¬ êµ¬í˜„
            answer_generation_started = False
            last_node_name = None
            event_count = 0
            stream_event_count = 0
            on_llm_stream_count = 0
            on_chat_model_stream_count = 0
            on_chain_stream_count = 0
            
            async for event in self.workflow_service.app.astream_events(
                initial_state,
                config,
                version="v2"
            ):
                event_count += 1
                event_type = event.get("event", "")
                event_name = event.get("name", "")
                event_parent = event.get("parent", {})
                
                # on_llm_stream ì´ë²¤íŠ¸ ë°œìƒ ì¶”ì 
                if event_type == "on_llm_stream":
                    on_llm_stream_count += 1
                    if on_llm_stream_count <= 10:  # ì²˜ìŒ 10ê°œë§Œ ìƒì„¸ ë¡œê¹…
                        logger.debug(
                            f"[stream_final_answer] on_llm_stream ì´ë²¤íŠ¸ #{on_llm_stream_count}: "
                            f"name={event_name}, "
                            f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}"
                        )
                elif event_type == "on_chat_model_stream":
                    on_chat_model_stream_count += 1
                    if on_chat_model_stream_count <= 10:
                        logger.debug(
                            f"[stream_final_answer] on_chat_model_stream ì´ë²¤íŠ¸ #{on_chat_model_stream_count}: "
                            f"name={event_name}, "
                            f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}"
                        )
                elif event_type == "on_chain_stream":
                    on_chain_stream_count += 1
                    if on_chain_stream_count <= 5:
                        logger.debug(
                            f"[stream_final_answer] on_chain_stream ì´ë²¤íŠ¸ #{on_chain_stream_count}: "
                            f"name={event_name}, "
                            f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}"
                        )
                
                # on_chain_start: ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹œì‘ ê°ì§€
                if event_type == "on_chain_start":
                    node_name = event_name
                    if node_name in ["generate_and_validate_answer", "generate_answer_enhanced"]:
                        answer_generation_started = True
                        last_node_name = node_name
                        logger.debug(f"[stream_final_answer] ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹œì‘: {node_name}")
                
                # on_chain_end: ë‹µë³€ ìƒì„± ë…¸ë“œ ì™„ë£Œ
                elif event_type == "on_chain_end":
                    node_name = event_name
                    if node_name == "generate_and_validate_answer":
                        answer_generation_started = False
                        logger.debug(f"[stream_final_answer] ë‹µë³€ ìƒì„± ë…¸ë“œ ì™„ë£Œ: {node_name}")
                
                # LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ (on_llm_stream, on_chat_model_streamë§Œ ì²˜ë¦¬)
                # on_chain_streamì€ ì œì™¸ (ì „ì²´ ë‹µë³€ì„ í•œ ë²ˆì— ì „ë‹¬í•˜ë¯€ë¡œ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ë¶ˆê°€)
                elif event_type in ["on_llm_stream", "on_chat_model_stream"]:
                    # on_llm_stream ì´ë²¤íŠ¸ ë°œìƒ ë¡œê¹…
                    logger.debug(
                        f"[stream_final_answer] on_llm_stream ì´ë²¤íŠ¸ ë°œìƒ: "
                        f"type={event_type}, name={event_name}, "
                        f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}, "
                        f"last_node={last_node_name}, started={answer_generation_started}"
                    )
                    
                    # ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                    if not answer_generation_started:
                        logger.debug(f"[stream_final_answer] ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì‹œì‘ë˜ì§€ ì•Šì•„ ê±´ë„ˆëœ€: {event_name}")
                        continue
                    
                    # ë‹µë³€ ìƒì„± ë…¸ë“œì¸ì§€ í™•ì¸ (StreamEventProcessorì˜ is_answer_node ë¡œì§ ì°¸ê³ )
                    is_target_node = False
                    
                    # ë°©ë²• 1: ì´ë²¤íŠ¸ ì´ë¦„ìœ¼ë¡œ ì§ì ‘ íŒë‹¨
                    if "generate_answer" in event_name.lower() or \
                       "generate_and_validate" in event_name.lower() or \
                       event_name in ["generate_answer_enhanced", "generate_and_validate_answer", "direct_answer"]:
                        is_target_node = True
                        logger.debug(f"[stream_final_answer] ë°©ë²• 1: ì´ë²¤íŠ¸ ì´ë¦„ìœ¼ë¡œ íƒ€ê²Ÿ ë…¸ë“œ í™•ì¸: {event_name}")
                    
                    # ë°©ë²• 2: parent í•„ë“œì—ì„œ ë…¸ë“œ ì´ë¦„ í™•ì¸
                    if not is_target_node:
                        parent_node_name = None
                        if isinstance(event_parent, dict):
                            parent_node_name = event_parent.get("name", "")
                        
                        if parent_node_name and (
                            "generate_answer" in parent_node_name.lower() or 
                            "generate_and_validate" in parent_node_name.lower() or
                            parent_node_name in ["generate_answer_enhanced", "generate_and_validate_answer"]
                        ):
                            is_target_node = True
                            logger.debug(f"[stream_final_answer] ë°©ë²• 2: parent í•„ë“œë¡œ íƒ€ê²Ÿ ë…¸ë“œ í™•ì¸: {parent_node_name}")
                    
                    # ë°©ë²• 3: last_node_nameìœ¼ë¡œ í™•ì¸ (LLM ëª¨ë¸ ì´ë¦„ì¸ ê²½ìš°)
                    if not is_target_node and last_node_name in ["generate_and_validate_answer", "generate_answer_enhanced"]:
                        if "Chat" in event_name or "LLM" in event_name or "Model" in event_name:
                            is_target_node = True
                            logger.debug(f"[stream_final_answer] ë°©ë²• 3: last_node_nameìœ¼ë¡œ íƒ€ê²Ÿ ë…¸ë“œ í™•ì¸: {last_node_name}, event_name={event_name}")
                    
                    if is_target_node:
                        logger.debug(f"[stream_final_answer] íƒ€ê²Ÿ ë…¸ë“œ í™•ì¸ë¨: {event_name}, í† í° ì¶”ì¶œ ì‹œì‘")
                        event_data = event.get("data", {})
                        
                        # on_llm_stream, on_chat_model_stream: StreamEventProcessorì˜ extract_chunk_from_llm_stream ë¡œì§ ì‚¬ìš©
                        chunk_obj = event_data.get("chunk")
                        token = None
                        
                        if chunk_obj:
                            if hasattr(chunk_obj, "content"):
                                content = chunk_obj.content
                                if isinstance(content, str):
                                    token = content
                                elif isinstance(content, list) and len(content) > 0:
                                    token = content[0] if isinstance(content[0], str) else str(content[0])
                                else:
                                    token = str(content) if content else None
                            elif isinstance(chunk_obj, str):
                                token = chunk_obj
                            elif isinstance(chunk_obj, dict):
                                token = chunk_obj.get("content") or chunk_obj.get("text")
                            elif hasattr(chunk_obj, "text"):
                                token = chunk_obj.text
                            elif hasattr(chunk_obj, "__class__") and "AIMessageChunk" in str(type(chunk_obj)):
                                try:
                                    content = getattr(chunk_obj, "content", None)
                                    if isinstance(content, str):
                                        token = content
                                    elif isinstance(content, list) and len(content) > 0:
                                        token = content[0] if isinstance(content[0], str) else str(content[0])
                                    elif content is not None:
                                        token = str(content)
                                except Exception:
                                    token = None
                            else:
                                token = str(chunk_obj) if chunk_obj else None
                        
                        # delta í˜•ì‹ ì²˜ë¦¬ (LangGraph v2)
                        if not token and "delta" in event_data:
                            delta = event_data["delta"]
                            if isinstance(delta, dict):
                                token = delta.get("content") or delta.get("text")
                            elif isinstance(delta, str):
                                token = delta
                        
                        # í† í°ì´ ìˆìœ¼ë©´ SSE í˜•ì‹ìœ¼ë¡œ ì „ë‹¬
                        if token and isinstance(token, str) and len(token) > 0:
                            stream_event_count += 1
                            logger.debug(
                                f"[stream_final_answer] í† í° ì „ì†¡: "
                                f"token_length={len(token)}, "
                                f"token_preview={token[:50]}..., "
                                f"stream_event_count={stream_event_count}"
                            )
                            stream_event = {
                                "type": "stream",
                                "content": token,
                                "timestamp": datetime.now().isoformat()
                            }
                            yield f"data: {json.dumps(stream_event, ensure_ascii=False)}\n\n"
                        else:
                            logger.debug(
                                f"[stream_final_answer] í† í° ì¶”ì¶œ ì‹¤íŒ¨: "
                                f"token={token}, "
                                f"chunk_obj={chunk_obj}, "
                                f"event_data_keys={list(event_data.keys()) if isinstance(event_data, dict) else []}"
                            )
                    else:
                        # ë””ë²„ê¹…: í•„í„°ë§ë˜ì§€ ì•Šì€ ì´ë²¤íŠ¸ ë¡œê¹…
                        logger.debug(
                            f"[stream_final_answer] íƒ€ê²Ÿ ë…¸ë“œê°€ ì•„ë‹˜ (í•„í„°ë§ë¨): "
                            f"type={event_type}, name={event_name}, "
                            f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}, "
                            f"last_node={last_node_name}, started={answer_generation_started}"
                        )
                
                # generate_and_validate_answer ë…¸ë“œ ì™„ë£Œ ì‹œì 
                elif event_type == "on_chain_end" and event_name == "generate_and_validate_answer":
                    logger.info(
                        f"[stream_final_answer] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: "
                        f"ì´ {event_count}ê°œ ì´ë²¤íŠ¸, "
                        f"ìŠ¤íŠ¸ë¦¼ ì´ë²¤íŠ¸ {stream_event_count}ê°œ, "
                        f"on_llm_stream={on_llm_stream_count}ê°œ, "
                        f"on_chat_model_stream={on_chat_model_stream_count}ê°œ, "
                        f"on_chain_stream={on_chain_stream_count}ê°œ"
                    )
                    
                    # ìµœì¢… ì™„ë£Œ ì´ë²¤íŠ¸ (metadata í¬í•¨)
                    try:
                        # State ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ ìµœì†Œí™”í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ ë¸”ë¡œí‚¹ ë°©ì§€)
                        final_state = None
                        state_values = None
                        
                        try:
                            # ì¦‰ì‹œ ì‹œë„ (íƒ€ì„ì•„ì›ƒ 2ì´ˆ)
                            final_state = await asyncio.wait_for(
                                self.workflow_service.app.aget_state(config),
                                timeout=2.0
                            )
                            if final_state and final_state.values:
                                state_values = final_state.values
                        except asyncio.TimeoutError:
                            logger.warning(f"[stream_final_answer] Timeout getting state, using empty metadata")
                        except Exception as e:
                            logger.warning(f"[stream_final_answer] Error getting state: {e}")
                        
                        if final_state and state_values:
                            # metadata ì¶”ì¶œ
                            sources = state_values.get("sources", [])
                            legal_references = state_values.get("legal_references", [])
                            sources_detail = state_values.get("sources_detail", [])
                            related_questions = state_values.get("metadata", {}).get("related_questions", [])
                            
                            # sourcesê°€ ì—¬ì „íˆ ì—†ìœ¼ë©´ retrieved_docsì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
                            if not sources and not sources_detail:
                                retrieved_docs = state_values.get("retrieved_docs", [])
                                if retrieved_docs:
                                    logger.info(f"[stream_final_answer] Sources not in state, extracting from {len(retrieved_docs)} retrieved_docs")
                                    # SourcesExtractorë¥¼ ì‚¬ìš©í•˜ì—¬ sources ì¶”ì¶œ
                                    if hasattr(self, 'sources_extractor') and self.sources_extractor:
                                        try:
                                            sources_data = self.sources_extractor._extract_sources(state_values)
                                            legal_references_data = self.sources_extractor._extract_legal_references(state_values)
                                            sources_detail_data = self.sources_extractor._extract_sources_detail(state_values)
                                            related_questions_data = self.sources_extractor._extract_related_questions(state_values)
                                            
                                            if sources_data:
                                                sources = sources_data
                                            if legal_references_data:
                                                legal_references = legal_references_data
                                            if sources_detail_data:
                                                sources_detail = sources_detail_data
                                            if related_questions_data:
                                                related_questions = related_questions_data
                                            
                                            logger.info(f"[stream_final_answer] Extracted {len(sources)} sources from retrieved_docs")
                                        except Exception as e:
                                            logger.warning(f"[stream_final_answer] Failed to extract sources from retrieved_docs: {e}")
                            
                            # related_questionsê°€ ì—†ìœ¼ë©´ sources_extractorë¡œ ì¶”ì¶œ ì‹œë„
                            if not related_questions:
                                if hasattr(self, 'sources_extractor') and self.sources_extractor:
                                    try:
                                        related_questions_data = self.sources_extractor._extract_related_questions(state_values)
                                        if related_questions_data:
                                            related_questions = related_questions_data
                                            logger.info(f"[stream_final_answer] Extracted {len(related_questions)} related_questions from state")
                                    except Exception as e:
                                        logger.warning(f"[stream_final_answer] Failed to extract related_questions from state: {e}")
                            
                            # LLM ê²€ì¦ ê²°ê³¼ ì¶”ì¶œ
                            llm_validation_result = state_values.get("metadata", {}).get("llm_validation_result", {})
                            
                            final_metadata = {
                                "sources": sources,
                                "legal_references": legal_references,
                                "sources_detail": sources_detail,
                                "related_questions": related_questions,
                                "llm_validation": llm_validation_result if llm_validation_result else None
                            }
                        else:
                            final_metadata = {}
                    except Exception as e:
                        logger.error(f"Error getting final state: {e}", exc_info=True)
                        final_metadata = {}
                    
                    # í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê²€ì¦ ì´ë²¤íŠ¸ ì „ì†¡
                    if final_metadata.get("llm_validation"):
                        validation_result = final_metadata["llm_validation"]
                        validation_event = {
                            "type": "validation",
                            "content": "ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ",
                            "metadata": {
                                "quality_score": validation_result.get("quality_score", 0.0),
                                "is_valid": validation_result.get("is_valid", False),
                                "needs_regeneration": validation_result.get("needs_regeneration", False),
                                "regeneration_reason": validation_result.get("regeneration_reason"),
                                "issues": validation_result.get("issues", []),
                                "strengths": validation_result.get("strengths", [])
                            },
                            "timestamp": datetime.now().isoformat()
                        }
                        yield f"data: {json.dumps(validation_event, ensure_ascii=False)}\n\n"
                    
                    # ìµœì¢… ë‹µë³€ ì´ë²¤íŠ¸
                    final_event = {
                        "type": "final",
                        "content": "",  # ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì´ë¯¸ ì „ì†¡ë¨
                        "metadata": final_metadata or {},
                        "timestamp": datetime.now().isoformat()
                    }
                    yield f"data: {json.dumps(final_event, ensure_ascii=False)}\n\n"
                    
                    # ì™„ë£Œ ì´ë²¤íŠ¸
                    done_event = {
                        "type": "done",
                        "content": "[DONE]",
                        "timestamp": datetime.now().isoformat()
                    }
                    yield f"data: {json.dumps(done_event, ensure_ascii=False)}\n\n"
                    break  # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ
                
            except GeneratorExit:
                # í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš° ì •ìƒ ì¢…ë£Œ
                logger.debug("[stream_final_answer] Client disconnected, closing stream")
                return
            except Exception as e:
                logger.error(f"Stream error: {e}", exc_info=True)
                try:
                    error_event = {
                        "type": "error",
                        "content": f"[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                        "timestamp": datetime.now().isoformat()
                    }
                    yield f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"
                except GeneratorExit:
                    # yield ì¤‘ í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš°
                    logger.debug("[stream_final_answer] Client disconnected during error handling")
                    return
                except Exception as yield_error:
                    # yield ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš° (ìŠ¤íŠ¸ë¦¼ì´ ì´ë¯¸ ë‹«í˜)
                    logger.error(f"Failed to yield error event: {yield_error}")
                    return
    
    def is_available(self) -> bool:
        """ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return self.workflow_service is not None


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì§€ì—° ì´ˆê¸°í™”)
chat_service: Optional[ChatService] = None

def get_chat_service() -> ChatService:
    """ChatService ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global chat_service
    if chat_service is None:
        try:
            logger.info("Initializing ChatService...")
            chat_service = ChatService()
            if chat_service.is_available():
                logger.info("âœ… ChatService initialized successfully with workflow service")
            else:
                logger.warning("âš ï¸  ChatService initialized but workflow service is not available")
                logger.warning("   Check API server logs for initialization errors")
        except Exception as e:
            logger.error(f"Failed to initialize ChatService: {e}", exc_info=True)
            import traceback
            tb = traceback.format_exc()
            logger.error(f"Traceback:\n{tb}")
            # ì‹¤íŒ¨í•´ë„ ChatService ì¸ìŠ¤í„´ìŠ¤ëŠ” ìƒì„± (workflow_serviceê°€ Noneì¼ ìˆ˜ ìˆìŒ)
            chat_service = ChatService()
    return chat_service

# ëª¨ë“ˆ import ì‹œì ì—ëŠ” ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ (ì§€ì—° ì´ˆê¸°í™”)
# ì²« ìš”ì²­ ì‹œ get_chat_service()ë¥¼ í†µí•´ ì´ˆê¸°í™”
# ì´ë ‡ê²Œ í•˜ë©´ api/main.pyì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ë¡œë“œí•œ í›„ ì´ˆê¸°í™” ê°€ëŠ¥
logger.info("ChatService module loaded. Will initialize on first request via get_chat_service().")

