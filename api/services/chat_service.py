"""
ì±„íŒ… ì„œë¹„ìŠ¤ (lawfirm_langgraph ë˜í¼)
"""
import sys
import logging
from pathlib import Path
from typing import Dict, Any, Optional, AsyncGenerator

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# lawfirm_langgraph ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€ (core ëª¨ë“ˆ importë¥¼ ìœ„í•´)
lawfirm_langgraph_path = project_root / "lawfirm_langgraph"
if lawfirm_langgraph_path.exists():
    sys.path.insert(0, str(lawfirm_langgraph_path))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ChatService ì´ˆê¸°í™” ì „ì— ë°˜ë“œì‹œ ë¡œë“œ)
try:
    from dotenv import load_dotenv
    
    # 1. lawfirm_langgraph/.env ë¡œë“œ (LangGraphConfigê°€ ì‚¬ìš©)
    langgraph_env = lawfirm_langgraph_path / ".env"
    if langgraph_env.exists():
        load_dotenv(dotenv_path=str(langgraph_env), override=False)
        logging.info(f"âœ… [ChatService] Loaded environment from: {langgraph_env}")
    else:
        logging.warning(f"âš ï¸  [ChatService] Environment file not found: {langgraph_env}")
    
    # 2. í”„ë¡œì íŠ¸ ë£¨íŠ¸ .env ë¡œë“œ (ê³µí†µ ì„¤ì •)
    root_env = project_root / ".env"
    if root_env.exists():
        load_dotenv(dotenv_path=str(root_env), override=False)
        logging.info(f"âœ… [ChatService] Loaded environment from: {root_env}")
    
    # 3. api/.env ë¡œë“œ (API ì„œë²„ ì „ìš© ì„¤ì •, ìµœìš°ì„ )
    api_env = Path(__file__).parent.parent / ".env"
    if api_env.exists():
        load_dotenv(dotenv_path=str(api_env), override=True)
        logging.info(f"âœ… [ChatService] Loaded environment from: {api_env}")
        
except ImportError:
    logging.warning("âš ï¸  python-dotenv not installed. Environment variables from .env files will not be loaded.")
except Exception as e:
    logging.warning(f"âš ï¸  Failed to load environment variables: {e}")

try:
    from lawfirm_langgraph.langgraph_core.workflow.workflow_service import LangGraphWorkflowService
    from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
    LANGGRAPH_AVAILABLE = True
except ImportError as e:
    LANGGRAPH_AVAILABLE = False
    logging.warning(f"LangGraph not available: {e}")

logger = logging.getLogger(__name__)

# ë¡œê±° ë ˆë²¨ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (ë£¨íŠ¸ ë¡œê±° ë ˆë²¨ê³¼ ë™ê¸°í™”)
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œê·¸ ë ˆë²¨ ì½ê¸°
import os
log_level_str = os.getenv("LOG_LEVEL", "info").upper()
log_level_map = {
    "CRITICAL": logging.CRITICAL,
    "ERROR": logging.ERROR,
    "WARNING": logging.WARNING,
    "INFO": logging.INFO,
    "DEBUG": logging.DEBUG,
}
log_level = log_level_map.get(log_level_str, logging.INFO)
logger.setLevel(log_level)
logger.disabled = False  # ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™”
logger.propagate = True  # ë£¨íŠ¸ ë¡œê±°ë¡œ ì „íŒŒ

# ë¡œê¹…ì´ ë¹„í™œì„±í™”ë˜ì§€ ì•Šë„ë¡ ë³´í˜¸
logging.disable(logging.NOTSET)  # ëª¨ë“  ë¡œê¹… í™œì„±í™”

# ë£¨íŠ¸ ë¡œê±°ì— í•¸ë“¤ëŸ¬ê°€ ì—†ìœ¼ë©´ ì¶”ê°€ (ëª¨ë“ˆ import ì‹œì ì— ë¡œê¹…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ)
root_logger = logging.getLogger()
if not root_logger.handlers:
    handler = logging.StreamHandler()
    handler.setLevel(log_level)
    handler.setFormatter(
        logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    )
    root_logger.addHandler(handler)
    root_logger.setLevel(log_level)
    root_logger.disabled = False

# í…ŒìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥ (ëª¨ë“ˆ import ì‹œì )
logger.info("âœ… ChatService logger initialized and enabled")
logger.debug("âœ… ChatService logger debug level enabled")


class ChatService:
    """ì±„íŒ… ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("ğŸš€ ChatService.__init__() called - Initializing ChatService...")
        self.workflow_service: Optional[LangGraphWorkflowService] = None
        self._initialize_workflow()
        logger.info("âœ… ChatService.__init__() completed")
    
    def _initialize_workflow(self):
        """ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”"""
        if not LANGGRAPH_AVAILABLE:
            logger.warning("LangGraph is not available. Service will continue without LangGraph features.")
            return
        
        try:
            import os
            # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
            google_api_key = os.getenv("GOOGLE_API_KEY", "")
            if not google_api_key:
                logger.warning("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                logger.warning("LangGraphëŠ” Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            logger.info("Loading LangGraphConfig from environment...")
            
            config = LangGraphConfig.from_env()
            logger.info(f"LangGraph Config loaded: langgraph_enabled={config.langgraph_enabled}, llm_provider={config.llm_provider}")
            
            if not config.langgraph_enabled:
                logger.warning("LangGraph is disabled in configuration")
                return
            
            logger.info("Initializing LangGraphWorkflowService...")
            
            self.workflow_service = LangGraphWorkflowService(config)
            logger.info("âœ… ChatService initialized successfully with LangGraph workflow")
        except ImportError as e:
            logger.error(f"Import error during workflow initialization: {e}", exc_info=True)
            self.workflow_service = None
        except Exception as e:
            logger.error(f"Failed to initialize workflow service: {e}", exc_info=True)
            import traceback
            tb = traceback.format_exc()
            logger.error(f"Traceback:\n{tb}")
            self.workflow_service = None
    
    async def process_message(
        self,
        message: str,
        session_id: Optional[str] = None,
        enable_checkpoint: bool = True
    ) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ ì²˜ë¦¬
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            enable_checkpoint: ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        if not self.workflow_service:
            import os
            error_details = []
            
            # ì›ì¸ ë¶„ì„
            if not LANGGRAPH_AVAILABLE:
                error_details.append("LangGraph ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                google_api_key = os.getenv("GOOGLE_API_KEY", "")
                if not google_api_key:
                    error_details.append("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                else:
                    error_details.append("ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
            error_msg = f"Workflow service unavailable. Details: {', '.join(error_details)}"
            logger.error(error_msg)
            
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\nì›ì¸:\n" + "\n".join(f"- {detail}" for detail in error_details) + "\n\nAPI ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "confidence": 0.0,
                "legal_references": [],
                "processing_steps": ["ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨"],
                "session_id": session_id or "error",
                "processing_time": 0.0,
                "query_type": "error",
                "metadata": {"error_details": error_details},
                "errors": error_details
            }
        
        try:
            result = await self.workflow_service.process_query(
                query=message,
                session_id=session_id,
                enable_checkpoint=enable_checkpoint
            )
            return result
        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0,
                "legal_references": [],
                "processing_steps": [f"ì˜¤ë¥˜: {str(e)}"],
                "session_id": session_id or "error",
                "processing_time": 0.0,
                "query_type": "error",
                "metadata": {"error": str(e)},
                "errors": [str(e)]
            }
    
    async def stream_message(
        self,
        message: str,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì²˜ë¦¬ (Server-Sent Events)
        ì‹¤ì œ LLM í† í° ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•©ë‹ˆë‹¤.
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            
        Yields:
            ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬ (í† í° ë‹¨ìœ„)
        """
        if not self.workflow_service:
            yield "[ì˜¤ë¥˜] ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            return
        
        try:
            import uuid
            import asyncio
            
            # ì„¸ì…˜ ID ìƒì„±
            if not session_id:
                session_id = str(uuid.uuid4())
            
            # ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            from lawfirm_langgraph.langgraph_core.state.state_definitions import create_initial_legal_state
            
            # ë¡œê¹…: message ê°’ í™•ì¸
            logger.info(f"stream_message: ë°›ì€ message='{message[:100] if message else 'EMPTY'}...', length={len(message) if message else 0}")
            
            # messageë¥¼ queryë¡œ ì‚¬ìš© (create_initial_legal_stateì˜ ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ëŠ” query)
            initial_state = create_initial_legal_state(message, session_id)
            
            # ì¤‘ìš”: initial_stateì— queryê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ë„ë¡ ê°•ì œ
            # LangGraphì— ì „ë‹¬í•˜ê¸° ì „ì— input ê·¸ë£¹ì— queryê°€ ìˆì–´ì•¼ í•¨
            if "input" not in initial_state:
                initial_state["input"] = {}
            if not initial_state["input"].get("query"):
                initial_state["input"]["query"] = message
                logger.debug(f"stream_message: input.queryì— message ì„¤ì •: '{message[:50]}...'")
            if not initial_state["input"].get("session_id"):
                initial_state["input"]["session_id"] = session_id
            
            # ìµœìƒìœ„ ë ˆë²¨ì—ë„ query í¬í•¨ (ì´ì¤‘ ë³´ì¥)
            if not initial_state.get("query"):
                initial_state["query"] = message
                logger.debug(f"stream_message: ìµœìƒìœ„ queryì— message ì„¤ì •: '{message[:50]}...'")
            if not initial_state.get("session_id"):
                initial_state["session_id"] = session_id
            
            # ì´ˆê¸° state ê²€ì¦
            initial_query = initial_state.get("input", {}).get("query", "") if initial_state.get("input") else initial_state.get("query", "")
            logger.info(f"stream_message: initial_state query length={len(initial_query)}, query='{initial_query[:100] if initial_query else 'EMPTY'}...'")
            logger.debug(f"stream_message: initial_state input.query='{initial_state.get('input', {}).get('query', 'NOT_FOUND')[:50] if initial_state.get('input', {}).get('query') else 'NOT_FOUND'}...'")
            logger.debug(f"stream_message: initial_state ìµœìƒìœ„ query='{initial_state.get('query', 'NOT_FOUND')[:50] if initial_state.get('query') else 'NOT_FOUND'}...'")
            
            if not initial_query or not str(initial_query).strip():
                logger.error(f"Initial state query is empty! Input message was: '{message[:50]}...'")
                logger.debug(f"stream_message: ERROR - initial_state query is empty!")
                logger.debug(f"stream_message: initial_state keys: {list(initial_state.keys())}")
                yield "[ì˜¤ë¥˜] ì§ˆë¬¸ì´ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                return
            
            config = {"configurable": {"thread_id": session_id}}
            
            # ì‹¤ì œ í† í° ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë³€ìˆ˜
            full_answer = ""
            answer_found = False
            tokens_received = 0
            last_node_name = None
            executed_nodes = set()  # ì‹¤í–‰ëœ ë…¸ë“œ ì¶”ì 
            answer_generation_started = False  # ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹œì‘ í”Œë˜ê·¸
            
            # ë…¸ë“œ ì´ë¦„ì„ ì‚¬ìš©ì ì¹œí™”ì ì¸ ë©”ì‹œì§€ë¡œ ë§¤í•‘
            node_name_mapping = {
                "classify_query_and_complexity": "ì§ˆë¬¸ ë¶„ì„ ì¤‘...",
                "classification_parallel": "ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘...",
                "route_expert": "ì „ë¬¸ê°€ ë¼ìš°íŒ… ì¤‘...",
                "expand_keywords": "í‚¤ì›Œë“œ í™•ì¥ ì¤‘...",
                "prepare_search_query": "ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ ì¤‘...",
                "execute_searches_parallel": "ê´€ë ¨ ë²•ë¥  ê²€ìƒ‰ ì¤‘...",
                "process_search_results_combined": "ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ì¤‘...",
                "prepare_documents_and_terms": "ë¬¸ì„œ ì¤€ë¹„ ì¤‘...",
                "generate_answer_enhanced": "ë‹µë³€ ìƒì„± ì¤‘...",
                "generate_and_validate_answer": "ë‹µë³€ ìƒì„± ì¤‘...",
                "validate_answer_quality": "ë‹µë³€ ê²€ì¦ ì¤‘...",
                "prepare_final_response": "ìµœì¢… ë‹µë³€ ì¤€ë¹„ ì¤‘..."
            }
            
            # astream_events()ë¥¼ ì‚¬ìš©í•˜ì—¬ LLM í† í° ìŠ¤íŠ¸ë¦¬ë° ê°ì§€
            try:
                # ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
                # LangGraph ë²„ì „ë³„ í˜¸í™˜ì„±: version íŒŒë¼ë¯¸í„°ê°€ ì—†ì„ ìˆ˜ë„ ìˆìŒ
                # wrapper í•¨ìˆ˜ë¡œ ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
                async def get_stream_events():
                    """ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ë˜í¼"""
                    try:
                        # version="v2" ì‹œë„ (LangGraph ìµœì‹  ë²„ì „)
                        logger.info("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: astream_events(version='v2') ì‚¬ìš©")
                        async for event in self.workflow_service.app.astream_events(
                            initial_state, 
                            config,
                            version="v2"
                        ):
                            yield event
                    except (TypeError, AttributeError) as ve:
                        # version íŒŒë¼ë¯¸í„°ê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° (êµ¬ë²„ì „)
                        logger.debug(f"astream_eventsì—ì„œ version íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›: {ve}, ê¸°ë³¸ ë²„ì „ ì‚¬ìš©")
                        logger.info("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: astream_events() ì‚¬ìš© (ê¸°ë³¸ ë²„ì „)")
                        async for event in self.workflow_service.app.astream_events(
                            initial_state, 
                            config
                        ):
                            yield event
                
                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
                event_count = 0
                llm_stream_count = 0
                event_types_seen = set()  # ë³¸ ì´ë²¤íŠ¸ íƒ€ì… ì¶”ì 
                node_names_seen = set()  # ë³¸ ë…¸ë“œ ì´ë¦„ ì¶”ì 
                
                async for event in get_stream_events():
                    event_count += 1
                    # ì´ë²¤íŠ¸ íƒ€ì… í™•ì¸
                    event_type = event.get("event", "")
                    event_name = event.get("name", "")
                    
                    # ì´ë²¤íŠ¸ íƒ€ì…ê³¼ ë…¸ë“œ ì´ë¦„ ì¶”ì 
                    event_types_seen.add(event_type)
                    if event_name:
                        node_names_seen.add(event_name)
                    
                    # ë””ë²„ê¹…: ì´ë²¤íŠ¸ íƒ€ì… ë¡œê¹… (ì²˜ìŒ 20ê°œë§Œ)
                    if event_count <= 20:
                        logger.debug(f"ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ #{event_count}: type={event_type}, name={event_name}")
                    
                    # LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ê°ì§€ (ë‹µë³€ ìƒì„± ë…¸ë“œì—ì„œë§Œ)
                    # LangGraph/LangChain ìµœì‹  ë²„ì „ì—ì„œëŠ” on_chat_model_streamë„ ì§€ì›
                    elif event_type in ["on_llm_stream", "on_chat_model_stream"]:
                        # ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        if not answer_generation_started:
                            # ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¬´ì‹œ
                            llm_stream_count += 1
                            if llm_stream_count <= 5:
                                logger.debug(f"ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ: {event_name} (ë¬´ì‹œ)")
                            continue
                        
                        llm_stream_count += 1
                        logger.debug(f"{event_type} ì´ë²¤íŠ¸ ë°œê²¬: name={event_name}, ì „ì²´ ì´ë²¤íŠ¸ í‚¤: {list(event.keys())}")
                        
                        # ì´ë²¤íŠ¸ì˜ ìƒìœ„ ë…¸ë“œ ì •ë³´ í™•ì¸
                        event_tags = event.get("tags", [])
                        event_metadata = event.get("metadata", {})
                        event_parent = event.get("parent", {})
                        
                        # ìƒìœ„ ë…¸ë“œ ì´ë¦„ í™•ì¸
                        parent_node_name = None
                        if isinstance(event_parent, dict):
                            parent_node_name = event_parent.get("name", "")
                        elif isinstance(event_tags, list):
                            # tagsì—ì„œ ë…¸ë“œ ì´ë¦„ ì°¾ê¸°
                            for tag in event_tags:
                                if isinstance(tag, str) and ("generate_answer" in tag.lower() or "generate_and_validate" in tag.lower()):
                                    parent_node_name = tag
                                    break
                        
                        # ë‹µë³€ ìƒì„± ë…¸ë“œ ë‚´ë¶€ì˜ LLM í˜¸ì¶œì¸ì§€ í™•ì¸
                        is_answer_node = False
                        
                        # ë°©ë²• 1: ì´ë²¤íŠ¸ ì´ë¦„ìœ¼ë¡œ ì§ì ‘ íŒë‹¨
                        if "generate_answer" in event_name.lower() or \
                           "generate_and_validate" in event_name.lower() or \
                           event_name in ["generate_answer_enhanced", "generate_and_validate_answer", "direct_answer"]:
                            is_answer_node = True
                        
                        # ë°©ë²• 2: ìƒìœ„ ë…¸ë“œê°€ ë‹µë³€ ìƒì„± ë…¸ë“œì¸ì§€ í™•ì¸
                        elif parent_node_name and (
                            "generate_answer" in parent_node_name.lower() or 
                            "generate_and_validate" in parent_node_name.lower() or
                            parent_node_name in ["generate_answer_enhanced", "generate_and_validate_answer"]
                        ):
                            is_answer_node = True
                        
                        # ë°©ë²• 3: ChatGoogleGenerativeAIì¸ ê²½ìš°, ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹¤í–‰ëœ ë…¸ë“œê°€ ë‹µë³€ ìƒì„± ë…¸ë“œì¸ì§€ í™•ì¸
                        elif event_name == "ChatGoogleGenerativeAI" and answer_generation_started:
                            # last_node_nameì´ ë‹µë³€ ìƒì„± ë…¸ë“œì¸ì§€ í™•ì¸
                            if last_node_name in ["generate_answer_enhanced", "generate_and_validate_answer"]:
                                is_answer_node = True
                            # ë˜ëŠ” executed_nodesì— ë‹µë³€ ìƒì„± ë…¸ë“œê°€ í¬í•¨ë˜ì–´ ìˆê³ , ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
                            elif "generate_answer_enhanced" in executed_nodes or "generate_and_validate_answer" in executed_nodes:
                                # ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ìŠ¤íŠ¸ë¦¬ë°
                                is_answer_node = True
                        
                        # ë””ë²„ê¹…: ëª¨ë“  ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ë¡œê¹… (ì²˜ìŒ 10ê°œë§Œ)
                        if llm_stream_count <= 10:
                            logger.debug(
                                f"{event_type} ì´ë²¤íŠ¸ #{llm_stream_count}: "
                                f"name={event_name}, parent={parent_node_name}, "
                                f"is_answer_node={is_answer_node}, "
                                f"answer_generation_started={answer_generation_started}, "
                                f"last_node={last_node_name}, "
                                f"tags={event_tags}, metadata={event_metadata}"
                            )
                        
                        if not is_answer_node:
                            # ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì•„ë‹ˆë©´ ë¬´ì‹œ
                            if llm_stream_count <= 5:
                                logger.debug(f"ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì•„ë‹˜: {event_name}, parent={parent_node_name} (ë¬´ì‹œ)")
                            continue
                        
                        logger.info(f"âœ… ë‹µë³€ ìƒì„± ë…¸ë“œì—ì„œ {event_type} ì´ë²¤íŠ¸ ê°ì§€: {event_name}, parent={parent_node_name}")
                        
                        if is_answer_node:
                            logger.debug(f"LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ê°ì§€: {event_name} (ì´ {llm_stream_count}ê°œ)")
                            # í† í° ì¶”ì¶œ (ë‹¤ì–‘í•œ ì´ë²¤íŠ¸ êµ¬ì¡° ì§€ì›)
                            chunk = None
                            event_data = event.get("data", {})
                            
                            try:
                                # ê²½ìš° 1: LangChain í‘œì¤€ í˜•ì‹ - data.chunk.content
                                if isinstance(event_data, dict):
                                    chunk_obj = event_data.get("chunk")
                                    if chunk_obj is not None:
                                        # AIMessageChunk ê°ì²´ ì²˜ë¦¬
                                        if hasattr(chunk_obj, "content"):
                                            content = chunk_obj.content
                                            # contentê°€ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                            if isinstance(content, str):
                                                chunk = content
                                            # contentê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (AIMessageChunkì˜ contentëŠ” ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ)
                                            elif isinstance(content, list) and len(content) > 0:
                                                # ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ ë¬¸ìì—´ì´ë©´ ì‚¬ìš©
                                                if isinstance(content[0], str):
                                                    chunk = content[0]
                                                else:
                                                    chunk = str(content[0])
                                            else:
                                                chunk = str(content)
                                        elif isinstance(chunk_obj, str):
                                            chunk = chunk_obj
                                        elif hasattr(chunk_obj, "text"):
                                            chunk = chunk_obj.text
                                        # AIMessageChunk ê°ì²´ì˜ ê²½ìš° ì§ì ‘ content ì ‘ê·¼ ì‹œë„
                                        elif hasattr(chunk_obj, "__class__") and "AIMessageChunk" in str(type(chunk_obj)):
                                            try:
                                                content = getattr(chunk_obj, "content", None)
                                                if isinstance(content, str):
                                                    chunk = content
                                                elif isinstance(content, list) and len(content) > 0:
                                                    if isinstance(content[0], str):
                                                        chunk = content[0]
                                                    else:
                                                        chunk = str(content[0])
                                                elif content is not None:
                                                    chunk = str(content)
                                            except Exception:
                                                pass
                                    
                                    # ê²½ìš° 2: ì§ì ‘ ë¬¸ìì—´ í˜•ì‹
                                    if not chunk:
                                        chunk = event_data.get("text") or event_data.get("content")
                                    
                                    # ê²½ìš° 3: delta í˜•ì‹ (LangGraph v2)
                                    if not chunk and "delta" in event_data:
                                        delta = event_data["delta"]
                                        if isinstance(delta, dict):
                                            chunk = delta.get("content") or delta.get("text")
                                        elif isinstance(delta, str):
                                            chunk = delta
                                
                                # ê²½ìš° 4: ì´ë²¤íŠ¸ ìµœìƒìœ„ ë ˆë²¨ì— ì§ì ‘ í¬í•¨
                                if not chunk:
                                    chunk = event.get("chunk") or event.get("text") or event.get("content")
                                
                                # í† í°ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ì „ì†¡
                                if chunk and isinstance(chunk, str):
                                    # JSON í˜•ì‹ ì¶œë ¥ ê°ì§€ ë° í•„í„°ë§ (ì¤‘ê°„ ë…¸ë“œì˜ JSON ì¶œë ¥ ì œê±°)
                                    chunk_stripped = chunk.strip()
                                    
                                    # JSON í˜•ì‹ ì‹œì‘ íŒ¨í„´ ê°ì§€
                                    is_json_output = False
                                    if chunk_stripped.startswith("{") or chunk_stripped.startswith("```json"):
                                        is_json_output = True
                                    elif chunk_stripped.startswith("```") and "json" in chunk_stripped[:20].lower():
                                        is_json_output = True
                                    
                                    # JSON í˜•ì‹ì´ë©´ ë¬´ì‹œ (ì¤‘ê°„ ë…¸ë“œì˜ JSON ì¶œë ¥)
                                    if is_json_output:
                                        logger.debug(f"JSON í˜•ì‹ ì¶œë ¥ ê°ì§€ ë° ë¬´ì‹œ: {chunk_stripped[:100]}...")
                                        continue
                                    
                                    # ê³µë°± í† í°ë„ í¬í•¨ (ì‹¤ì œ í† í° ìŠ¤íŠ¸ë¦¬ë°)
                                    # ë‹¨, ì™„ì „íˆ ë¹ˆ ë¬¸ìì—´ì€ ì œì™¸
                                    if len(chunk) > 0:
                                        full_answer += chunk
                                        tokens_received += 1
                                        answer_found = True
                                        yield chunk
                                        
                            except (AttributeError, TypeError, KeyError) as e:
                                # ì´ë²¤íŠ¸ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ê²½ìš° ë¡œê¹…ë§Œ í•˜ê³  ê³„ì† ì§„í–‰
                                logger.debug(f"í† í° ì¶”ì¶œ ì‹¤íŒ¨ (ì´ë²¤íŠ¸ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦„): {e}, event_keys={list(event.keys()) if isinstance(event, dict) else 'N/A'}")
                                # ë””ë²„ê¹…: ì´ë²¤íŠ¸ êµ¬ì¡° ìƒì„¸ ë¡œê¹… (ì²˜ìŒ 3ê°œë§Œ)
                                if llm_stream_count <= 3:
                                    logger.debug(f"ì´ë²¤íŠ¸ êµ¬ì¡° ìƒì„¸: event_data={event_data}, event_data type={type(event_data)}")
                                    if isinstance(event_data, dict):
                                        logger.debug(f"event_data keys: {list(event_data.keys())}")
                                continue
                    
                    # LLM ì™„ë£Œ ì´ë²¤íŠ¸ (on_llm_end ë˜ëŠ” on_chat_model_end)
                    elif event_type in ["on_llm_end", "on_chat_model_end"]:
                        # ìµœì¢… ë‹µë³€ í™•ì¸ (ëˆ„ë½ëœ ë¶€ë¶„ì´ ìˆëŠ”ì§€ ì²´í¬)
                        try:
                            event_data = event.get("data", {})
                            if isinstance(event_data, dict):
                                output = event_data.get("output")
                                if output is not None:
                                    final_answer = None
                                    
                                    # ë‹¤ì–‘í•œ ì¶œë ¥ í˜•ì‹ ì§€ì›
                                    if hasattr(output, "content"):
                                        final_answer = output.content
                                    elif isinstance(output, str):
                                        final_answer = output
                                    elif isinstance(output, dict):
                                        final_answer = output.get("content") or output.get("text") or str(output)
                                    else:
                                        final_answer = str(output)
                                    
                                    # ëˆ„ë½ëœ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì „ì†¡ (ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì¼ë¶€ í† í°ì´ ëˆ„ë½ëœ ê²½ìš°)
                                    if final_answer and isinstance(final_answer, str):
                                        if len(final_answer) > len(full_answer):
                                            missing_part = final_answer[len(full_answer):]
                                            if missing_part:
                                                full_answer = final_answer
                                                yield missing_part
                                                logger.debug(f"ëˆ„ë½ëœ ë¶€ë¶„ ì „ì†¡: {len(missing_part)}ì")
                        except (AttributeError, TypeError, KeyError) as e:
                            logger.debug(f"on_llm_end ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                            pass
                    
                    # ë…¸ë“œ ì‹¤í–‰ ì´ë²¤íŠ¸ (ì§„í–‰ ìƒí™© í‘œì‹œ)
                    elif event_type == "on_chain_start":
                        node_name = event.get("name", "")
                        
                        # ì£¼ìš” ë…¸ë“œì˜ ì§„í–‰ ìƒí™© í‘œì‹œ
                        if node_name in node_name_mapping:
                            if node_name not in executed_nodes:
                                progress_message = node_name_mapping.get(node_name, f"[{node_name} ì‹¤í–‰ ì¤‘...]")
                                # ì§„í–‰ ìƒí™© ë©”ì‹œì§€ëŠ” íŠ¹ë³„í•œ í˜•ì‹ìœ¼ë¡œ ì „ì†¡ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ êµ¬ë¶„ ê°€ëŠ¥í•˜ë„ë¡)
                                yield f"[ì§„í–‰ìƒí™©]{progress_message}\n"
                                executed_nodes.add(node_name)
                                logger.debug(f"ì§„í–‰ ìƒí™© ë©”ì‹œì§€ ì „ì†¡: {progress_message}")
                        
                        # ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹œì‘ ì‹œ í”Œë˜ê·¸ ì„¤ì •
                        if node_name in ["generate_answer_enhanced", "generate_and_validate_answer"]:
                            answer_generation_started = True
                            if not answer_found:
                                yield "[ì§„í–‰ìƒí™©]ë‹µë³€ ìƒì„± ì¤‘...\n"
                                last_node_name = node_name
                                logger.debug(f"ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹œì‘: {node_name}, answer_generation_started=True")
                    
                    # ë…¸ë“œ ì™„ë£Œ ì´ë²¤íŠ¸ (í¬ë§·íŒ…ëœ ë‹µë³€ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
                    elif event_type == "on_chain_end":
                        # í¬ë§·íŒ…ëœ ë‹µë³€ì€ ì›ë³¸ì´ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                        # ìŠ¤íŠ¸ë¦¬ë°ëœ ì›ì‹œ ë‹µë³€ë§Œ ì‚¬ìš©
                        node_name = event.get("name", "")
                        if node_name in ["generate_answer_enhanced", "generate_and_validate_answer"]:
                            # ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì™„ë£Œë˜ë©´ í”Œë˜ê·¸ í•´ì œ
                            answer_generation_started = False
                            logger.debug(f"ë‹µë³€ ìƒì„± ë…¸ë“œ ì™„ë£Œ: {node_name}, answer_generation_started=False")
                            
                            if not answer_found:
                                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ê°€ ì „í˜€ ë°œìƒí•˜ì§€ ì•Šì€ ê²½ìš°
                                logger.warning("ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í¬ë§·íŒ…ëœ ë‹µë³€ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                                yield "[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                                answer_found = True
                            else:
                                # ìŠ¤íŠ¸ë¦¬ë°ì´ ìˆì—ˆì„ ë•Œ: í¬ë§·íŒ…ëœ ë‹µë³€ì€ ì™„ì „íˆ ë¬´ì‹œ
                                logger.debug("ìŠ¤íŠ¸ë¦¬ë°ëœ ë‹µë³€ì´ ìˆìŠµë‹ˆë‹¤. í¬ë§·íŒ…ëœ ë‹µë³€ì€ ë¬´ì‹œë©ë‹ˆë‹¤.")
                
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ìµœì¢… í™•ì¸
                logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: ì´ {event_count}ê°œ ì´ë²¤íŠ¸, LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ {llm_stream_count}ê°œ, í† í° ìˆ˜ì‹  {tokens_received}ê°œ")
                logger.info(f"ë°œìƒí•œ ì´ë²¤íŠ¸ íƒ€ì…: {sorted(event_types_seen)}")
                logger.info(f"ë°œìƒí•œ ë…¸ë“œ ì´ë¦„ (ë‹µë³€ ìƒì„± ê´€ë ¨): {[n for n in sorted(node_names_seen) if 'answer' in n.lower() or 'generate' in n.lower()]}")
                
                # ë””ë²„ê¹…: ë°œìƒí•œ ëª¨ë“  ì´ë²¤íŠ¸ íƒ€ì…ê³¼ ë…¸ë“œ ì´ë¦„ ë¡œê¹…
                if llm_stream_count == 0:
                    logger.warning("âš ï¸ LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    logger.debug(f"ë°œìƒí•œ ëª¨ë“  ì´ë²¤íŠ¸ íƒ€ì…: {sorted(event_types_seen)}")
                    logger.debug(f"ë°œìƒí•œ ëª¨ë“  ë…¸ë“œ ì´ë¦„: {sorted(node_names_seen)}")
                    # ë‹µë³€ ìƒì„± ê´€ë ¨ ë…¸ë“œê°€ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    answer_nodes_executed = [n for n in sorted(node_names_seen) if 'answer' in n.lower() or 'generate' in n.lower()]
                    if answer_nodes_executed:
                        logger.info(f"ë‹µë³€ ìƒì„± ê´€ë ¨ ë…¸ë“œ ì‹¤í–‰ë¨: {answer_nodes_executed}")
                    else:
                        logger.warning("ë‹µë³€ ìƒì„± ê´€ë ¨ ë…¸ë“œê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                if not answer_found:
                    # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
                    # process_messageë¥¼ í˜¸ì¶œí•˜ë©´ ì¤‘ë³µ ì‹¤í–‰ì´ë¯€ë¡œ, ìµœì¢… ê²°ê³¼ë§Œ ê°€ì ¸ì˜¤ëŠ” ë°©ë²• ì‚¬ìš©
                    logger.warning(f"LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì´ë²¤íŠ¸ ìˆ˜: {event_count}, LLM ìŠ¤íŠ¸ë¦¬ë°: {llm_stream_count})")
                    logger.info("ìµœì¢… ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤...")
                    # ìµœì¢… ê²°ê³¼ë§Œ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)
                    try:
                        result = await self.process_message(message, session_id)
                        final_answer = result.get("answer", "")
                        if final_answer and len(final_answer) > len(full_answer):
                            # ëˆ„ë½ëœ ë¶€ë¶„ë§Œ ì „ì†¡
                            missing_part = final_answer[len(full_answer):]
                            if missing_part:
                                full_answer = final_answer
                                chunk_size = 10
                                for i in range(0, len(missing_part), chunk_size):
                                    chunk = missing_part[i:i + chunk_size]
                                    if chunk.strip():
                                        yield chunk
                                        await asyncio.sleep(0.03)
                                logger.info(f"ìµœì¢… ë‹µë³€ì—ì„œ ëˆ„ë½ëœ ë¶€ë¶„ ì „ì†¡: {len(missing_part)}ì")
                    except Exception as e:
                        logger.error(f"ìµœì¢… ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}", exc_info=True)
            
            except Exception as stream_error:
                # astream_events ì‹¤íŒ¨ ì‹œ astreamìœ¼ë¡œ í´ë°±
                logger.warning(f"astream_events ì‹¤íŒ¨, astreamìœ¼ë¡œ í´ë°±: {stream_error}")
                # stream_mode="updates" ì‚¬ìš© ì‹œ ë³€ê²½ëœ í•„ë“œë§Œ í¬í•¨ë˜ë¯€ë¡œ ì§ì ‘ í™•ì¸ ê°€ëŠ¥
                async for event in self.workflow_service.app.astream(initial_state, config, stream_mode="updates"):
                    for node_name, node_state in event.items():
                        if isinstance(node_state, dict):
                            answer = None
                            # answer ê·¸ë£¹ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
                            if "answer" in node_state:
                                answer = node_state.get("answer", "")
                            # common ê·¸ë£¹ì—ì„œ answer í™•ì¸ (ë³€ê²½ëœ ê²½ìš°ì—ë§Œ í¬í•¨)
                            elif "common" in node_state and isinstance(node_state["common"], dict):
                                common = node_state["common"]
                                if "answer" in common:
                                    answer = common.get("answer", "")
                            
                            if answer and isinstance(answer, str) and len(answer) > len(full_answer):
                                new_part = answer[len(full_answer):]
                                if new_part:
                                    full_answer = answer
                                    chunk_size = 10
                                    for i in range(0, len(new_part), chunk_size):
                                        chunk = new_part[i:i + chunk_size]
                                        if chunk.strip():
                                            yield chunk
                                            await asyncio.sleep(0.05)
                                    answer_found = True
            
            # ì™„ë£Œ ë©”íƒ€ë°ì´í„° (ë‹µë³€ì´ ì—†ì–´ë„ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡)
            if full_answer:
                logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {len(full_answer)}ì, {tokens_received}ê°œ í† í° ìˆ˜ì‹ ")
            else:
                logger.warning("ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            # ì™„ë£Œ ì‹ í˜¸ëŠ” chat.pyì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì „ì†¡í•˜ì§€ ì•ŠìŒ
            # (ì¤‘ë³µ ì „ì†¡ ë°©ì§€ ë° SSE í˜•ì‹ ì¼ê´€ì„± ìœ ì§€)
            
        except Exception as e:
            logger.error(f"Error in stream_message: {e}", exc_info=True)
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ë§Œ ì „ì†¡ (ì™„ë£Œ ì‹ í˜¸ëŠ” chat.pyì—ì„œ ì²˜ë¦¬)
            try:
                yield f"[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            except Exception as yield_error:
                logger.error(f"Error yielding error message: {yield_error}")
    
    def is_available(self) -> bool:
        """ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return self.workflow_service is not None


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì§€ì—° ì´ˆê¸°í™”)
chat_service: Optional[ChatService] = None

def get_chat_service() -> ChatService:
    """ChatService ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global chat_service
    if chat_service is None:
        try:
            logger.info("Initializing ChatService...")
            chat_service = ChatService()
            if chat_service.is_available():
                logger.info("âœ… ChatService initialized successfully with workflow service")
            else:
                logger.warning("âš ï¸  ChatService initialized but workflow service is not available")
                logger.warning("   Check API server logs for initialization errors")
        except Exception as e:
            logger.error(f"Failed to initialize ChatService: {e}", exc_info=True)
            import traceback
            tb = traceback.format_exc()
            logger.error(f"Traceback:\n{tb}")
            # ì‹¤íŒ¨í•´ë„ ChatService ì¸ìŠ¤í„´ìŠ¤ëŠ” ìƒì„± (workflow_serviceê°€ Noneì¼ ìˆ˜ ìˆìŒ)
            chat_service = ChatService()
    return chat_service

# ëª¨ë“ˆ import ì‹œì ì—ëŠ” ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ (ì§€ì—° ì´ˆê¸°í™”)
# ì²« ìš”ì²­ ì‹œ get_chat_service()ë¥¼ í†µí•´ ì´ˆê¸°í™”
# ì´ë ‡ê²Œ í•˜ë©´ api/main.pyì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¨¼ì € ë¡œë“œí•œ í›„ ì´ˆê¸°í™” ê°€ëŠ¥
logger.info("ChatService module loaded. Will initialize on first request via get_chat_service().")

