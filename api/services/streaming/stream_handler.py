"""
ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì „ìš© í´ë˜ìŠ¤
"""
import asyncio
import logging
import uuid
from datetime import datetime
from typing import Dict, Any, Optional, AsyncGenerator, List

from .constants import StreamingConstants
from .event_builder import StreamEventBuilder
from .token_extractor import TokenExtractor
from .node_filter import NodeFilter
from api.utils.sse_formatter import format_sse_event
from api.utils.source_type_mapper import get_default_sources_by_type
from api.utils.langgraph_config_helper import create_langgraph_config_with_callbacks

logger = logging.getLogger(__name__)

# ê¸°ë³¸ sources_by_type êµ¬ì¡° (ì‹¤ì œ PostgreSQL í…Œì´ë¸”ëª… ê¸°ë°˜)
DEFAULT_SOURCES_BY_TYPE = get_default_sources_by_type()


class StreamHandler:
    """ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        workflow_service,
        sources_extractor,
        extract_related_questions_fn=None
    ):
        self.workflow_service = workflow_service
        self.sources_extractor = sources_extractor
        self.extract_related_questions_fn = extract_related_questions_fn
        self.token_extractor = TokenExtractor()
        self.node_filter = NodeFilter()
        self.event_builder = StreamEventBuilder()
        # ë¡œê·¸ ì¹´ìš´í„° ì´ˆê¸°í™”
        self._skip_log_count = 0
        self._classification_skip_count = 0
    
    async def stream_final_answer(
        self,
        message: str,
        session_id: Optional[str] = None,
        validate_and_augment_state_fn=None
    ) -> AsyncGenerator[str, None]:
        """
        LangGraphì˜ astream_events()ë¥¼ ì‚¬ìš©í•˜ì—¬ 
        generate_and_validate_answer ë…¸ë“œì˜ LLM ì‘ë‹µë§Œ ìŠ¤íŠ¸ë¦¼ í˜•íƒœë¡œ ì „ë‹¬
        """
        if not self.workflow_service:
            error_event = self.event_builder.create_error_event(
                "[ì˜¤ë¥˜] ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            )
            yield format_sse_event(error_event)
            return
        
        try:
            if not session_id:
                session_id = str(uuid.uuid4())
            
            from lawfirm_langgraph.core.workflow.state.state_definitions import create_initial_legal_state
            initial_state = create_initial_legal_state(message, session_id)
            
            if validate_and_augment_state_fn:
                initial_query = validate_and_augment_state_fn(initial_state, message, session_id)
            else:
                initial_query = self._validate_and_augment_state(initial_state, message, session_id)
            
            if not initial_query:
                error_event = self.event_builder.create_error_event(
                    "[ì˜¤ë¥˜] ì§ˆë¬¸ì´ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                )
                yield format_sse_event(error_event)
                return
            
            callback_handler, config = await self._setup_callback_handler(session_id)
            
            # âš ï¸ ì£¼ì˜: stateì— ì½œë°±ì„ ì €ì¥í•˜ë©´ LangGraph ì²´í¬í¬ì¸íŠ¸ ì§ë ¬í™” ì‹œ ì˜¤ë¥˜ ë°œìƒ
            # configì˜ callbacksë§Œ ì‚¬ìš©í•˜ê³  stateì—ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
            # LangGraphëŠ” configì˜ callbacksë¥¼ ìë™ìœ¼ë¡œ ë…¸ë“œì— ì „ë‹¬í•¨
            if callback_handler:
                logger.debug(f"[stream_final_answer] âœ… ì½œë°±ì´ configì— ì„¤ì •ë¨: {len(config.get('callbacks', []))}ê°œ")
            
            progress_event = self.event_builder.create_progress_event("ë‹µë³€ ìƒì„± ì¤‘...")
            yield format_sse_event(progress_event)
            
            # ì´ˆê¸° State ë¡œê¹…
            logger.info(
                f"[stream_final_answer] ì´ˆê¸° State í™•ì¸: "
                f"query={initial_state.get('query', '')[:50] if initial_state.get('query') else 'N/A'}..., "
                f"session_id={initial_state.get('session_id', 'N/A')}, "
                f"state_keys={list(initial_state.keys())[:20]}"
            )
            
            # Config ë¡œê¹…
            logger.info(
                f"[stream_final_answer] Config í™•ì¸: "
                f"thread_id={config.get('configurable', {}).get('thread_id', 'N/A')}, "
                f"has_callbacks={bool(config.get('callbacks'))}, "
                f"config_keys={list(config.keys())}"
            )
            
            try:
                async for chunk in self._process_stream_events(
                    initial_state, config, callback_handler, message, session_id
                ):
                    yield chunk
            except Exception as process_error:
                logger.error(
                    f"[stream_final_answer] âš ï¸ _process_stream_events ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {process_error}",
                    exc_info=True
                )
                # ì—ëŸ¬ ì´ë²¤íŠ¸ ì „ì†¡
                try:
                    error_event = self.event_builder.create_error_event(
                        f"[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(process_error)}"
                    )
                    yield format_sse_event(error_event)
                except Exception:
                    pass
                
                # ğŸ”¥ ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€: ì˜ˆì™¸ ë°œìƒí•´ë„ done ì´ë²¤íŠ¸ ì „ì†¡
                try:
                    minimal_done = {"type": "done", "timestamp": datetime.now().isoformat(), "error": str(process_error)}
                    yield format_sse_event(minimal_done)
                    logger.debug("[stream_final_answer] Minimal done event sent after process error")
                except Exception:
                    pass
                raise
                
        except (GeneratorExit, asyncio.CancelledError) as cancel_error:
            # í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš° - ì •ìƒì ì¸ ì¢…ë£Œ
            logger.debug(f"[stream_final_answer] Stream cancelled or client disconnected: {cancel_error}")
            # GeneratorExitì™€ CancelledErrorëŠ” ìƒìœ„ë¡œ ì „íŒŒí•˜ì—¬ ì œë„ˆë ˆì´í„° ì¢…ë£Œ
            raise
        except Exception as e:
            logger.error(f"Stream error: {e}", exc_info=True)
            try:
                error_event = self.event_builder.create_error_event(
                    f"[ì˜¤ë¥˜] ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    type(e).__name__
                )
                yield format_sse_event(error_event)
                
                # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ done ì´ë²¤íŠ¸ ì „ì†¡í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¥¼ ëª…í™•íˆ í•¨
                done_event = self.event_builder.create_done_event("", {})
                yield format_sse_event(done_event)
            except (GeneratorExit, asyncio.CancelledError):
                logger.debug("[stream_final_answer] Client disconnected or cancelled during error handling")
                raise
            except Exception as yield_error:
                logger.error(f"Failed to yield error event: {yield_error}")
                # ìµœì¢… í´ë°±: ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•´ ì•„ë¬´ê²ƒë„ yieldí•˜ì§€ ì•Šê³  ì¢…ë£Œ
    
    async def _setup_callback_handler(self, session_id: str):
        """ì½œë°± í•¸ë“¤ëŸ¬ ì„¤ì •"""
        callback_queue = asyncio.Queue()
        callback_handler = None
        
        if self.workflow_service and hasattr(self.workflow_service, 'create_streaming_callback_handler'):
            callback_handler = self.workflow_service.create_streaming_callback_handler(queue=callback_queue)
            if callback_handler:
                logger.info("[stream_final_answer] âœ… StreamingCallbackHandler created and ready")
            else:
                logger.warning("[stream_final_answer] âš ï¸ Failed to create StreamingCallbackHandler")
        
        # LangGraph config ìƒì„± (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©)
        if callback_handler:
            config = create_langgraph_config_with_callbacks(
                session_id=session_id,
                callbacks=[callback_handler]
            )
            logger.info(f"[stream_final_answer] âœ… Callbacks added to config: {len(config.get('callbacks', []))} callback(s)")
        else:
            # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ config ìƒì„±
            from api.utils.langgraph_config_helper import create_langgraph_config
            config = create_langgraph_config(session_id=session_id)
            logger.warning("[stream_final_answer] âš ï¸ No callback handler, streaming may not work optimally")
        
        return callback_handler, config
    
    async def _process_stream_events(
        self,
        initial_state: Dict[str, Any],
        config: Dict[str, Any],
        callback_handler: Any,
        message: str,
        session_id: str
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¼ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        # ë¡œê·¸ ì¹´ìš´í„° ì´ˆê¸°í™” (ê° ìŠ¤íŠ¸ë¦¼ ì„¸ì…˜ë§ˆë‹¤)
        self._skip_log_count = 0
        self._classification_skip_count = 0
        
        answer_generation_started = False
        last_node_name = None
        event_count = 0
        stream_event_count = 0
        on_llm_stream_count = 0
        on_chat_model_stream_count = 0
        on_chain_stream_count = 0
        full_answer = ""
        callback_chunks_received = 0
        processed_callback_chunks = set()
        
        callback_queue = None
        if callback_handler and hasattr(callback_handler, 'queue'):
            callback_queue = callback_handler.queue
        chunk_output_queue = asyncio.Queue() if callback_queue else None
        
        callback_monitoring_active = True
        callback_task = None
        
        async def monitor_callback_queue():
            """ì½œë°± í ëª¨ë‹ˆí„°ë§"""
            nonlocal callback_monitoring_active
            while callback_monitoring_active:
                try:
                    if callback_queue and chunk_output_queue:
                        try:
                            chunk_data = await asyncio.wait_for(
                                callback_queue.get(),
                                timeout=StreamingConstants.CALLBACK_QUEUE_TIMEOUT
                            )
                            if chunk_data and chunk_data.get("type") == StreamingConstants.CALLBACK_CHUNK_TYPE:
                                await chunk_output_queue.put(chunk_data)
                        except asyncio.TimeoutError:
                            await asyncio.sleep(StreamingConstants.CALLBACK_MONITORING_INTERVAL)
                            continue
                        except asyncio.QueueEmpty:
                            await asyncio.sleep(StreamingConstants.CALLBACK_MONITORING_INTERVAL)
                            continue
                    else:
                        await asyncio.sleep(0.1)
                except Exception as e:
                    logger.debug(f"[stream_final_answer] Error in callback queue monitoring: {e}")
                    await asyncio.sleep(0.1)
        
        if callback_queue and chunk_output_queue:
            callback_task = asyncio.create_task(monitor_callback_queue())
            logger.info("[stream_final_answer] âœ… Callback queue monitoring task started")
        
        try:
            # ìµœì‹  LangGraph API í˜¸í™˜: version íŒŒë¼ë¯¸í„° ì—†ì´ ì‹œë„, ì‹¤íŒ¨ ì‹œ êµ¬ë²„ì „ í´ë°±
            logger.info(
                f"[_process_stream_events] ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: "
                f"session_id={session_id}, "
                f"message={message[:50]}..., "
                f"initial_state_keys={list(initial_state.keys())[:20]}, "
                f"config_thread_id={config.get('configurable', {}).get('thread_id', 'N/A')}"
            )
            
            try:
                logger.info(f"[_process_stream_events] astream_events() í˜¸ì¶œ ì‹œì‘")
                try:
                    astream_events_iter = self.workflow_service.app.astream_events(
                        initial_state,
                        config
                    )
                    logger.info(f"[_process_stream_events] âœ… astream_events() ì œë„ˆë ˆì´í„° ìƒì„± ì™„ë£Œ, ì´ë²¤íŠ¸ ëŒ€ê¸° ì‹œì‘")
                except Exception as iter_error:
                    logger.error(f"[_process_stream_events] âš ï¸ astream_events() ì œë„ˆë ˆì´í„° ìƒì„± ì‹¤íŒ¨: {iter_error}", exc_info=True)
                    # ğŸ”¥ ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€: ì œë„ˆë ˆì´í„° ìƒì„± ì‹¤íŒ¨í•´ë„ done ì´ë²¤íŠ¸ ì „ì†¡
                    # done ì´ë²¤íŠ¸ë¥¼ ë³´ë‚¸ í›„ì—ëŠ” raiseí•˜ì§€ ì•Šê³  ì •ìƒ ì¢…ë£Œ (ìŠ¤íŠ¸ë¦¼ì€ ì™„ë£Œë¨)
                    try:
                        error_event = self.event_builder.create_error_event(str(iter_error))
                        yield format_sse_event(error_event)
                        minimal_done = {"type": "done", "timestamp": datetime.now().isoformat(), "error": str(iter_error)}
                        yield format_sse_event(minimal_done)
                        logger.debug("[_process_stream_events] Error and done event sent after generator creation error")
                    except (GeneratorExit, asyncio.CancelledError):
                        # í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš°
                        raise
                    except Exception as yield_error:
                        logger.error(f"[_process_stream_events] Failed to send error/done event: {yield_error}")
                    # ì˜ˆì™¸ëŠ” ë¡œê¹…ë§Œ í•˜ê³  raiseí•˜ì§€ ì•ŠìŒ (done ì´ë²¤íŠ¸ë¥¼ ë³´ëƒˆìœ¼ë¯€ë¡œ ìŠ¤íŠ¸ë¦¼ì€ ì •ìƒ ì¢…ë£Œ)
                
                try:
                    async for event in astream_events_iter:
                        logger.info(f"[_process_stream_events] âœ… ì´ë²¤íŠ¸ ìˆ˜ì‹  #{event_count + 1}: event_type={event.get('event', 'N/A')}, name={event.get('name', 'N/A')}")
                        
                        # ì½œë°± íì—ì„œ ì²­í¬ ì²˜ë¦¬
                        if chunk_output_queue:
                            chunks_received, full_answer, chunks_to_yield = self._process_callback_queue_chunks(
                                chunk_output_queue, processed_callback_chunks, full_answer
                            )
                            callback_chunks_received += chunks_received
                            
                            # ì½œë°± ì²­í¬ë¥¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡
                            for content in chunks_to_yield:
                                stream_event = self.event_builder.create_stream_event(
                                    content, source="callback"
                                )
                                yield format_sse_event(stream_event)
                        
                        event_count += 1
                        event_type = event.get("event", "")
                        event_name = event.get("name", "")
                        event_parent = event.get("parent", {})
                        event_data = event.get("data", {})
                        
                        # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì¹´ìš´í„° ë° ë¡œê¹…
                        if event_type == "on_llm_stream":
                            on_llm_stream_count += 1
                            if on_llm_stream_count <= StreamingConstants.MAX_DEBUG_LOGS:
                                logger.debug(
                                    f"[stream_final_answer] on_llm_stream ì´ë²¤íŠ¸ #{on_llm_stream_count}: "
                                    f"name={event_name}, "
                                    f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}"
                                )
                        elif event_type == "on_chat_model_stream":
                            on_chat_model_stream_count += 1
                            if on_chat_model_stream_count <= StreamingConstants.MAX_DEBUG_LOGS:
                                logger.debug(
                                    f"[stream_final_answer] on_chat_model_stream ì´ë²¤íŠ¸ #{on_chat_model_stream_count}: "
                                    f"name={event_name}, "
                                    f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}"
                                )
                        elif event_type == "on_chain_stream":
                            on_chain_stream_count += 1
                            if on_chain_stream_count <= StreamingConstants.MAX_DETAILED_LOGS:
                                logger.debug(
                                    f"[stream_final_answer] on_chain_stream ì´ë²¤íŠ¸ #{on_chain_stream_count}: "
                                    f"name={event_name}, "
                                    f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}"
                                )
                        
                        # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì²˜ë¦¬
                        if event_type == "on_chain_start":
                            answer_generation_started, last_node_name = self._handle_on_chain_start_event(
                                event_name, answer_generation_started, last_node_name
                            )
                        
                        elif event_type == "on_chain_end":
                            answer_generation_started = self._handle_on_chain_end_event(
                                event_name, answer_generation_started
                            )
                            
                            # ë‹µë³€ ì™„ë£Œ ë…¸ë“œì¸ ê²½ìš° ë‚¨ì€ ì½œë°± ì²­í¬ ì²˜ë¦¬
                            if self.node_filter.is_answer_completion_node(event_name):
                                if chunk_output_queue:
                                    chunks_received, full_answer, chunks_to_yield = self._process_callback_queue_chunks(
                                        chunk_output_queue, processed_callback_chunks, full_answer
                                    )
                                    callback_chunks_received += chunks_received
                                    
                                    # ì½œë°± ì²­í¬ë¥¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡
                                    for content in chunks_to_yield:
                                        stream_event = self.event_builder.create_stream_event(
                                            content, source="callback"
                                        )
                                        yield format_sse_event(stream_event)
                        
                        elif event_type in ["on_llm_stream", "on_chat_model_stream"]:
                            should_continue, full_answer, stream_event_count, token = self._handle_streaming_event(
                                event_type, event_name, event_parent, event_data,
                                answer_generation_started, last_node_name,
                                full_answer, stream_event_count
                            )
                            
                            if not should_continue:
                                continue
                            
                            # í† í°ì´ ìˆìœ¼ë©´ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡
                            if token:
                                stream_event = self.event_builder.create_stream_event(token)
                                yield format_sse_event(stream_event)
                
                # async for ë£¨í”„ ì¢…ë£Œ í›„ ì²˜ë¦¬ (ëª¨ë“  ì´ë²¤íŠ¸ ìˆ˜ì‹  ì™„ë£Œ)
                except StopAsyncIteration:
                    logger.info(f"[_process_stream_events] âœ… astream_events() ì œë„ˆë ˆì´í„° ì •ìƒ ì¢…ë£Œ (StopAsyncIteration)")
                    # ì •ìƒ ì¢…ë£Œì´ë¯€ë¡œ ê³„ì† ì§„í–‰
                except Exception as iter_error:
                    logger.error(f"[_process_stream_events] âš ï¸ astream_events() ì´í„°ë ˆì´í„° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {iter_error}", exc_info=True)
                    # ğŸ”¥ ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€: ì˜ˆì™¸ ë°œìƒí•´ë„ done ì´ë²¤íŠ¸ ì „ì†¡
                    # done ì´ë²¤íŠ¸ë¥¼ ë³´ë‚¸ í›„ì—ëŠ” raiseí•˜ì§€ ì•Šê³  ì •ìƒ ì¢…ë£Œ (ìŠ¤íŠ¸ë¦¼ì€ ì™„ë£Œë¨)
                    try:
                        error_event = self.event_builder.create_error_event(str(iter_error))
                        yield format_sse_event(error_event)
                        minimal_done = {"type": "done", "timestamp": datetime.now().isoformat(), "error": str(iter_error)}
                        yield format_sse_event(minimal_done)
                        logger.debug("[_process_stream_events] Error and done event sent after iterator error")
                    except (GeneratorExit, asyncio.CancelledError):
                        # í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš°
                        raise
                    except Exception as yield_error:
                        logger.error(f"[_process_stream_events] Failed to send error/done event: {yield_error}")
                    # ì˜ˆì™¸ëŠ” ë¡œê¹…ë§Œ í•˜ê³  raiseí•˜ì§€ ì•ŠìŒ (done ì´ë²¤íŠ¸ë¥¼ ë³´ëƒˆìœ¼ë¯€ë¡œ ìŠ¤íŠ¸ë¦¼ì€ ì •ìƒ ì¢…ë£Œ)
                
                logger.info(
                    f"[stream_final_answer] âœ… astream_events() ë£¨í”„ ì™„ë£Œ: "
                    f"ì´ {event_count}ê°œ ì´ë²¤íŠ¸, "
                    f"ìŠ¤íŠ¸ë¦¼ ì´ë²¤íŠ¸ {stream_event_count}ê°œ, "
                    f"ì½œë°± ì²­í¬ {callback_chunks_received}ê°œ, "
                    f"on_llm_stream={on_llm_stream_count}ê°œ, "
                    f"on_chat_model_stream={on_chat_model_stream_count}ê°œ, "
                    f"on_chain_stream={on_chain_stream_count}ê°œ, "
                    f"full_answer_length={len(full_answer)}"
                )
                
                # ì´ë²¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê²½ê³ 
                if event_count < 5:
                    logger.warning(
                        f"[stream_final_answer] âš ï¸ ì´ë²¤íŠ¸ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({event_count}ê°œ). "
                        f"ì›Œí¬í”Œë¡œìš°ê°€ ì œëŒ€ë¡œ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )
                
                await asyncio.sleep(StreamingConstants.STATE_RETRY_DELAY)
                
                # final_metadata ê°€ì ¸ì˜¤ê¸° (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
                final_metadata = {}
                try:
                    logger.info(f"[stream_final_answer] final_metadata ê°€ì ¸ì˜¤ê¸° ì‹œë„: session_id={session_id}")
                    final_metadata = await self._get_final_metadata(
                        config, initial_state, message, full_answer, session_id
                    )
                    logger.info(
                        f"[stream_final_answer] âœ… final_metadata ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: "
                        f"sources_detail={len(final_metadata.get('sources_detail', []))}, "
                        f"sources_by_type={bool(final_metadata.get('sources_by_type'))}, "
                        f"sources={len(final_metadata.get('sources', []))}, "
                        f"legal_references={len(final_metadata.get('legal_references', []))}, "
                        f"metadata_keys={list(final_metadata.keys())[:20]}"
                    )
                except Exception as metadata_error:
                    logger.warning(
                        f"[stream_final_answer] âš ï¸ Failed to get final metadata: {metadata_error}",
                        exc_info=True
                    )
                    # metadata ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                
                # validation ì´ë²¤íŠ¸ ì „ì†¡ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
                if final_metadata.get("llm_validation"):
                    try:
                        validation_event = self.event_builder.create_validation_event(
                            final_metadata["llm_validation"]
                        )
                        yield format_sse_event(validation_event)
                    except Exception as validation_error:
                        logger.warning(f"[stream_final_answer] Failed to send validation event: {validation_error}")
                        # validation ì´ë²¤íŠ¸ ì „ì†¡ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                
                # final_event ì „ì†¡ (ì‹¤íŒ¨í•´ë„ done ì´ë²¤íŠ¸ëŠ” ì „ì†¡)
                try:
                    final_event = self.event_builder.create_final_event(full_answer, final_metadata)
                    yield format_sse_event(final_event)
                except (GeneratorExit, asyncio.CancelledError):
                    # í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš°
                    raise
                except Exception as final_error:
                    logger.warning(f"[stream_final_answer] Failed to send final event: {final_error}")
                    # final ì´ë²¤íŠ¸ ì „ì†¡ ì‹¤íŒ¨í•´ë„ done ì´ë²¤íŠ¸ëŠ” ì „ì†¡
                
                # done_eventëŠ” ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¥¼ ì•Œë¦¬ëŠ” ìš©ë„ë¡œ ë°˜ë“œì‹œ ì „ì†¡ (ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€)
                # ì¤‘ìš”: done ì´ë²¤íŠ¸ëŠ” ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ë°˜ë“œì‹œ ì „ì†¡ë˜ì–´ì•¼ í•¨
                done_event_sent = False
                try:
                    done_event = self.event_builder.create_done_event(full_answer, final_metadata)
                    yield format_sse_event(done_event)
                    done_event_sent = True
                    logger.debug("[stream_final_answer] Done event sent successfully")
                except (GeneratorExit, asyncio.CancelledError):
                    # í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš°
                    logger.debug("[stream_final_answer] Client disconnected while sending done event")
                    raise
                except Exception as done_error:
                    logger.error(f"[stream_final_answer] Failed to send done event: {done_error}", exc_info=True)
                    # done ì´ë²¤íŠ¸ ì „ì†¡ ì‹¤íŒ¨ ì‹œ ìµœì†Œí•œì˜ done ì´ë²¤íŠ¸ë¼ë„ ì „ì†¡ ì‹œë„
                    if not done_event_sent:
                        try:
                            minimal_done = {"type": "done", "timestamp": datetime.now().isoformat()}
                            yield format_sse_event(minimal_done)
                            logger.debug("[stream_final_answer] Minimal done event sent as fallback")
                        except Exception:
                            logger.error("[stream_final_answer] Failed to send minimal done event", exc_info=True)
            except (TypeError, AttributeError) as e:
                # êµ¬ë²„ì „ API í´ë°±: version="v2" íŒŒë¼ë¯¸í„° ì‚¬ìš©
                logger.warning(f"[_process_stream_events] ìµœì‹  API ì‹¤íŒ¨, êµ¬ë²„ì „ API(version='v2') ì‹œë„: {e}")
                try:
                    logger.info(f"[_process_stream_events] astream_events(version='v2') í˜¸ì¶œ ì‹œì‘")
                    async for event in self.workflow_service.app.astream_events(
                        initial_state,
                        config,
                        version="v2"
                    ):
                        logger.info(f"[_process_stream_events] âœ… ì´ë²¤íŠ¸ ìˆ˜ì‹  (v2): event_type={event.get('event', 'N/A')}, name={event.get('name', 'N/A')}")
                        # ë™ì¼í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬ ë¡œì§ ì ìš© (ìœ„ì˜ ë¡œì§ ì¬ì‚¬ìš©)
                        # TODO: ì´ë²¤íŠ¸ ì²˜ë¦¬ ë¡œì§ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¬ì‚¬ìš©
                        event_count += 1
                        # ê¸°ë³¸ ì´ë²¤íŠ¸ ì²˜ë¦¬...
                except Exception as ve2:
                    logger.error(f"[_process_stream_events] êµ¬ë²„ì „ APIë„ ì‹¤íŒ¨: {ve2}", exc_info=True)
                    # ğŸ”¥ ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€: ì˜ˆì™¸ ë°œìƒí•´ë„ done ì´ë²¤íŠ¸ ì „ì†¡
                    # done ì´ë²¤íŠ¸ë¥¼ ë³´ë‚¸ í›„ì—ëŠ” raiseí•˜ì§€ ì•Šê³  ì •ìƒ ì¢…ë£Œ (ìŠ¤íŠ¸ë¦¼ì€ ì™„ë£Œë¨)
                    try:
                        error_event = self.event_builder.create_error_event(str(ve2))
                        yield format_sse_event(error_event)
                        minimal_done = {"type": "done", "timestamp": datetime.now().isoformat(), "error": str(ve2)}
                        yield format_sse_event(minimal_done)
                        logger.debug("[_process_stream_events] Error and done event sent after v2 API error")
                    except (GeneratorExit, asyncio.CancelledError):
                        # í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš°
                        raise
                    except Exception as yield_error:
                        logger.error(f"[_process_stream_events] Failed to send error/done event: {yield_error}")
                    # ì˜ˆì™¸ëŠ” ë¡œê¹…ë§Œ í•˜ê³  raiseí•˜ì§€ ì•ŠìŒ (done ì´ë²¤íŠ¸ë¥¼ ë³´ëƒˆìœ¼ë¯€ë¡œ ìŠ¤íŠ¸ë¦¼ì€ ì •ìƒ ì¢…ë£Œ)
            except Exception as e:
                logger.error(f"[_process_stream_events] âš ï¸ astream_events() ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
                # ğŸ”¥ ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€: ì˜ˆì™¸ ë°œìƒí•´ë„ done ì´ë²¤íŠ¸ ì „ì†¡
                # done ì´ë²¤íŠ¸ë¥¼ ë³´ë‚¸ í›„ì—ëŠ” raiseí•˜ì§€ ì•Šê³  ì •ìƒ ì¢…ë£Œ (ìŠ¤íŠ¸ë¦¼ì€ ì™„ë£Œë¨)
                try:
                    error_event = self.event_builder.create_error_event(str(e))
                    yield format_sse_event(error_event)
                    minimal_done = {"type": "done", "timestamp": datetime.now().isoformat(), "error": str(e)}
                    yield format_sse_event(minimal_done)
                    logger.debug("[_process_stream_events] Error and done event sent after unexpected error")
                except (GeneratorExit, asyncio.CancelledError):
                    # í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ì„ ëŠì€ ê²½ìš°
                    raise
                except Exception as yield_error:
                    logger.error(f"[_process_stream_events] Failed to send error/done event: {yield_error}")
                # ì˜ˆì™¸ëŠ” ë¡œê¹…ë§Œ í•˜ê³  raiseí•˜ì§€ ì•ŠìŒ (done ì´ë²¤íŠ¸ë¥¼ ë³´ëƒˆìœ¼ë¯€ë¡œ ìŠ¤íŠ¸ë¦¼ì€ ì •ìƒ ì¢…ë£Œ)
                # ìƒìœ„ì—ì„œ ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•  í•„ìš”ê°€ ìˆìœ¼ë©´ ë¡œê¹…ëœ ì˜ˆì™¸ ì •ë³´ë¥¼ ì‚¬ìš©
        
        except asyncio.CancelledError:
            logger.debug("[stream_final_answer] Stream cancelled (client disconnected)")
            # ğŸ”¥ ê°œì„ : ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€ë¥¼ ìœ„í•´ done ì´ë²¤íŠ¸ ì „ì†¡ ì‹œë„
            try:
                done_event = self.event_builder.create_done_event("", {})
                yield format_sse_event(done_event)
            except Exception:
                pass  # ì´ë¯¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ë¬´ì‹œ
            raise
        except GeneratorExit:
            # GeneratorExitëŠ” ì œë„ˆë ˆì´í„°ê°€ ì´ë¯¸ ì¢…ë£Œëœ ìƒíƒœì´ë¯€ë¡œ yieldë¥¼ ì‹œë„í•˜ë©´ ì•ˆ ë¨
            logger.debug("[stream_final_answer] Generator exit (client disconnected)")
            raise
        except Exception as e:
            logger.error(f"[stream_final_answer] Unexpected error: {e}", exc_info=True)
            # ğŸ”¥ ê°œì„ : ERR_INCOMPLETE_CHUNKED_ENCODING ë°©ì§€ë¥¼ ìœ„í•´ done ì´ë²¤íŠ¸ ì „ì†¡ ì‹œë„
            try:
                error_event = self.event_builder.create_error_event(str(e))
                yield format_sse_event(error_event)
                done_event = self.event_builder.create_done_event("", {})
                yield format_sse_event(done_event)
            except Exception:
                pass  # ì´ë¯¸ ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ë¬´ì‹œ
            raise
        finally:
            callback_monitoring_active = False
            if callback_task:
                callback_task.cancel()
                try:
                    await callback_task
                except asyncio.CancelledError:
                    pass
            
            if callback_handler and hasattr(callback_handler, 'get_stats'):
                stats = callback_handler.get_stats()
                logger.info(
                    f"[stream_final_answer] Callback stats: "
                    f"chunks={callback_chunks_received}, "
                    f"total_chunks={stats.get('total_chunks', 0)}, "
                    f"streaming_active={stats.get('streaming_active', False)}"
                )
    
    
    async def _get_final_metadata(
        self,
        config: Dict[str, Any],
        initial_state: Dict[str, Any],
        message: str,
        full_answer: str,
        session_id: str
    ) -> Dict[str, Any]:
        """ìµœì¢… ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            final_state = await asyncio.wait_for(
                self.workflow_service.app.aget_state(config),
                timeout=StreamingConstants.STATE_TIMEOUT
            )
            if final_state and final_state.values:
                state_values = final_state.values
                logger.debug(
                    f"[stream_final_answer] State retrieved: "
                    f"answer_length={len(state_values.get('answer', ''))}, "
                    f"full_answer_length={len(full_answer)}"
                )
                
                # sources ì¶”ì¶œ ê°•í™” (ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ í™•ì¸, ìš°ì„ ìˆœìœ„ ì ìš©)
                sources_from_top = state_values.get("sources", [])
                sources_from_common = (state_values.get("common", {}).get("sources") if isinstance(state_values.get("common"), dict) else None) or []
                sources_from_metadata = (state_values.get("metadata", {}).get("sources") if isinstance(state_values.get("metadata"), dict) else None) or []
                # ìš°ì„ ìˆœìœ„: top > common > metadata
                sources = sources_from_top if sources_from_top else (sources_from_common if sources_from_common else sources_from_metadata)
                if not sources:
                    sources = []
                
                # legal_references ì¶”ì¶œ ê°•í™” (ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ í™•ì¸, ìš°ì„ ìˆœìœ„ ì ìš©)
                legal_refs_from_top = state_values.get("legal_references", [])
                legal_refs_from_common = (state_values.get("common", {}).get("legal_references") if isinstance(state_values.get("common"), dict) else None) or []
                legal_refs_from_metadata = (state_values.get("metadata", {}).get("legal_references") if isinstance(state_values.get("metadata"), dict) else None) or []
                # ìš°ì„ ìˆœìœ„: top > common > metadata
                legal_references = legal_refs_from_top if legal_refs_from_top else (legal_refs_from_common if legal_refs_from_common else legal_refs_from_metadata)
                if not legal_references:
                    legal_references = []
                
                # sources_detail ì¶”ì¶œ ê°•í™” (ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ í™•ì¸, ìš°ì„ ìˆœìœ„ ì ìš©)
                sources_detail_from_top = state_values.get("sources_detail", [])
                sources_detail_from_common = (state_values.get("common", {}).get("sources_detail") if isinstance(state_values.get("common"), dict) else None) or []
                sources_detail_from_metadata = (state_values.get("metadata", {}).get("sources_detail") if isinstance(state_values.get("metadata"), dict) else None) or []
                
                # ìƒì„¸ ë¡œê¹…: ê° ìœ„ì¹˜ì—ì„œ sources_detail í™•ì¸
                logger.info(
                    f"[_get_final_metadata] sources_detail ì¶”ì¶œ ì‹œë„: "
                    f"top={len(sources_detail_from_top) if isinstance(sources_detail_from_top, list) else 'not_list'}, "
                    f"common={len(sources_detail_from_common) if isinstance(sources_detail_from_common, list) else 'not_list'}, "
                    f"metadata={len(sources_detail_from_metadata) if isinstance(sources_detail_from_metadata, list) else 'not_list'}"
                )
                
                # state_valuesì˜ ëª¨ë“  í‚¤ í™•ì¸ (ë””ë²„ê¹…ìš©)
                state_keys = list(state_values.keys())
                logger.debug(
                    f"[_get_final_metadata] State keys: {state_keys[:30]}... "
                    f"(total: {len(state_keys)})"
                )
                
                # commonê³¼ metadata êµ¬ì¡° í™•ì¸
                if isinstance(state_values.get("common"), dict):
                    common_keys = list(state_values["common"].keys())
                    logger.debug(f"[_get_final_metadata] Common keys: {common_keys[:20]}...")
                if isinstance(state_values.get("metadata"), dict):
                    metadata_keys = list(state_values["metadata"].keys())
                    logger.debug(f"[_get_final_metadata] Metadata keys: {metadata_keys[:20]}...")
                
                # ìš°ì„ ìˆœìœ„: top > common > metadata
                sources_detail = sources_detail_from_top if sources_detail_from_top else (sources_detail_from_common if sources_detail_from_common else sources_detail_from_metadata)
                if not sources_detail:
                    sources_detail = []
                
                sources_source = "top" if sources_from_top else ("common" if sources_from_common else ("metadata" if sources_from_metadata else "none"))
                sources_detail_source = "top" if sources_detail_from_top else ("common" if sources_detail_from_common else ("metadata" if sources_detail_from_metadata else "none"))
                
                logger.info(
                    f"[_get_final_metadata] Sources extraction result: "
                    f"state_sources={len(sources)} (from {sources_source}), "
                    f"state_legal_references={len(legal_references)}, "
                    f"state_sources_detail={len(sources_detail)} (from {sources_detail_source})"
                )
                
                # sources_detailì´ ë¹„ì–´ìˆìœ¼ë©´ ìƒì„¸ ë¡œê¹…
                if not sources_detail:
                    logger.warning(
                        f"[_get_final_metadata] âš ï¸ sources_detailì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. "
                        f"state_values êµ¬ì¡° í™•ì¸ í•„ìš”. "
                        f"top_type={type(sources_detail_from_top).__name__}, "
                        f"common_type={type(sources_detail_from_common).__name__}, "
                        f"metadata_type={type(sources_detail_from_metadata).__name__}"
                    )
                
                if not sources_detail:
                    structured_docs_from_top = state_values.get("structured_documents")
                    structured_docs_from_search = (state_values.get("search", {}).get("structured_documents") if isinstance(state_values.get("search"), dict) else None)
                    structured_docs_from_common = (state_values.get("common", {}).get("search", {}).get("structured_documents") if isinstance(state_values.get("common"), dict) and isinstance(state_values["common"].get("search"), dict) else None)
                    
                    structured_docs = (
                        structured_docs_from_top or
                        structured_docs_from_search or
                        structured_docs_from_common
                    )
                    
                    prompt_used_docs = []
                    if structured_docs and isinstance(structured_docs, dict):
                        documents_in_prompt = structured_docs.get("documents", [])
                        if documents_in_prompt and isinstance(documents_in_prompt, list):
                            min_relevance_score = 0.80
                            filtered_docs = []
                            for doc in documents_in_prompt:
                                if not isinstance(doc, dict):
                                    continue
                                
                                relevance_score = (
                                    doc.get("relevance_score") or
                                    doc.get("score") or
                                    doc.get("final_weighted_score") or
                                    doc.get("similarity") or
                                    0.0
                                )
                                
                                if relevance_score >= min_relevance_score:
                                    filtered_docs.append(doc)
                                else:
                                    logger.debug(
                                        f"[stream_final_answer] Document filtered out due to low relevance: "
                                        f"score={relevance_score:.3f} < {min_relevance_score}, "
                                        f"doc_id={doc.get('doc_id') or doc.get('id') or 'unknown'}"
                                    )
                            
                            prompt_used_docs = filtered_docs
                            logger.info(
                                f"[stream_final_answer] Filtered documents by relevance (>= {min_relevance_score}): "
                                f"{len(prompt_used_docs)}/{len(documents_in_prompt)} documents passed"
                            )
                            
                            if not prompt_used_docs and documents_in_prompt:
                                logger.warning(
                                    f"[stream_final_answer] No documents with relevance >= {min_relevance_score} found. "
                                    f"All {len(documents_in_prompt)} documents were filtered out. "
                                    f"Consider lowering the threshold or checking document quality."
                                )
                    
                    retrieved_docs_from_top = state_values.get("retrieved_docs")
                    retrieved_docs_from_search = (state_values.get("search", {}).get("retrieved_docs") if isinstance(state_values.get("search"), dict) else None)
                    retrieved_docs_from_common = (state_values.get("common", {}).get("search", {}).get("retrieved_docs") if isinstance(state_values.get("common"), dict) and isinstance(state_values["common"].get("search"), dict) else None)
                    retrieved_docs_from_metadata = (state_values.get("metadata", {}).get("retrieved_docs") if isinstance(state_values.get("metadata"), dict) else None)
                    retrieved_docs_from_metadata_search = (state_values.get("metadata", {}).get("search", {}).get("retrieved_docs") if isinstance(state_values.get("metadata"), dict) and isinstance(state_values["metadata"].get("search"), dict) else None)
                    
                    all_retrieved_docs = (
                        retrieved_docs_from_top or
                        retrieved_docs_from_search or
                        retrieved_docs_from_common or
                        retrieved_docs_from_metadata or
                        retrieved_docs_from_metadata_search
                    )
                    
                    logger.info(
                        f"[_get_final_metadata] retrieved_docs í™•ì¸: "
                        f"top={len(retrieved_docs_from_top) if isinstance(retrieved_docs_from_top, list) else 0}, "
                        f"search={len(retrieved_docs_from_search) if isinstance(retrieved_docs_from_search, list) else 0}, "
                        f"common={len(retrieved_docs_from_common) if isinstance(retrieved_docs_from_common, list) else 0}, "
                        f"metadata={len(retrieved_docs_from_metadata) if isinstance(retrieved_docs_from_metadata, list) else 0}, "
                        f"all_retrieved_docs={len(all_retrieved_docs) if isinstance(all_retrieved_docs, list) else 0}"
                    )
                    
                    if prompt_used_docs:
                        retrieved_docs = prompt_used_docs
                        logger.info(
                            f"[stream_final_answer] Using {len(retrieved_docs)} documents from structured_documents "
                            f"(actual documents used in prompt) instead of all {len(all_retrieved_docs) if all_retrieved_docs else 0} retrieved_docs"
                        )
                    else:
                        retrieved_docs = all_retrieved_docs
                        if retrieved_docs:
                            logger.debug(
                                f"[stream_final_answer] structured_documents not found, "
                                f"using all {len(retrieved_docs)} retrieved_docs"
                            )
                    
                    if prompt_used_docs:
                        retrieved_docs_source = "structured_documents"
                    else:
                        retrieved_docs_source = (
                            "top" if retrieved_docs_from_top else
                            ("search" if retrieved_docs_from_search else
                            ("common.search" if retrieved_docs_from_common else
                            ("metadata" if retrieved_docs_from_metadata else
                            ("metadata.search" if retrieved_docs_from_metadata_search else "none"))))
                        )
                    
                    logger.debug(
                        f"[stream_final_answer] Retrieved docs check: "
                        f"count={len(retrieved_docs) if retrieved_docs else 0}, "
                        f"source={retrieved_docs_source}"
                    )
                    
                    # stateì— retrieved_docsê°€ ì—†ìœ¼ë©´ global cacheì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
                    if not retrieved_docs:
                        logger.debug(f"[stream_final_answer] State has no retrieved_docs, attempting to restore from global cache")
                        try:
                            # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
                            _global_search_results_cache = None
                            import_errors = []
                            
                            # ê²½ë¡œ 1: core.shared.wrappers.node_wrappers (ê°€ì¥ ì¼ë°˜ì )
                            try:
                                from core.shared.wrappers.node_wrappers import _global_search_results_cache
                                logger.debug(f"[stream_final_answer] Successfully imported _global_search_results_cache from core.shared.wrappers.node_wrappers")
                            except (ImportError, AttributeError) as e:
                                import_errors.append(f"core.shared.wrappers.node_wrappers: {e}")
                                
                                # ê²½ë¡œ 2: lawfirm_langgraph.core.shared.wrappers.node_wrappers
                                try:
                                    from lawfirm_langgraph.core.shared.wrappers.node_wrappers import _global_search_results_cache
                                    logger.debug(f"[stream_final_answer] Successfully imported _global_search_results_cache from lawfirm_langgraph.core.shared.wrappers.node_wrappers")
                                except (ImportError, AttributeError) as e2:
                                    import_errors.append(f"lawfirm_langgraph.core.shared.wrappers.node_wrappers: {e2}")
                                    
                                    # ê²½ë¡œ 3: core.agents.node_wrappers
                                    try:
                                        from core.agents.node_wrappers import _global_search_results_cache
                                        logger.debug(f"[stream_final_answer] Successfully imported _global_search_results_cache from core.agents.node_wrappers")
                                    except (ImportError, AttributeError) as e3:
                                        import_errors.append(f"core.agents.node_wrappers: {e3}")
                            
                            if _global_search_results_cache is not None:
                                logger.debug(f"[stream_final_answer] Global cache exists: {type(_global_search_results_cache).__name__}, keys: {list(_global_search_results_cache.keys()) if isinstance(_global_search_results_cache, dict) else 'N/A'}")
                                
                                cached_structured_docs = None
                                if isinstance(_global_search_results_cache, dict) and "search" in _global_search_results_cache:
                                    cached_search = _global_search_results_cache["search"]
                                    if isinstance(cached_search, dict):
                                        cached_structured_docs = cached_search.get("structured_documents")
                                
                                if cached_structured_docs and isinstance(cached_structured_docs, dict):
                                    cached_prompt_docs = cached_structured_docs.get("documents", [])
                                    if cached_prompt_docs and isinstance(cached_prompt_docs, list) and len(cached_prompt_docs) > 0:
                                        min_relevance_score = 0.80
                                        filtered_cached_docs = []
                                        for doc in cached_prompt_docs:
                                            if not isinstance(doc, dict):
                                                continue
                                            
                                            relevance_score = (
                                                doc.get("relevance_score") or
                                                doc.get("score") or
                                                doc.get("final_weighted_score") or
                                                doc.get("similarity") or
                                                0.0
                                            )
                                            
                                            if relevance_score >= min_relevance_score:
                                                filtered_cached_docs.append(doc)
                                        
                                        if filtered_cached_docs:
                                            retrieved_docs = filtered_cached_docs
                                            retrieved_docs_source = "global_cache.structured_documents"
                                            logger.info(
                                                f"[stream_final_answer] Restored {len(retrieved_docs)}/{len(cached_prompt_docs)} documents "
                                                f"from global cache structured_documents (filtered by relevance >= {min_relevance_score})"
                                            )
                                        else:
                                            logger.warning(
                                                f"[stream_final_answer] All {len(cached_prompt_docs)} documents from global cache "
                                                f"were filtered out (relevance < {min_relevance_score})"
                                            )
                                
                                if not retrieved_docs:
                                    if isinstance(_global_search_results_cache, dict) and "search" in _global_search_results_cache:
                                        cached_search = _global_search_results_cache["search"]
                                        if isinstance(cached_search, dict):
                                            cached_docs = cached_search.get("retrieved_docs", [])
                                            if isinstance(cached_docs, list) and len(cached_docs) > 0:
                                                retrieved_docs = cached_docs
                                                retrieved_docs_source = "global_cache.search.retrieved_docs"
                                                logger.debug(f"[stream_final_answer] Restored {len(retrieved_docs)} retrieved_docs from global cache search group")
                                            else:
                                                cached_merged = cached_search.get("merged_documents", [])
                                                if isinstance(cached_merged, list) and len(cached_merged) > 0:
                                                    retrieved_docs = cached_merged
                                                    retrieved_docs_source = "global_cache.search.merged_documents"
                                                    logger.debug(f"[stream_final_answer] Restored {len(retrieved_docs)} merged_documents from global cache search group")
                                
                                if not retrieved_docs and isinstance(_global_search_results_cache, dict):
                                    cached_docs = _global_search_results_cache.get("retrieved_docs", [])
                                    if isinstance(cached_docs, list) and len(cached_docs) > 0:
                                        retrieved_docs = cached_docs
                                        retrieved_docs_source = "global_cache.top"
                                        logger.debug(f"[stream_final_answer] Restored {len(retrieved_docs)} retrieved_docs from global cache top level")
                                
                                if not retrieved_docs:
                                    logger.debug(f"[stream_final_answer] Global cache exists but no retrieved_docs found in it")
                            else:
                                logger.debug(f"[stream_final_answer] Global cache is None after import attempts")
                                
                        except Exception as e:
                            logger.warning(f"[stream_final_answer] Failed to access global cache: {e}", exc_info=True)
                    
                    logger.debug(
                        f"[stream_final_answer] Attempting to extract sources: "
                        f"retrieved_docs_count={len(retrieved_docs) if retrieved_docs else 0}, "
                        f"retrieved_docs_source={retrieved_docs_source}, "
                        f"sources_extractor={self.sources_extractor is not None}"
                    )
                    
                    # ê°œì„ : sources, legal_references, sources_detailì´ ì—†ìœ¼ë©´ retrieved_docsì—ì„œ ì¶”ì¶œ ì‹œë„
                    if retrieved_docs and self.sources_extractor:
                        try:
                            # ğŸ”¥ retrieved_docs ì •ê·œí™” (type í†µí•©) - ì¶”ì¶œ ì „ì— ì •ê·œí™”
                            from lawfirm_langgraph.core.utils.document_type_normalizer import normalize_documents_type
                            retrieved_docs = normalize_documents_type(retrieved_docs) if retrieved_docs else []
                            
                            # retrieved_docsë¥¼ state_valuesì— ì„ì‹œë¡œ ì¶”ê°€í•˜ì—¬ ì¶”ì¶œ í•¨ìˆ˜ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•¨
                            temp_state = {**state_values, "retrieved_docs": retrieved_docs}
                            
                            # sources ì¶”ì¶œ ì‹œ ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ìŠ¤íŠ¸ë¦¬ë°ì´ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡ ê°ê° try-except ì²˜ë¦¬
                            # sourcesê°€ ì—†ì„ ë•Œë§Œ ì¶”ì¶œ ì‹œë„
                            if not sources:
                                try:
                                    sources_data = self.sources_extractor._extract_sources(temp_state)
                                    if sources_data:
                                        sources = sources_data
                                        state_values["sources"] = sources_data
                                        logger.info(f"[stream_final_answer] âœ… Extracted {len(sources)} sources from retrieved_docs")
                                except Exception as e:
                                    logger.warning(f"[stream_final_answer] Failed to extract sources: {e}", exc_info=True)
                            
                            # legal_referencesê°€ ì—†ì„ ë•Œë§Œ ì¶”ì¶œ ì‹œë„
                            if not legal_references:
                                try:
                                    legal_references_data = self.sources_extractor._extract_legal_references(temp_state)
                                    if legal_references_data:
                                        legal_references = legal_references_data
                                        logger.info(f"[stream_final_answer] âœ… Extracted {len(legal_references)} legal_references from retrieved_docs")
                                except Exception as e:
                                    logger.warning(f"[stream_final_answer] Failed to extract legal_references: {e}", exc_info=True)
                            
                            # sources_by_typeì´ ì—†ì„ ë•Œë§Œ retrieved_docsì—ì„œ ìƒì„± ì‹œë„
                            sources_by_type = temp_state.get("sources_by_type")
                            if not sources_by_type:
                                retrieved_docs = temp_state.get("retrieved_docs", [])
                                if retrieved_docs and isinstance(retrieved_docs, list):
                                    try:
                                        sources_by_type = self.sources_extractor._generate_sources_by_type_from_retrieved_docs(retrieved_docs)
                                        temp_state["sources_by_type"] = sources_by_type
                                        logger.info(f"[stream_final_answer] âœ… Generated sources_by_type from {len(retrieved_docs)} retrieved_docs")
                                    except Exception as e:
                                        logger.warning(f"[stream_final_answer] Failed to generate sources_by_type from retrieved_docs: {e}", exc_info=True)
                            
                            logger.debug(
                                f"[stream_final_answer] Sources extraction result: "
                                f"sources={len(sources) if sources else 0}, "
                                f"legal_references={len(legal_references) if legal_references else 0}, "
                                f"sources_detail={len(sources_detail) if sources_detail else 0}"
                            )
                        except Exception as e:
                            logger.warning(f"[stream_final_answer] Failed to extract sources from retrieved_docs: {e}", exc_info=True)
                    else:
                        logger.debug(
                            f"[stream_final_answer] Skipping sources extraction from state: "
                            f"retrieved_docs={retrieved_docs is not None and len(retrieved_docs) > 0}, "
                            f"sources_extractor={self.sources_extractor is not None}"
                        )
                        
                        # stateì— retrieved_docsê°€ ì—†ìœ¼ë©´ extract_from_message_metadataì™€ extract_from_stateë¥¼ ì‹œë„
                        if not retrieved_docs and self.sources_extractor and session_id:
                            try:
                                logger.debug(f"[stream_final_answer] Attempting to extract sources from session: session_id={session_id}")
                                
                                # ë¨¼ì € ë©”ì‹œì§€ metadataì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
                                message_id = state_values.get("metadata", {}).get("message_id") if isinstance(state_values.get("metadata"), dict) else None
                                session_sources = await self.sources_extractor.extract_from_message_metadata(session_id, message_id)
                                
                                # ì—†ìœ¼ë©´ stateì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
                                if not any(session_sources.values()):
                                    logger.debug(f"[stream_final_answer] No sources in message metadata, trying extract_from_state")
                                    session_sources = await self.sources_extractor.extract_from_state(session_id)
                                
                                if session_sources:
                                    session_sources_list = session_sources.get("sources", [])
                                    session_legal_refs = session_sources.get("legal_references", [])
                                    session_sources_detail = session_sources.get("sources_detail", [])
                                    
                                    logger.debug(
                                        f"[stream_final_answer] Sources extracted from session: "
                                        f"sources={len(session_sources_list)}, "
                                        f"legal_references={len(session_legal_refs)}, "
                                        f"sources_detail={len(session_sources_detail)}"
                                    )
                                    
                                    if session_sources_list:
                                        sources = session_sources_list
                                    if session_legal_refs:
                                        legal_references = session_legal_refs
                                    if session_sources_detail:
                                        sources_detail = session_sources_detail
                            except Exception as e:
                                logger.warning(f"[stream_final_answer] Failed to extract sources from session: {e}", exc_info=True)
                
                # related_questions ì¶”ì¶œ ê°•í™”
                related_questions = (
                    (state_values.get("metadata", {}).get("related_questions") if isinstance(state_values.get("metadata"), dict) else None) or
                    []
                )
                if not related_questions and self.extract_related_questions_fn:
                    try:
                        related_questions = await self.extract_related_questions_fn(
                            state_values, initial_state, message, full_answer, session_id
                        )
                    except Exception as e:
                        logger.warning(f"[stream_final_answer] Failed to extract related_questions: {e}", exc_info=True)
                        related_questions = []
                
                # llm_validation_result ì¶”ì¶œ ê°•í™”
                llm_validation_result = (
                    (state_values.get("metadata", {}).get("llm_validation_result", {}) if isinstance(state_values.get("metadata"), dict) else {}) or
                    {}
                )
                
                # message_id ì¶”ì¶œ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë©”ì‹œì§€ ë§¤ì¹­ì— ì‚¬ìš©)
                message_id = (
                    (state_values.get("metadata", {}).get("message_id") if isinstance(state_values.get("metadata"), dict) else None) or
                    None
                )
                
                # íƒ€ì…ë³„ ê·¸ë£¹í™” (ìƒˆë¡œìš´ ê¸°ëŠ¥) - íŒë¡€ì˜ ì°¸ì¡° ë²•ë ¹ í¬í•¨
                sources_by_type = self._generate_sources_by_type(sources_detail)
                
                final_metadata = {
                    "sources_by_type": sources_by_type,  # ìœ ì¼í•œ í•„ìš”í•œ í•„ë“œ
                    "related_questions": related_questions,
                    "llm_validation": llm_validation_result if llm_validation_result else None,
                    "message_id": message_id,  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë©”ì‹œì§€ ë§¤ì¹­ì— ì‚¬ìš©
                    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ deprecated í•„ë“œë„ í¬í•¨ (ì ì§„ì  ì œê±°)
                    "sources": sources,  # deprecated: sources_by_typeì—ì„œ ì¬êµ¬ì„± ê°€ëŠ¥
                    "legal_references": legal_references,  # deprecated: sources_by_typeì—ì„œ ì¬êµ¬ì„± ê°€ëŠ¥
                    "sources_detail": sources_detail,  # deprecated: sources_by_typeì—ì„œ ì¬êµ¬ì„± ê°€ëŠ¥
                }
                
                logger.info(
                    f"[stream_final_answer] âœ… Final metadata extracted: "
                    f"sources={len(final_metadata['sources'])}, "
                    f"legal_references={len(final_metadata['legal_references'])}, "
                    f"sources_detail={len(final_metadata['sources_detail'])}, "
                    f"related_questions={len(final_metadata['related_questions'])}, "
                    f"sources_by_type={bool(final_metadata.get('sources_by_type'))}"
                )
                
                return final_metadata
        except asyncio.TimeoutError:
            logger.warning(f"[stream_final_answer] Timeout getting state, using empty metadata")
        except Exception as e:
            logger.warning(f"[stream_final_answer] Error getting state: {e}")
        
        return {}
    
    def _generate_sources_by_type(self, sources_detail: List[Dict[str, Any]]) -> Optional[Dict[str, List[Any]]]:
        """
        sources_by_type ìƒì„± (íŒë¡€ì˜ ì°¸ì¡° ë²•ë ¹ í¬í•¨)
        ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ì•ˆì „í•˜ê²Œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
        """
        if not sources_detail or not self.sources_extractor:
            return None
        
        try:
            sources_by_type = self.sources_extractor._get_sources_by_type_with_reference_statutes(sources_detail)
            logger.debug(f"[stream_final_answer] Generated sources_by_type with reference statutes: {len(sources_by_type.get('statute_article', []))} statutes")
            return sources_by_type
        except Exception as e:
            logger.warning(f"[stream_final_answer] Failed to generate sources_by_type: {e}", exc_info=True)
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ sources_by_type ìƒì„± (ì°¸ì¡° ë²•ë ¹ ì—†ì´)
            try:
                sources_by_type = self.sources_extractor._get_sources_by_type(sources_detail) if sources_detail else DEFAULT_SOURCES_BY_TYPE.copy()
                return sources_by_type
            except Exception as fallback_error:
                logger.error(f"[stream_final_answer] Failed to generate fallback sources_by_type: {fallback_error}", exc_info=True)
                return DEFAULT_SOURCES_BY_TYPE.copy()
    
    def _process_callback_queue_chunks(
        self,
        chunk_output_queue: asyncio.Queue,
        processed_callback_chunks: set,
        full_answer: str
    ) -> tuple[int, str, list[str]]:
        """
        ì½œë°± íì—ì„œ ì²­í¬ë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬
        
        Returns:
            (callback_chunks_received, updated_full_answer, chunks_to_yield)
            chunks_to_yield: ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        callback_chunks_received = 0
        updated_full_answer = full_answer
        chunks_to_yield = []
        
        if not chunk_output_queue:
            return callback_chunks_received, updated_full_answer, chunks_to_yield
        
        try:
            while True:
                try:
                    chunk_data = chunk_output_queue.get_nowait()
                    if chunk_data and chunk_data.get("type") == StreamingConstants.CALLBACK_CHUNK_TYPE:
                        content = chunk_data.get("content", "")
                        chunk_index = chunk_data.get("chunk_index", 0)
                        chunk_key = f"{chunk_index}_{content[:10]}"
                        
                        if chunk_key not in processed_callback_chunks and content:
                            processed_callback_chunks.add(chunk_key)
                            callback_chunks_received += 1
                            updated_full_answer += content
                            chunks_to_yield.append(content)
                            
                            if callback_chunks_received <= StreamingConstants.MAX_DEBUG_LOGS:
                                logger.info(
                                    f"[stream_final_answer] âœ… Callback chunk #{callback_chunks_received}: "
                                    f"length={len(content)}, content={content[:50]}..."
                                )
                except asyncio.QueueEmpty:
                    break
        except Exception as e:
            logger.debug(f"[stream_final_answer] Error checking callback output queue: {e}")
        
        return callback_chunks_received, updated_full_answer, chunks_to_yield
    
    def _handle_on_chain_start_event(
        self,
        event_name: str,
        answer_generation_started: bool,
        last_node_name: Optional[str]
    ) -> tuple[bool, Optional[str]]:
        """
        on_chain_start ì´ë²¤íŠ¸ ì²˜ë¦¬
        
        Returns:
            (updated_answer_generation_started, updated_last_node_name)
        """
        node_name = event_name
        is_answer_node = self.node_filter.is_answer_generation_node(node_name)
        logger.info(
            f"[stream_final_answer] on_chain_start: "
            f"node_name={node_name}, "
            f"is_answer_generation_node={is_answer_node}"
        )
        
        if is_answer_node:
            answer_generation_started = True
            last_node_name = node_name
            logger.info(f"[stream_final_answer] âœ… ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹œì‘: {node_name}, answer_generation_started=True")
        
        return answer_generation_started, last_node_name
    
    def _handle_on_chain_end_event(
        self,
        event_name: str,
        answer_generation_started: bool
    ) -> bool:
        """
        on_chain_end ì´ë²¤íŠ¸ ì²˜ë¦¬
        
        Returns:
            updated_answer_generation_started
        """
        node_name = event_name
        if self.node_filter.is_answer_completion_node(node_name):
            answer_generation_started = False
            logger.debug(f"[stream_final_answer] ë‹µë³€ ìƒì„± ë…¸ë“œ ì™„ë£Œ: {node_name}")
        
        return answer_generation_started
    
    def _handle_streaming_event(
        self,
        event_type: str,
        event_name: str,
        event_parent: Dict[str, Any],
        event_data: Dict[str, Any],
        answer_generation_started: bool,
        last_node_name: Optional[str],
        full_answer: str,
        stream_event_count: int
    ) -> tuple[bool, str, int, Optional[str]]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ (on_llm_stream, on_chat_model_stream)
        
        Returns:
            (should_continue, updated_full_answer, updated_stream_event_count, token_to_yield)
            should_continue: Falseë©´ continue, Trueë©´ ê³„ì† ì²˜ë¦¬
            token_to_yield: Noneì´ ì•„ë‹ˆë©´ yieldí•´ì•¼ í•  í† í°
        """
        # ë¶„ë¥˜ ë…¸ë“œëŠ” ì •ìƒ ë™ì‘ì´ë¯€ë¡œ ì¡°ìš©íˆ ê±´ë„ˆëœ€
        if self.node_filter.is_classification_node(event_name):
            self._classification_skip_count += 1
            return False, full_answer, stream_event_count, None
        
        # answer_generation_startedê°€ Falseì¸ ê²½ìš° ë¡œê¹… í›„ ê±´ë„ˆë›°ê¸°
        if not answer_generation_started:
            # ë¡œê·¸ ì¹´ìš´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œí•œëœ íšŸìˆ˜ë§Œ ë¡œê·¸ ì¶œë ¥
            if self._skip_log_count < StreamingConstants.MAX_SKIP_LOGS:
                logger.warning(
                    f"[stream_final_answer] âš ï¸ ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì‹œì‘ë˜ì§€ ì•Šì•„ ê±´ë„ˆëœ€: "
                    f"event_name={event_name}, "
                    f"event_type={event_type}, "
                    f"last_node={last_node_name}, "
                    f"answer_generation_started={answer_generation_started}"
                )
                self._skip_log_count += 1
            return False, full_answer, stream_event_count, None
        
        # íƒ€ê²Ÿ ë…¸ë“œ í™•ì¸ ë° í† í° ì¶”ì¶œ
        if self.node_filter.is_target_node(event_name, event_parent, last_node_name):
            logger.info(f"[stream_final_answer] âœ… íƒ€ê²Ÿ ë…¸ë“œ í™•ì¸ë¨: {event_name}, í† í° ì¶”ì¶œ ì‹œì‘")
            
            # ğŸ”¥ [END] í‚¤ì›Œë“œê°€ ì´ë¯¸ ë°œê²¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
            # ì´ë¯¸ [END] ì´í›„ë¼ë©´ ë” ì´ìƒ ì „ì†¡í•˜ì§€ ì•ŠìŒ
            if '[END]' in full_answer.upper():
                logger.debug(
                    f"[stream_final_answer] âš ï¸ [END] í‚¤ì›Œë“œê°€ ì´ë¯¸ ë°œê²¬ë˜ì–´ ì¶”ê°€ í† í° ì „ì†¡ ì¤‘ë‹¨"
                )
                return False, full_answer, stream_event_count, None
            
            token = self.token_extractor.extract_from_event(event_data)
            
            if token:
                stream_event_count += 1
                updated_full_answer = full_answer + token
                
                # ğŸ”¥ [END] í‚¤ì›Œë“œ ì´í›„ ë‚´ìš© í•„í„°ë§
                # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ [END] í‚¤ì›Œë“œ ì°¾ê¸°
                end_keyword_pos = -1
                updated_full_answer_upper = updated_full_answer.upper()
                for keyword in ['[END]', '[END', 'END]']:
                    pos = updated_full_answer_upper.find(keyword.upper())
                    if pos != -1:
                        end_keyword_pos = pos
                        break
                
                if end_keyword_pos != -1:
                    # [END] í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ë©´ ê·¸ ì´í›„ ë‚´ìš©ì€ ì œì™¸
                    updated_full_answer = updated_full_answer[:end_keyword_pos].rstrip()
                    token = updated_full_answer[len(full_answer):] if len(updated_full_answer) > len(full_answer) else ""
                    logger.info(
                        f"[stream_final_answer] âœ… [END] í‚¤ì›Œë“œ ë°œê²¬, ì´í›„ ë‚´ìš© í•„í„°ë§ "
                        f"(ìœ„ì¹˜: {end_keyword_pos}, í•„í„°ë§ëœ í† í° ê¸¸ì´: {len(token)})"
                    )
                
                if token:
                    logger.info(
                        f"[stream_final_answer] âœ… í† í° ì „ì†¡ #{stream_event_count}: "
                        f"token_length={len(token)}, "
                        f"token_preview={token[:50]}..., "
                        f"full_answer_length={len(updated_full_answer)}"
                    )
                    return True, updated_full_answer, stream_event_count, token
                else:
                    # [END] ì´í›„ ë‚´ìš©ë§Œ ìˆì–´ì„œ í•„í„°ë§ë¨
                    logger.debug(
                        f"[stream_final_answer] âš ï¸ [END] ì´í›„ ë‚´ìš©ë§Œ ìˆì–´ í† í° ì „ì†¡ ì¤‘ë‹¨"
                    )
                    return False, updated_full_answer, stream_event_count, None
            else:
                logger.warning(
                    f"[stream_final_answer] âš ï¸ í† í° ì¶”ì¶œ ì‹¤íŒ¨: "
                    f"event_name={event_name}, "
                    f"event_data_keys={list(event_data.keys()) if isinstance(event_data, dict) else []}, "
                    f"event_data_type={type(event_data).__name__}"
                )
        else:
            logger.debug(
                f"[stream_final_answer] íƒ€ê²Ÿ ë…¸ë“œê°€ ì•„ë‹˜ (í•„í„°ë§ë¨): "
                f"type={event_type}, name={event_name}, "
                f"parent={event_parent.get('name', '') if isinstance(event_parent, dict) else ''}, "
                f"last_node={last_node_name}, started={answer_generation_started}"
            )
        
        return True, full_answer, stream_event_count, None
    
    def _validate_and_augment_state(
        self,
        initial_state: Dict[str, Any],
        message: str,
        session_id: str
    ) -> Optional[str]:
        """ìƒíƒœ ê²€ì¦ ë° ë³´ê°•"""
        if "input" not in initial_state:
            initial_state["input"] = {}
        if not initial_state["input"].get("query"):
            initial_state["input"]["query"] = message
        if not initial_state["input"].get("session_id"):
            initial_state["input"]["session_id"] = session_id
        
        if not initial_state.get("query"):
            initial_state["query"] = message
        if not initial_state.get("session_id"):
            initial_state["session_id"] = session_id
        
        initial_query = initial_state.get("input", {}).get("query") or initial_state.get("query")
        if not initial_query or not str(initial_query).strip():
            logger.error(f"Initial state query is empty! Input message was: '{message[:50]}...'")
            return None
        return initial_query

