# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ LangGraph Legal Workflow
ë‹µë³€ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ í–¥ìƒëœ ì›Œí¬í”Œë¡œìš° êµ¬í˜„

ì£¼ìš” ê¸°ëŠ¥:
- ê¸´ê¸‰ë„ í‰ê°€ (Urgency Assessment)
- ë²•ë¥ ë¶„ì•¼ ë¶„ë¥˜ ê°•í™” (Legal Field Classification)
- ë²•ë ¹ ê²€ì¦ (Legal Basis Validation)
- ë¬¸ì„œ ë¶„ì„ (Document Analysis) - ê³„ì•½ì„œ/ê³ ì†Œì¥ ë“±
- ì „ë¬¸ê°€ ë¼ìš°íŒ… (Expert Router) - ê°€ì¡±ë²•/ê¸°ì—…ë²•/ì§€ì ì¬ì‚°ê¶Œ
- ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬ (Multi-turn Conversation)
"""

import asyncio
import logging
import re
import time
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Union

from langchain_community.llms import Ollama
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import END, StateGraph

# Langfuse observe ë°ì½”ë ˆì´í„° ì¶”ê°€
try:
    from langfuse import observe
    LANGFUSE_OBSERVE_AVAILABLE = True
except ImportError:
    LANGFUSE_OBSERVE_AVAILABLE = False
    # Mock observe decorator
    def observe(**kwargs):
        def decorator(func):
            return func
        return decorator

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from core.agents.answer_formatter import AnswerFormatterHandler
from core.agents.answer_generator import AnswerGenerator
from core.agents.chain_builders import (
    AnswerGenerationChainBuilder,
    ClassificationChainBuilder,
    DirectAnswerChainBuilder,
    DocumentAnalysisChainBuilder,
    QueryEnhancementChainBuilder,
)
from core.agents.classification_handler import ClassificationHandler
from core.agents.context_builder import ContextBuilder
from core.agents.extractors import (
    DocumentExtractor,
    QueryExtractor,
    ResponseExtractor,
)
from core.agents.keyword_mapper import LegalKeywordMapper
from core.agents.legal_data_connector_v2 import LegalDataConnectorV2
from core.agents.node_wrappers import with_state_optimization
from core.agents.performance_optimizer import PerformanceOptimizer
from core.agents.prompt_builders import PromptBuilder, QueryBuilder
from core.agents.prompt_chain_executor import PromptChainExecutor
from core.agents.quality_validators import (
    AnswerValidator,
    ContextValidator,
    SearchValidator,
)
from core.agents.query_enhancer import QueryEnhancer
from core.agents.reasoning_extractor import ReasoningExtractor
from core.agents.response_parsers import (
    AnswerParser,
    ClassificationParser,
    DocumentParser,
    QueryParser,
)
from core.agents.search_handler import SearchHandler
from core.agents.state_definitions import LegalWorkflowState
from core.agents.state_utils import (
    MAX_DOCUMENT_CONTENT_LENGTH,
    MAX_PROCESSING_STEPS,
    MAX_RETRIEVED_DOCS,
    prune_processing_steps,
    prune_retrieved_docs,
)
from core.agents.workflow_constants import (
    AnswerExtractionPatterns,
    QualityThresholds,
    RetryConfig,
    WorkflowConstants,
)
from core.agents.workflow_routes import WorkflowRoutes
from core.agents.workflow_utils import WorkflowUtils
from core.services.search.question_classifier import QuestionType
from core.services.search.result_merger import ResultMerger, ResultRanker
from infrastructure.utils.langgraph_config import LangGraphConfig
from source.services.term_integration_system import TermIntegrator
from source.services.unified_prompt_manager import (
    LegalDomain,
    ModelType,
    UnifiedPromptManager,
)

# Logger ì´ˆê¸°í™”
logger = logging.getLogger(__name__)

# AnswerStructureEnhancer í†µí•© (ë‹µë³€ êµ¬ì¡°í™” ë° ë²•ì  ê·¼ê±° ê°•í™”)
try:
    from source.services.answer_structure_enhancer import AnswerStructureEnhancer
    ANSWER_STRUCTURE_ENHANCER_AVAILABLE = True
except ImportError:
    ANSWER_STRUCTURE_ENHANCER_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("AnswerStructureEnhancer not available")


class QueryComplexity(str, Enum):
    """ì§ˆë¬¸ ë³µì¡ë„"""
    SIMPLE = "simple"           # ê²€ìƒ‰ ë¶ˆí•„ìš” (ì¼ë°˜ ìƒì‹, ì •ì˜)
    MODERATE = "moderate"       # ë‹¨ì¼ ê²€ìƒ‰ í•„ìš”
    COMPLEX = "complex"         # ë‹¤ì¤‘ ê²€ìƒ‰ í•„ìš”
    MULTI_HOP = "multi_hop"     # ì¶”ë¡  ì²´ì¸ í•„ìš”


class RetryCounterManager:
    """ì¬ì‹œë„ ì¹´ìš´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, logger):
        self.logger = logger

    def get_retry_counts(self, state: LegalWorkflowState) -> Dict[str, int]:
        """
        ëª¨ë“  ê²½ë¡œì—ì„œ ì¬ì‹œë„ ì¹´ìš´í„° ì•ˆì „í•˜ê²Œ ì½ê¸°

        Args:
            state: LegalWorkflowState

        Returns:
            ì¬ì‹œë„ ì¹´ìš´í„° ë”•ì…”ë„ˆë¦¬ (generation, validation, total)
        """
        generation_retry = 0
        validation_retry = 0

        # 1ìˆœìœ„: common.metadata (ìƒíƒœ ìµœì í™”ì—ì„œ í•­ìƒ í¬í•¨ë¨)
        if "common" in state and isinstance(state.get("common"), dict):
            common_meta = state["common"].get("metadata", {})
            if isinstance(common_meta, dict):
                generation_retry = max(generation_retry, common_meta.get("generation_retry_count", 0))
                validation_retry = max(validation_retry, common_meta.get("validation_retry_count", 0))

        # 2ìˆœìœ„: ìµœìƒìœ„ ë ˆë²¨ retry_count
        top_level_retry = state.get("retry_count", 0)
        generation_retry = max(generation_retry, top_level_retry)

        # 3ìˆœìœ„: ìµœìƒìœ„ ë ˆë²¨ _generation_retry_count, _validation_retry_count
        if isinstance(state, dict):
            generation_retry = max(generation_retry, state.get("_generation_retry_count", 0))
            validation_retry = max(validation_retry, state.get("_validation_retry_count", 0))

        # 4ìˆœìœ„: metadata ì§ì ‘ í™•ì¸
        metadata = state.get("metadata", {})
        if isinstance(metadata, dict):
            generation_retry = max(generation_retry, metadata.get("generation_retry_count", 0))
            validation_retry = max(validation_retry, metadata.get("validation_retry_count", 0))

        total = generation_retry + validation_retry

        return {
            "generation": generation_retry,
            "validation": validation_retry,
            "total": total
        }

    def increment_retry_count(self, state: LegalWorkflowState, retry_type: str) -> int:
        """
        ì¬ì‹œë„ ì¹´ìš´í„° ì•ˆì „í•˜ê²Œ ì¦ê°€ (ëª¨ë“  ê²½ë¡œì— ì €ì¥)

        Args:
            state: LegalWorkflowState
            retry_type: "generation" ë˜ëŠ” "validation"

        Returns:
            ì¦ê°€åçš„ ì¬ì‹œë„ íšŸìˆ˜
        """
        counts = self.get_retry_counts(state)

        if retry_type == "generation":
            new_count = counts["generation"] + 1
            if new_count > RetryConfig.MAX_GENERATION_RETRIES:
                self.logger.warning(
                    f"Generation retry count would exceed limit: {new_count} > {RetryConfig.MAX_GENERATION_RETRIES}"
                )
                new_count = RetryConfig.MAX_GENERATION_RETRIES
        elif retry_type == "validation":
            new_count = counts["validation"] + 1
            if new_count > RetryConfig.MAX_VALIDATION_RETRIES:
                self.logger.warning(
                    f"Validation retry count would exceed limit: {new_count} > {RetryConfig.MAX_VALIDATION_RETRIES}"
                )
                new_count = RetryConfig.MAX_VALIDATION_RETRIES
        else:
            self.logger.error(f"Unknown retry_type: {retry_type}")
            return counts.get(retry_type, 0)

        # ëª¨ë“  ê²½ë¡œì— ì €ì¥
        self._save_retry_count(state, retry_type, new_count)

        self.logger.info(
            f"âœ… [RETRY] {retry_type.capitalize()} retry count: {counts[retry_type]} â†’ {new_count}"
        )

        return new_count

    def _save_retry_count(self, state: LegalWorkflowState, retry_type: str, count: int) -> None:
        """ì¬ì‹œë„ ì¹´ìš´í„°ë¥¼ ëª¨ë“  ê²½ë¡œì— ì €ì¥"""
        key = f"{retry_type}_retry_count"

        # metadataì— ì €ì¥
        if "metadata" not in state or not isinstance(state.get("metadata"), dict):
            state["metadata"] = {}
        state["metadata"][key] = count

        # common.metadataì— ì €ì¥
        if "common" not in state or not isinstance(state.get("common"), dict):
            state["common"] = {}
        if "metadata" not in state["common"]:
            state["common"]["metadata"] = {}
        state["common"]["metadata"][key] = count

        # ìµœìƒìœ„ ë ˆë²¨ì— ì €ì¥ (ì¡°ê±´ë¶€ ì—£ì§€ ì ‘ê·¼ìš©)
        if retry_type == "generation":
            state["retry_count"] = count
            state["_generation_retry_count"] = count
        elif retry_type == "validation":
            state["_validation_retry_count"] = count

    def should_allow_retry(self, state: LegalWorkflowState, retry_type: str) -> bool:
        """
        ì¬ì‹œë„ í—ˆìš© ì—¬ë¶€ í™•ì¸

        Args:
            state: LegalWorkflowState
            retry_type: "generation" ë˜ëŠ” "validation"

        Returns:
            ì¬ì‹œë„ í—ˆìš© ì—¬ë¶€
        """
        counts = self.get_retry_counts(state)

        # ì „ì—­ ì¬ì‹œë„ íšŸìˆ˜ ì²´í¬
        if counts["total"] >= RetryConfig.MAX_TOTAL_RETRIES:
            self.logger.warning(
                f"Maximum total retry count ({RetryConfig.MAX_TOTAL_RETRIES}) reached"
            )
            return False

        # ê°œë³„ ì¬ì‹œë„ íšŸìˆ˜ ì²´í¬
        if retry_type == "generation":
            if counts["generation"] >= RetryConfig.MAX_GENERATION_RETRIES:
                return False
        elif retry_type == "validation":
            if counts["validation"] >= RetryConfig.MAX_VALIDATION_RETRIES:
                return False

        return True


class EnhancedLegalQuestionWorkflow:
    """ê°œì„ ëœ ë²•ë¥  ì§ˆë¬¸ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°"""

    def __init__(self, config: LangGraphConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # í†µí•© í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™” (ìš°ì„ )
        self.unified_prompt_manager = UnifiedPromptManager()

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.keyword_mapper = LegalKeywordMapper()
        self.data_connector = LegalDataConnectorV2()
        self.performance_optimizer = PerformanceOptimizer()
        self.term_integrator = TermIntegrator()
        self.result_merger = ResultMerger()
        self.result_ranker = ResultRanker()

        # ì¬ì‹œë„ ì¹´ìš´í„° ê´€ë¦¬ì ì´ˆê¸°í™”
        self.retry_manager = RetryCounterManager(self.logger)

        # ì¶”ë¡  ê³¼ì • ë¶„ë¦¬ ëª¨ë“ˆ ì´ˆê¸°í™” (Phase 1 ë¦¬íŒ©í† ë§)
        self.reasoning_extractor = ReasoningExtractor(logger=self.logger)

        # AnswerStructureEnhancer ì´ˆê¸°í™” (ë‹µë³€ êµ¬ì¡°í™” ë° ë²•ì  ê·¼ê±° ê°•í™”)
        if ANSWER_STRUCTURE_ENHANCER_AVAILABLE:
            self.answer_structure_enhancer = AnswerStructureEnhancer()
            self.logger.info("AnswerStructureEnhancer initialized for answer quality enhancement")
        else:
            self.answer_structure_enhancer = None
            self.logger.warning("AnswerStructureEnhancer not available")

        # AnswerFormatter ì´ˆê¸°í™” (ì‹œê°ì  í¬ë§·íŒ…)
        try:
            from core.services.generation.answer_formatter import AnswerFormatter
            self.answer_formatter = AnswerFormatter()
            self.logger.info("AnswerFormatter initialized for visual formatting")
        except Exception as e:
            self.logger.warning(f"Failed to initialize AnswerFormatter: {e}")
            self.answer_formatter = None

        # Semantic Search Engine ì´ˆê¸°í™” (ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ - lawfirm_v2_faiss.index ì‚¬ìš©)
        try:
            from source.services.semantic_search_engine_v2 import SemanticSearchEngineV2
            from source.utils.config import Config
            # lawfirm_v2.db ê¸°ë°˜ìœ¼ë¡œ ìë™ìœ¼ë¡œ ./data/lawfirm_v2_faiss.index ì‚¬ìš©
            config = Config()
            db_path = config.database_path
            self.semantic_search = SemanticSearchEngineV2(db_path=db_path)
            self.logger.info(f"SemanticSearchEngineV2 initialized successfully with {db_path}")
        except Exception as e:
            self.logger.warning(f"Failed to initialize SemanticSearchEngineV2: {e}")
            self.semantic_search = None

        # ê²€ìƒ‰ í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” (Phase 2 ë¦¬íŒ©í† ë§) - semantic_search ì´ˆê¸°í™” ì´í›„
        self.search_handler = SearchHandler(
            semantic_search=self.semantic_search,
            keyword_mapper=self.keyword_mapper,
            data_connector=self.data_connector,
            result_merger=self.result_merger,
            result_ranker=self.result_ranker,
            performance_optimizer=self.performance_optimizer,
            config=self.config,
            logger=self.logger
        )

        # ì»¨í…ìŠ¤íŠ¸ ë¹Œë” ì´ˆê¸°í™” (Phase 6 ë¦¬íŒ©í† ë§) - semantic_search ì´ˆê¸°í™” ì´í›„
        self.context_builder = ContextBuilder(
            semantic_search=self.semantic_search,
            config=self.config,
            logger=self.logger
        )

        # MultiTurnQuestionHandler ì´ˆê¸°í™” (ë©€í‹°í„´ ì§ˆë¬¸ ì²˜ë¦¬)
        try:
            from source.services.conversation_manager import ConversationManager
            from source.services.multi_turn_handler import MultiTurnQuestionHandler
            self.multi_turn_handler = MultiTurnQuestionHandler()
            self.conversation_manager = ConversationManager()
            self.logger.info("MultiTurnQuestionHandler initialized successfully")
        except Exception as e:
            self.logger.warning(f"Failed to initialize MultiTurnQuestionHandler: {e}")
            self.multi_turn_handler = None
            self.conversation_manager = None

        # AIKeywordGenerator ì´ˆê¸°í™” (AI í‚¤ì›Œë“œ í™•ì¥)
        try:
            from source.services.ai_keyword_generator import AIKeywordGenerator
            self.ai_keyword_generator = AIKeywordGenerator()
            self.logger.info("AIKeywordGenerator initialized successfully")
        except Exception as e:
            self.logger.warning(f"Failed to initialize AIKeywordGenerator: {e}")
            self.ai_keyword_generator = None

        # EmotionIntentAnalyzer ì´ˆê¸°í™” (ê¸´ê¸‰ë„ í‰ê°€ìš©)
        try:
            from source.services.emotion_intent_analyzer import EmotionIntentAnalyzer
            self.emotion_analyzer = EmotionIntentAnalyzer()
            self.logger.info("EmotionIntentAnalyzer initialized")
        except Exception as e:
            self.logger.warning(f"Failed to initialize EmotionIntentAnalyzer: {e}")
            self.emotion_analyzer = None

        # LegalBasisValidator ì´ˆê¸°í™” (ë²•ë ¹ ê²€ì¦ìš©)
        try:
            from source.services.legal_basis_validator import LegalBasisValidator
            self.legal_validator = LegalBasisValidator()
            self.logger.info("LegalBasisValidator initialized")
        except Exception as e:
            self.logger.warning(f"Failed to initialize LegalBasisValidator: {e}")
            self.legal_validator = None

        # DocumentProcessor ì´ˆê¸°í™” (ë¬¸ì„œ ë¶„ì„ìš©)
        try:
            from infrastructure.utils.config import Config as UtilsConfig
            from source.services.document_processor import LegalDocumentProcessor
            utils_config = UtilsConfig()
            self.document_processor = LegalDocumentProcessor(utils_config)
            self.logger.info("LegalDocumentProcessor initialized")
        except Exception as e:
            self.logger.warning(f"Failed to initialize LegalDocumentProcessor: {e}")
            self.document_processor = None

        # ConfidenceCalculator ì´ˆê¸°í™” (ì‹ ë¢°ë„ ê³„ì‚°ìš©)
        try:
            from core.services.enhancement.confidence_calculator import (
                ConfidenceCalculator,
            )
            self.confidence_calculator = ConfidenceCalculator()
            self.logger.info("ConfidenceCalculator initialized")
        except Exception as e:
            self.logger.warning(f"Failed to initialize ConfidenceCalculator: {e}")
            self.confidence_calculator = None

        # LLM ì´ˆê¸°í™”
        self.llm = self._initialize_llm()

        # ë¹ ë¥¸ LLM ì´ˆê¸°í™” (ê°„ë‹¨í•œ ì§ˆë¬¸ìš©)
        self.llm_fast = self._initialize_llm_fast()

        # ë‹µë³€ ìƒì„± í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” (Phase 5 ë¦¬íŒ©í† ë§) - LLM ì´ˆê¸°í™” ì´í›„
        self.answer_generator = AnswerGenerator(
            llm=self.llm,
            logger=self.logger
        )

        # ì›Œí¬í”Œë¡œìš° ë¼ìš°íŒ… í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” (Phase 9 ë¦¬íŒ©í† ë§) - answer_generator ì´ˆê¸°í™” ì´í›„
        self.workflow_routes = WorkflowRoutes(
            retry_manager=self.retry_manager,
            answer_generator=self.answer_generator,
            ai_keyword_generator=self.ai_keyword_generator,
            logger=self.logger
        )

        # ë‹µë³€ í¬ë§·íŒ… í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” (Phase 4 ë¦¬íŒ©í† ë§) - í•„ìš”í•œ ì˜ì¡´ì„± ì´ˆê¸°í™” ì´í›„
        self.answer_formatter_handler = AnswerFormatterHandler(
            keyword_mapper=self.keyword_mapper,
            answer_structure_enhancer=self.answer_structure_enhancer,
            answer_formatter=self.answer_formatter,
            confidence_calculator=self.confidence_calculator,
            reasoning_extractor=self.reasoning_extractor,
            answer_generator=self.answer_generator,
            logger=self.logger
        )

        # ì¿¼ë¦¬ ê°•í™” ìºì‹œ ì´ˆê¸°í™”
        # Phase 7 ë¦¬íŒ©í† ë§: ì¿¼ë¦¬ ê°•í™” ìºì‹œëŠ” QueryEnhancerë¡œ ì´ë™ë¨

        # ë³µì¡ë„ ë¶„ë¥˜ ìºì‹œ ì´ˆê¸°í™”
        self._complexity_cache: Dict[str, Tuple[QueryComplexity, bool]] = {}

        # í†µí•© ë¶„ë¥˜ ìºì‹œ ì´ˆê¸°í™” (ì§ˆë¬¸ ìœ í˜• + ë³µì¡ë„ ë™ì‹œ ë¶„ë¥˜)
        self._classification_cache: Dict[str, Tuple[QuestionType, float, QueryComplexity, bool]] = {}


        # í†µê³„ ê´€ë¦¬ (configì—ì„œ í™œì„±í™” ì—¬ë¶€ í™•ì¸)
        self.stats = {
            'total_queries': 0,
            'total_documents_retrieved': 0,
            'avg_response_time': 0.0,
            'avg_confidence': 0.0,
            'total_errors': 0,
            'llm_complexity_classifications': 0,
            'complexity_cache_hits': 0,
            'complexity_cache_misses': 0,
            'complexity_fallback_count': 0,
            'avg_complexity_classification_time': 0.0
        } if self.config.enable_statistics else None

        # ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶•
        self.graph = self._build_graph()
        logger.info("EnhancedLegalQuestionWorkflow initialized with UnifiedPromptManager.")

    def _initialize_llm(self):
        """LLM ì´ˆê¸°í™” (Google Gemini ìš°ì„ , Ollama ë°±ì—…)"""
        if self.config.llm_provider == "google":
            try:
                return self._initialize_gemini()
            except Exception as e:
                logger.warning(f"Failed to initialize Google Gemini LLM: {e}. Falling back to Ollama.")

        if self.config.llm_provider == "ollama":
            try:
                return self._initialize_ollama()
            except Exception as e:
                logger.warning(f"Failed to initialize Ollama LLM: {e}. Using Mock LLM.")

        return self._create_mock_llm()

    def _initialize_gemini(self):
        """Google Gemini LLM ì´ˆê¸°í™”"""
        gemini_llm = ChatGoogleGenerativeAI(
            model=self.config.google_model,
            temperature=WorkflowConstants.TEMPERATURE,
            max_output_tokens=WorkflowConstants.MAX_OUTPUT_TOKENS,
            timeout=WorkflowConstants.TIMEOUT,
            api_key=self.config.google_api_key
        )
        logger.info(f"Initialized Google Gemini LLM: {self.config.google_model}")
        return gemini_llm

    def _initialize_ollama(self):
        """Ollama LLM ì´ˆê¸°í™”"""
        ollama_llm = Ollama(
            model=self.config.ollama_model,
            base_url=self.config.ollama_base_url,
            temperature=WorkflowConstants.TEMPERATURE,
            num_predict=WorkflowConstants.MAX_OUTPUT_TOKENS,
            timeout=20
        )
        logger.info(f"Initialized Ollama LLM: {self.config.ollama_model}")
        return ollama_llm

    def _create_mock_llm(self):
        """Mock LLM ìƒì„±"""
        class MockLLM:
            def invoke(self, prompt):
                return "Mock LLM response for: " + prompt
            async def ainvoke(self, prompt):
                return "Mock LLM async response for: " + prompt

        logger.warning("No valid LLM provider configured or failed to initialize. Using Mock LLM.")
        return MockLLM()

    def _initialize_llm_fast(self):
        """ë¹ ë¥¸ LLM ì´ˆê¸°í™” (ê°„ë‹¨í•œ ì§ˆë¬¸ìš© - Gemini Flash ë˜ëŠ” ì‘ì€ ëª¨ë¸)"""
        if self.config.llm_provider == "google":
            try:
                # Gemini Flash ëª¨ë¸ ì‚¬ìš© (ë” ë¹ ë¦„)
                flash_model = "gemini-1.5-flash"
                if self.config.google_model and "flash" in self.config.google_model.lower():
                    flash_model = self.config.google_model

                gemini_llm_fast = ChatGoogleGenerativeAI(
                    model=flash_model,
                    temperature=0.3,
                    max_output_tokens=500,  # ê°„ë‹¨í•œ ë‹µë³€ë§Œ í•„ìš”
                    timeout=10,
                    api_key=self.config.google_api_key
                )
                logger.info(f"Initialized fast LLM: {flash_model}")
                return gemini_llm_fast
            except Exception as e:
                logger.warning(f"Failed to initialize fast LLM: {e}. Using main LLM.")
                return self.llm

        # ê¸°ë³¸ LLM ì‚¬ìš©
        return self.llm

    def _build_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶• - Adaptive RAG ë° ìµœì í™” ì ìš©"""
        workflow = StateGraph(LegalWorkflowState)

        # í†µí•©ëœ ì§ˆë¬¸ ë¶„ë¥˜ ë° ë³µì¡ë„ íŒë‹¨ ë…¸ë“œ (Phase 4)
        workflow.add_node("classify_query_and_complexity", self.classify_query_and_complexity)

        # ì§ì ‘ ë‹µë³€ (ê°„ë‹¨í•œ ì§ˆë¬¸ìš©)
        workflow.add_node("direct_answer", self.direct_answer_node)

        # ë³‘ë ¬ ë¶„ë¥˜ (ê¸´ê¸‰ë„ + ë©€í‹°í„´)
        workflow.add_node("classification_parallel", self.classification_parallel)
        workflow.add_node("assess_urgency", self.assess_urgency)
        workflow.add_node("resolve_multi_turn", self.resolve_multi_turn)
        workflow.add_node("route_expert", self.route_expert)  # Phase 9: route_expertëŠ” ë˜í¼ ë©”ì„œë“œë¡œ ìœ ì§€
        workflow.add_node("analyze_document", self.analyze_document)

        # í‚¤ì›Œë“œ í™•ì¥ ë° ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ ë…¸ë“œ (Phase 6 - ì„œë¸Œë…¸ë“œë¡œ ë¶„ë¦¬)
        workflow.add_node("expand_keywords", self.expand_keywords)
        workflow.add_node("prepare_search_query", self.prepare_search_query)

        # ê°œì„ ëœ ê²€ìƒ‰ ë…¸ë“œë“¤
        workflow.add_node("execute_searches_parallel", self.execute_searches_parallel)

        # ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ í†µí•© (6ê°œ ë…¸ë“œ ë³‘í•©)
        workflow.add_node("process_search_results_combined", self.process_search_results_combined)

        # í†µí•©ëœ ë¬¸ì„œ ì¤€ë¹„ ë° ìš©ì–´ ì²˜ë¦¬ ë…¸ë“œ (Phase 3)
        workflow.add_node("prepare_documents_and_terms", self.prepare_documents_and_terms)

        # í†µí•©ëœ ë‹µë³€ ìƒì„±, ê²€ì¦, í¬ë§·íŒ… ë° ìµœì¢… ì¤€ë¹„ ë…¸ë“œ (Phase 5 + Phase 2 í†µí•©)
        workflow.add_node("generate_and_validate_answer", self.generate_and_validate_answer)


        # Entry point
        workflow.set_entry_point("classify_query_and_complexity")

        # ë³µì¡ë„ ë¶„ë¥˜ í›„ ë¼ìš°íŒ… (Phase 4)
        # Phase 9 ë¦¬íŒ©í† ë§: ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ë¥¼ WorkflowRoutesë¡œ ì´ë™
        workflow.add_conditional_edges(
            "classify_query_and_complexity",
            self._route_by_complexity,  # ë˜í¼ ë©”ì„œë“œ ì‚¬ìš©
            {
                "simple": "direct_answer",      # ê°„ë‹¨í•œ ì§ˆë¬¸ â†’ ì§ì ‘ ë‹µë³€
                "moderate": "classification_parallel",  # ì¤‘ê°„ ì§ˆë¬¸ â†’ ë³‘ë ¬ ë¶„ë¥˜
                "complex": "classification_parallel",  # ë³µì¡í•œ ì§ˆë¬¸ â†’ ë³‘ë ¬ ë¶„ë¥˜
            }
        )

        # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ì§ì ‘ ë‹µë³€ ìƒì„± í›„ ENDë¡œ (í¬ë§·íŒ…ì€ direct_answer_node ë‚´ë¶€ì—ì„œ ì²˜ë¦¬)
        workflow.add_edge("direct_answer", END)

        # ë³‘ë ¬ ë¶„ë¥˜ í›„ ì „ë¬¸ê°€ ë¼ìš°íŒ…
        workflow.add_edge("classification_parallel", "route_expert")

        # ì¡°ê±´ë¶€: ì „ë¬¸ê°€ ë¼ìš°íŒ… í›„ ë¬¸ì„œ ë¶„ì„ ì—¬ë¶€
        # Phase 9 ë¦¬íŒ©í† ë§: ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ë¥¼ WorkflowRoutesë¡œ ì´ë™
        workflow.add_conditional_edges(
            "route_expert",
            self._should_analyze_document,  # ë˜í¼ ë©”ì„œë“œ ì‚¬ìš©
            {
                "analyze": "analyze_document",
                "skip": "expand_keywords"
            }
        )

        workflow.add_edge("analyze_document", "expand_keywords")

        # í‚¤ì›Œë“œ í™•ì¥ â†’ ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ (Phase 6)
        workflow.add_edge("expand_keywords", "prepare_search_query")

        # ê°œì„ ëœ ê²€ìƒ‰ í”Œë¡œìš° (Adaptive RAG ì ìš©, Phase 6)
        # Phase 9 ë¦¬íŒ©í† ë§: ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ë¥¼ WorkflowRoutesë¡œ ì´ë™
        workflow.add_conditional_edges(
            "prepare_search_query",
            self._should_skip_search_adaptive,  # ë˜í¼ ë©”ì„œë“œ ì‚¬ìš©
            {
                "skip": "prepare_documents_and_terms",
                "continue": "execute_searches_parallel"
            }
        )

        # ë³‘ë ¬ ê²€ìƒ‰ â†’ í†µí•© ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ (6ê°œ ë…¸ë“œ ë³‘í•©)
        workflow.add_edge("execute_searches_parallel", "process_search_results_combined")
        workflow.add_edge("process_search_results_combined", "prepare_documents_and_terms")
        workflow.add_edge("prepare_documents_and_terms", "generate_and_validate_answer")

        # í†µí•©ëœ ë‹µë³€ ìƒì„±, ê²€ì¦, í¬ë§·íŒ… í›„ ì œí•œëœ ì¬ì‹œë„ (Phase 5 + Phase 2 í†µí•©)
        # Phase 9 ë¦¬íŒ©í† ë§: ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ë¥¼ WorkflowRoutesë¡œ ì´ë™
        workflow.add_conditional_edges(
            "generate_and_validate_answer",
            self._should_retry_validation,  # ë˜í¼ ë©”ì„œë“œ ì‚¬ìš©
            {
                "accept": END,  # í¬ë§·íŒ… ì™„ë£Œ í›„ ì§ì ‘ ENDë¡œ
                "retry_generate": "generate_and_validate_answer",
                "retry_search": "expand_keywords"
            }
        )

        return workflow

    @observe(name="expand_keywords")
    @with_state_optimization("expand_keywords", enable_reduction=False)
    def expand_keywords(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """í‚¤ì›Œë“œ í™•ì¥ ì „ìš© ë…¸ë“œ (Part 1)"""
        try:
            start_time = time.time()

            # metadata ë³´ì¡´
            preserved_complexity = state.get("metadata", {}).get("query_complexity") if isinstance(state.get("metadata"), dict) else None
            preserved_needs_search = state.get("metadata", {}).get("needs_search") if isinstance(state.get("metadata"), dict) else None

            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            state["metadata"] = dict(state["metadata"])
            if preserved_complexity:
                state["metadata"]["query_complexity"] = preserved_complexity
            if preserved_needs_search is not None:
                state["metadata"]["needs_search"] = preserved_needs_search
            state["metadata"]["_last_executed_node"] = "expand_keywords"

            if "common" not in state or not isinstance(state.get("common"), dict):
                state["common"] = {}
            if "metadata" not in state["common"]:
                state["common"]["metadata"] = {}
            state["common"]["metadata"]["_last_executed_node"] = "expand_keywords"

            # AI í‚¤ì›Œë“œ í™•ì¥ (ì¡°ê±´ë¶€)
            if self.ai_keyword_generator:
                keywords = self._get_state_value(state, "extracted_keywords", [])
                if len(keywords) == 0:
                    query = self._get_state_value(state, "query", "")
                    query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", "general_question"))
                    keywords = self.keyword_mapper.get_keywords_for_question(query, query_type_str)
                    keywords = [kw for kw in keywords if isinstance(kw, (str, int, float, tuple)) and kw is not None]
                    keywords = list(set(keywords))
                    self._set_state_value(state, "extracted_keywords", keywords)

                query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
                domain = self._get_domain_from_query_type(query_type_str)

                try:
                    expansion_result = asyncio.run(
                        self.ai_keyword_generator.expand_domain_keywords(
                            domain=domain,
                            base_keywords=keywords,
                            target_count=30
                        )
                    )

                    if expansion_result.api_call_success:
                        all_keywords = keywords + expansion_result.expanded_keywords
                        all_keywords = [kw for kw in all_keywords if isinstance(kw, (str, int, float, tuple)) and kw is not None]
                        all_keywords = list(set(all_keywords))
                        self._set_state_value(state, "extracted_keywords", all_keywords)
                        self._set_state_value(state, "ai_keyword_expansion", {
                            "domain": expansion_result.domain,
                            "original_keywords": expansion_result.base_keywords,
                            "expanded_keywords": expansion_result.expanded_keywords,
                            "confidence": expansion_result.confidence,
                            "method": expansion_result.expansion_method
                        })
                        self.logger.info(
                            f"âœ… [KEYWORD EXPANSION] Expanded {len(keywords)} â†’ {len(all_keywords)} keywords "
                            f"(domain: {domain}, method: {expansion_result.expansion_method})"
                        )
                    else:
                        fallback_keywords = self.ai_keyword_generator.expand_keywords_with_fallback(domain, keywords)
                        all_keywords = keywords + fallback_keywords
                        all_keywords = [kw for kw in all_keywords if isinstance(kw, (str, int, float, tuple)) and kw is not None]
                        all_keywords = list(set(all_keywords))
                        self._set_state_value(state, "extracted_keywords", all_keywords)
                        self._set_state_value(state, "ai_keyword_expansion", {
                            "domain": domain,
                            "original_keywords": keywords,
                            "expanded_keywords": fallback_keywords,
                            "confidence": 0.5,
                            "method": "fallback"
                        })
                        self.logger.info(
                            f"âš ï¸ [KEYWORD EXPANSION] Used fallback: {len(keywords)} â†’ {len(all_keywords)} keywords"
                        )
                except Exception as e:
                    self.logger.warning(f"AI keyword expansion failed: {e}")

            self._save_metadata_safely(state, "_last_executed_node", "expand_keywords")
            self._update_processing_time(state, start_time)
            self._add_step(state, "í‚¤ì›Œë“œ í™•ì¥", f"í‚¤ì›Œë“œ í™•ì¥ ì™„ë£Œ: {len(self._get_state_value(state, 'extracted_keywords', []))}ê°œ")

        except Exception as e:
            self._handle_error(state, str(e), "í‚¤ì›Œë“œ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

        return state

    # Phase 9 ë¦¬íŒ©í† ë§: ë¼ìš°íŒ… ê´€ë ¨ ë©”ì„œë“œëŠ” WorkflowRoutesë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _should_expand_keywords_ai(self, state: LegalWorkflowState) -> str:
        """WorkflowRoutes.should_expand_keywords_ai ë˜í¼"""
        return self.workflow_routes.should_expand_keywords_ai(state)

    def _should_retry_generation(self, state: LegalWorkflowState) -> str:
        """WorkflowRoutes.should_retry_generation ë˜í¼"""
        return self.workflow_routes.should_retry_generation(state)

    def _should_retry_validation(self, state: LegalWorkflowState) -> str:
        """WorkflowRoutes.should_retry_validation ë˜í¼"""
        return self.workflow_routes.should_retry_validation(state, answer_generator=self.answer_generator)


    @observe(name="validate_answer_quality")
    @with_state_optimization("validate_answer_quality", enable_reduction=False)
    def validate_answer_quality(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ë‹µë³€ í’ˆì§ˆ ë° ë²•ë ¹ ê²€ì¦"""
        try:
            # ì‹¤í–‰ ê¸°ë¡ ì €ì¥ (í—¬í¼ ë©”ì„œë“œ ì‚¬ìš©)
            self._save_metadata_safely(state, "_last_executed_node", "validate_answer_quality")

            start_time = time.time()
            answer = self._normalize_answer(self._get_state_value(state, "answer", ""))
            errors = self._get_state_value(state, "errors", [])
            sources = self._get_state_value(state, "sources", [])

            # ë‹µë³€ ë‚´ìš© ë¡œê¹… (í’ˆì§ˆ ê²€ì¦ ì „)
            # answerê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸ í›„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            answer_content_preview = ""
            if isinstance(answer, str):
                answer_content_preview = answer[:500] if len(answer) > 500 else answer
            else:
                answer_str = str(answer)
                answer_content_preview = answer_str[:500] if len(answer_str) > 500 else answer_str

            answer_length = len(answer) if isinstance(answer, str) else len(str(answer))
            self.logger.info(
                f"ğŸ” [QUALITY VALIDATION] Answer received for validation:\n"
                f"   Answer length: {answer_length} characters\n"
                f"   Answer content: '{answer_content_preview}'\n"
                f"   Answer type: {type(answer).__name__}\n"
                f"   Error count: {len(errors)}\n"
                f"   Source count: {len(sources)}"
            )

            # í’ˆì§ˆ ê²€ì¦
            # answerê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸ í›„ ê¸¸ì´ ê³„ì‚°
            answer_str_for_check = answer if isinstance(answer, str) else str(answer) if answer else ""
            quality_checks = {
                "has_answer": len(answer_str_for_check) > 0,
                "min_length": len(answer_str_for_check) >= WorkflowConstants.MIN_ANSWER_LENGTH_VALIDATION,
                "no_errors": len(errors) == 0,
                "has_sources": len(sources) > 0 or len(self._get_state_value(state, "retrieved_docs", [])) > 0
            }

            # ë²•ë ¹ ê²€ì¦ ì¶”ê°€
            if self.legal_validator and len(answer_str_for_check) > 0:
                try:
                    query = self._get_state_value(state, "query", "")
                    # answerê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸ (ë²•ë ¹ ê²€ì¦ì—ëŠ” ë¬¸ìì—´ë§Œ ì „ë‹¬)
                    answer_for_validation = answer if isinstance(answer, str) else answer_str_for_check
                    validation_result = self.legal_validator.validate_legal_basis(query, answer_for_validation)
                    self._set_state_value(state, "legal_validity_check", validation_result.is_valid)
                    self._set_state_value(state, "legal_basis_validation", {
                        "confidence": validation_result.confidence,
                        "issues": validation_result.issues,
                        "recommendations": validation_result.recommendations
                    })
                    quality_checks["legal_basis_valid"] = validation_result.is_valid
                    self.logger.info(f"Legal basis validation: {validation_result.is_valid}")
                except Exception as e:
                    self.logger.warning(f"Legal validation failed: {e}")
                    self._set_state_value(state, "legal_validity_check", True)
                    quality_checks["legal_basis_valid"] = True
            else:
                self._set_state_value(state, "legal_validity_check", True)
                quality_checks["legal_basis_valid"] = True

            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            passed_checks = sum(quality_checks.values())
            total_checks = len(quality_checks)
            quality_score = passed_checks / total_checks

            # í’ˆì§ˆ ê²€ì¦ ìƒì„¸ ê²°ê³¼ ë¡œê¹…
            self.logger.info(
                f"ğŸ“Š [QUALITY CHECKS] Detailed validation results:\n"
                f"   Quality checks: {quality_checks}\n"
                f"   Passed checks: {passed_checks}/{total_checks}\n"
                f"   Quality score: {quality_score:.2f} (threshold: {QualityThresholds.QUALITY_PASS_THRESHOLD})"
            )

            # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ í†µê³¼ ì—¬ë¶€ ê²°ì •
            quality_check_passed = quality_score >= QualityThresholds.QUALITY_PASS_THRESHOLD

            # ì¡°ê±´ë¶€ ì—£ì§€ ë° ê³µìš© ë©”íƒ€ ì ‘ê·¼ì„ ìœ„í•´ ì €ì¥ (ìµœìƒìœ„+common.metadata)
            self._save_metadata_safely(state, "quality_score", quality_score, save_to_top_level=True)
            self._save_metadata_safely(state, "quality_check_passed", quality_check_passed, save_to_top_level=True)

            self._update_processing_time(state, start_time)

            quality_status = "í†µê³¼" if quality_check_passed else "ì‹¤íŒ¨"
            legal_validity = self._get_state_value(state, "legal_validity_check", True)
            self._add_step(state, "ë‹µë³€ ê²€ì¦",
                         f"í’ˆì§ˆ: {quality_score:.2f}, ë²•ë ¹: {legal_validity}")

            self.logger.info(
                f"Answer quality validation: {quality_status}, "
                f"score: {quality_score:.2f}, checks: {passed_checks}/{total_checks}"
            )

        except Exception as e:
            self._handle_error(state, str(e), "ë‹µë³€ ê²€ì¦ ì¤‘ ì˜¤ë¥˜")
            self._set_state_value(state, "legal_validity_check", True)

            # ì—ëŸ¬ ì‹œì—ë„ ë©”íƒ€ë°ì´í„° ì €ì¥
            self._save_metadata_safely(state, "quality_score", 0.0, save_to_top_level=True)
            self._save_metadata_safely(state, "quality_check_passed", False, save_to_top_level=True)

        return state

    @observe(name="generate_and_validate_answer")
    @with_state_optimization("generate_and_validate_answer", enable_reduction=True)
    def generate_and_validate_answer(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """í†µí•©ëœ ë‹µë³€ ìƒì„±, ê²€ì¦, í¬ë§·íŒ… ë° ìµœì¢… ì¤€ë¹„"""
        try:
            overall_start_time = time.time()

            # Part 1: ë‹µë³€ ìƒì„± (generate_answer_enhanced ì‹¤í–‰)
            generation_start_time = time.time()

            # ì´ì „ì— ì‹¤í–‰ëœ ë…¸ë“œ í™•ì¸
            metadata = state.get("metadata", {})
            if not isinstance(metadata, dict):
                metadata = {}
            if "common" in state and isinstance(state.get("common"), dict):
                common_metadata = state["common"].get("metadata", {})
                if isinstance(common_metadata, dict):
                    metadata = {**metadata, **common_metadata}

            last_executed_node = metadata.get("_last_executed_node", "")
            is_retry = (last_executed_node == "validate_answer_quality" or last_executed_node == "generate_and_validate_answer")
            if is_retry:
                if self.retry_manager.should_allow_retry(state, "validation"):
                    self.retry_manager.increment_retry_count(state, "validation")

            # generate_answer_enhanced ì‹¤í–‰
            state = self.generate_answer_enhanced(state)

            self._update_processing_time(state, generation_start_time)
            self._save_metadata_safely(state, "_last_executed_node", "generate_and_validate_answer")

            # Part 2: í’ˆì§ˆ ê²€ì¦ (validate_answer_quality ë¡œì§)
            validation_start_time = time.time()

            quality_check_passed = self._validate_answer_quality_internal(state)

            self._update_processing_time(state, validation_start_time)

            # Part 3: ê²€ì¦ í†µê³¼ ì‹œ í¬ë§·íŒ… ë° ìµœì¢… ì¤€ë¹„
            if quality_check_passed:
                formatting_start_time = time.time()
                try:
                    state = self._format_and_finalize_answer(state)
                    self._update_processing_time(state, formatting_start_time)

                    elapsed = time.time() - overall_start_time
                    confidence = state.get("confidence", 0.0)
                    self.logger.info(
                        f"generate_and_validate_answer completed (with formatting) in {elapsed:.2f}s, "
                        f"confidence: {confidence:.3f}"
                    )
                except Exception as format_error:
                    self.logger.warning(f"Formatting failed: {format_error}, using basic format")
                    state["answer"] = self._normalize_answer(state.get("answer", ""))
                    self._prepare_final_response_minimal(state)
                    self._update_processing_time(state, formatting_start_time)

            self._update_processing_time(state, overall_start_time)

        except Exception as e:
            self._handle_error(state, str(e), "ë‹µë³€ ìƒì„± ë° ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # ê¸°ë³¸ê°’ ì„¤ì •
            if "answer" not in state or not state.get("answer"):
                state["answer"] = ""
            self._set_state_value(state, "legal_validity_check", True)
            self._save_metadata_safely(state, "quality_score", 0.0, save_to_top_level=True)
            self._save_metadata_safely(state, "quality_check_passed", False, save_to_top_level=True)

        return state

    def _validate_answer_quality_internal(self, state: LegalWorkflowState) -> bool:
        """í’ˆì§ˆ ê²€ì¦ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        answer = self._normalize_answer(self._get_state_value(state, "answer", ""))
        errors = self._get_state_value(state, "errors", [])
        sources = self._get_state_value(state, "sources", [])

        # í’ˆì§ˆ ê²€ì¦
        answer_str_for_check = answer if isinstance(answer, str) else str(answer) if answer else ""
        quality_checks = {
            "has_answer": len(answer_str_for_check) > 0,
            "min_length": len(answer_str_for_check) >= WorkflowConstants.MIN_ANSWER_LENGTH_VALIDATION,
            "no_errors": len(errors) == 0,
            "has_sources": len(sources) > 0 or len(self._get_state_value(state, "retrieved_docs", [])) > 0
        }

        # ë²•ë ¹ ê²€ì¦
        query = self._get_state_value(state, "query", "")
        if self.legal_validator and len(answer_str_for_check) > 0:
            try:
                answer_for_validation = answer if isinstance(answer, str) else answer_str_for_check
                validation_result = self.legal_validator.validate_legal_basis(query, answer_for_validation)
                self._set_state_value(state, "legal_validity_check", validation_result.is_valid)
                self._set_state_value(state, "legal_basis_validation", {
                    "confidence": validation_result.confidence,
                    "issues": validation_result.issues,
                    "recommendations": validation_result.recommendations
                })
                quality_checks["legal_basis_valid"] = validation_result.is_valid
            except Exception as e:
                self.logger.warning(f"Legal validation failed: {e}")
                self._set_state_value(state, "legal_validity_check", True)
                quality_checks["legal_basis_valid"] = True
        else:
            self._set_state_value(state, "legal_validity_check", True)
            quality_checks["legal_basis_valid"] = True

        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        passed_checks = sum(quality_checks.values())
        total_checks = len(quality_checks)
        quality_score = passed_checks / total_checks if total_checks > 0 else 0.0
        quality_check_passed = quality_score >= QualityThresholds.QUALITY_PASS_THRESHOLD

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self._save_metadata_safely(state, "quality_score", quality_score, save_to_top_level=True)
        self._save_metadata_safely(state, "quality_check_passed", quality_check_passed, save_to_top_level=True)

        legal_validity = self._get_state_value(state, "legal_validity_check", True)
        self._add_step(state, "ë‹µë³€ ê²€ì¦",
                     f"í’ˆì§ˆ: {quality_score:.2f}, ë²•ë ¹: {legal_validity}")

        return quality_check_passed

    def _format_and_finalize_answer(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """í¬ë§·íŒ… ë° ìµœì¢… ì¤€ë¹„ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        # AnswerFormatterHandler ì‚¬ìš©
        preserved_values = self.answer_formatter_handler.extract_preserved_values(state)

        # í¬ë§·íŒ…
        formatted_answer = self.answer_formatter_handler.format_answer_part(state)
        state["answer"] = formatted_answer

        # ìµœì¢… ì¤€ë¹„
        self.answer_formatter_handler.prepare_final_response_part(
            state,
            preserved_values["query_complexity"],
            preserved_values["needs_search"]
        )

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.update_statistics(state)

        return state

    def _prepare_final_response_minimal(self, state: LegalWorkflowState) -> None:
        """ìµœì†Œí•œì˜ ìµœì¢… ì¤€ë¹„ (í¬ë§·íŒ… ì‹¤íŒ¨ ì‹œ)"""
        query_complexity = state.get("metadata", {}).get("query_complexity")
        needs_search = state.get("metadata", {}).get("needs_search", False)
        self.answer_formatter_handler.prepare_final_response_part(state, query_complexity, needs_search)

    # Phase 8 ë¦¬íŒ©í† ë§: Helper methodsëŠ” WorkflowUtilsë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ (ê¸°ì¡´ ì½”ë“œì—ì„œ self._method() í˜¸ì¶œ ì‹œ ì‚¬ìš©)
    def _get_state_value(self, state: LegalWorkflowState, key: str, default: Any = None) -> Any:
        """WorkflowUtils.get_state_value ë˜í¼"""
        return WorkflowUtils.get_state_value(state, key, default)

    def _set_state_value(self, state: LegalWorkflowState, key: str, value: Any) -> None:
        """WorkflowUtils.set_state_value ë˜í¼"""
        WorkflowUtils.set_state_value(state, key, value, self.logger)

    def _update_processing_time(self, state: LegalWorkflowState, start_time: float):
        """WorkflowUtils.update_processing_time ë˜í¼"""
        return WorkflowUtils.update_processing_time(state, start_time)

    def _add_step(self, state: LegalWorkflowState, step_prefix: str, step_message: str):
        """WorkflowUtils.add_step ë˜í¼"""
        WorkflowUtils.add_step(state, step_prefix, step_message)

    def _handle_error(self, state: LegalWorkflowState, error_msg: str, context: str = ""):
        """WorkflowUtils.handle_error ë˜í¼"""
        WorkflowUtils.handle_error(state, error_msg, context, self.logger)

    def _normalize_answer(self, answer_raw: Any) -> str:
        """WorkflowUtils.normalize_answer ë˜í¼"""
        return WorkflowUtils.normalize_answer(answer_raw)

    # Phase 1 ë¦¬íŒ©í† ë§: ì¶”ë¡  ê³¼ì • ë¶„ë¦¬ ë©”ì„œë“œë“¤ì€ ReasoningExtractorë¡œ ì´ë™ë¨
    # Phase 8 ë¦¬íŒ©í† ë§: ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ì€ WorkflowUtilsë¡œ ì´ë™ë¨

    def _save_metadata_safely(self, state: LegalWorkflowState, key: str, value: Any,
                             save_to_top_level: bool = False) -> None:
        """WorkflowUtils.save_metadata_safely ë˜í¼"""
        WorkflowUtils.save_metadata_safely(state, key, value, save_to_top_level)

    def _get_quality_metadata(self, state: LegalWorkflowState) -> Dict[str, Any]:
        """WorkflowUtils.get_quality_metadata ë˜í¼"""
        return WorkflowUtils.get_quality_metadata(state)

    def _get_category_mapping(self) -> Dict[str, List[str]]:
        """WorkflowUtils.get_category_mapping ë˜í¼"""
        return WorkflowUtils.get_category_mapping()

    @observe(name="classify_query")
    @with_state_optimization("classify_query", enable_reduction=True)
    def classify_query(self, state: LegalWorkflowState) -> LegalWorkflowState:
        # ì¤‘ìš”: ë…¸ë“œ ì‹œì‘ ì‹œ input ê·¸ë£¹ ë³´ì¥
        # LangGraphê°€ ì´ˆê¸° stateë¥¼ ì œëŒ€ë¡œ ì „ë‹¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° ëŒ€ë¹„
        print(f"[DEBUG] classify_query: START - State keys={list(state.keys()) if isinstance(state, dict) else 'N/A'}")

        # input ê·¸ë£¹ í™•ì¸ ë° ìƒì„±
        if "input" not in state or not isinstance(state.get("input"), dict):
            state["input"] = {}
            print(f"[DEBUG] classify_query: Created empty input group")

        # queryê°€ ì—†ìœ¼ë©´ ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ ì°¾ê¸°
        current_query = state["input"].get("query", "")
        if not current_query:
            # 1. ìµœìƒìœ„ ë ˆë²¨ì—ì„œ ì°¾ê¸°
            query_from_top = state.get("query", "")
            session_id_from_top = state.get("session_id", "")
            if query_from_top:
                state["input"]["query"] = query_from_top
                if session_id_from_top:
                    state["input"]["session_id"] = session_id_from_top
                print(f"[DEBUG] classify_query: Restored query from top-level: length={len(query_from_top)}")

        # ë””ë²„ê¹…: ì´ˆê¸° stateì˜ query í™•ì¸
        query_value = self._get_state_value(state, "query", "")
        if not query_value or not str(query_value).strip():
            # stateì—ì„œ ì§ì ‘ í™•ì¸
            if "input" in state and isinstance(state.get("input"), dict):
                query_value = state["input"].get("query", "")
                print(f"[DEBUG] classify_query: query from state['input']: '{query_value[:50] if query_value else 'EMPTY'}...'")
            elif isinstance(state, dict) and "query" in state:
                query_value = state["query"]
                print(f"[DEBUG] classify_query: query from state directly: '{query_value[:50] if query_value else 'EMPTY'}...'")
            else:
                print(f"[DEBUG] classify_query: query NOT FOUND, state keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")
                # queryë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ inputì— ë¹ˆ ë¬¸ìì—´ ì„¤ì • (ë‚˜ì¤‘ì— ë³µì› ì‹œë„)
                if "input" not in state:
                    state["input"] = {}
                state["input"]["query"] = ""
        else:
            print(f"[DEBUG] classify_query: query from _get_state_value: '{query_value[:50]}...'")
            # queryë¥¼ ì°¾ì•˜ìœ¼ë©´ inputì— ì €ì¥
            if "input" not in state:
                state["input"] = {}
            state["input"]["query"] = query_value

        """
        ì§ˆë¬¸ ë¶„ë¥˜ (LLM ê¸°ë°˜)

        ì‚¬ìš©í•˜ëŠ” State ê·¸ë£¹:
        - input: query, session_id
        - classification: query_type, confidence, legal_field, legal_domain (ì¶œë ¥)
        - common: processing_steps, errors
        """
        try:
            start_time = time.time()

            query = self._get_state_value(state, "query", "")
            classified_type, confidence = self._classify_with_llm(query)

            # QuestionType enumì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            query_type_str = classified_type.value if hasattr(classified_type, 'value') else str(classified_type)
            self._set_state_value(state, "query_type", query_type_str)
            self._set_state_value(state, "confidence", confidence)

            # ë¡œê¹…: LLM ë¶„ë¥˜ ê²°ê³¼
            self.logger.info(
                f"âœ… [QUESTION CLASSIFICATION] "
                f"QuestionType={classified_type.name if hasattr(classified_type, 'name') else classified_type} "
                f"(confidence: {confidence:.2f})"
            )

            # ë²•ë¥  ë¶„ì•¼ ì¶”ì¶œ
            legal_field = self._extract_legal_field(query_type_str, query)
            self._set_state_value(state, "legal_field", legal_field)
            self._set_state_value(state, "legal_domain", self._map_to_legal_domain(legal_field))

            processing_time = self._update_processing_time(state, start_time)
            self._add_step(state, "ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ",
                         f"ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ: {query_type_str}, ë²•ë¥ ë¶„ì•¼: {legal_field} (ì‹œê°„: {processing_time:.3f}s)")

            self.logger.info(f"LLM classified query as {query_type_str} with confidence {confidence}, field: {legal_field}")

            # ì¤‘ìš”: stateì— input ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ìƒì„± (LangGraph state ë³‘í•© ì‹œ inputì´ ì‚¬ë¼ì§€ëŠ” ê²ƒì„ ë°©ì§€)
            # LangGraphëŠ” TypedDictì˜ ê° í•„ë“œë¥¼ ë³‘í•©í•˜ëŠ”ë°, input í•„ë“œê°€ ê²°ê³¼ì— ì—†ìœ¼ë©´ ì´ì „ ê°’ì´ ì‚¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
            query_value = self._get_state_value(state, "query", "")
            session_id_value = self._get_state_value(state, "session_id", "")

            # í•­ìƒ input ê·¸ë£¹ì„ resultì— í¬í•¨ (LangGraph state ë³‘í•© ì‹œ ë³´ì¡´)
            if "input" not in state:
                state["input"] = {}
            state["input"]["query"] = query_value or state.get("input", {}).get("query", "")
            state["input"]["session_id"] = session_id_value or state.get("input", {}).get("session_id", "")

            if not state["input"]["query"]:
                self.logger.warning(f"classify_query: query is empty after ensuring input group!")
            else:
                self.logger.debug(f"Ensured input group in state after classify_query: query length={len(state['input']['query'])}")

        except Exception as e:
            self._handle_error(state, str(e), "LLM ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            query = self._get_state_value(state, "query", "")
            classified_type, confidence = self._fallback_classification(query)
            query_type_str = classified_type.value if hasattr(classified_type, 'value') else str(classified_type)
            self._set_state_value(state, "query_type", query_type_str)
            self._set_state_value(state, "confidence", confidence)

            # ë²•ë¥  ë¶„ì•¼ ì¶”ì¶œ (í´ë°±)
            legal_field = self._extract_legal_field(query_type_str, query)
            self._set_state_value(state, "legal_field", legal_field)
            self._set_state_value(state, "legal_domain", self._map_to_legal_domain(legal_field))

            self._add_step(state, "í´ë°± í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ ì‚¬ìš©", "í´ë°± í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ ì‚¬ìš©")

            # ì¤‘ìš”: stateì— input ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ìƒì„± (í´ë°± ê²½ë¡œì—ì„œë„ ë³´ì¥)
            query_value = self._get_state_value(state, "query", "")
            session_id_value = self._get_state_value(state, "session_id", "")

            if "input" not in state:
                state["input"] = {}
            state["input"]["query"] = query_value or state.get("input", {}).get("query", "")
            state["input"]["session_id"] = session_id_value or state.get("input", {}).get("session_id", "")

            if not state["input"]["query"]:
                self.logger.warning(f"classify_query (fallback): query is empty after ensuring input group!")
            else:
                self.logger.debug(f"Ensured input group in state after classify_query (fallback): query length={len(state['input']['query'])}")

        # ì¤‘ìš”: ë°˜í™˜ ì „ì— í•­ìƒ input ê·¸ë£¹ ë³´ì¥ (ëª¨ë“  ê²½ë¡œì—ì„œ)
        # LangGraphëŠ” ë…¸ë“œê°€ ë°˜í™˜í•œ stateë§Œ ë‹¤ìŒ ë…¸ë“œì— ì „ë‹¬í•˜ë¯€ë¡œ, inputì„ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•¨

        # 1. í˜„ì¬ stateì—ì„œ query ì°¾ê¸°
        query_value = None
        session_id_value = None

        # ìš°ì„ ìˆœìœ„ 1: state["input"]["query"]
        if "input" in state and isinstance(state.get("input"), dict):
            query_value = state["input"].get("query", "")
            session_id_value = state["input"].get("session_id", "")

        # ìš°ì„ ìˆœìœ„ 2: ìµœìƒìœ„ ë ˆë²¨ query
        if not query_value:
            query_value = state.get("query", "")
            session_id_value = state.get("session_id", "")

        # ìš°ì„ ìˆœìœ„ 3: search.search_query
        if not query_value and "search" in state and isinstance(state.get("search"), dict):
            query_value = state["search"].get("search_query", "")

        # input ê·¸ë£¹ ìƒì„± ë° ì„¤ì •
        if "input" not in state:
            state["input"] = {}

        # queryê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì €ì¥
        if query_value and str(query_value).strip():
            state["input"]["query"] = query_value
            if session_id_value:
                state["input"]["session_id"] = session_id_value
            self.logger.debug(f"classify_query returning with input.query length={len(query_value)}")
            print(f"[DEBUG] classify_query: Returning state with input.query length={len(query_value)}")
        else:
            # queryë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì—ëŸ¬ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰
            # (ë‹¤ìŒ ë…¸ë“œë‚˜ workflow_serviceì—ì„œ ë³µì›í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€)
            self.logger.error(f"classify_query returning with EMPTY input.query! State keys: {list(state.keys())}")
            print(f"[DEBUG] classify_query: ERROR - Returning state with EMPTY input.query! State keys: {list(state.keys())}")
            print(f"[DEBUG] classify_query: State structure - input={state.get('input')}, has query key={bool(state.get('query'))}")

        return state

    @observe(name="classify_complexity")
    @with_state_optimization("classify_complexity", enable_reduction=False)  # ë¼ìš°íŒ…ì— í•„ìš”í•œ ê°’ ë³´ì¡´ì„ ìœ„í•´ reduction ë¹„í™œì„±í™”
    def classify_complexity(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì§ˆë¬¸ ë³µì¡ë„ë¥¼ íŒë‹¨í•˜ê³  ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ ê²°ì • (Adaptive RAG)"""
        try:
            start_time = time.time()

            query = self._get_state_value(state, "query", "")
            if not query:
                # ê¸°ë³¸ê°’: ì¤‘ê°„ ë³µì¡ë„ (ê²€ìƒ‰ í•„ìš”)
                self._set_state_value(state, "query_complexity", QueryComplexity.MODERATE.value)
                self._set_state_value(state, "needs_search", True)
                return state

            query_lower = query.lower()

            # 1. ì¸ì‚¬ë§/ê°„ë‹¨í•œ ì§ˆë¬¸ ì²´í¬
            simple_greetings = ["ì•ˆë…•", "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ë„ì›€", "ì„¤ëª…", "ì•ˆë…•í•˜ì„¸ìš”", "ê³ ë§ˆì›Œìš”", "ê°ì‚¬í•©ë‹ˆë‹¤"]
            if any(pattern in query_lower for pattern in simple_greetings):
                if len(query) < 20:  # ë§¤ìš° ì§§ì€ ì¸ì‚¬ë§
                    complexity = QueryComplexity.SIMPLE
                    needs_search = False
                    self.logger.info(f"âœ… ê°„ë‹¨í•œ ì§ˆë¬¸ ê°ì§€ (ì¸ì‚¬ë§): {query[:50]}...")
                    # classification ê·¸ë£¹ê³¼ ìµœìƒìœ„ ë ˆë²¨ ëª¨ë‘ì— ì €ì¥ (ë¼ìš°íŒ… ë° ìµœì¢… ê²°ê³¼ ì¶”ì¶œì„ ìœ„í•´)
                    self._set_state_value(state, "query_complexity", complexity.value)
                    self._set_state_value(state, "needs_search", needs_search)
                    # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì§ì ‘ ì €ì¥
                    if "classification" not in state:
                        state["classification"] = {}
                    state["classification"]["query_complexity"] = complexity.value
                    state["classification"]["needs_search"] = needs_search
                    # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì €ì¥
                    state["query_complexity"] = complexity.value
                    state["needs_search"] = needs_search
                    # common ê·¸ë£¹ê³¼ metadataì—ë„ ì €ì¥ (reducerê°€ ë³´ì¡´í•˜ëŠ” ê·¸ë£¹)
                    if "common" not in state:
                        state["common"] = {}
                    state["common"]["query_complexity"] = complexity.value
                    state["common"]["needs_search"] = needs_search
                    # metadataì—ë„ ì €ì¥ (ê¸°ì¡´ ë‚´ìš© ë³´ì¡´)
                    if "metadata" not in state:
                        state["metadata"] = {}
                    elif not isinstance(state.get("metadata"), dict):
                        state["metadata"] = {}
                    # ê¸°ì¡´ metadata ë‚´ìš© ë³´ì¡´í•˜ë©´ì„œ query_complexity ì¶”ê°€
                    state["metadata"]["query_complexity"] = complexity.value
                    state["metadata"]["needs_search"] = needs_search

                    # ì¤‘ìš”: Global cacheì—ë„ ì €ì¥ (reducer ì†ì‹¤ ë°©ì§€)
                    try:
                        from core.agents import node_wrappers
                        # ëª¨ë“ˆ ë ˆë²¨ ë³€ìˆ˜ì— ì§ì ‘ ì ‘ê·¼
                        if not hasattr(node_wrappers, '_global_search_results_cache') or node_wrappers._global_search_results_cache is None:
                            node_wrappers._global_search_results_cache = {}
                        # query_complexity ì •ë³´ ì €ì¥
                        node_wrappers._global_search_results_cache["query_complexity"] = complexity.value
                        node_wrappers._global_search_results_cache["needs_search"] = needs_search
                        print(f"[DEBUG] classify_complexity (ê°„ë‹¨): âœ… Global cache ì €ì¥ ì™„ë£Œ - complexity={complexity.value}, needs_search={needs_search}")
                    except Exception as e:
                        print(f"[DEBUG] classify_complexity (ê°„ë‹¨): âŒ Global cache ì €ì¥ ì‹¤íŒ¨: {e}")
                        import traceback
                        print(f"[DEBUG] classify_complexity (ê°„ë‹¨): Exception traceback: {traceback.format_exc()}")

                    # ë””ë²„ê¹…: ì €ì¥ í™•ì¸
                    saved_complexity = self._get_state_value(state, "query_complexity", None)
                    saved_needs_search = self._get_state_value(state, "needs_search", None)
                    top_level_complexity = state.get("query_complexity")
                    top_level_needs_search = state.get("needs_search")
                    common_complexity = state.get("common", {}).get("query_complexity")
                    metadata_complexity = state.get("metadata", {}).get("query_complexity")
                    print(f"[DEBUG] classify_complexity: ì €ì¥ ì™„ë£Œ")
                    print(f"  - ìµœìƒìœ„ ë ˆë²¨: complexity={top_level_complexity}, needs_search={top_level_needs_search}")
                    print(f"  - classification ê·¸ë£¹: complexity={state.get('classification', {}).get('query_complexity')}")
                    print(f"  - common ê·¸ë£¹: complexity={common_complexity}")
                    print(f"  - metadata: complexity={metadata_complexity}")

                    processing_time = self._update_processing_time(state, start_time)
                    self._add_step(state, "ë³µì¡ë„ ë¶„ë¥˜", f"ê°„ë‹¨í•œ ì§ˆë¬¸ (ì¸ì‚¬ë§) - ê²€ìƒ‰ ë¶ˆí•„ìš” (ì‹œê°„: {processing_time:.3f}s)")
                    return state

            # 2. ë²•ë¥  ìš©ì–´ ì •ì˜ ì§ˆë¬¸ ì²´í¬
            definition_keywords = ["ëœ»", "ì˜ë¯¸", "ì •ì˜", "ì´ë€", "ë€ ë¬´ì—‡", "ë¬´ì—‡ì¸ê°€", "ë¬´ì—‡ì´ì•¼", "ë¬´ì—‡ì´ëƒ"]
            if any(pattern in query_lower for pattern in definition_keywords):
                # ë‹¨ìˆœ ì •ì˜ ì§ˆë¬¸ì¸ì§€ í™•ì¸ (ê¸¸ì´ì™€ í‚¤ì›Œë“œë¡œ íŒë‹¨)
                if len(query) < 30 and any(word in query for word in definition_keywords):
                    # ê°„ë‹¨í•œ ì •ì˜ ì§ˆë¬¸
                    complexity = QueryComplexity.SIMPLE
                    needs_search = False
                    self.logger.info(f"âœ… ê°„ë‹¨í•œ ì§ˆë¬¸ ê°ì§€ (ìš©ì–´ ì •ì˜): {query[:50]}...")
                    self._set_state_value(state, "query_complexity", complexity.value)
                    self._set_state_value(state, "needs_search", needs_search)
                    # Global cacheì—ë„ ì €ì¥
                    try:
                        from core.agents import node_wrappers
                        if not hasattr(node_wrappers, '_global_search_results_cache') or node_wrappers._global_search_results_cache is None:
                            node_wrappers._global_search_results_cache = {}
                        node_wrappers._global_search_results_cache["query_complexity"] = complexity.value
                        node_wrappers._global_search_results_cache["needs_search"] = needs_search
                        print(f"[DEBUG] classify_complexity (ìš©ì–´ì •ì˜): âœ… Global cache ì €ì¥ ì™„ë£Œ")
                    except Exception as e:
                        print(f"[DEBUG] classify_complexity (ìš©ì–´ì •ì˜): âŒ Global cache ì €ì¥ ì‹¤íŒ¨: {e}")
                    processing_time = self._update_processing_time(state, start_time)
                    self._add_step(state, "ë³µì¡ë„ ë¶„ë¥˜", f"ê°„ë‹¨í•œ ì§ˆë¬¸ (ìš©ì–´ ì •ì˜) - ê²€ìƒ‰ ë¶ˆí•„ìš” (ì‹œê°„: {processing_time:.3f}s)")
                    return state

            # 3. íŠ¹ì • ì¡°ë¬¸/ë²•ë ¹ ì§ˆì˜ (ì¤‘ê°„ ë³µì¡ë„)
            if ("ì¡°" in query or "ë²•" in query or "ë²•ë ¹" in query or "ë²•ë¥ " in query) and len(query) < 50:
                complexity = QueryComplexity.MODERATE
                needs_search = True
                self.logger.info(f"ğŸ“‹ ì¤‘ê°„ ë³µì¡ë„ ì§ˆë¬¸ (ë²•ë ¹ ì¡°íšŒ): {query[:50]}...")

            # 4. ë³µì¡í•œ ì§ˆë¬¸ (ë¹„êµ, ì ˆì°¨, ì‚¬ë¡€ ë¶„ì„)
            elif any(keyword in query for keyword in ["ë¹„êµ", "ì°¨ì´", "ì–´ë–»ê²Œ", "ë°©ë²•", "ì ˆì°¨", "ì‚¬ë¡€", "íŒë¡€ ë¹„êµ"]):
                complexity = QueryComplexity.COMPLEX
                needs_search = True
                self.logger.info(f"ğŸ” ë³µì¡í•œ ì§ˆë¬¸ ê°ì§€: {query[:50]}...")

            # 5. ê¸°ë³¸ê°’ (ì¤‘ê°„ ë³µì¡ë„)
            else:
                complexity = QueryComplexity.MODERATE
                needs_search = True

            # Stateì— ì €ì¥ (classification ê·¸ë£¹ê³¼ ìµœìƒìœ„ ë ˆë²¨ ëª¨ë‘ì— ì €ì¥)
            self._set_state_value(state, "query_complexity", complexity.value)
            self._set_state_value(state, "needs_search", needs_search)

            # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì§ì ‘ ì €ì¥ (ë¼ìš°íŒ… ë° ìµœì¢… ê²°ê³¼ ì¶”ì¶œì„ ìœ„í•´)
            if "classification" not in state:
                state["classification"] = {}
            state["classification"]["query_complexity"] = complexity.value
            state["classification"]["needs_search"] = needs_search
            # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì €ì¥
            state["query_complexity"] = complexity.value
            state["needs_search"] = needs_search
            # common ê·¸ë£¹ê³¼ metadataì—ë„ ì €ì¥ (reducerê°€ ë³´ì¡´í•˜ëŠ” ê·¸ë£¹)
            if "common" not in state:
                state["common"] = {}
            state["common"]["query_complexity"] = complexity.value
            state["common"]["needs_search"] = needs_search
            # metadataì—ë„ ì €ì¥ (ê¸°ì¡´ ë‚´ìš© ë³´ì¡´)
            if "metadata" not in state:
                state["metadata"] = {}
            elif not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            # ê¸°ì¡´ metadata ë‚´ìš© ë³´ì¡´í•˜ë©´ì„œ query_complexity ì¶”ê°€
            state["metadata"]["query_complexity"] = complexity.value
            state["metadata"]["needs_search"] = needs_search

            # ì¤‘ìš”: Global cacheì—ë„ ì €ì¥ (reducer ì†ì‹¤ ë°©ì§€)
            try:
                from core.agents import node_wrappers
                # ëª¨ë“ˆ ë ˆë²¨ ë³€ìˆ˜ì— ì§ì ‘ ì ‘ê·¼
                if not hasattr(node_wrappers, '_global_search_results_cache') or node_wrappers._global_search_results_cache is None:
                    node_wrappers._global_search_results_cache = {}
                # query_complexity ì •ë³´ ì €ì¥
                node_wrappers._global_search_results_cache["query_complexity"] = complexity.value
                node_wrappers._global_search_results_cache["needs_search"] = needs_search
                print(f"[DEBUG] classify_complexity: âœ… Global cache ì €ì¥ ì™„ë£Œ - complexity={complexity.value}, needs_search={needs_search}")
                print(f"[DEBUG] classify_complexity: Global cache keys={list(node_wrappers._global_search_results_cache.keys())[:10]}")
            except Exception as e:
                print(f"[DEBUG] classify_complexity: âŒ Global cache ì €ì¥ ì‹¤íŒ¨: {e}")
                import traceback
                print(f"[DEBUG] classify_complexity: Exception traceback: {traceback.format_exc()}")

            # ë””ë²„ê¹…: ì €ì¥ í™•ì¸
            top_level_complexity = state.get("query_complexity")
            top_level_needs_search = state.get("needs_search")
            common_complexity = state.get("common", {}).get("query_complexity")
            metadata_complexity = state.get("metadata", {}).get("query_complexity")
            print(f"[DEBUG] classify_complexity: ì €ì¥ ì™„ë£Œ (ìµœì¢…)")
            print(f"  - ìµœìƒìœ„ ë ˆë²¨: complexity={top_level_complexity}, needs_search={top_level_needs_search}")
            print(f"  - classification ê·¸ë£¹: complexity={state.get('classification', {}).get('query_complexity')}")
            print(f"  - common ê·¸ë£¹: complexity={common_complexity}")
            print(f"  - metadata: complexity={metadata_complexity}")

            processing_time = self._update_processing_time(state, start_time)
            self._add_step(
                state,
                "ë³µì¡ë„ ë¶„ë¥˜",
                f"ì§ˆë¬¸ ë³µì¡ë„: {complexity.value}, ê²€ìƒ‰ í•„ìš”: {needs_search} (ì‹œê°„: {processing_time:.3f}s)"
            )

        except Exception as e:
            self._handle_error(state, str(e), "ë³µì¡ë„ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # ê¸°ë³¸ê°’: ì¤‘ê°„ ë³µì¡ë„ (ê²€ìƒ‰ í•„ìš”)
            self._set_state_value(state, "query_complexity", QueryComplexity.MODERATE.value)
            self._set_state_value(state, "needs_search", True)

        return state

    @observe(name="classify_query_and_complexity")
    @with_state_optimization("classify_query_and_complexity", enable_reduction=False)
    def classify_query_and_complexity(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """í†µí•©ëœ ì§ˆë¬¸ ë¶„ë¥˜ ë° ë³µì¡ë„ íŒë‹¨ (classify_query + classify_complexity)"""
        try:
            overall_start_time = time.time()

            # ========== Part 1: classify_query ë¡œì§ ==========
            query_start_time = time.time()

            # ì¤‘ìš”: ë…¸ë“œ ì‹œì‘ ì‹œ input ê·¸ë£¹ ë³´ì¥
            if "input" not in state or not isinstance(state.get("input"), dict):
                state["input"] = {}

            # queryê°€ ì—†ìœ¼ë©´ ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ ì°¾ê¸°
            current_query = state["input"].get("query", "")
            if not current_query:
                query_from_top = state.get("query", "")
                session_id_from_top = state.get("session_id", "")
                if query_from_top:
                    state["input"]["query"] = query_from_top
                    if session_id_from_top:
                        state["input"]["session_id"] = session_id_from_top

            # query í™•ì¸
            query_value = self._get_state_value(state, "query", "")
            if not query_value or not str(query_value).strip():
                if "input" in state and isinstance(state.get("input"), dict):
                    query_value = state["input"].get("query", "")
                elif isinstance(state, dict) and "query" in state:
                    query_value = state["query"]
                else:
                    if "input" not in state:
                        state["input"] = {}
                    state["input"]["query"] = ""
            else:
                if "input" not in state:
                    state["input"] = {}
                state["input"]["query"] = query_value

            # ========== í†µí•© ë¶„ë¥˜: ì§ˆë¬¸ ìœ í˜• + ë³µì¡ë„ ë™ì‹œ ë¶„ë¥˜ ==========
            query = self._get_state_value(state, "query", "")

            if not query:
                # ê¸°ë³¸ê°’ ì„¤ì •
                classified_type, confidence = self._fallback_classification("")
                complexity = QueryComplexity.MODERATE
                needs_search = True
            else:
                # LLM ê¸°ë°˜ ì²´ì¸ ë¶„ë¥˜ (ì§ˆë¬¸ ìœ í˜• â†’ ë²•ë¥  ë¶„ì•¼ â†’ ë³µì¡ë„ â†’ ê²€ìƒ‰ í•„ìš”ì„±)
                if self.config.use_llm_for_complexity:
                    try:
                        classified_type, confidence, complexity, needs_search = self._classify_query_with_chain(query)
                        self.logger.info(
                            f"âœ… [CHAIN CLASSIFICATION] "
                            f"QuestionType={classified_type.value}, complexity={complexity.value}, "
                            f"needs_search={needs_search}, confidence={confidence:.2f}"
                        )
                    except Exception as e:
                        self.logger.warning(f"ì²´ì¸ LLM ë¶„ë¥˜ ì‹¤íŒ¨, í´ë°± ì‚¬ìš©: {e}")
                        classified_type, confidence = self._fallback_classification(query)
                        complexity, needs_search = self._fallback_complexity_classification(query)
                else:
                    # í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ì§ì ‘ ì‚¬ìš©
                    classified_type, confidence = self._fallback_classification(query)
                    complexity, needs_search = self._fallback_complexity_classification(query)

            # ì§ˆë¬¸ ìœ í˜• ì²˜ë¦¬
            query_type_str = classified_type.value if hasattr(classified_type, 'value') else str(classified_type)
            legal_field = self._extract_legal_field(query_type_str, query)

            # Stateì— ì €ì¥ (ì§ˆë¬¸ ìœ í˜•)
            self._set_state_value(state, "query_type", query_type_str)
            self._set_state_value(state, "confidence", confidence)
            self._set_state_value(state, "legal_field", legal_field)
            self._set_state_value(state, "legal_domain", self._map_to_legal_domain(legal_field))

            self._update_processing_time(state, query_start_time)
            self._add_step(state, "ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ",
                         f"ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ: {query_type_str}, ë²•ë¥ ë¶„ì•¼: {legal_field}")

            # input ê·¸ë£¹ ë³´ì¥
            query_value = self._get_state_value(state, "query", "")
            session_id_value = self._get_state_value(state, "session_id", "")
            if "input" not in state:
                state["input"] = {}
            state["input"]["query"] = query_value or state.get("input", {}).get("query", "")
            state["input"]["session_id"] = session_id_value or state.get("input", {}).get("session_id", "")

            # ========== ë³µì¡ë„ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥ ==========
            # Stateì— ì €ì¥ (ëª¨ë“  ìœ„ì¹˜ì—)
            self._set_state_value(state, "query_complexity", complexity.value)
            self._set_state_value(state, "needs_search", needs_search)

            # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì§ì ‘ ì €ì¥
            if "classification" not in state:
                state["classification"] = {}
            state["classification"]["query_complexity"] = complexity.value
            state["classification"]["needs_search"] = needs_search
            state["query_complexity"] = complexity.value
            state["needs_search"] = needs_search

            # common ê·¸ë£¹ê³¼ metadataì—ë„ ì €ì¥
            if "common" not in state:
                state["common"] = {}
            state["common"]["query_complexity"] = complexity.value
            state["common"]["needs_search"] = needs_search
            if "metadata" not in state:
                state["metadata"] = {}
            elif not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            state["metadata"]["query_complexity"] = complexity.value
            state["metadata"]["needs_search"] = needs_search

            # Global cacheì—ë„ ì €ì¥
            try:
                from core.agents import node_wrappers
                if not hasattr(node_wrappers, '_global_search_results_cache') or node_wrappers._global_search_results_cache is None:
                    node_wrappers._global_search_results_cache = {}
                node_wrappers._global_search_results_cache["query_complexity"] = complexity.value
                node_wrappers._global_search_results_cache["needs_search"] = needs_search
            except Exception as e:
                self.logger.warning(f"Global cache ì €ì¥ ì‹¤íŒ¨: {e}")

            self._add_step(
                state,
                "ë³µì¡ë„ ë¶„ë¥˜",
                f"ì§ˆë¬¸ ë³µì¡ë„: {complexity.value}, ê²€ìƒ‰ í•„ìš”: {needs_search}"
            )

            self._update_processing_time(state, overall_start_time)

        except Exception as e:
            self._handle_error(state, str(e), "ì§ˆë¬¸ ë¶„ë¥˜ ë° ë³µì¡ë„ íŒë‹¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # ê¸°ë³¸ê°’ ì„¤ì •
            try:
                query = self._get_state_value(state, "query", "")
                classified_type, confidence = self._fallback_classification(query)
                query_type_str = classified_type.value if hasattr(classified_type, 'value') else str(classified_type)
                legal_field = self._extract_legal_field(query_type_str, query)
                self._set_state_value(state, "query_type", query_type_str)
                self._set_state_value(state, "confidence", confidence)
                self._set_state_value(state, "legal_field", legal_field)
                self._set_state_value(state, "legal_domain", self._map_to_legal_domain(legal_field))
            except:
                self._set_state_value(state, "query_type", "general_question")
                self._set_state_value(state, "confidence", 0.5)
                self._set_state_value(state, "legal_field", "general")
                self._set_state_value(state, "legal_domain", "general")

            # ê¸°ë³¸ ë³µì¡ë„
            self._set_state_value(state, "query_complexity", QueryComplexity.MODERATE.value)
            self._set_state_value(state, "needs_search", True)

        # input ê·¸ë£¹ ë³´ì¥
        query_value = self._get_state_value(state, "query", "")
        session_id_value = self._get_state_value(state, "session_id", "")
        if "input" not in state:
            state["input"] = {}
        state["input"]["query"] = query_value or state.get("input", {}).get("query", "")
        state["input"]["session_id"] = session_id_value or state.get("input", {}).get("session_id", "")

        return state

    @observe(name="direct_answer")
    @with_state_optimization("direct_answer", enable_reduction=True)
    def direct_answer_node(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ê°„ë‹¨í•œ ì§ˆë¬¸ - ê²€ìƒ‰ ì—†ì´ LLMë§Œ ì‚¬ìš©í•˜ê³  í¬ë§·íŒ…ê¹Œì§€ í†µí•© ì²˜ë¦¬"""
        try:
            start_time = time.time()

            query = self._get_state_value(state, "query", "")
            if not query:
                self.logger.warning("direct_answer_node: queryê°€ ì—†ìŠµë‹ˆë‹¤")
                return state

            # ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš© (Flash)
            llm = self.llm_fast if hasattr(self, 'llm_fast') and self.llm_fast else self.llm

            # Phase 10 ë¦¬íŒ©í† ë§: DirectAnswerHandler ì‚¬ìš©
            # Prompt Chainingì„ ì‚¬ìš©í•œ ì§ì ‘ ë‹µë³€ ìƒì„±
            answer = self.direct_answer_handler.generate_direct_answer_with_chain(query)

            # ì²´ì¸ ì‹¤íŒ¨ ì‹œ í´ë°±
            if not answer or len(answer.strip()) < 10:
                self.logger.debug("Chain direct answer failed, using fallback")
                answer = self.direct_answer_handler.generate_fallback_answer(query)

                # ìµœì†Œ ê¸¸ì´ ì²´í¬
                if not answer or len(answer.strip()) < 10:
                    # í´ë°±: ê²€ìƒ‰ ê²½ë¡œë¡œ
                    answer_length = len(answer) if answer else 0
                    self.logger.warning(f"ì§ì ‘ ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ (ê¸¸ì´: {answer_length}), ê²€ìƒ‰ ê²½ë¡œë¡œ ì „í™˜")
                    self._set_state_value(state, "needs_search", True)
                    return state

            # ë‹µë³€ ì €ì¥ (ì²´ì¸ ì„±ê³µ ë˜ëŠ” í´ë°± ì„±ê³µ)
            self._set_state_value(state, "answer", answer)
            self._set_state_value(state, "sources", [])  # ê²€ìƒ‰ ì—†ìŒ

            processing_time = self._update_processing_time(state, start_time)
            self._add_step(
                state,
                "ì§ì ‘ ë‹µë³€ ìƒì„±",
                f"ê²€ìƒ‰ ì—†ì´ ì§ì ‘ ë‹µë³€ ìƒì„± ì™„ë£Œ (ì‹œê°„: {processing_time:.3f}s)"
            )

            # í¬ë§·íŒ… ë° ìµœì¢… ì¤€ë¹„ (í†µí•© ì²˜ë¦¬)
            formatting_start_time = time.time()
            try:
                state = self._format_and_finalize_answer(state)
                self._update_processing_time(state, formatting_start_time)

                total_time = time.time() - start_time
                confidence = state.get("confidence", 0.0)
                self.logger.info(
                    f"âœ… ì§ì ‘ ë‹µë³€ ìƒì„± ë° í¬ë§·íŒ… ì™„ë£Œ (ê²€ìƒ‰ ìŠ¤í‚µ): {query[:50]}... "
                    f"(ì´ ì‹œê°„: {total_time:.2f}s, confidence: {confidence:.3f})"
                )
            except Exception as format_error:
                self.logger.warning(f"Direct answer formatting failed: {format_error}, using basic format")
                state["answer"] = self._normalize_answer(state.get("answer", ""))
                self._prepare_final_response_minimal(state)
                self._update_processing_time(state, formatting_start_time)

        except Exception as e:
            self._handle_error(state, str(e), "direct_answer_node ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # í´ë°±: ê²€ìƒ‰ ê²½ë¡œë¡œ
            self._set_state_value(state, "needs_search", True)

        return state

    @observe(name="resolve_multi_turn")
    @with_state_optimization("resolve_multi_turn", enable_reduction=True)
    def resolve_multi_turn(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ë©€í‹°í„´ ì§ˆë¬¸ í•´ê²° ë…¸ë“œ"""
        try:
            start_time = time.time()

            # ë©€í‹°í„´ í•¸ë“¤ëŸ¬ì™€ ì„¸ì…˜ ê´€ë¦¬ìê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if not self.multi_turn_handler or not self.conversation_manager:
                self._set_state_value(state, "is_multi_turn", False)
                query = self._get_state_value(state, "query", "")
                self._set_state_value(state, "search_query", query)
                self.logger.debug("Multi-turn handler not available, skipping multi-turn resolution")
                return state

            query = self._get_state_value(state, "query", "")
            session_id = self._get_state_value(state, "session_id", "")

            # ì„¸ì…˜ì—ì„œ ëŒ€í™” ë§¥ë½ ê°€ì ¸ì˜¤ê¸°
            conversation_context = self._get_or_create_conversation_context(session_id)

            if conversation_context and conversation_context.turns:
                # ë©€í‹°í„´ ì§ˆë¬¸ ê°ì§€
                is_multi_turn = self.multi_turn_handler.detect_multi_turn_question(query, conversation_context)
                self._set_state_value(state, "is_multi_turn", is_multi_turn)

                if is_multi_turn:
                    # ì™„ì „í•œ ì§ˆë¬¸ êµ¬ì„±
                    multi_turn_result = self.multi_turn_handler.build_complete_query(query, conversation_context)

                    resolved_query = multi_turn_result.get("resolved_query", query)
                    self._set_state_value(state, "multi_turn_confidence", multi_turn_result.get("confidence", 1.0))

                    # ëŒ€í™” ë§¥ë½ ì •ë³´ ì €ì¥
                    self._set_state_value(state, "conversation_context", self._build_conversation_context_dict(conversation_context))

                    # ê²€ìƒ‰ ì¿¼ë¦¬ ì—…ë°ì´íŠ¸ (í•´ê²°ëœ ì¿¼ë¦¬ ì‚¬ìš©)
                    self._set_state_value(state, "search_query", resolved_query)

                    self.logger.info(f"Multi-turn question resolved: '{query}' -> '{resolved_query}'")
                    self._add_step(state, "ë©€í‹°í„´ ì²˜ë¦¬",
                                 f"ë©€í‹°í„´ ì§ˆë¬¸ í•´ê²°: {multi_turn_result.get('reasoning', '')}")
                else:
                    # ë©€í‹°í„´ ì§ˆë¬¸ì´ ì•„ë‹˜
                    self._set_state_value(state, "multi_turn_confidence", 1.0)

                    # ë‹¨ì¼ í„´ì´ë¯€ë¡œ search_queryëŠ” ê·¸ëŒ€ë¡œ
                    self._set_state_value(state, "search_query", query)
            else:
                # ëŒ€í™” ë§¥ë½ì´ ì—†ìŒ
                self._set_state_value(state, "is_multi_turn", False)
                self._set_state_value(state, "multi_turn_confidence", 1.0)
                self._set_state_value(state, "search_query", query)

            self._update_processing_time(state, start_time)

        except Exception as e:
            self.logger.error(f"Error in resolve_multi_turn: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì›ë³¸ ì¿¼ë¦¬ ìœ ì§€
            self._set_state_value(state, "is_multi_turn", False)
            search_query = self._get_state_value(state, "search_query")
            if not search_query:
                search_query = self._get_state_value(state, "query", "")
            self._set_state_value(state, "search_query", search_query)
            self._handle_error(state, str(e), "ë©€í‹°í„´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

        return state

    def _get_or_create_conversation_context(self, session_id: str):
        """ëŒ€í™” ë§¥ë½ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        try:
            if not self.conversation_manager:
                return None

            # ì„¸ì…˜ì—ì„œ ëŒ€í™” ë§¥ë½ ì¡°íšŒ
            # ConversationManagerì˜ sessions ë”•ì…”ë„ˆë¦¬ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            sessions = getattr(self.conversation_manager, 'sessions', {})
            context = sessions.get(session_id)

            return context
        except Exception as e:
            self.logger.error(f"Error getting conversation context: {e}")
            return None

    def _build_conversation_context_dict(self, context):
        """ConversationContextë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        result = QueryBuilder.build_conversation_context_dict(context)
        if result is None and context is not None:
            self.logger.error(f"Error building conversation context dict")
        return result

    # Phase 10 ë¦¬íŒ©í† ë§: ì§ì ‘ ë‹µë³€ ìƒì„± ë©”ì„œë“œëŠ” DirectAnswerHandlerë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ (í•„ìš”ì‹œ)
    def _generate_direct_answer_with_chain(self, query: str, llm) -> Optional[str]:
        """DirectAnswerHandler.generate_direct_answer_with_chain ë˜í¼ (LLM íŒŒë¼ë¯¸í„° ë¬´ì‹œ)"""
        return self.direct_answer_handler.generate_direct_answer_with_chain(query)

    def _parse_query_type_analysis_response(self, response: str) -> Dict[str, Any]:
        """WorkflowUtils.parse_query_type_analysis_response ë˜í¼"""
        return WorkflowUtils.parse_query_type_analysis_response(response, self.logger)

    def _parse_quality_validation_response(self, response: str) -> Dict[str, Any]:
        """WorkflowUtils.parse_quality_validation_response ë˜í¼"""
        return WorkflowUtils.parse_quality_validation_response(response, self.logger)

    # Phase 3 ë¦¬íŒ©í† ë§: ë¶„ë¥˜ ê´€ë ¨ ë©”ì„œë“œëŠ” ClassificationHandlerë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _classify_with_llm(self, query: str) -> Tuple[QuestionType, float]:
        """ClassificationHandler.classify_with_llm ë˜í¼"""
        return self.classification_handler.classify_with_llm(query)

    def _fallback_classification(self, query: str) -> Tuple[QuestionType, float]:
        """ClassificationHandler.fallback_classification ë˜í¼"""
        return self.classification_handler.fallback_classification(query)

    def _fallback_complexity_classification(self, query: str) -> Tuple[QueryComplexity, bool]:
        """ClassificationHandler.fallback_complexity_classification ë˜í¼"""
        return self.classification_handler.fallback_complexity_classification(query)

    def _parse_complexity_response(self, response: str) -> Optional[Dict[str, Any]]:
        """ClassificationHandler.parse_complexity_response ë˜í¼"""
        return self.classification_handler.parse_complexity_response(response)

    def _parse_unified_classification_response(self, response: str) -> Optional[Dict[str, Any]]:
        """ClassificationHandler.parse_unified_classification_response ë˜í¼"""
        return self.classification_handler.parse_unified_classification_response(response)

    def _classify_query_with_chain(self, query: str) -> Tuple[QuestionType, float, QueryComplexity, bool]:
        """
        Prompt Chainingì„ ì‚¬ìš©í•œ ì§ˆë¬¸ ë¶„ë¥˜ (ë‹¤ë‹¨ê³„ ì²´ì¸)

        Step 1: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        Step 2: ë²•ë¥  ë¶„ì•¼ ì¶”ì¶œ (ì§ˆë¬¸ ìœ í˜• ê¸°ë°˜)
        Step 3: ë³µì¡ë„ í‰ê°€ (ì§ˆë¬¸ + ìœ í˜• + ë¶„ì•¼ ê¸°ë°˜)
        Step 4: ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ (ë³µì¡ë„ ê¸°ë°˜)

        Returns:
            Tuple[QuestionType, float, QueryComplexity, bool]: (ì§ˆë¬¸ ìœ í˜•, ì‹ ë¢°ë„, ë³µì¡ë„, ê²€ìƒ‰ í•„ìš” ì—¬ë¶€)
        """
        try:
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = f"query_chain:{query}"

            # ìºì‹œ í™•ì¸
            if cache_key in self._classification_cache:
                self.logger.debug(f"Using cached chain classification for: {query[:50]}...")
                if hasattr(self, 'stats'):
                    self.stats['complexity_cache_hits'] = self.stats.get('complexity_cache_hits', 0) + 1
                return self._classification_cache[cache_key]

            if hasattr(self, 'stats'):
                self.stats['complexity_cache_misses'] = self.stats.get('complexity_cache_misses', 0) + 1

            start_time = time.time()

            # PromptChainExecutor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            llm = self.llm_fast if hasattr(self, 'llm_fast') and self.llm_fast else self.llm
            chain_executor = PromptChainExecutor(llm, self.logger)

            # ì²´ì¸ ìŠ¤í… ì •ì˜
            chain_steps = []

            # Step 1: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
            def build_question_type_prompt(prev_output, initial_input):
                query = prev_output.get("query") if isinstance(prev_output, dict) else (initial_input.get("query") if isinstance(initial_input, dict) else "")
                if not query:
                    query = str(prev_output) if not isinstance(prev_output, dict) else ""

                return f"""ë‹¤ìŒ ë²•ë¥  ì§ˆë¬¸ì˜ ìœ í˜•ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ë‹¤ìŒ ìœ í˜• ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:
1. precedent_search - íŒë¡€, ì‚¬ê±´, ë²•ì› íŒê²°, íŒì‹œì‚¬í•­ ê´€ë ¨
2. law_inquiry - ë²•ë¥  ì¡°ë¬¸, ë²•ë ¹, ê·œì •ì˜ ë‚´ìš©ì„ ë¬»ëŠ” ì§ˆë¬¸
3. legal_advice - ë²•ë¥  ì¡°ì–¸, í•´ì„, ê¶Œë¦¬ êµ¬ì œ ë°©ë²•ì„ ë¬»ëŠ” ì§ˆë¬¸
4. procedure_guide - ë²•ì  ì ˆì°¨, ì†Œì†¡ ë°©ë²•, ëŒ€ì‘ ë°©ë²•ì„ ë¬»ëŠ” ì§ˆë¬¸
5. term_explanation - ë²•ë¥  ìš©ì–´ì˜ ì •ì˜ë‚˜ ì˜ë¯¸ë¥¼ ë¬»ëŠ” ì§ˆë¬¸
6. general_question - ë²”ìš©ì ì¸ ë²•ë¥  ì§ˆë¬¸

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "question_type": "precedent_search" | "law_inquiry" | "legal_advice" | "procedure_guide" | "term_explanation" | "general_question",
    "confidence": 0.0-1.0,
    "reasoning": "íŒë‹¨ ê·¼ê±° (í•œêµ­ì–´)"
}}
"""

            chain_steps.append({
                "name": "question_type_classification",
                "prompt_builder": build_question_type_prompt,
                "input_extractor": lambda prev: {"query": query} if isinstance(prev, dict) or not prev else prev,
                "output_parser": lambda response, prev: ClassificationParser.parse_question_type_response(response),
                "validator": lambda output: output and isinstance(output, dict) and "question_type" in output,
                "required": True
            })

            # Step 2: ë²•ë¥  ë¶„ì•¼ ì¶”ì¶œ (ì§ˆë¬¸ ìœ í˜• ê¸°ë°˜)
            def build_legal_field_prompt(prev_output, initial_input):
                # prev_outputì€ Step 1ì˜ ê²°ê³¼ (question_type í¬í•¨)
                if not isinstance(prev_output, dict):
                    return None

                question_type = prev_output.get("question_type", "")
                query_value = initial_input.get("query") if isinstance(initial_input, dict) else query

                if not question_type:
                    return None

                return f"""ë‹¤ìŒ ì§ˆë¬¸ê³¼ ì§ˆë¬¸ ìœ í˜•ì„ ë°”íƒ•ìœ¼ë¡œ ë²•ë¥  ë¶„ì•¼ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query_value}
ì§ˆë¬¸ ìœ í˜•: {question_type}

ë²•ë¥  ë¶„ì•¼ ì˜ˆì‹œ:
- family_law (ê°€ì¡±ë²•): ì´í˜¼, ì–‘ìœ¡ê¶Œ, ìƒì†, ë¶€ì–‘ ë“±
- civil_law (ë¯¼ë²•): ê³„ì•½, ì†í•´ë°°ìƒ, ë¬¼ê¶Œ, ì±„ê¶Œ ë“±
- corporate_law (ê¸°ì—…ë²•): íšŒì‚¬ë²•, ìƒë²•, ê¸ˆìœµë²• ë“±
- intellectual_property (ì§€ì ì¬ì‚°ê¶Œ): íŠ¹í—ˆ, ìƒí‘œ, ì €ì‘ê¶Œ ë“±
- criminal_law (í˜•ë²•): í˜•ì‚¬ì†Œì†¡, ë²”ì£„ ë“±
- labor_law (ë…¸ë™ë²•): ê·¼ë¡œë²•, ê·¼ë¡œê¸°ì¤€ë²• ë“±
- administrative_law (í–‰ì •ë²•): í–‰ì •ì²˜ë¶„, í–‰ì •ì†Œì†¡ ë“±
- general (ì¼ë°˜): ë¶„ë¥˜ë˜ì§€ ì•ŠëŠ” ê²½ìš°

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "legal_field": "family_law" | "civil_law" | "corporate_law" | "intellectual_property" | "criminal_law" | "labor_law" | "administrative_law" | "general",
    "confidence": 0.0-1.0,
    "reasoning": "íŒë‹¨ ê·¼ê±° (í•œêµ­ì–´)"
}}
"""

            chain_steps.append({
                "name": "legal_field_extraction",
                "prompt_builder": build_legal_field_prompt,
                "input_extractor": lambda prev: prev,  # Step 1ì˜ ì¶œë ¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                "output_parser": lambda response, prev: ClassificationParser.parse_legal_field_response(response),
                "validator": lambda output: output is None or (isinstance(output, dict) and "legal_field" in output),
                "required": False,  # ì„ íƒ ë‹¨ê³„ (ì—†ì–´ë„ ì§„í–‰ ê°€ëŠ¥)
                "skip_if": lambda prev: not isinstance(prev, dict) or not prev.get("question_type")
            })

            # Step 3: ë³µì¡ë„ í‰ê°€ (ì§ˆë¬¸ + ìœ í˜• + ë¶„ì•¼ ê¸°ë°˜)
            def build_complexity_prompt(prev_output, initial_input):
                # prev_outputì€ Step 2ì˜ ê²°ê³¼ ë˜ëŠ” Step 1ì˜ ê²°ê³¼
                if not isinstance(prev_output, dict):
                    prev_output = {}

                # Step 1ê³¼ Step 2ì˜ ê²°ê³¼ í†µí•©
                question_type = prev_output.get("question_type", "")
                legal_field = prev_output.get("legal_field", "")
                query_value = initial_input.get("query") if isinstance(initial_input, dict) else query

                # Step 1 ê²°ê³¼ì—ì„œ ì§ˆë¬¸ ìœ í˜• ì°¾ê¸°
                if not question_type:
                    # prev_outputì—ì„œ ì§ˆë¬¸ ìœ í˜• ì°¾ê¸° ì‹œë„ (ì´ì „ ë‹¨ê³„ ì¶œë ¥ í†µí•©)
                    if isinstance(prev_output, dict):
                        question_type = prev_output.get("question_type", "")

                return f"""ë‹¤ìŒ ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query_value}
ì§ˆë¬¸ ìœ í˜•: {question_type}
ë²•ë¥  ë¶„ì•¼: {legal_field if legal_field else "ë¯¸ì§€ì •"}

ë‹¤ìŒ ë³µì¡ë„ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:
1. simple (ê°„ë‹¨):
   - ë‹¨ìˆœ ì¸ì‚¬ë§: "ì•ˆë…•í•˜ì„¸ìš”", "ê³ ë§ˆì›Œìš”" ë“±
   - ë§¤ìš° ê°„ë‹¨í•œ ë²•ë¥  ìš©ì–´ ì •ì˜ (10ì ì´ë‚´, ì¼ë°˜ ìƒì‹ ìˆ˜ì¤€)
   - ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ê²½ìš°

2. moderate (ì¤‘ê°„):
   - íŠ¹ì • ë²•ë ¹ ì¡°ë¬¸ ì¡°íšŒ: "ë¯¼ë²• ì œ123ì¡°", "í˜•ë²• ì œ250ì¡°" ë“±
   - ë‹¨ì¼ ë²•ë¥  ê°œë… ì§ˆë¬¸: "ê³„ì•½ì´ë€?", "ì†í•´ë°°ìƒì˜ ìš”ê±´ì€?"
   - ë‹¨ì¼ íŒë¡€ ê²€ìƒ‰: "XX ì‚¬ê±´ íŒë¡€"
   - ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ë§Œ ë‹¨ìˆœí•œ ê²½ìš°

3. complex (ë³µì¡):
   - ë¹„êµ ë¶„ì„ ì§ˆë¬¸: "ê³„ì•½ í•´ì§€ì™€ í•´ì œì˜ ì°¨ì´", "ì´í˜¼ê³¼ ì¬í˜¼ì˜ ì°¨ì´"
   - ì ˆì°¨/ë°©ë²• ì§ˆë¬¸: "ì´í˜¼ ì ˆì°¨ëŠ”?", "ì†Œì†¡ ë°©ë²•ì€?"
   - ë‹¤ì¤‘ ë²•ë ¹/íŒë¡€ í•„ìš”: "ì†í•´ë°°ìƒ ê´€ë ¨ ìµœê·¼ íŒë¡€ì™€ ë²•ë ¹"
   - ë³µí•©ì  ë²•ë¥  ë¶„ì„: "ê³„ì•½ í•´ì§€ ì‹œ ìœ„ì•½ê¸ˆê³¼ ì†í•´ë°°ìƒ"
   - ê²€ìƒ‰ê³¼ ë¶„ì„ì´ ëª¨ë‘ í•„ìš”í•œ ê²½ìš°

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "complexity": "simple" | "moderate" | "complex",
    "confidence": 0.0-1.0,
    "reasoning": "íŒë‹¨ ê·¼ê±° (í•œêµ­ì–´)"
}}
"""

            chain_steps.append({
                "name": "complexity_assessment",
                "prompt_builder": build_complexity_prompt,
                "input_extractor": lambda prev: prev,  # ì´ì „ ë‹¨ê³„ì˜ í†µí•© ê²°ê³¼ ì‚¬ìš©
                "output_parser": lambda response, prev: ClassificationParser.parse_complexity_response(response),
                "validator": lambda output: output and isinstance(output, dict) and "complexity" in output,
                "required": True
            })

            # Step 4: ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ (ë³µì¡ë„ ê¸°ë°˜)
            def build_search_necessity_prompt(prev_output, initial_input):
                # prev_outputì€ Step 3ì˜ ê²°ê³¼ (complexity í¬í•¨)
                if not isinstance(prev_output, dict):
                    return None

                complexity = prev_output.get("complexity", "")
                query_value = initial_input.get("query") if isinstance(initial_input, dict) else query

                if not complexity:
                    return None

                return f"""ë‹¤ìŒ ì§ˆë¬¸ì˜ ê²€ìƒ‰ í•„ìš”ì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query_value}
ë³µì¡ë„: {complexity}

ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°:
- simpleì´ ì•„ë‹Œ ê²½ìš° (moderate ë˜ëŠ” complex)
- ë²•ë¥  ì¡°ë¬¸, íŒë¡€, ê·œì •ì„ ì°¾ì•„ì•¼ í•˜ëŠ” ê²½ìš°
- ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°

ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ê²½ìš°:
- simple ë³µì¡ë„ì¸ ê²½ìš°
- ì¼ë°˜ì ì¸ ë²•ë¥  ìƒì‹ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ê²½ìš°
- ë‹¨ìˆœ ì¸ì‚¬ë§ì´ë‚˜ ì •ì˜ ì§ˆë¬¸

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "needs_search": true | false,
    "confidence": 0.0-1.0,
    "reasoning": "íŒë‹¨ ê·¼ê±° (í•œêµ­ì–´)"
}}
"""

            chain_steps.append({
                "name": "search_necessity_assessment",
                "prompt_builder": build_search_necessity_prompt,
                "input_extractor": lambda prev: prev,  # Step 3ì˜ ì¶œë ¥ ì‚¬ìš©
                "output_parser": lambda response, prev: ClassificationParser.parse_search_necessity_response(response),
                "validator": lambda output: output is None or (isinstance(output, dict) and "needs_search" in output),
                "required": False,  # ì„ íƒ ë‹¨ê³„
                "skip_if": lambda prev: not isinstance(prev, dict) or not prev.get("complexity")
            })

            # ì²´ì¸ ì‹¤í–‰
            initial_input_dict = {"query": query}
            chain_result = chain_executor.execute_chain(
                chain_steps=chain_steps,
                initial_input=initial_input_dict,
                max_iterations=2,
                stop_on_failure=False
            )

            # ê²°ê³¼ ì¶”ì¶œ ë° ë³€í™˜
            chain_history = chain_result.get("chain_history", [])

            # Step 1 ê²°ê³¼: ì§ˆë¬¸ ìœ í˜•
            question_type_result = None
            for step in chain_history:
                if step.get("step_name") == "question_type_classification" and step.get("success"):
                    question_type_result = step.get("output", {})
                    break

            # Step 2 ê²°ê³¼: ë²•ë¥  ë¶„ì•¼ (ì„ íƒì )
            legal_field_result = None
            for step in chain_history:
                if step.get("step_name") == "legal_field_extraction" and step.get("success"):
                    legal_field_result = step.get("output", {})
                    break

            # Step 3 ê²°ê³¼: ë³µì¡ë„
            complexity_result = None
            for step in chain_history:
                if step.get("step_name") == "complexity_assessment" and step.get("success"):
                    complexity_result = step.get("output", {})
                    break

            # Step 4 ê²°ê³¼: ê²€ìƒ‰ í•„ìš”ì„±
            search_necessity_result = None
            for step in chain_history:
                if step.get("step_name") == "search_necessity_assessment" and step.get("success"):
                    search_necessity_result = step.get("output", {})
                    break

            # ê²°ê³¼ ë³€í™˜
            if not question_type_result or not isinstance(question_type_result, dict):
                raise ValueError("Question type classification failed")

            # QuestionType ë³€í™˜
            question_type_mapping = {
                "precedent_search": QuestionType.PRECEDENT_SEARCH,
                "law_inquiry": QuestionType.LAW_INQUIRY,
                "legal_advice": QuestionType.LEGAL_ADVICE,
                "procedure_guide": QuestionType.PROCEDURE_GUIDE,
                "term_explanation": QuestionType.TERM_EXPLANATION,
                "general_question": QuestionType.GENERAL_QUESTION,
            }
            question_type_str = question_type_result.get("question_type", "general_question")
            classified_type = question_type_mapping.get(question_type_str, QuestionType.GENERAL_QUESTION)
            confidence = float(question_type_result.get("confidence", 0.85))

            # QueryComplexity ë³€í™˜
            if complexity_result and isinstance(complexity_result, dict):
                complexity_str = complexity_result.get("complexity", "moderate")
            else:
                complexity_str = "moderate"

            complexity_mapping = {
                "simple": QueryComplexity.SIMPLE,
                "moderate": QueryComplexity.MODERATE,
                "complex": QueryComplexity.COMPLEX,
            }
            complexity = complexity_mapping.get(complexity_str, QueryComplexity.MODERATE)

            # ê²€ìƒ‰ í•„ìš”ì„±
            if search_necessity_result and isinstance(search_necessity_result, dict):
                needs_search = search_necessity_result.get("needs_search", True)
            else:
                # ë³µì¡ë„ ê¸°ë°˜ ê¸°ë³¸ê°’
                needs_search = complexity != QueryComplexity.SIMPLE

            elapsed_time = time.time() - start_time

            self.logger.info(
                f"âœ… [CHAIN CLASSIFICATION] "
                f"question_type={classified_type.value}, complexity={complexity.value}, "
                f"needs_search={needs_search}, confidence={confidence:.2f}, "
                f"(ì‹œê°„: {elapsed_time:.3f}s)"
            )

            result_tuple = (classified_type, confidence, complexity, needs_search)

            # ìºì‹œì— ì €ì¥
            if len(self._classification_cache) >= 100:
                oldest_key = next(iter(self._classification_cache))
                del self._classification_cache[oldest_key]

            self._classification_cache[cache_key] = result_tuple

            return result_tuple

        except Exception as e:
            self.logger.warning(f"Chain classification failed: {e}, using fallback")
            if hasattr(self, 'stats'):
                self.stats['complexity_fallback_count'] = self.stats.get('complexity_fallback_count', 0) + 1
            question_type, confidence = self._fallback_classification(query)
            complexity, needs_search = self._fallback_complexity_classification(query)
            return (question_type, confidence, complexity, needs_search)

    def _parse_question_type_response(self, response: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            import re

            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)

            # ê¸°ë³¸ê°’
            return {
                "question_type": "general_question",
                "confidence": 0.7,
                "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨"
            }
        except Exception as e:
            self.logger.warning(f"Failed to parse question type response: {e}")
            return {
                "question_type": "general_question",
                "confidence": 0.7,
                "reasoning": f"íŒŒì‹± ì—ëŸ¬: {e}"
            }

    def _parse_legal_field_response(self, response: str) -> Optional[Dict[str, Any]]:
        """ë²•ë¥  ë¶„ì•¼ ì¶”ì¶œ ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            import re

            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)

            return None
        except Exception as e:
            self.logger.warning(f"Failed to parse legal field response: {e}")
            return None

    def _parse_complexity_response(self, response: str) -> Dict[str, Any]:
        """ë³µì¡ë„ í‰ê°€ ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            import re

            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)

            # ê¸°ë³¸ê°’
            return {
                "complexity": "moderate",
                "confidence": 0.7,
                "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨"
            }
        except Exception as e:
            self.logger.warning(f"Failed to parse complexity response: {e}")
            return {
                "complexity": "moderate",
                "confidence": 0.7,
                "reasoning": f"íŒŒì‹± ì—ëŸ¬: {e}"
            }

    def _parse_search_necessity_response(self, response: str) -> Optional[Dict[str, Any]]:
        """ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            import re

            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)

            return None
        except Exception as e:
            self.logger.warning(f"Failed to parse search necessity response: {e}")
            return None

    def _classify_query_and_complexity_with_llm(self, query: str) -> Tuple[QuestionType, float, QueryComplexity, bool]:
        """ClassificationHandler.classify_query_and_complexity_with_llm ë˜í¼"""
        return self.classification_handler.classify_query_and_complexity_with_llm(query)

    def _classify_complexity_with_llm(self, query: str, query_type: str = "") -> Tuple[QueryComplexity, bool]:
        """ClassificationHandler.classify_complexity_with_llm ë˜í¼"""
        return self.classification_handler.classify_complexity_with_llm(query, query_type)

    # Helper methods for retrieve_documents
    def _get_query_type_str(self, query_type) -> str:
        """WorkflowUtils.get_query_type_str ë˜í¼"""
        return WorkflowUtils.get_query_type_str(query_type)

    def _normalize_query_type_for_prompt(self, query_type) -> str:
        """WorkflowUtils.normalize_query_type_for_prompt ë˜í¼"""
        return WorkflowUtils.normalize_query_type_for_prompt(query_type, self.logger)

    def _optimize_search_query(
        self,
        query: str,
        query_type: str,
        extracted_keywords: List[str],
        legal_field: str
    ) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” (LLM ê°•í™” í¬í•¨, í´ë°± ì§€ì›)

        Returns:
            {
                "semantic_query": "ì˜ë¯¸ì  ê²€ìƒ‰ìš© ì¿¼ë¦¬",
                "keyword_queries": ["í‚¤ì›Œë“œ query 1", ...],
                "expanded_keywords": ["í™•ì¥ëœ í‚¤ì›Œë“œ", ...],
                "llm_enhanced": bool  # LLM ê°•í™” ì‚¬ìš© ì—¬ë¶€
            }
        """
        """QueryEnhancer.optimize_search_query ë˜í¼"""
        return self.query_enhancer.optimize_search_query(query, query_type, extracted_keywords, legal_field)

    def _normalize_legal_terms(self, query: str, keywords: List[str]) -> List[str]:
        """QueryEnhancer.normalize_legal_terms ë˜í¼"""
        return self.query_enhancer.normalize_legal_terms(query, keywords)

    def _expand_legal_terms(
        self,
        terms: List[str],
        legal_field: str
    ) -> List[str]:
        """QueryEnhancer.expand_legal_terms ë˜í¼"""
        return self.query_enhancer.expand_legal_terms(terms, legal_field)

    def _clean_query_for_fallback(self, query: str) -> str:
        """QueryEnhancer.clean_query_for_fallback ë˜í¼"""
        return self.query_enhancer.clean_query_for_fallback(query)

    def _build_semantic_query(self, query: str, expanded_terms: List[str]) -> str:
        """QueryEnhancer.build_semantic_query ë˜í¼"""
        return self.query_enhancer.build_semantic_query(query, expanded_terms)

    def _build_keyword_queries(
        self,
        query: str,
        expanded_terms: List[str],
        query_type: str
    ) -> List[str]:
        """QueryEnhancer.build_keyword_queries ë˜í¼"""
        return self.query_enhancer.build_keyword_queries(query, expanded_terms, query_type)

    # Phase 7 ë¦¬íŒ©í† ë§: ì¿¼ë¦¬ ê°•í™” ê´€ë ¨ ë©”ì„œë“œëŠ” QueryEnhancerë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _enhance_query_with_llm(
        self,
        query: str,
        query_type: str,
        extracted_keywords: List[str],
        legal_field: str
    ) -> Optional[Dict[str, Any]]:
        """QueryEnhancer.enhance_query_with_llm ë˜í¼"""
        return self.query_enhancer.enhance_query_with_llm(query, query_type, extracted_keywords, legal_field)

    # Phase 7 ë¦¬íŒ©í† ë§: ì¿¼ë¦¬ ê°•í™” ê´€ë ¨ ë©”ì„œë“œëŠ” QueryEnhancerë¡œ ì´ë™ë¨
    def _build_query_enhancement_prompt(
        self,
        query: str,
        query_type: str,
        extracted_keywords: List[str],
        legal_field: str
    ) -> str:
        """QueryEnhancer.build_query_enhancement_prompt ë˜í¼"""
        return self.query_enhancer.build_query_enhancement_prompt(query, query_type, extracted_keywords, legal_field)

    def _format_field_info(self, legal_field: str, field_info: Dict[str, Any]) -> str:
        """QueryEnhancer.format_field_info ë˜í¼"""
        return self.query_enhancer.format_field_info(legal_field, field_info)


    def _enhance_query_with_chain(
        self,
        query: str,
        query_type: str,
        extracted_keywords: List[str],
        legal_field: str
    ) -> Optional[Dict[str, Any]]:
        """QueryEnhancer.enhance_query_with_chain ë˜í¼"""
        return self.query_enhancer.enhance_query_with_chain(query, query_type, extracted_keywords, legal_field)

    # Phase 7 ë¦¬íŒ©í† ë§: íŒŒì‹± ë©”ì„œë“œë“¤ì€ QueryParserë¡œ ì´ë™ë¨ (response_parsers ëª¨ë“ˆ ì‚¬ìš©)
    def _parse_llm_query_enhancement(self, llm_output: str) -> Optional[Dict[str, Any]]:
        """QueryEnhancer.parse_llm_query_enhancement ë˜í¼"""
        return self.query_enhancer.parse_llm_query_enhancement(llm_output)

    def _determine_search_parameters(
        self,
        query_type: str,
        query_complexity: int,
        keyword_count: int,
        is_retry: bool
    ) -> Dict[str, Any]:
        """QueryEnhancer.determine_search_parameters ë˜í¼"""
        return self.query_enhancer.determine_search_parameters(query_type, query_complexity, keyword_count, is_retry)

    # Phase 2 ë¦¬íŒ©í† ë§: ê²€ìƒ‰ ê´€ë ¨ ë©”ì„œë“œëŠ” SearchHandlerë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _check_cache(self, state: LegalWorkflowState, query: str, query_type_str: str, start_time: float) -> bool:
        """SearchHandler.check_cache ë˜í¼"""
        return self.search_handler.check_cache(state, query, query_type_str, start_time)

    def _semantic_search(self, query: str, k: Optional[int] = None) -> Tuple[List[Dict[str, Any]], int]:
        """SearchHandler.semantic_search ë˜í¼"""
        return self.search_handler.semantic_search(query, k)

    def _keyword_search(
        self,
        query: str,
        query_type_str: str,
        limit: Optional[int] = None,
        legal_field: str = "",
        extracted_keywords: List[str] = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """í–¥ìƒëœ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        try:
            category_mapping = self._get_category_mapping()
            categories_to_search = category_mapping.get(query_type_str, ["civil_law"])

            # ì§€ì›ë˜ëŠ” ë²•ë¥  ë¶„ì•¼ë§Œ ë§¤í•‘ (ë¯¼ì‚¬ë²•, ì§€ì‹ì¬ì‚°ê¶Œë²•, í–‰ì •ë²•, í˜•ì‚¬ë²•ë§Œ)
            field_category_map = {
                "civil": "civil_law",
                "criminal": "criminal_law",
                "intellectual_property": "intellectual_property",
                "administrative": "administrative_law"
            }

            preferred_category = None
            if legal_field and legal_field in field_category_map:
                preferred_category = field_category_map[legal_field]
                if preferred_category in categories_to_search:
                    categories_to_search.remove(preferred_category)
                    categories_to_search.insert(0, preferred_category)

            keyword_results = []
            search_limit = limit if limit is not None else WorkflowConstants.CATEGORY_SEARCH_LIMIT

            # í™•ì¥ëœ í‚¤ì›Œë“œë¥¼ ì¿¼ë¦¬ì— ì¶”ê°€
            enhanced_query = query
            if extracted_keywords and len(extracted_keywords) > 0:
                # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì¶”ê°€ (ì¿¼ë¦¬ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡)
                safe_keywords = [kw for kw in extracted_keywords[:3] if isinstance(kw, str)]
                if safe_keywords:
                    enhanced_query = f"{query} {' '.join(safe_keywords)}"

            print(f"[DEBUG] _keyword_search: Searching {len(categories_to_search)} categories with query='{enhanced_query[:50]}...', limit={search_limit}")

            for category in categories_to_search:
                # í‚¤ì›Œë“œ ê²€ìƒ‰ì€ í•­ìƒ FTS5 ê²€ìƒ‰ ìˆ˜í–‰ (force_fts=True)
                print(f"[DEBUG] _keyword_search: Searching category={category}")
                category_docs = self.data_connector.search_documents(
                    enhanced_query, category, limit=search_limit, force_fts=True
                )
                print(f"[DEBUG] _keyword_search: Category {category} returned {len(category_docs)} documents")

                for doc in category_docs:
                    doc['search_type'] = 'keyword'
                    doc['category'] = category
                    # ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„ ì ìˆ˜ ì¶”ê°€
                    if preferred_category and category == preferred_category:
                        doc['category_boost'] = 1.2
                    else:
                        doc['category_boost'] = 1.0

                keyword_results.extend(category_docs)
                self.logger.info(f"Found {len(category_docs)} documents in category: {category}")

            print(f"[DEBUG] _keyword_search: Total results={len(keyword_results)}")
            return keyword_results, len(keyword_results)
        except Exception as e:
            self.logger.warning(f"Keyword search failed: {e}")
            return [], 0

    def _merge_and_rerank_search_results(
        self,
        semantic_results: List[Dict],
        keyword_results: List[Dict],
        query: str,
        optimized_queries: Dict[str, Any],
        rerank_params: Dict[str, Any]
    ) -> List[Dict]:
        """SearchHandler.merge_and_rerank_search_results ë˜í¼"""
        return self.search_handler.merge_and_rerank_search_results(
            semantic_results, keyword_results, query, optimized_queries, rerank_params
        )

    def _filter_low_quality_results(
        self,
        documents: List[Dict],
        min_relevance: float,
        max_diversity: int
    ) -> List[Dict]:
        """SearchHandler.filter_low_quality_results ë˜í¼"""
        return self.search_handler.filter_low_quality_results(
            documents, min_relevance, max_diversity
        )

    def _apply_metadata_filters(
        self,
        documents: List[Dict],
        query_type: str,
        legal_field: str
    ) -> List[Dict]:
        """SearchHandler.apply_metadata_filters ë˜í¼"""
        return self.search_handler.apply_metadata_filters(
            documents, query_type, legal_field
        )

    def _calculate_field_match(self, legal_field: str, doc_category: str) -> float:
        """SearchHandler.calculate_field_match ë˜í¼"""
        return self.search_handler.calculate_field_match(legal_field, doc_category)

    def _calculate_recency_score(self, doc_date: Any) -> float:
        """SearchHandler.calculate_recency_score ë˜í¼"""
        return self.search_handler.calculate_recency_score(doc_date)

    def _calculate_source_credibility(self, source: str) -> float:
        """SearchHandler.calculate_source_credibility ë˜í¼"""
        return self.search_handler.calculate_source_credibility(source)

    def _merge_search_results(self, semantic_results: List[Dict], keyword_results: List[Dict]) -> List[Dict]:
        """SearchHandler.merge_search_results ë˜í¼"""
        return self.search_handler.merge_search_results(semantic_results, keyword_results)

    def _update_search_metadata(
        self,
        state: LegalWorkflowState,
        semantic_count: int,
        keyword_count: int,
        documents: List[Dict],
        query_type_str: str,
        start_time: float,
        optimized_queries: Optional[Dict[str, Any]] = None
    ) -> None:
        """SearchHandler.update_search_metadata ë˜í¼"""
        self.search_handler.update_search_metadata(
            state, semantic_count, keyword_count, documents,
            query_type_str, start_time, optimized_queries
        )

    def _fallback_search(self, state: LegalWorkflowState) -> None:
        """SearchHandler.fallback_search ë˜í¼"""
        self.search_handler.fallback_search(state)


    @observe(name="expand_keywords_ai")
    @with_state_optimization("expand_keywords_ai", enable_reduction=True)
    def expand_keywords_ai(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """AI í‚¤ì›Œë“œ í™•ì¥ (AIKeywordGenerator ì‚¬ìš©)"""
        try:
            # ë°©ë²• 1: ë…¸ë“œ í˜¸ì¶œ ì¶”ì  - ì‹¤í–‰ ê¸°ë¡ ë‚¨ê¸°ê¸°
            # ì§ì ‘ ì„¤ì •í•˜ì—¬ ìƒíƒœ ìµœì í™”ë¡œ ì¸í•œ ì†ì‹¤ ë°©ì§€
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            # ì¤‘ìš”: query_complexityì™€ needs_search ë³´ì¡´
            preserved_complexity = state.get("metadata", {}).get("query_complexity")
            preserved_needs_search = state.get("metadata", {}).get("needs_search")
            state["metadata"] = dict(state["metadata"])  # ë³µì‚¬ë³¸ ìƒì„±
            # ë³´ì¡´ëœ ê°’ ë³µì›
            if preserved_complexity:
                state["metadata"]["query_complexity"] = preserved_complexity
            if preserved_needs_search is not None:
                state["metadata"]["needs_search"] = preserved_needs_search
            state["metadata"]["_last_executed_node"] = "expand_keywords_ai"
            # common ê·¸ë£¹ì—ë„ ì„¤ì • (nested êµ¬ì¡° ì§€ì›)
            if "common" not in state or not isinstance(state.get("common"), dict):
                state["common"] = {}
            if "metadata" not in state["common"]:
                state["common"]["metadata"] = {}
            state["common"]["metadata"]["_last_executed_node"] = "expand_keywords_ai"

            start_time = time.time()

            if not self.ai_keyword_generator:
                self.logger.debug("AIKeywordGenerator not available, skipping")
                return state

            # ê¸°ì¡´ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„±
            keywords = self._get_state_value(state, "extracted_keywords", [])
            if len(keywords) == 0:
                query = self._get_state_value(state, "query", "")
                query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", "general_question"))
                # LegalKeywordMapperë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self.keyword_mapper.get_keywords_for_question(query, query_type_str)
                # ì¤‘ë³µ ì œê±° (hashable íƒ€ì…ë§Œ í•„í„°ë§)
                keywords = [kw for kw in keywords if isinstance(kw, (str, int, float, tuple)) and kw is not None]
                keywords = list(set(keywords))  # ì¤‘ë³µ ì œê±°
                self._set_state_value(state, "extracted_keywords", keywords)
                self.logger.info(f"Generated base keywords: {len(keywords)} keywords from query")

            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            domain = self._get_domain_from_query_type(query_type_str)

            # AI í‚¤ì›Œë“œ í™•ì¥ (ë™ê¸° ì‹¤í–‰ - ë¹„ë™ê¸°ëŠ” ì§€ì› ì•ˆë¨)
            expansion_result = asyncio.run(
                self.ai_keyword_generator.expand_domain_keywords(
                    domain=domain,
                    base_keywords=keywords,
                    target_count=30
                )
            )

            if expansion_result.api_call_success:
                # í™•ì¥ëœ í‚¤ì›Œë“œ ì¶”ê°€
                all_keywords = keywords + expansion_result.expanded_keywords
                # hashable íƒ€ì…ë§Œ í•„í„°ë§ (ìŠ¬ë¼ì´ìŠ¤ ê°ì²´ ë“± unhashable íƒ€ì… ë°©ì§€)
                all_keywords = [kw for kw in all_keywords if isinstance(kw, (str, int, float, tuple)) and kw is not None]
                all_keywords = list(set(all_keywords))
                self._set_state_value(state, "extracted_keywords", all_keywords)

                # ë©”íƒ€ë°ì´í„° ì €ì¥ (ìƒˆ ë”•ì…”ë„ˆë¦¬ ìƒì„±)
                self._set_state_value(state, "ai_keyword_expansion", {
                    "domain": expansion_result.domain,
                    "original_keywords": expansion_result.base_keywords,
                    "expanded_keywords": expansion_result.expanded_keywords,
                    "confidence": expansion_result.confidence,
                    "method": expansion_result.expansion_method
                })

                processing_time = self._update_processing_time(state, start_time)
                self._add_step(state, "AI í‚¤ì›Œë“œ í™•ì¥",
                             f"AI í‚¤ì›Œë“œ í™•ì¥ ì™„ë£Œ: +{len(expansion_result.expanded_keywords)}ê°œ (ì´ {len(all_keywords)}ê°œ, {processing_time:.3f}s)")

                self.logger.info(f"AI keyword expansion: {len(expansion_result.expanded_keywords)} keywords added in {processing_time:.3f}s")
            else:
                # í´ë°± ì‚¬ìš©
                self.logger.warning("AI keyword expansion failed, using fallback")
                fallback_keywords = self.ai_keyword_generator.expand_keywords_with_fallback(
                    domain, keywords
                )
                all_keywords = keywords + fallback_keywords
                # hashable íƒ€ì…ë§Œ í•„í„°ë§ (ìŠ¬ë¼ì´ìŠ¤ ê°ì²´ ë“± unhashable íƒ€ì… ë°©ì§€)
                all_keywords = [kw for kw in all_keywords if isinstance(kw, (str, int, float, tuple)) and kw is not None]
                all_keywords = list(set(all_keywords))
                self._set_state_value(state, "extracted_keywords", all_keywords)

                # ë©”íƒ€ë°ì´í„° ì €ì¥ (ìƒˆ ë”•ì…”ë„ˆë¦¬ ìƒì„±)
                self._set_state_value(state, "ai_keyword_expansion", {
                    "domain": domain,
                    "original_keywords": keywords,
                    "expanded_keywords": fallback_keywords,
                    "confidence": 0.5,
                    "method": "fallback"
                })

                processing_time = self._update_processing_time(state, start_time)
                self._add_step(state, "AI í‚¤ì›Œë“œ í™•ì¥ (í´ë°±)",
                             f"AI í‚¤ì›Œë“œ í™•ì¥ (í´ë°±): +{len(fallback_keywords)}ê°œ ({processing_time:.3f}s)")
                self.logger.info(f"Fallback keyword expansion: {len(fallback_keywords)} keywords added in {processing_time:.3f}s")

        except Exception as e:
            self._handle_error(state, str(e), "AI í‚¤ì›Œë“œ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

        return state

    def _get_domain_from_query_type(self, query_type: str) -> str:
        """WorkflowUtils.get_domain_from_query_type ë˜í¼"""
        return WorkflowUtils.get_domain_from_query_type(query_type)

    def _get_supported_domains(self) -> List[LegalDomain]:
        """WorkflowUtils.get_supported_domains ë˜í¼"""
        return WorkflowUtils.get_supported_domains()

    def _is_supported_domain(self, domain: Optional[LegalDomain]) -> bool:
        """WorkflowUtils.is_supported_domain ë˜í¼"""
        return WorkflowUtils.is_supported_domain(domain)

    @observe(name="process_legal_terms")
    @with_state_optimization("process_legal_terms", enable_reduction=True)
    def process_legal_terms(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ë²•ë¥  ìš©ì–´ ì¶”ì¶œ ë° í†µí•© (ë¬¸ì„œ ê²€ìƒ‰ í›„, ë‹µë³€ ìƒì„± ì „)"""
        try:
            # ë°©ë²• 1: ë…¸ë“œ í˜¸ì¶œ ì¶”ì  - ì‹¤í–‰ ê¸°ë¡ ë‚¨ê¸°ê¸°
            # ì§ì ‘ ì„¤ì •í•˜ì—¬ ìƒíƒœ ìµœì í™”ë¡œ ì¸í•œ ì†ì‹¤ ë°©ì§€
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            state["metadata"] = dict(state["metadata"])  # ë³µì‚¬ë³¸ ìƒì„±
            state["metadata"]["_last_executed_node"] = "process_legal_terms"
            # common ê·¸ë£¹ì—ë„ ì„¤ì • (nested êµ¬ì¡° ì§€ì›)
            if "common" not in state or not isinstance(state.get("common"), dict):
                state["common"] = {}
            if "metadata" not in state["common"]:
                state["common"]["metadata"] = {}
            state["common"]["metadata"]["_last_executed_node"] = "process_legal_terms"

            start_time = time.time()

            retrieved_docs = self._get_state_value(state, "retrieved_docs", [])
            all_terms = self._extract_terms_from_documents(retrieved_docs)
            self.logger.info(f"ì¶”ì¶œëœ ìš©ì–´ ìˆ˜: {len(all_terms)}")

            # ê¸°ì¡´ ì¬ì‹œë„ ì¹´ìš´í„° ë³´ì¡´ (ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œ ì†ì‹¤ ë°©ì§€)
            # ê°•í™”ëœ ë¡œê¹…ìœ¼ë¡œ ìƒíƒœ í™•ì¸
            existing_metadata_direct = state.get("metadata", {})
            existing_metadata_common = state.get("common", {}).get("metadata", {}) if isinstance(state.get("common"), dict) else {}
            existing_metadata = existing_metadata_direct if isinstance(existing_metadata_direct, dict) else {}
            if isinstance(existing_metadata_common, dict):
                existing_metadata = {**existing_metadata, **existing_metadata_common}

            existing_top_level = state.get("retry_count", 0)

            # ê¸°ì¡´ ì¬ì‹œë„ ì¹´ìš´í„° ì €ì¥
            saved_gen_retry = max(
                existing_metadata.get("generation_retry_count", 0),
                existing_top_level
            )
            saved_val_retry = existing_metadata.get("validation_retry_count", 0)

            self.logger.debug(
                f"[METADATA READ] process_legal_terms: gen_retry={saved_gen_retry}, "
                f"val_retry={saved_val_retry}, top_level={existing_top_level}"
            )

            if all_terms:
                representative_terms = self._integrate_and_process_terms(all_terms)
                metadata = dict(existing_metadata)  # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë³µì‚¬
                metadata["extracted_terms"] = representative_terms
                metadata["total_terms_extracted"] = len(all_terms)
                metadata["unique_terms"] = len(representative_terms)
                # ì¬ì‹œë„ ì¹´ìš´í„° ë³´ì¡´
                metadata["generation_retry_count"] = saved_gen_retry
                metadata["validation_retry_count"] = saved_val_retry
                metadata["_last_executed_node"] = "process_legal_terms"
                state["metadata"] = metadata
                # common ê·¸ë£¹ì—ë„ ë™ê¸°í™”
                if "common" not in state or not isinstance(state.get("common"), dict):
                    state["common"] = {}
                if "metadata" not in state["common"]:
                    state["common"]["metadata"] = {}
                state["common"]["metadata"].update(metadata)
                # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì €ì¥
                state["retry_count"] = saved_gen_retry

                self.logger.debug(
                    f"[METADATA SAVE] process_legal_terms: gen_retry={saved_gen_retry}, "
                    f"val_retry={saved_val_retry} (preserved)"
                )

                self._set_state_value(state, "metadata", metadata)
                self._add_step(state, "ìš©ì–´ í†µí•© ì™„ë£Œ", f"ìš©ì–´ í†µí•© ì™„ë£Œ: {len(representative_terms)}ê°œ")
                self.logger.info(f"í†µí•©ëœ ìš©ì–´ ìˆ˜: {len(representative_terms)}")
            else:
                metadata = dict(existing_metadata)  # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë³µì‚¬
                metadata["extracted_terms"] = []
                # ì¬ì‹œë„ ì¹´ìš´í„° ë³´ì¡´
                metadata["generation_retry_count"] = saved_gen_retry
                metadata["validation_retry_count"] = saved_val_retry
                metadata["_last_executed_node"] = "process_legal_terms"
                state["metadata"] = metadata
                # common ê·¸ë£¹ì—ë„ ë™ê¸°í™”
                if "common" not in state or not isinstance(state.get("common"), dict):
                    state["common"] = {}
                if "metadata" not in state["common"]:
                    state["common"]["metadata"] = {}
                state["common"]["metadata"].update(metadata)
                # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì €ì¥
                state["retry_count"] = saved_gen_retry

                self.logger.debug(
                    f"[METADATA SAVE] process_legal_terms (no terms): gen_retry={saved_gen_retry}, "
                    f"val_retry={saved_val_retry} (preserved)"
                )

                self._set_state_value(state, "metadata", metadata)
                self._add_step(state, "ìš©ì–´ ì¶”ì¶œ ì—†ìŒ", "ìš©ì–´ ì¶”ì¶œ ì—†ìŒ (ë¬¸ì„œ ë‚´ìš© ë¶€ì¡±)")

            # ë°˜í™˜ ì§ì „ ìµœì¢… ìƒíƒœ í™•ì¸
            final_meta_direct = state.get("metadata", {})
            final_meta_common = state.get("common", {}).get("metadata", {}) if isinstance(state.get("common"), dict) else {}
            final_gen_check = final_meta_direct.get("generation_retry_count", 0) if isinstance(final_meta_direct, dict) else 0
            final_gen_common = final_meta_common.get("generation_retry_count", 0) if isinstance(final_meta_common, dict) else 0
            final_top_check = state.get("retry_count", 0)

            self.logger.debug(
                f"[METADATA FINAL] process_legal_terms: gen_retry={max(final_gen_check, final_gen_common, final_top_check)} "
                f"(metadata={final_gen_check}, common={final_gen_common}, top={final_top_check})"
            )

            self._update_processing_time(state, start_time)
        except Exception as e:
            self._handle_error(state, str(e), "ë²•ë¥  ìš©ì–´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            metadata = self._get_state_value(state, "metadata", {})
            if not isinstance(metadata, dict):
                metadata = {}
            metadata["extracted_terms"] = []
            self._set_state_value(state, "metadata", metadata)
        return state

    def _extract_terms_from_documents(self, docs: List[Dict]) -> List[str]:
        """ë¬¸ì„œì—ì„œ ë²•ë¥  ìš©ì–´ ì¶”ì¶œ"""
        return DocumentExtractor.extract_terms_from_documents(docs)

    def _integrate_and_process_terms(self, all_terms: List[str]) -> List[str]:
        """ìš©ì–´ í†µí•© ë° ì²˜ë¦¬"""
        processed_terms = self.term_integrator.integrate_terms(all_terms)
        return [term["representative_term"] for term in processed_terms]

    @observe(name="generate_answer_enhanced")
    @with_state_optimization("generate_answer_enhanced", enable_reduction=True)
    def generate_answer_enhanced(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ê°œì„ ëœ ë‹µë³€ ìƒì„± - UnifiedPromptManager í™œìš©"""
        try:
            # ì´ì „ì— ì‹¤í–‰ëœ ë…¸ë“œ í™•ì¸
            metadata = state.get("metadata", {})
            if not isinstance(metadata, dict):
                metadata = {}
            # common ê·¸ë£¹ì—ì„œë„ í™•ì¸ (nested êµ¬ì¡° ì§€ì›)
            if "common" in state and isinstance(state.get("common"), dict):
                common_metadata = state["common"].get("metadata", {})
                if isinstance(common_metadata, dict):
                    metadata = {**metadata, **common_metadata}

            last_executed_node = metadata.get("_last_executed_node", "")

            # validation ì¬ì‹œë„ë¡œ í˜¸ì¶œëœ ê²½ìš° (validate_answer_quality ì´í›„)
            is_retry = (last_executed_node == "validate_answer_quality")
            if is_retry:
                if self.retry_manager.should_allow_retry(state, "validation"):
                    self.retry_manager.increment_retry_count(state, "validation")

            start_time = time.time()

            query_type = self._get_state_value(state, "query_type", "")
            query = self._get_state_value(state, "query", "")
            question_type, domain = self._get_question_type_and_domain(query_type, query)
            model_type = ModelType.GEMINI if self.config.llm_provider == "google" else ModelType.OLLAMA
            extracted_keywords = self._get_state_value(state, "extracted_keywords", [])

            # í”„ë¡¬í”„íŠ¸ ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© (ìˆëŠ” ê²½ìš°)
            prompt_optimized_context = self._get_state_value(state, "prompt_optimized_context", {})
            retrieved_docs = self._get_state_value(state, "retrieved_docs", [])

            # ê°œì„  ì‚¬í•­ 9: retrieved_docsë¥¼ ìµœì¢… stateì— ëª…í™•í•˜ê²Œ ë³´ì¡´
            if retrieved_docs:
                # ìµœìƒìœ„ ë ˆë²¨ê³¼ search ê·¸ë£¹ ëª¨ë‘ì— ì €ì¥
                self._set_state_value(state, "retrieved_docs", retrieved_docs)
                # search ê·¸ë£¹ ì§ì ‘ ì €ì¥
                if "search" not in state:
                    state["search"] = {}
                state["search"]["retrieved_docs"] = retrieved_docs
                # common ê·¸ë£¹ì—ë„ ì €ì¥í•˜ì—¬ reduction í›„ì—ë„ ìœ ì§€
                if "common" not in state:
                    state["common"] = {}
                if "search" not in state["common"]:
                    state["common"]["search"] = {}
                state["common"]["search"]["retrieved_docs"] = retrieved_docs

            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ global cacheì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° ì‹œë„
            if not retrieved_docs or len(retrieved_docs) == 0:
                try:
                    from core.agents.node_wrappers import _global_search_results_cache
                    if _global_search_results_cache:
                        cached_docs = _global_search_results_cache.get("retrieved_docs", [])
                        if cached_docs:
                            retrieved_docs = cached_docs
                            self.logger.info(
                                f"ğŸ”„ [ANSWER GENERATION] Restored {len(retrieved_docs)} retrieved_docs from global cache"
                            )
                            # stateì—ë„ ì €ì¥í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡
                            self._set_state_value(state, "retrieved_docs", retrieved_docs)
                except (ImportError, AttributeError, TypeError) as e:
                    self.logger.debug(f"Could not restore from global cache: {e}")

            # ê²€ìƒ‰ ê²°ê³¼ í†µê³„ ê³„ì‚° (ë‹µë³€ ìƒì„±ì— í™œìš©)
            semantic_results_count = sum(1 for doc in retrieved_docs if doc.get("search_type") == "semantic") if retrieved_docs else 0
            keyword_results_count = sum(1 for doc in retrieved_docs if doc.get("search_type") == "keyword") if retrieved_docs else 0

            # ê°œì„  ì‚¬í•­ 9 ê³„ì†: ê²€ìƒ‰ ê²°ê³¼ í†µê³„ë„ metadataì— ì €ì¥
            metadata = self._get_state_value(state, "metadata", {})
            if not isinstance(metadata, dict):
                metadata = {}
            if retrieved_docs:
                metadata["search_results"] = {
                    "count": len(retrieved_docs),
                    "semantic_count": semantic_results_count,
                    "keyword_count": keyword_results_count,
                    "sources": [doc.get("source", "Unknown") for doc in retrieved_docs[:10]]
                }
                self._set_state_value(state, "metadata", metadata)

            if retrieved_docs:
                self.logger.info(
                    f"ğŸ“Š [ANSWER GENERATION] Using {len(retrieved_docs)} documents for answer generation: "
                    f"Semantic: {semantic_results_count}, Keyword: {keyword_results_count}"
                )

            # prompt_optimized_context ê²€ì¦
            has_valid_optimized_context = (
                prompt_optimized_context
                and isinstance(prompt_optimized_context, dict)
                and prompt_optimized_context.get("prompt_optimized_text")
                and len(prompt_optimized_context.get("prompt_optimized_text", "").strip()) > 0
            )

            if has_valid_optimized_context:
                prompt_text = prompt_optimized_context["prompt_optimized_text"]
                doc_count = prompt_optimized_context.get("document_count", 0)
                context_length = prompt_optimized_context.get("total_context_length", 0)
                structured_docs_from_context = prompt_optimized_context.get("structured_documents", {})

                # ê²€ì¦: retrieved_docsê°€ ìˆëŠ”ë° prompt_optimized_contextì˜ ë¬¸ì„œ ìˆ˜ê°€ 0ì´ë©´ ê²½ê³ 
                if retrieved_docs and len(retrieved_docs) > 0 and doc_count == 0:
                    self.logger.warning(
                        f"âš ï¸ [PROMPT VALIDATION] retrieved_docs exists ({len(retrieved_docs)} docs) "
                        f"but prompt_optimized_context has 0 documents. "
                        f"This will trigger forced conversion from retrieved_docs."
                    )

                # ì¶”ê°€ ê²€ì¦: structured_documentsê°€ ë¹„ì–´ìˆê±°ë‚˜ retrieved_docsë³´ë‹¤ í˜„ì €íˆ ì ìœ¼ë©´ ê²½ê³ 
                structured_docs_count = 0
                if isinstance(structured_docs_from_context, dict):
                    structured_docs_count = len(structured_docs_from_context.get("documents", []))

                if retrieved_docs and len(retrieved_docs) > 0:
                    if structured_docs_count == 0:
                        self.logger.warning(
                            f"âš ï¸ [PROMPT VALIDATION] retrieved_docs exists ({len(retrieved_docs)} docs) "
                            f"but prompt_optimized_context.structured_documents is empty. "
                            f"This will trigger forced conversion from retrieved_docs."
                        )
                    elif structured_docs_count < len(retrieved_docs) * 0.3:
                        self.logger.warning(
                            f"âš ï¸ [PROMPT VALIDATION] retrieved_docs has {len(retrieved_docs)} docs "
                            f"but prompt_optimized_context has only {structured_docs_count} documents. "
                            f"This may trigger forced conversion to include more documents."
                        )

                # ê²€ì¦: prompt_textê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê²½ê³  (ì§€ì‹œì‚¬í•­ë§Œ ìˆê³  ë¬¸ì„œ ë‚´ìš©ì´ ì—†ì„ ìˆ˜ ìˆìŒ)
                if context_length < 500:  # ìµœì†Œ 500ì ì´ìƒì´ì–´ì•¼ ì‹¤ì œ ë¬¸ì„œ ë‚´ìš© í¬í•¨
                    self.logger.warning(
                        f"âš ï¸ [PROMPT VALIDATION] prompt_optimized_text is too short ({context_length} chars). "
                        f"May contain only instructions without document content."
                    )
                    # retrieved_docsê°€ ìˆìœ¼ë©´ í´ë°±ìœ¼ë¡œ ì „í™˜ ê³ ë ¤
                    if retrieved_docs and len(retrieved_docs) > 0:
                        self.logger.info(
                            f"ğŸ”„ [FALLBACK] Switching to _build_intelligent_context due to short prompt_optimized_text"
                        )
                        context_dict = self._build_intelligent_context(state)
                    else:
                        # optimized context ì‚¬ìš© (ë¹ˆ ë¬¸ì„œì¼ ìˆ˜ ìˆìŒ)
                        context_dict = {
                            "context": prompt_text,
                            "structured_documents": prompt_optimized_context.get("structured_documents", {}),
                            "document_count": doc_count,
                            "legal_references": self._extract_legal_references_from_docs(retrieved_docs),
                            "query_type": query_type,
                            "context_length": context_length,
                            "docs_included": doc_count
                        }
                else:
                    # í”„ë¡¬í”„íŠ¸ ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© (ì •ìƒ)
                    # structured_documentsê°€ ë¹„ì–´ìˆê±°ë‚˜ retrieved_docsì™€ ë¶ˆì¼ì¹˜í•˜ë©´ ë³´ê°•
                    structured_docs = prompt_optimized_context.get("structured_documents", {})

                    # retrieved_docsê°€ ìˆëŠ”ë° structured_documentsê°€ ë¹„ì–´ìˆê±°ë‚˜ ì ìœ¼ë©´ ë³´ê°•
                    if retrieved_docs and len(retrieved_docs) > 0:
                        docs_in_structured = structured_docs.get("documents", []) if isinstance(structured_docs, dict) else []
                        # ìµœì†Œ ìš”êµ¬ì‚¬í•­: retrieved_docsì˜ 50% ì´ìƒ ë˜ëŠ” ìµœì†Œ 1ê°œ
                        min_required = max(1, min(3, int(len(retrieved_docs) * 0.5))) if len(retrieved_docs) > 5 else 1

                        if not docs_in_structured or len(docs_in_structured) < min_required:
                            # retrieved_docsë¥¼ structured_documents í˜•íƒœë¡œ ê°•ì œ ë³€í™˜í•˜ì—¬ ë³´ê°•
                            normalized_documents = []
                            for idx, doc in enumerate(retrieved_docs[:10], 1):
                                if isinstance(doc, dict):
                                    content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                                    source = doc.get("source") or doc.get("title") or f"Document_{idx}"
                                    relevance_score = doc.get("relevance_score") or doc.get("final_weighted_score", 0.0)

                                    if content and len(content.strip()) > 10:
                                        normalized_documents.append({
                                            "document_id": idx,
                                            "source": source,
                                            "content": content[:2000],
                                            "relevance_score": float(relevance_score),
                                            "metadata": doc.get("metadata", {})
                                        })

                            if normalized_documents:
                                if not isinstance(structured_docs, dict):
                                    structured_docs = {}
                                structured_docs["documents"] = normalized_documents
                                structured_docs["total_count"] = len(normalized_documents)
                                self.logger.info(
                                    f"âœ… [SEARCH RESULTS ENFORCED] Added {len(normalized_documents)} documents "
                                    f"from retrieved_docs to structured_documents "
                                    f"(original structured_docs had {len(docs_in_structured)} docs)"
                                )
                                # ë¬¸ì„œ ìˆ˜ ì—…ë°ì´íŠ¸
                                doc_count = len(normalized_documents)

                    # prompt_optimized_textë¥¼ contextì™€ prompt_optimized_text ë‘ í‚¤ì— ëª¨ë‘ í¬í•¨í•˜ì—¬
                    # UnifiedPromptManagerê°€ í™•ì‹¤íˆ ì¸ì‹í•˜ë„ë¡ í•¨
                    context_dict = {
                        "context": prompt_text,  # ë©”ì¸ ì»¨í…ìŠ¤íŠ¸
                        "prompt_optimized_text": prompt_text,  # í”„ë¡¬í”„íŠ¸ ìµœì í™” í…ìŠ¤íŠ¸ë„ ì§ì ‘ í¬í•¨
                        "structured_documents": structured_docs,  # ê°•ì œ ë³´ê°•ëœ structured_documents
                        "document_count": doc_count,
                        "legal_references": self._extract_legal_references_from_docs(retrieved_docs),
                        "query_type": query_type,
                        "context_length": context_length,
                        "docs_included": len(structured_docs.get("documents", [])) if isinstance(structured_docs, dict) else 0
                    }

                    # content_validation ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ í¬í•¨
                    content_validation = prompt_optimized_context.get("content_validation")
                    if content_validation:
                        context_dict["content_validation"] = content_validation
                        if not content_validation.get("has_document_content", False):
                            self.logger.warning(
                                f"âš ï¸ [PROMPT VALIDATION] content_validation indicates no document content in prompt"
                            )

                    self.logger.info(
                        f"âœ… [PROMPT OPTIMIZED] Using optimized document context "
                        f"({doc_count} docs, {context_length} chars)"
                    )
            else:
                # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                if retrieved_docs and len(retrieved_docs) > 0:
                    self.logger.warning(
                        f"âš ï¸ [FALLBACK] prompt_optimized_context is missing or invalid, "
                        f"but retrieved_docs exists ({len(retrieved_docs)} docs). "
                        f"Using _build_intelligent_context as fallback."
                    )
                else:
                    self.logger.info(
                        f"â„¹ï¸ [FALLBACK] No prompt_optimized_context and no retrieved_docs. "
                        f"Using _build_intelligent_context."
                    )
                context_dict = self._build_intelligent_context(state)

            # ìµœì¢… ê²€ì¦: context_dictì— ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ì´ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            context_text = context_dict.get("context", "")
            if retrieved_docs and len(retrieved_docs) > 0:
                # retrieved_docsê°€ ìˆëŠ”ë° context_textê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ê²½ê³ 
                if not context_text or len(context_text.strip()) < 100:
                    self.logger.error(
                        f"âš ï¸ [CONTEXT VALIDATION] retrieved_docs exists ({len(retrieved_docs)} docs) "
                        f"but context_dict['context'] is empty or too short ({len(context_text)} chars). "
                        f"This may cause LLM to generate answer without document references!"
                    )
                else:
                    # context_textì— ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê°„ë‹¨íˆ í™•ì¸
                    # (ìµœì†Œí•œ í•˜ë‚˜ì˜ ë¬¸ì„œ sourceë‚˜ content ì¼ë¶€ê°€ í¬í•¨ë˜ì–´ì•¼ í•¨)
                    has_doc_reference = False
                    for doc in retrieved_docs[:3]:  # ìƒìœ„ 3ê°œë§Œ í™•ì¸
                        source = doc.get("source", "")
                        content_preview = (doc.get("content") or doc.get("text", ""))[:50]
                        if source and source in context_text:
                            has_doc_reference = True
                            break
                        if content_preview and content_preview in context_text:
                            has_doc_reference = True
                            break

                    if not has_doc_reference:
                        self.logger.warning(
                            f"âš ï¸ [CONTEXT VALIDATION] context_text does not seem to contain references to retrieved_docs. "
                            f"Length: {len(context_text)} chars"
                        )

            # SQL 0ê±´ í´ë°± ì‹ í˜¸ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì»¨í…ìŠ¤íŠ¸ ë³´ê°• ì¬ì‹œë„
            try:
                meta_forced = self._get_state_value(state, "metadata", {})
                if isinstance(meta_forced, dict) and meta_forced.get("force_rag_fallback"):
                    self.logger.info("[ROUTER FALLBACK] SQL 0ê±´ â†’ í‚¤ì›Œë“œ+ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ë³´ê°• ì¬ì‹œë„")
                    state = self._adaptive_context_expansion(state, {"reason": "router_zero_rows", "needs_expansion": True})
                    context_dict = self._build_intelligent_context(state)
            except Exception as e:
                self.logger.warning(f"Router fallback expansion skipped due to error: {e}")

            # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦
            validation_results = self._validate_context_quality(
                context_dict,
                query,
                query_type,
                extracted_keywords
            )

            # í’ˆì§ˆ ë¶€ì¡± ì‹œ ì»¨í…ìŠ¤íŠ¸ í™•ì¥
            if validation_results.get("needs_expansion", False):
                state = self._adaptive_context_expansion(state, validation_results)
                # í™•ì¥ í›„ ì»¨í…ìŠ¤íŠ¸ ì¬êµ¬ì¶•
                context_dict = self._build_intelligent_context(state)
                # ì¬ê²€ì¦ (ì„ íƒì )
                validation_results = self._validate_context_quality(
                    context_dict, query, query_type, extracted_keywords
                )

            # ê²€ì¦ ê²°ê³¼ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥
            if not isinstance(metadata, dict):
                metadata = {}
            metadata["context_validation"] = validation_results

            # ì¬ì‹œë„ ì‹œ í’ˆì§ˆ í”¼ë“œë°± ê°€ì ¸ì˜¤ê¸°
            quality_feedback = None
            base_prompt_type = "korean_legal_expert"

            if is_retry:
                quality_feedback = self.answer_generator.get_quality_feedback_for_retry(state)
                base_prompt_type = self.answer_generator.determine_retry_prompt_type(quality_feedback)

                self.logger.info(
                    f"ğŸ”„ [RETRY WITH FEEDBACK] Previous score: {quality_feedback.get('previous_score', 0):.2f}, "
                    f"Failed checks: {len(quality_feedback.get('failed_checks', []))}, "
                    f"Prompt type: {base_prompt_type}"
                )

                # í”¼ë“œë°±ì„ ë©”íƒ€ë°ì´í„°ì— ì €ì¥ (í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  ìˆ˜ ìˆë„ë¡)
                if not isinstance(metadata, dict):
                    metadata = {}
                metadata["retry_feedback"] = quality_feedback
                self._set_state_value(state, "metadata", metadata)

            # í”¼ë“œë°±ì´ ìˆìœ¼ë©´ context_dictì— ì¶”ê°€
            if quality_feedback:
                context_dict["quality_feedback"] = quality_feedback

            # ğŸ” ê²€ìƒ‰ ê²°ê³¼ ê°•ì œ í¬í•¨ ë³´ê°• ë¡œì§ (ì¤‘ìš”!)
            # retrieved_docsê°€ ì—†ëŠ” ê²½ìš° ê²½ê³  ë° ì²˜ë¦¬
            if not retrieved_docs or len(retrieved_docs) == 0:
                self.logger.warning(
                    f"âš ï¸ [NO SEARCH RESULTS] retrieved_docs is empty. "
                    f"LLM will generate answer without document references. "
                    f"Query: '{query[:50]}...'"
                )
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ context_dictì— ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œ
                context_dict["has_search_results"] = False
                context_dict["search_results_note"] = (
                    "í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë²•ë¥  ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
                    "ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•˜ë˜, "
                    "êµ¬ì²´ì ì¸ ì¡°ë¬¸ì´ë‚˜ íŒë¡€ë¥¼ ì¸ìš©í•  ìˆ˜ ì—†ìŒì„ ëª…ì‹œí•˜ì„¸ìš”."
                )
            else:
                context_dict["has_search_results"] = True

            # retrieved_docsê°€ ìˆëŠ”ë° structured_documentsê°€ ë¹„ì–´ìˆê±°ë‚˜ ì—†ìœ¼ë©´ ê°•ì œ ë³€í™˜
            # ê°œì„ : prompt_optimized_context ì‚¬ìš© ì‹œì—ë„ structured_documentsê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ
            # retrieved_docsì™€ ë¹„êµí•˜ì—¬ ì‹¤ì œ ë¬¸ì„œê°€ ì¶©ë¶„íˆ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if retrieved_docs and len(retrieved_docs) > 0:
                structured_docs = context_dict.get("structured_documents", {})
                documents_in_structured = []

                if isinstance(structured_docs, dict):
                    documents_in_structured = structured_docs.get("documents", [])

                # has_valid_documents ì²´í¬ ê°œì„ :
                # 1. documentsê°€ ì¡´ì¬í•´ì•¼ í•¨
                # 2. documents ìˆ˜ê°€ retrieved_docsì˜ ìµœì†Œ 30% ì´ìƒì´ì–´ì•¼ í•¨ (ë„ˆë¬´ ì—„ê²©í•˜ì§€ ì•Šê²Œ)
                # 3. ë˜ëŠ” retrieved_docsê°€ ì ì€ ê²½ìš°(5ê°œ ì´í•˜) ìµœì†Œ 1ê°œ ì´ìƒì´ì–´ì•¼ í•¨
                min_required_docs = max(1, min(3, int(len(retrieved_docs) * 0.3))) if len(retrieved_docs) > 5 else 1

                has_valid_documents = (
                    isinstance(structured_docs, dict)
                    and documents_in_structured
                    and len(documents_in_structured) > 0
                    and len(documents_in_structured) >= min_required_docs
                )

                # ë¡œê¹… ì¶”ê°€ (ë””ë²„ê¹…ìš©)
                self.logger.debug(
                    f"ğŸ” [STRUCTURED DOCS CHECK] retrieved_docs={len(retrieved_docs)}, "
                    f"structured_docs_count={len(documents_in_structured)}, "
                    f"min_required={min_required_docs}, "
                    f"has_valid={has_valid_documents}"
                )

                if not has_valid_documents:
                    # retrieved_docsë¥¼ structured_documents í˜•íƒœë¡œ ê°•ì œ ë³€í™˜
                    normalized_documents = []
                    for idx, doc in enumerate(retrieved_docs[:10], 1):  # ìƒìœ„ 10ê°œë§Œ
                        if isinstance(doc, dict):
                            # ë‹¤ì–‘í•œ í•„ë“œëª…ì—ì„œ content ì¶”ì¶œ
                            content = (
                                doc.get("content")
                                or doc.get("text")
                                or doc.get("content_text")
                                or doc.get("summary")
                                or ""
                            )

                            # source ì¶”ì¶œ
                            source = (
                                doc.get("source")
                                or doc.get("title")
                                or doc.get("document_id")
                                or f"Document_{idx}"
                            )

                            # relevance_score ì¶”ì¶œ
                            relevance_score = (
                                doc.get("relevance_score")
                                or doc.get("score")
                                or doc.get("final_weighted_score")
                                or 0.5
                            )

                            # contentê°€ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ì¶”ê°€
                            if content and len(content.strip()) > 10:
                                normalized_documents.append({
                                    "document_id": idx,
                                    "source": source,
                                    "content": content[:2000],  # ìµœëŒ€ 2000ìë¡œ ì œí•œ
                                    "relevance_score": float(relevance_score),
                                    "metadata": doc.get("metadata", {})
                                })

                    if normalized_documents:
                        # structured_documents ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
                        if not isinstance(structured_docs, dict):
                            structured_docs = {}

                        structured_docs["documents"] = normalized_documents
                        structured_docs["total_count"] = len(normalized_documents)
                        context_dict["structured_documents"] = structured_docs
                        context_dict["document_count"] = len(normalized_documents)
                        context_dict["docs_included"] = len(normalized_documents)

                        # ê°œì„  ì‚¬í•­ 4: ê²€ìƒ‰ ê²°ê³¼ì™€ structured_documents ê°„ì˜ ë§¤í•‘ ì •ë³´ ì¶”ê°€
                        structured_docs["source_mapping"] = [
                            {
                                "original_index": idx,
                                "document_id": doc.get("document_id"),
                                "source": doc.get("source"),
                                "transformed": True
                            }
                            for idx, doc in enumerate(normalized_documents)
                        ]
                        context_dict["retrieved_to_structured_mapping"] = {
                            "total_retrieved": len(retrieved_docs),
                            "total_transformed": len(normalized_documents),
                            "transformation_rate": len(normalized_documents) / len(retrieved_docs) if retrieved_docs else 0
                        }

                        # ê°œì„  ì‚¬í•­ 2: structured_documentsë¥¼ stateì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
                        if structured_docs and isinstance(structured_docs, dict):
                            # search ê·¸ë£¹ì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
                            self._set_state_value(state, "structured_documents", structured_docs)
                            # search ê·¸ë£¹ ì§ì ‘ ì €ì¥
                            if "search" not in state:
                                state["search"] = {}
                            state["search"]["structured_documents"] = structured_docs

                        # ê°œì„  ì‚¬í•­ 7: state reductionìœ¼ë¡œ ì¸í•œ ë°ì´í„° ì†ì‹¤ ë°©ì§€
                        # common ê·¸ë£¹ì—ë„ ì €ì¥í•˜ì—¬ reduction í›„ì—ë„ ìœ ì§€
                        if "common" not in state:
                            state["common"] = {}
                        if "search" not in state["common"]:
                            state["common"]["search"] = {}
                        state["common"]["search"]["structured_documents"] = structured_docs

                        self.logger.info(
                            f"âœ… [SEARCH RESULTS INJECTION] Added {len(normalized_documents)} documents "
                            f"from retrieved_docs to context_dict.structured_documents"
                        )
                    else:
                        self.logger.warning(
                            f"âš ï¸ [SEARCH RESULTS INJECTION] retrieved_docs has {len(retrieved_docs)} docs "
                            f"but none have valid content (>10 chars)"
                        )
                else:
                    # ì´ë¯¸ validí•œ structured_documentsê°€ ìˆìŒ
                    doc_count = len(documents_in_structured)
                    self.logger.info(
                        f"âœ… [SEARCH RESULTS] structured_documents already has {doc_count} valid documents "
                        f"(retrieved_docs: {len(retrieved_docs)}, required: {min_required_docs})"
                    )

                    # ê°œì„  ì‚¬í•­ 2 ê³„ì†: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” structured_documentsë„ stateì— ì €ì¥
                    if structured_docs and isinstance(structured_docs, dict):
                        # search ê·¸ë£¹ì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
                        self._set_state_value(state, "structured_documents", structured_docs)
                        # search ê·¸ë£¹ ì§ì ‘ ì €ì¥
                        if "search" not in state:
                            state["search"] = {}
                        state["search"]["structured_documents"] = structured_docs

                    # ê°œì„  ì‚¬í•­ 7 ê³„ì†: common ê·¸ë£¹ì—ë„ ì €ì¥í•˜ì—¬ reduction í›„ì—ë„ ìœ ì§€
                    if structured_docs:
                        if "common" not in state:
                            state["common"] = {}
                        if "search" not in state["common"]:
                            state["common"]["search"] = {}
                        state["common"]["search"]["structured_documents"] = structured_docs

                    # ì¶”ê°€ ê²€ì¦: structured_documentsì˜ ë¬¸ì„œë“¤ì´ retrieved_docsì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    # (ì™„ë²½í•œ ì¼ì¹˜ëŠ” ì•„ë‹ˆì§€ë§Œ, ìµœì†Œí•œ retrieved_docsì— ìˆëŠ” ë¬¸ì„œë“¤ì´ í¬í•¨ë˜ì–´ì•¼ í•¨)
                    if doc_count < len(retrieved_docs) * 0.5:
                        self.logger.warning(
                            f"âš ï¸ [SEARCH RESULTS] structured_documents has only {doc_count} documents "
                            f"while retrieved_docs has {len(retrieved_docs)}. "
                            f"This may indicate some documents were lost during preparation."
                        )

            optimized_prompt = self.unified_prompt_manager.get_optimized_prompt(
                query=query,
                question_type=question_type,
                domain=domain,
                context=context_dict,
                model_type=model_type,
                base_prompt_type=base_prompt_type
            )

            # í”„ë¡¬í”„íŠ¸ ìƒì„± í›„ ìƒì„¸ ë¡œê¹…
            prompt_length = len(optimized_prompt)
            context_length_in_dict = context_dict.get("context_length", 0)
            docs_included = context_dict.get("docs_included", context_dict.get("document_count", 0))

            # í”„ë¡¬í”„íŠ¸ì— ë¬¸ì„œ ì„¹ì…˜ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            has_documents_section = "ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ" in optimized_prompt or "## ğŸ”" in optimized_prompt
            documents_in_prompt = optimized_prompt.count("ë¬¸ì„œ") if has_documents_section else 0
            structured_docs_count = 0
            structured_docs_in_context = context_dict.get("structured_documents", {})
            if isinstance(structured_docs_in_context, dict):
                structured_docs_count = len(structured_docs_in_context.get("documents", []))

            self.logger.info(
                f"âœ… [PROMPT GENERATED] Final prompt created: "
                f"{prompt_length} chars, "
                f"context: {context_length_in_dict} chars, "
                f"docs_in_context_dict: {docs_included}, "
                f"structured_docs: {structured_docs_count}, "
                f"has_documents_section: {has_documents_section}, "
                f"'ë¬¸ì„œ' mentions in prompt: {documents_in_prompt}"
            )

            # ê°œì„  ì‚¬í•­ 3: optimized_promptë¥¼ stateì— ì €ì¥
            metadata = self._get_state_value(state, "metadata", {})
            if not isinstance(metadata, dict):
                metadata = {}
            if len(optimized_prompt) > 10000:
                metadata["optimized_prompt"] = optimized_prompt[:10000] + "... (truncated)"
                metadata["optimized_prompt_length"] = len(optimized_prompt)
            else:
                metadata["optimized_prompt"] = optimized_prompt
                metadata["optimized_prompt_length"] = len(optimized_prompt)

            # ê°œì„  ì‚¬í•­ 5: í”„ë¡¬í”„íŠ¸ ìƒì„± ê²°ê³¼ ì •ë³´ë¥¼ metadataì— ì €ì¥
            prompt_generation_info = {
                "prompt_length": prompt_length,
                "context_length": context_length_in_dict,
                "docs_in_context_dict": docs_included,
                "structured_docs_count": structured_docs_count,
                "has_documents_section": has_documents_section,
                "documents_in_prompt": documents_in_prompt,
                "retrieved_docs_count": len(retrieved_docs) if retrieved_docs else 0
            }
            metadata["prompt_generation_info"] = prompt_generation_info
            self._set_state_value(state, "metadata", metadata)

            # ê²€ì¦: retrieved_docsê°€ ìˆëŠ”ë° í”„ë¡¬í”„íŠ¸ì— ë¬¸ì„œ ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ê²½ê³ 
            if retrieved_docs and len(retrieved_docs) > 0:
                if not has_documents_section:
                    self.logger.error(
                        f"âŒ [PROMPT VALIDATION ERROR] retrieved_docs has {len(retrieved_docs)} documents "
                        f"but prompt does not contain documents_section! "
                        f"This may cause LLM to generate answer without sources!"
                    )
                else:
                    self.logger.info(
                        f"âœ… [PROMPT VALIDATION] Documents section found in prompt "
                        f"(retrieved_docs: {len(retrieved_docs)}, structured_docs: {structured_docs_count})"
                    )
                    # í”„ë¡¬í”„íŠ¸ì—ì„œ ë¬¸ì„œ ì„¹ì…˜ ì¼ë¶€ ì¶œë ¥ (í™•ì¸ìš©)
                    doc_section_start = optimized_prompt.find("## ğŸ”")
                    if doc_section_start >= 0:
                        doc_section_preview = optimized_prompt[doc_section_start:doc_section_start+500]
                        self.logger.debug(
                            f"ğŸ“„ [PROMPT PREVIEW] Documents section preview:\n{doc_section_preview}..."
                        )

            # í”„ë¡¬í”„íŠ¸ì— ë¬¸ì„œ ë‚´ìš© í¬í•¨ ì—¬ë¶€ í™•ì¸ (ê°•í™”ëœ ê²€ì¦)
            context_text = context_dict.get("context", "")
            structured_docs_in_context = context_dict.get("structured_documents", {})
            documents_in_context = []
            if isinstance(structured_docs_in_context, dict):
                documents_in_context = structured_docs_in_context.get("documents", [])

            # ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš° í”„ë¡¬í”„íŠ¸ì— í¬í•¨ ì—¬ë¶€ í™•ì¸
            if retrieved_docs and len(retrieved_docs) > 0:
                # 1. context_text í™•ì¸
                if context_text and len(context_text) > 100:
                    context_preview = context_text[:100]
                    if context_preview in optimized_prompt:
                        self.logger.info(
                            f"âœ… [PROMPT VALIDATION] Context text confirmed in final prompt "
                            f"({len(context_text)} chars included)"
                        )
                    else:
                        self.logger.warning(
                            f"âš ï¸ [PROMPT VALIDATION] Context text may not be fully included in final prompt. "
                            f"Context length: {len(context_text)} chars, Prompt length: {prompt_length} chars"
                        )

                # 2. structured_documents í™•ì¸
                if documents_in_context:
                    doc_found_count = 0
                    for doc in documents_in_context[:5]:  # ìƒìœ„ 5ê°œë§Œ í™•ì¸
                        if isinstance(doc, dict):
                            doc_content = doc.get("content", "")
                            doc_source = doc.get("source", "")

                            # ë¬¸ì„œ ë‚´ìš©ì´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                            if doc_content and len(doc_content) > 50:
                                content_preview = doc_content[:150].strip()
                                if content_preview and content_preview in optimized_prompt:
                                    doc_found_count += 1
                                elif doc_source and doc_source in optimized_prompt:
                                    doc_found_count += 1

                    if doc_found_count > 0:
                        self.logger.info(
                            f"âœ… [PROMPT VALIDATION] Found {doc_found_count}/{min(5, len(documents_in_context))} "
                            f"documents in final prompt"
                        )
                    else:
                        self.logger.error(
                            f"âŒ [PROMPT VALIDATION FAILED] No documents from structured_documents "
                            f"found in final prompt! Documents in context: {len(documents_in_context)}, "
                            f"Prompt length: {prompt_length} chars. "
                            f"This may cause LLM to generate answer without document references!"
                        )
                else:
                    self.logger.warning(
                        f"âš ï¸ [PROMPT VALIDATION] retrieved_docs exists ({len(retrieved_docs)} docs) "
                        f"but structured_documents is empty. Prompt may not include search results."
                    )

                # 3. í”„ë¡¬í”„íŠ¸ì— "ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ" ì„¹ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
                search_section_keywords = [
                    "ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ",
                    "ê²€ìƒ‰ ê²°ê³¼",
                    "ë°˜ë“œì‹œ ì°¸ê³ ",
                    "ë¬¸ì„œë“¤",
                    "structured_documents"
                ]
                has_search_section = any(keyword in optimized_prompt for keyword in search_section_keywords)

                if not has_search_section and documents_in_context:
                    self.logger.warning(
                        f"âš ï¸ [PROMPT VALIDATION] Search results section keywords not found in prompt "
                        f"despite having {len(documents_in_context)} documents in context."
                    )
            else:
                # retrieved_docsê°€ ì—†ëŠ” ê²½ìš°ëŠ” ì •ìƒ (ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°)
                self.logger.debug(
                    "â„¹ï¸ [PROMPT VALIDATION] No retrieved_docs to validate against"
                )

            # ê°œì„  ì‚¬í•­ 8: ê²€ì¦ ë¡œì§ ê²°ê³¼ë¥¼ stateì— ì²´ê³„ì ìœ¼ë¡œ ì €ì¥
            prompt_validation_result = {
                "has_documents_section": has_documents_section,
                "documents_in_prompt": documents_in_prompt,
                "structured_docs_in_prompt": doc_found_count if 'doc_found_count' in locals() else 0,
                "has_search_section": has_search_section if 'has_search_section' in locals() else False,
                "validation_warnings": [],  # ê²€ì¦ ê²½ê³  ëª©ë¡
                "validation_errors": []     # ê²€ì¦ ì˜¤ë¥˜ ëª©ë¡
            }
            # ê²€ì¦ ê²½ê³  ë° ì˜¤ë¥˜ ìˆ˜ì§‘
            if retrieved_docs and len(retrieved_docs) > 0:
                if not has_documents_section:
                    prompt_validation_result["validation_errors"].append(
                        f"retrieved_docs has {len(retrieved_docs)} documents but prompt does not contain documents_section"
                    )
                if not has_search_section and documents_in_context:
                    prompt_validation_result["validation_warnings"].append(
                        f"Search results section keywords not found in prompt despite having {len(documents_in_context)} documents"
                    )
                if 'doc_found_count' in locals() and doc_found_count == 0 and documents_in_context:
                    prompt_validation_result["validation_errors"].append(
                        f"No documents from structured_documents found in final prompt"
                    )
            metadata["prompt_validation"] = prompt_validation_result
            self._set_state_value(state, "metadata", metadata)

            # í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            self.logger.debug(
                f"ğŸ“ [PROMPT PREVIEW] Final prompt preview (last 300 chars):\n"
                f"{optimized_prompt[-300:] if len(optimized_prompt) > 300 else optimized_prompt}"
            )

            # ğŸ”´ í”„ë¡¬í”„íŠ¸ ì „ì²´ ì €ì¥ (í‰ê°€ìš©)
            prompt_file = None
            try:
                debug_dir = Path("debug/prompts")
                debug_dir.mkdir(parents=True, exist_ok=True)
                prompt_file = debug_dir / f"prompt_{int(time.time())}.txt"
                with open(prompt_file, "w", encoding="utf-8") as f:
                    f.write(optimized_prompt)
                self.logger.info(f"ğŸ’¾ [PROMPT SAVED] Full prompt saved to {prompt_file} ({prompt_length} chars)")
            except Exception as e:
                self.logger.debug(f"Could not save prompt to file: {e}")

            # ğŸ”„ Prompt Chainingì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„± ë° ê°œì„  (Phase 5 ë¦¬íŒ©í† ë§)
            normalized_response = self.answer_generator.generate_answer_with_chain(
                optimized_prompt=optimized_prompt,
                query=query,
                context_dict=context_dict,
                quality_feedback=quality_feedback,
                is_retry=is_retry
            )

            # ì‘ë‹µ ìƒì„± ì§í›„ ìƒì„¸ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            self.logger.info(
                f"ğŸ“ [ANSWER GENERATED] Response received:\n"
                f"   Normalized response length: {len(normalized_response)} characters\n"
                f"   Normalized response content: '{normalized_response[:300]}'\n"
                f"   Normalized response repr: {repr(normalized_response[:100])}"
            )

            self._set_state_value(state, "answer", normalized_response)

            # ê°œì„  ì‚¬í•­ 10: í”„ë¡¬í”„íŠ¸-ë‹µë³€ ê°„ ì—°ê²° ì •ë³´ ì¶”ê°€
            metadata = self._get_state_value(state, "metadata", {})
            if not isinstance(metadata, dict):
                metadata = {}
            metadata["answer_generation"] = {
                "prompt_length": prompt_length,
                "answer_length": len(normalized_response),
                "prompt_file": str(prompt_file) if 'prompt_file' in locals() else None,
                "generation_timestamp": time.time(),
                "used_search_results": bool(retrieved_docs and len(retrieved_docs) > 0),
                "structured_docs_used": structured_docs_count
            }
            self._set_state_value(state, "metadata", metadata)

            # ë‹µë³€-ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„ ê²€ì¦ (ì¬ìƒì„± ì—†ì´ ê²€ì¦ ê²°ê³¼ë§Œ ê¸°ë¡) (Phase 5 ë¦¬íŒ©í† ë§)
            validation_result = self.answer_generator.validate_answer_uses_context(
                answer=normalized_response,
                context=context_dict,
                query=query,
                retrieved_docs=retrieved_docs  # ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ ì¶”ê°€
            )

            # ê²€ì¦ ê²°ê³¼ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥ (ì¬ìƒì„± ë¡œì§ ì œê±°)
            metadata = self._get_state_value(state, "metadata", {})
            if not isinstance(metadata, dict):
                metadata = {}

            # ê²€ì¦ ê²°ê³¼ ì €ì¥
            metadata["answer_validation"] = validation_result

            # ê°œì„  ì‚¬í•­ 6: ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš© ì¶”ì  ì •ë³´ ì¶”ê°€
            search_usage_tracking = {
                "retrieved_docs_count": len(retrieved_docs) if retrieved_docs else 0,
                "structured_docs_count": structured_docs_count,
                "citation_count": validation_result.get("citation_count", 0),
                "coverage_score": validation_result.get("coverage_score", 0.0),
                "has_document_references": validation_result.get("has_document_references", False),
                "sources_in_answer": []  # ì‹¤ì œ ì‚¬ìš©ëœ ì†ŒìŠ¤ ëª©ë¡ (ë‹µë³€ì—ì„œ ì¶”ì¶œ)
            }
            # ë‹µë³€ì—ì„œ ì†ŒìŠ¤ ì¶”ì¶œ
            if retrieved_docs and isinstance(normalized_response, str):
                sources_found = []
                for doc in retrieved_docs:
                    source = doc.get("source") or doc.get("title") or ""
                    if source and source in normalized_response:
                        sources_found.append(source)
                search_usage_tracking["sources_in_answer"] = sources_found[:10]  # ìƒìœ„ 10ê°œë§Œ
            metadata["search_usage_tracking"] = search_usage_tracking

            # ê²€ìƒ‰ ê²°ê³¼ í™œìš©ë„ ìƒì„¸ ë¡œê¹…
            citation_count = validation_result.get("citation_count", 0)
            coverage_score = validation_result.get("coverage_score", 0.0)
            has_document_references = validation_result.get("has_document_references", False)

            if retrieved_docs and len(retrieved_docs) > 0:
                if citation_count < 2:
                    self.logger.warning(
                        f"âš ï¸ [VALIDATION] Low citation count: {citation_count} (expected >= 2) "
                        f"for {len(retrieved_docs)} documents. "
                        f"Coverage: {coverage_score:.2f}, Has refs: {has_document_references}"
                    )
                elif not has_document_references:
                    self.logger.warning(
                        f"âš ï¸ [VALIDATION] Citations found ({citation_count}) but no document source references detected. "
                        f"Answer may not be using retrieved documents effectively."
                    )
                else:
                    self.logger.info(
                        f"âœ… [VALIDATION] Good context usage: {citation_count} citations, "
                        f"coverage: {coverage_score:.2f}, document references: {has_document_references}"
                    )

            # ì¬ìƒì„±ì´ í•„ìš”í–ˆëŠ”ì§€ ë¡œê·¸ë§Œ ê¸°ë¡ (ì‹¤ì œ ì¬ìƒì„±ì€ í•˜ì§€ ì•ŠìŒ)
            if validation_result.get("needs_regeneration", False):
                self.logger.warning(
                    f"âš ï¸ [VALIDATION] Context usage low (coverage: {validation_result.get('coverage_score', 0.0):.2f}), "
                    f"but regeneration is disabled. Answer generated with current validation result."
                )

            # ê°œì„  ì‚¬í•­ 1: context_dictë¥¼ stateì— ì €ì¥
            metadata["context_dict"] = context_dict  # ê²€ì¦ ë° ë””ë²„ê¹…ì„ ìœ„í•´ ì €ì¥

            self._set_state_value(state, "metadata", metadata)

            processing_time = self._update_processing_time(state, start_time)
            self._add_step(state, "ë‹µë³€ ìƒì„± ì™„ë£Œ", "ë‹µë³€ ìƒì„± ì™„ë£Œ")

            # ì‹¤í–‰ ê¸°ë¡ ì €ì¥ (ì¬ì‹œë„ ì¹´ìš´í„°ëŠ” RetryCounterManagerì—ì„œ ê´€ë¦¬)
            self._save_metadata_safely(state, "_last_executed_node", "generate_answer_enhanced")

            self.logger.info(f"Enhanced answer generated with UnifiedPromptManager in {processing_time:.2f}s")
        except Exception as e:
            self._handle_error(state, str(e), "ê°œì„ ëœ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            self._set_state_value(state, "answer", self.answer_generator.generate_fallback_answer(state))
        return state

    def _get_question_type_and_domain(self, query_type, query: str = "") -> Tuple[QuestionType, Optional[LegalDomain]]:
        """WorkflowUtils.get_question_type_and_domain ë˜í¼"""
        return WorkflowUtils.get_question_type_and_domain(query_type, query, self.logger)

    def _normalize_question_type(self, query_type) -> QuestionType:
        """WorkflowUtils.normalize_question_type ë˜í¼"""
        return WorkflowUtils.normalize_question_type(query_type, self.logger)

    def _extract_supported_domain_from_query(self, query: str) -> Optional[LegalDomain]:
        """WorkflowUtils.extract_supported_domain_from_query ë˜í¼"""
        return WorkflowUtils.extract_supported_domain_from_query(query)

    # Phase 5 ë¦¬íŒ©í† ë§: ë‹µë³€ ìƒì„± ê´€ë ¨ ë©”ì„œë“œëŠ” AnswerGeneratorë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _get_quality_feedback_for_retry(self, state: LegalWorkflowState) -> Dict[str, Any]:
        """AnswerGenerator.get_quality_feedback_for_retry ë˜í¼"""
        return self.answer_generator.get_quality_feedback_for_retry(state)

    def _determine_retry_prompt_type(self, quality_feedback: Dict[str, Any]) -> str:
        """AnswerGenerator.determine_retry_prompt_type ë˜í¼"""
        return self.answer_generator.determine_retry_prompt_type(quality_feedback)

    def _assess_improvement_potential(
        self,
        quality_score: float,
        quality_checks: Dict[str, bool],
        state: LegalWorkflowState
    ) -> Dict[str, Any]:
        """AnswerGenerator.assess_improvement_potential ë˜í¼"""
        result = self.answer_generator.assess_improvement_potential(
            quality_score,
            quality_checks,
            state
        )
        # í˜¸í™˜ì„±ì„ ìœ„í•´ ë°˜í™˜ í˜•ì‹ ë³€í™˜
        return {
            "should_retry": result.get("potential", 0.0) >= 0.3,
            "confidence": result.get("potential", 0.0),
            "best_strategy": result.get("strategy") or "retry_generate",
            "reasons": result.get("reasons", [])
        }

    def _improve_search_query_for_retry(
        self,
        original_query: str,
        quality_feedback: Dict[str, Any],
        state: LegalWorkflowState
    ) -> str:
        """ì¬ì‹œë„ë¥¼ ìœ„í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ê°œì„ """
        improvements = []

        # ë²•ë ¹ ê²€ì¦ ì‹¤íŒ¨ â†’ ë²•ë ¹ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
        failed_checks = quality_feedback.get("failed_checks", [])
        if any("ë²•ë ¹" in check or "ë²•" in check for check in failed_checks):
            improvements.append("ë²•ë ¹ ì¡°í•­")
            improvements.append("ë²•ë¥  ê·œì •")

        # ì†ŒìŠ¤ ì—†ìŒ â†’ ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€
        if any("ì†ŒìŠ¤" in check or "ì¶œì²˜" in check for check in failed_checks):
            # í‚¤ì›Œë“œ í™•ì¥
            extracted_keywords = self._get_state_value(state, "extracted_keywords", [])
            if isinstance(extracted_keywords, list) and len(extracted_keywords) > 0:
                # hashable íƒ€ì…ë§Œ ì¶”ê°€
                safe_keywords = [kw for kw in extracted_keywords[:3] if isinstance(kw, (str, int, float))]
                improvements.extend([str(kw) for kw in safe_keywords])

        # ë‹µë³€ì´ ì§§ìŒ â†’ ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ
        if any("ì§§" in check for check in failed_checks):
            query_type = self._get_state_value(state, "query_type", "")
            if "ê³„ì•½" in str(query_type) or "ê³„ì•½" in original_query:
                improvements.append("ê³„ì•½ì„œ ì‘ì„± ìš”ê±´")
            elif "ì†Œì†¡" in str(query_type) or "ì†Œì†¡" in original_query:
                improvements.append("ì†Œì†¡ ì ˆì°¨")
            elif "ì†í•´" in original_query or "ë°°ìƒ" in original_query:
                improvements.append("ì†í•´ë°°ìƒ ìš”ê±´")

        if improvements:
            improved_query = f"{original_query} {' '.join(improvements)}"
            return improved_query.strip()

        return original_query

    def _rerank_documents_by_relevance(
        self,
        documents: List[Dict],
        query: str,
        extracted_keywords: List[str]
    ) -> List[Dict]:
        """ContextBuilder.rerank_documents_by_relevance ë˜í¼"""
        return self.context_builder.rerank_documents_by_relevance(documents, query, extracted_keywords)

    def _select_high_value_documents(
        self,
        documents: List[Dict],
        query: str,
        min_relevance: float = 0.7,
        max_docs: int = 5
    ) -> List[Dict]:
        """ì •ë³´ ë°€ë„ ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ"""
        if not documents:
            return documents

        try:
            high_value_docs = []

            for doc in documents:
                doc_content = doc.get("content", "")
                if not doc_content or len(doc_content) < 20:
                    continue

                # 1. ë²•ë¥  ì¡°í•­ ì¸ìš© ìˆ˜ ê³„ì‚°
                citation_pattern = r'[ê°€-í£]+ë²•\s*ì œ?\s*\d+\s*ì¡°'
                citations = re.findall(citation_pattern, doc_content)
                citation_count = len(citations)
                citation_score = min(1.0, citation_count / 5.0)

                # 2. í•µì‹¬ ê°œë… ì„¤ëª… ì™„ì„±ë„ í‰ê°€
                query_words = set(query.lower().split())
                content_words = set(doc_content.lower().split())
                explanation_completeness = 0.0
                if query_words and content_words:
                    overlap = len(query_words.intersection(content_words))
                    explanation_completeness = min(1.0, overlap / max(1, len(query_words)))

                sentences = doc_content.split('ã€‚') or doc_content.split('.')
                avg_sentence_length = sum(len(s.strip()) for s in sentences if s.strip()) / max(1, len(sentences))

                descriptive_score_bonus = 0.0
                if 20 <= avg_sentence_length <= 100:
                    descriptive_score_bonus = 0.2
                elif avg_sentence_length > 100:
                    descriptive_score_bonus = 0.1

                explanation_completeness = min(1.0, explanation_completeness + descriptive_score_bonus)

                # 3. ì§ˆë¬¸ í‚¤ì›Œë“œ í¬í•¨ë„
                keyword_coverage = 0.0
                if query_words and content_words:
                    keyword_coverage = len(query_words.intersection(content_words)) / max(1, len(query_words))

                # 4. ì •ë³´ ë°€ë„ ì¢…í•© ì ìˆ˜
                relevance_score = doc.get("final_relevance_score") or doc.get("combined_score", 0.0) or doc.get("relevance_score", 0.0)

                information_density = (
                    0.3 * citation_score +
                    0.3 * explanation_completeness +
                    0.2 * keyword_coverage +
                    0.2 * min(1.0, relevance_score)
                )

                doc["information_density_score"] = information_density
                doc["citation_count"] = citation_count
                doc["explanation_completeness"] = explanation_completeness

                # ê´€ë ¨ì„± ì ìˆ˜ì™€ ì •ë³´ ë°€ë„ ì ìˆ˜ ê°€ì¤‘ í‰ê· 
                combined_value_score = 0.6 * relevance_score + 0.4 * information_density
                doc["combined_value_score"] = combined_value_score

                # ì„ê³„ê°’ ì²´í¬
                if combined_value_score >= min_relevance:
                    high_value_docs.append(doc)

            # combined_value_scoreë¡œ ì •ë ¬
            high_value_docs.sort(key=lambda x: x.get("combined_value_score", 0.0), reverse=True)

            # ìµœëŒ€ ë¬¸ì„œ ìˆ˜ ì œí•œ
            selected_docs = high_value_docs[:max_docs]

            self.logger.info(
                f"ğŸ“š [HIGH VALUE SELECTION] Selected {len(selected_docs)}/{len(documents)} documents. "
                f"Avg density: {sum(d.get('information_density_score', 0.0) for d in selected_docs) / max(1, len(selected_docs)):.3f}"
            )

            return selected_docs

        except Exception as e:
            self.logger.warning(f"High value document selection failed: {e}, using first {max_docs} documents")
            return documents[:max_docs]

    def _extract_key_insights(
        self,
        documents: List[Dict],
        query: str
    ) -> List[str]:
        """í•µì‹¬ ì •ë³´ ì¶”ì¶œ - ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ"""
        insights = DocumentExtractor.extract_key_insights(documents, query)
        self.logger.debug(f"ğŸ“ [KEY INSIGHTS] Extracted {len(insights)} key insights")
        return insights

    def _extract_legal_citations(
        self,
        documents: List[Dict]
    ) -> List[Dict[str, str]]:
        """ë²•ë¥  ì¸ìš© ì •ë³´ ì¶”ì¶œ"""
        citations = DocumentExtractor.extract_legal_citations(documents)
        self.logger.debug(f"âš–ï¸ [LEGAL CITATIONS] Extracted {len(citations)} citations")
        return citations

    def _optimize_context_composition(
        self,
        high_value_docs: List[Dict],
        query: str,
        max_length: int
    ) -> Dict[str, Any]:
        """ContextBuilder.optimize_context_composition ë˜í¼"""
        return self.context_builder.optimize_context_composition(high_value_docs, query, max_length)

    def _build_intelligent_context(
        self,
        state: LegalWorkflowState
    ) -> Dict[str, Any]:
        """ContextBuilder.build_intelligent_context ë˜í¼"""
        return self.context_builder.build_intelligent_context(state)

    def _calculate_context_relevance(
        self,
        context: Dict[str, Any],
        query: str
    ) -> float:
        """ContextBuilder.calculate_context_relevance ë˜í¼"""
        return self.context_builder.calculate_context_relevance(context, query)

    def _calculate_information_coverage(
        self,
        context: Dict[str, Any],
        query: str,
        query_type: str,
        extracted_keywords: List[str]
    ) -> float:
        """ContextBuilder.calculate_information_coverage ë˜í¼"""
        return self.context_builder.calculate_information_coverage(context, query, query_type, extracted_keywords)

    def _calculate_context_sufficiency(
        self,
        context: Dict[str, Any],
        query_type: str
    ) -> float:
        """ContextBuilder.calculate_context_sufficiency ë˜í¼"""
        return self.context_builder.calculate_context_sufficiency(context, query_type)

    def _identify_missing_information(
        self,
        context: Dict[str, Any],
        query: str,
        query_type: str,
        extracted_keywords: List[str]
    ) -> List[str]:
        """ContextBuilder.identify_missing_information ë˜í¼"""
        return self.context_builder.identify_missing_information(context, query, query_type, extracted_keywords)

    def _validate_context_quality(
        self,
        context: Dict[str, Any],
        query: str,
        query_type: str,
        extracted_keywords: List[str]
    ) -> Dict[str, Any]:
        """
        ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ (quality_validators ëª¨ë“ˆ ì‚¬ìš©)
        """
        # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (semantic_search ì‚¬ìš©)
        def calculate_relevance(context_text: str, query: str) -> float:
            try:
                if self.semantic_search and hasattr(self.semantic_search, '_calculate_semantic_score'):
                    return self.semantic_search._calculate_semantic_score(query, context_text)
            except Exception:
                pass
            return ContextValidator.calculate_relevance(context_text, query)

        return ContextValidator.validate_context_quality(
            context=context,
            query=query,
            query_type=query_type,
            extracted_keywords=extracted_keywords,
            calculate_relevance_func=calculate_relevance,
            calculate_coverage_func=None
        )

    def _validate_context_quality_original(
        self,
        context: Dict[str, Any],
        query: str,
        query_type: str,
        extracted_keywords: List[str]
    ) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ì í•©ì„± ê²€ì¦"""
        try:
            # 1. ê´€ë ¨ì„± ì ìˆ˜
            relevance_score = self._calculate_context_relevance(context, query)

            # 2. ì •ë³´ ì»¤ë²„ë¦¬ì§€
            coverage_score = self._calculate_information_coverage(
                context, query, query_type, extracted_keywords
            )

            # 3. ì¶©ë¶„ì„± í‰ê°€
            sufficiency_score = self._calculate_context_sufficiency(context, query_type)

            # 4. ë¶€ì¡±í•œ ì •ë³´ ì‹ë³„
            missing_info = self._identify_missing_information(
                context, query, query_type, extracted_keywords
            )

            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            overall_score = (relevance_score * 0.4 + coverage_score * 0.3 + sufficiency_score * 0.3)

            # ê²€ì¦ ê²°ê³¼
            validation_result = {
                "relevance_score": relevance_score,
                "coverage_score": coverage_score,
                "sufficiency_score": sufficiency_score,
                "overall_score": overall_score,
                "missing_information": missing_info,
                "is_sufficient": overall_score >= 0.6,
                "needs_expansion": overall_score < 0.6 or len(missing_info) > 3
            }

            self.logger.info(
                f"ğŸ” [CONTEXT VALIDATION] Relevance: {relevance_score:.2f}, "
                f"Coverage: {coverage_score:.2f}, Sufficiency: {sufficiency_score:.2f}, "
                f"Overall: {overall_score:.2f}, Missing: {len(missing_info)} items"
            )

            return validation_result

        except Exception as e:
            self.logger.warning(f"Context validation failed: {e}")
            return {
                "relevance_score": 0.5,
                "coverage_score": 0.5,
                "sufficiency_score": 0.5,
                "overall_score": 0.5,
                "missing_information": [],
                "is_sufficient": True,
                "needs_expansion": False
            }

    def _adaptive_context_expansion(
        self,
        state: LegalWorkflowState,
        validation_results: Dict[str, Any]
    ) -> LegalWorkflowState:
        """ì ì‘í˜• ì»¨í…ìŠ¤íŠ¸ í™•ì¥"""
        try:
            if not validation_results.get("needs_expansion", False):
                return state

            # í™•ì¥ íšŸìˆ˜ í™•ì¸ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            metadata = self._get_state_value(state, "metadata", {})
            if not isinstance(metadata, dict):
                metadata = {}

            expansion_count = metadata.get("context_expansion_count", 0)
            if expansion_count >= 1:  # ìµœëŒ€ 1íšŒ í™•ì¥
                self.logger.info("Context expansion skipped: maximum expansion count reached")
                return state

            missing_info = validation_results.get("missing_information", [])
            query = self._get_state_value(state, "query", "")
            query_type = self._get_state_value(state, "query_type", "")

            if not missing_info:
                return state

            self.logger.info(f"ğŸ”§ [CONTEXT EXPANSION] Expanding context for missing: {missing_info[:3]}")

            # ë¶€ì¡±í•œ ì •ë³´ë¡œ ì¶”ê°€ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            expanded_query = query
            if missing_info:
                # ìƒìœ„ 3ê°œ ëˆ„ë½ ì •ë³´ë¥¼ ì¿¼ë¦¬ì— ì¶”ê°€
                safe_missing = [m for m in missing_info[:3] if isinstance(m, str)]
                if safe_missing:
                    expanded_query = f"{query} {' '.join(safe_missing)}"

            # ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰
            try:
                semantic_results, semantic_count = self._semantic_search(expanded_query, k=5)
                keyword_results, keyword_count = self._keyword_search(
                    expanded_query,
                    query_type,
                    limit=3
                )

                # ê¸°ì¡´ ë¬¸ì„œì™€ í†µí•©
                existing_docs = self._get_state_value(state, "retrieved_docs", [])
                all_docs = existing_docs + semantic_results + keyword_results

                # ì¤‘ë³µ ì œê±°
                seen_ids = set()
                unique_docs = []
                for doc in all_docs:
                    doc_id = doc.get("id") or hash(doc.get("content", "")[:100])
                    if isinstance(doc_id, (str, int)) and doc_id not in seen_ids:
                        seen_ids.add(doc_id)
                        unique_docs.append(doc)

                self._set_state_value(state, "retrieved_docs", unique_docs[:10])  # ìµœëŒ€ 10ê°œ
                metadata["context_expansion_count"] = expansion_count + 1
                self._set_state_value(state, "metadata", metadata)

                self.logger.info(
                    f"âœ… [CONTEXT EXPANSION] Added {len(unique_docs) - len(existing_docs)} documents, "
                    f"total: {len(unique_docs)}"
                )

            except Exception as e:
                self.logger.warning(f"Context expansion search failed: {e}")

            return state

        except Exception as e:
            self.logger.error(f"Adaptive context expansion failed: {e}")
            return state

    # Phase 5 ë¦¬íŒ©í† ë§: ë‹µë³€ ìƒì„± ê´€ë ¨ ë©”ì„œë“œëŠ” AnswerGeneratorë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _validate_answer_uses_context(
        self,
        answer: str,
        context: Dict[str, Any],
        query: str,
        retrieved_docs: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """AnswerGenerator.validate_answer_uses_context ë˜í¼"""
        return self.answer_generator.validate_answer_uses_context(
            answer=answer,
            context=context,
            query=query,
            retrieved_docs=retrieved_docs
        )

    def _track_search_to_answer_pipeline(
        self,
        state: LegalWorkflowState
    ) -> Dict[str, Any]:
        """AnswerGenerator.track_search_to_answer_pipeline ë˜í¼"""
        return self.answer_generator.track_search_to_answer_pipeline(state)

    # Phase 6 ë¦¬íŒ©í† ë§: ì»¨í…ìŠ¤íŠ¸ ë¹Œë” ê´€ë ¨ ë©”ì„œë“œëŠ” ContextBuilderë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _build_context(self, state: LegalWorkflowState) -> Dict[str, Any]:
        """ContextBuilder.build_context ë˜í¼"""
        return self.context_builder.build_context(state)

    def _call_llm_with_retry(self, prompt: str, max_retries: int = WorkflowConstants.MAX_RETRIES) -> str:
        """AnswerGenerator.call_llm_with_retry ë˜í¼"""
        return self.answer_generator.call_llm_with_retry(prompt, max_retries)

    def _generate_answer_with_chain(
        self,
        optimized_prompt: str,
        query: str,
        context_dict: Dict[str, Any],
        quality_feedback: Optional[Dict[str, Any]] = None,
        is_retry: bool = False
    ) -> str:
        """AnswerGenerator.generate_answer_with_chain ë˜í¼"""
        return self.answer_generator.generate_answer_with_chain(
            optimized_prompt=optimized_prompt,
            query=query,
            context_dict=context_dict,
            quality_feedback=quality_feedback,
            is_retry=is_retry
        )

    def _parse_validation_response(self, response: str) -> Dict[str, Any]:
        """WorkflowUtils.parse_validation_response ë˜í¼"""
        return WorkflowUtils.parse_validation_response(response, self.logger)

    def _parse_improvement_instructions(self, response: str) -> Optional[Dict[str, Any]]:
        """WorkflowUtils.parse_improvement_instructions ë˜í¼"""
        return WorkflowUtils.parse_improvement_instructions(response, self.logger)

    def _parse_final_validation_response(self, response: str) -> Optional[Dict[str, Any]]:
        """WorkflowUtils.parse_final_validation_response ë˜í¼"""
        return WorkflowUtils.parse_final_validation_response(response, self.logger)

    def _extract_response_content(self, response) -> str:
        """WorkflowUtils.extract_response_content ë˜í¼"""
        return WorkflowUtils.extract_response_content(response)


    # Phase 5 ë¦¬íŒ©í† ë§: ë‹µë³€ ìƒì„± ê´€ë ¨ ë©”ì„œë“œëŠ” AnswerGeneratorë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _generate_fallback_answer(self, state: LegalWorkflowState) -> str:
        """AnswerGenerator.generate_fallback_answer ë˜í¼"""
        return self.answer_generator.generate_fallback_answer(state)

    @observe(name="format_answer")
    @with_state_optimization("format_answer", enable_reduction=True)
    # Phase 4 ë¦¬íŒ©í† ë§: ë‹µë³€ í¬ë§·íŒ… ê´€ë ¨ ë©”ì„œë“œëŠ” AnswerFormatterHandlerë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def format_answer(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """AnswerFormatterHandler.format_answer ë˜í¼"""
        return self.answer_formatter_handler.format_answer(state)

    @observe(name="prepare_final_response")
    @with_state_optimization("prepare_final_response", enable_reduction=False)
    def prepare_final_response(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """AnswerFormatterHandler.prepare_final_response ë˜í¼"""
        return self.answer_formatter_handler.prepare_final_response(state)

    # Phase 4 ë¦¬íŒ©í† ë§: ë‹µë³€ í¬ë§·íŒ… ê´€ë ¨ ë©”ì„œë“œëŠ” AnswerFormatterHandlerë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _format_answer_part(self, state: LegalWorkflowState) -> str:
        """AnswerFormatterHandler.format_answer_part ë˜í¼"""
        return self.answer_formatter_handler.format_answer_part(state)

    def _prepare_final_response_part(
        self,
        state: LegalWorkflowState,
        query_complexity: Optional[str],
        needs_search: bool
    ) -> None:
        """AnswerFormatterHandler.prepare_final_response_part ë˜í¼"""
        self.answer_formatter_handler.prepare_final_response_part(state, query_complexity, needs_search)

    def _extract_preserved_values(self, state: LegalWorkflowState) -> Dict[str, Any]:
        """AnswerFormatterHandler.extract_preserved_values ë˜í¼"""
        return self.answer_formatter_handler.extract_preserved_values(state)

    def _preserve_and_store_values(
        self,
        state: LegalWorkflowState,
        query_complexity: Optional[str],
        needs_search: bool
    ) -> None:
        """AnswerFormatterHandler.preserve_and_store_values ë˜í¼"""
        self.answer_formatter_handler.preserve_and_store_values(state, query_complexity, needs_search)

    def _map_confidence_level(self, confidence: float):
        """AnswerFormatterHandler.map_confidence_level ë˜í¼"""
        return self.answer_formatter_handler.map_confidence_level(confidence)

    def _calculate_keyword_coverage(
        self,
        state: LegalWorkflowState,
        answer: Union[str, Dict[str, Any], None]
    ) -> float:
        """AnswerFormatterHandler.calculate_keyword_coverage ë˜í¼"""
        return self.answer_formatter_handler.calculate_keyword_coverage(state, answer)

    def _set_metadata(
        self,
        state: LegalWorkflowState,
        answer: Union[str, Dict[str, Any], None],
        keyword_coverage: float
    ) -> None:
        """AnswerFormatterHandler.set_metadata ë˜í¼"""
        self.answer_formatter_handler.set_metadata(state, answer, keyword_coverage)

    # Phase 11 ë¦¬íŒ©í† ë§: ì¤‘ë³µëœ ì›ë³¸ ë©”ì„œë“œ ì½”ë“œ ì œê±° ì™„ë£Œ
    # ë‹µë³€ í¬ë§·íŒ… ê´€ë ¨ ë©”ì„œë“œëŠ” AnswerFormatterHandlerë¡œ ì´ë™ë˜ì–´ ë˜í¼ë§Œ ë‚¨ìŒ
    #
    # DEPRECATED: ì´ ë©”ì„œë“œëŠ” ë” ì´ìƒ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ì—ì„œ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    # í¬ë§·íŒ… ë¡œì§ì€ generate_and_validate_answerì™€ direct_answer_nodeì— í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.
    # í˜¸í™˜ì„±ì„ ìœ„í•´ ë‚¨ê²¨ë‘ì—ˆì§€ë§Œ, í–¥í›„ ì œê±°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    @observe(name="format_and_prepare_final")
    @with_state_optimization("format_and_prepare_final", enable_reduction=False)
    def format_and_prepare_final(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """í†µí•©ëœ ë‹µë³€ í¬ë§·íŒ… ë° ìµœì¢… ì¤€ë¹„ (DEPRECATED: generate_and_validate_answerì— í†µí•©ë¨)"""
        try:
            # Phase 4 ë¦¬íŒ©í† ë§: AnswerFormatterHandler ì‚¬ìš©
            state = self.answer_formatter_handler.format_and_prepare_final(state)

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.update_statistics(state)

            confidence = state.get("confidence", 0.0)
            self.logger.info(
                f"format_and_prepare_final completed, confidence: {confidence:.3f}"
            )

        except Exception as e:
            self._handle_error(state, str(e), "ë‹µë³€ í¬ë§·íŒ… ë° ìµœì¢… ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            answer = self._get_state_value(state, "answer", "")
            if not state.get("answer"):
                state["answer"] = self._normalize_answer(answer)

        return state

    def update_statistics(self, state: LegalWorkflowState):
        """í†µê³„ ì—…ë°ì´íŠ¸ (ì´ë™ í‰ê·  ì‚¬ìš©)"""
        if not self.stats:
            return

        try:
            self.stats['total_queries'] += 1
            processing_time = state.get("processing_time", 0.0)
            confidence = state.get("confidence", 0.0)
            docs_count = len(state.get("retrieved_docs", []))
            errors_count = len(state.get("errors", []))

            # ì´ë™ í‰ê·  ê³„ì‚°
            alpha = self.config.stats_update_alpha

            if self.stats['total_queries'] == 1:
                self.stats['avg_response_time'] = processing_time
                self.stats['avg_confidence'] = confidence
            else:
                # ì´ë™ í‰ê·  ì—…ë°ì´íŠ¸
                self.stats['avg_response_time'] = (
                    (1 - alpha) * self.stats['avg_response_time'] +
                    alpha * processing_time
                )
                self.stats['avg_confidence'] = (
                    (1 - alpha) * self.stats['avg_confidence'] +
                    alpha * confidence
                )

            # ëˆ„ì  í†µê³„
            self.stats['total_documents_retrieved'] += docs_count
            self.stats['total_errors'] += errors_count

            self.logger.debug(
                f"Statistics updated: queries={self.stats['total_queries']}, "
                f"avg_time={self.stats['avg_response_time']:.2f}s, "
                f"avg_conf={self.stats['avg_confidence']:.2f}"
            )
        except Exception as e:
            self.logger.warning(f"Statistics update failed: {e}")

    def get_statistics(self) -> Dict[str, Any]:
        """í†µê³„ ì¡°íšŒ"""
        if not self.stats:
            return {"enabled": False}

        return {
            "enabled": True,
            "total_queries": self.stats['total_queries'],
            "total_documents_retrieved": self.stats['total_documents_retrieved'],
            "avg_response_time": round(self.stats['avg_response_time'], 3),
            "avg_confidence": round(self.stats['avg_confidence'], 3),
            "total_errors": self.stats['total_errors'],
            "avg_docs_per_query": (
                round(self.stats['total_documents_retrieved'] / self.stats['total_queries'], 2)
                if self.stats['total_queries'] > 0 else 0
            )
        }

    def _extract_legal_field(self, query_type: str, query: str) -> str:
        """ë²•ë¥  ë¶„ì•¼ ì¶”ì¶œ"""
        return QueryExtractor.extract_legal_field(query_type, query)

    def _map_to_legal_domain(self, legal_field: str) -> str:
        """
        LegalDomain enumìœ¼ë¡œ ë§¤í•‘ - ì§€ì›ë˜ëŠ” ë„ë©”ì¸ë§Œ

        í˜„ì¬ ì§€ì› ë„ë©”ì¸:
        - ë¯¼ì‚¬ë²• (CIVIL_LAW)
        - ì§€ì‹ì¬ì‚°ê¶Œë²• (INTELLECTUAL_PROPERTY)
        - í–‰ì •ë²• (ADMINISTRATIVE_LAW)
        - í˜•ì‚¬ë²• (CRIMINAL_LAW)

        ì´ì™¸ëŠ” "ê¸°íƒ€/ì¼ë°˜"ìœ¼ë¡œ ì²˜ë¦¬
        """
        # ì§€ì›ë˜ëŠ” ë„ë©”ì¸ë§Œ ë§¤í•‘
        mapping = {
            "civil": LegalDomain.CIVIL_LAW.value if hasattr(LegalDomain.CIVIL_LAW, 'value') else "ë¯¼ì‚¬ë²•",
            "criminal": LegalDomain.CRIMINAL_LAW.value if hasattr(LegalDomain.CRIMINAL_LAW, 'value') else "í˜•ì‚¬ë²•",
            "intellectual_property": LegalDomain.INTELLECTUAL_PROPERTY.value if hasattr(LegalDomain.INTELLECTUAL_PROPERTY, 'value') else "ì§€ì ì¬ì‚°ê¶Œë²•",
            "administrative": LegalDomain.ADMINISTRATIVE_LAW.value if hasattr(LegalDomain.ADMINISTRATIVE_LAW, 'value') else "í–‰ì •ë²•",
        }
        return mapping.get(legal_field, "ê¸°íƒ€/ì¼ë°˜")

    @observe(name="classification_parallel")
    @with_state_optimization("classification_parallel", enable_reduction=True)
    def classification_parallel(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ë¶„ë¥˜ ì‘ì—… ë³‘ë ¬ ì‹¤í–‰ (ê¸´ê¸‰ë„ í‰ê°€ + ë©€í‹°í„´ ì²˜ë¦¬)"""
        try:
            start_time = time.time()
            from concurrent.futures import ThreadPoolExecutor

            query = self._get_state_value(state, "query", "")
            session_id = self._get_state_value(state, "session_id", "")

            # ë³‘ë ¬ ì‘ì—… ê²°ê³¼ ì €ì¥
            urgency_level = None
            urgency_reasoning = None
            is_multi_turn = False
            search_query = query

            with ThreadPoolExecutor(max_workers=2) as executor:
                # ë³‘ë ¬ ì‘ì—… ì •ì˜
                futures = {
                    'urgency': executor.submit(self._assess_urgency_internal, query),
                    'multi_turn': executor.submit(self._resolve_multi_turn_internal, query, session_id),
                }

                # ê²°ê³¼ ìˆ˜ì§‘
                results = {}
                for key, future in futures.items():
                    try:
                        results[key] = future.result(timeout=10)
                    except Exception as e:
                        self.logger.error(f"{key} ë³‘ë ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                        results[key] = None

                # State ì—…ë°ì´íŠ¸
                if results['urgency']:
                    urgency_level, urgency_reasoning = results['urgency']
                    self._set_state_value(state, "urgency_level", urgency_level)
                    self._set_state_value(state, "urgency_reasoning", urgency_reasoning)

                if results['multi_turn']:
                    is_multi_turn, search_query = results['multi_turn']
                    self._set_state_value(state, "is_multi_turn", is_multi_turn)
                    if search_query:
                        self._set_state_value(state, "search_query", search_query)

            processing_time = self._update_processing_time(state, start_time)
            self._add_step(
                state,
                "ë³‘ë ¬ ë¶„ë¥˜ ì™„ë£Œ",
                f"ê¸´ê¸‰ë„ í‰ê°€ ë° ë©€í‹°í„´ ì²˜ë¦¬ ë³‘ë ¬ ì™„ë£Œ (ì‹œê°„: {processing_time:.3f}s)"
            )

            self.logger.info(
                f"âœ… ë³‘ë ¬ ë¶„ë¥˜ ì™„ë£Œ: ê¸´ê¸‰ë„={urgency_level}, ë©€í‹°í„´={is_multi_turn} (ì‹œê°„: {processing_time:.3f}s)"
            )

        except Exception as e:
            self._handle_error(state, str(e), "ë³‘ë ¬ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # í´ë°±: ê¸°ë³¸ê°’ ì„¤ì •
            self._set_state_value(state, "urgency_level", "medium")
            self._set_state_value(state, "is_multi_turn", False)
            self._set_state_value(state, "search_query", query)

        return state

    def _assess_urgency_internal(self, query: str) -> Tuple[str, str]:
        """ê¸´ê¸‰ë„ í‰ê°€ (ë‚´ë¶€ ë¡œì§)"""
        try:
            if self.emotion_analyzer:
                intent_result = self.emotion_analyzer.analyze_intent(query, None)

                # ê¸´ê¸‰ë„ ì„¤ì •
                if intent_result and hasattr(intent_result, 'emergency_level'):
                    if hasattr(intent_result.emergency_level, 'value'):
                        urgency_level = intent_result.emergency_level.value
                    elif hasattr(intent_result.emergency_level, 'lower'):
                        urgency_level = intent_result.emergency_level.lower()
                    else:
                        urgency_level = str(intent_result.emergency_level).lower()

                    urgency_reasoning = getattr(intent_result, 'reasoning', None) or "ê¸´ê¸‰ë„ ë¶„ì„ ì™„ë£Œ"
                    return urgency_level, urgency_reasoning

            # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ í‰ê°€
            urgency_level = self._assess_urgency_fallback(query)
            return urgency_level, "í‚¤ì›Œë“œ ê¸°ë°˜ í‰ê°€"
        except Exception as e:
            self.logger.error(f"ê¸´ê¸‰ë„ í‰ê°€ ë‚´ë¶€ ë¡œì§ ì‹¤íŒ¨: {e}")
            return "medium", "ì˜¤ë¥˜ ë°œìƒ, ê¸°ë³¸ê°’ ì‚¬ìš©"

    def _resolve_multi_turn_internal(self, query: str, session_id: str) -> Tuple[bool, str]:
        """ë©€í‹°í„´ ì²˜ë¦¬ (ë‚´ë¶€ ë¡œì§)"""
        try:
            if not self.multi_turn_handler or not self.conversation_manager:
                return False, query

            conversation_context = self._get_or_create_conversation_context(session_id)

            if conversation_context and conversation_context.turns:
                is_multi_turn = self.multi_turn_handler.detect_multi_turn_question(query, conversation_context)

                if is_multi_turn:
                    multi_turn_result = self.multi_turn_handler.build_complete_query(query, conversation_context)
                    search_query = multi_turn_result.complete_query if multi_turn_result else query
                    return True, search_query

            return False, query
        except Exception as e:
            self.logger.error(f"ë©€í‹°í„´ ì²˜ë¦¬ ë‚´ë¶€ ë¡œì§ ì‹¤íŒ¨: {e}")
            return False, query

    @observe(name="assess_urgency")
    @with_state_optimization("assess_urgency", enable_reduction=True)
    def assess_urgency(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ê¸´ê¸‰ë„ í‰ê°€ ë…¸ë“œ"""
        try:
            start_time = time.time()
            query = self._get_state_value(state, "query", "")

            if self.emotion_analyzer:
                # ê°ì • ë° ì˜ë„ ë¶„ì„ (ì˜ë„ ë¶„ì„ë§Œ ì‚¬ìš©)
                intent_result = self.emotion_analyzer.analyze_intent(query, None)

                # ê¸´ê¸‰ë„ ì„¤ì •
                urgency_level = intent_result.urgency_level.value
                self._set_state_value(state, "urgency_level", urgency_level)
                self._set_state_value(state, "urgency_reasoning", intent_result.reasoning)

                # ê¸´ê¸‰ ìœ í˜• íŒë³„
                if "ê¸°í•œ" in query or "ë§ˆê°" in query or "ë°ë“œë¼ì¸" in query:
                    self._set_state_value(state, "emergency_type", "legal_deadline")
                elif "ì†Œì†¡" in query or "ì¬íŒ" in query or "ë²•ì›" in query:
                    self._set_state_value(state, "emergency_type", "case_progress")
                else:
                    self._set_state_value(state, "emergency_type", None)

                self.logger.info(f"Urgency assessed: {urgency_level}")
            else:
                # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ê¸´ê¸‰ë„ í‰ê°€
                urgency_level = self._assess_urgency_fallback(query)
                self._set_state_value(state, "urgency_level", urgency_level)
                self._set_state_value(state, "urgency_reasoning", "í‚¤ì›Œë“œ ê¸°ë°˜ í‰ê°€")
                self._set_state_value(state, "emergency_type", None)

            self._update_processing_time(state, start_time)
            self._add_step(state, "ê¸´ê¸‰ë„ í‰ê°€", f"ê¸´ê¸‰ë„: {urgency_level}")

        except Exception as e:
            self._handle_error(state, str(e), "ê¸´ê¸‰ë„ í‰ê°€ ì¤‘ ì˜¤ë¥˜")
            self._set_state_value(state, "urgency_level", "medium")
            self._set_state_value(state, "urgency_reasoning", "ê¸°ë³¸ê°’")
            self._set_state_value(state, "emergency_type", None)

        return state

    def _assess_urgency_fallback(self, query: str) -> str:
        """í´ë°± ê¸´ê¸‰ë„ í‰ê°€"""
        urgent_keywords = ["ê¸´ê¸‰", "ê¸‰í•´", "ë¹¨ë¦¬", "ì¦‰ì‹œ", "ë‹¹ì¥"]
        high_keywords = ["ì˜¤ëŠ˜", "ë‚´ì¼", "ì´ë²ˆì£¼", "ë§ˆê°"]

        query_lower = query.lower()
        if any(k in query_lower for k in urgent_keywords):
            return "critical"
        elif any(k in query_lower for k in high_keywords):
            return "high"
        else:
            return "medium"

    @observe(name="analyze_document")
    @with_state_optimization("analyze_document", enable_reduction=True)
    def analyze_document(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ë¶„ì„"""
        try:
            start_time = time.time()

            # Check if document analysis is available in metadata or session context
            # Document would be passed via external parameter or session
            doc_text = state.get("document_analysis", {}).get("raw_text") if isinstance(state.get("document_analysis"), dict) else None

            if not doc_text:
                # Check if there's a document in document_analysis field
                if state.get("document_analysis") and isinstance(state["document_analysis"], str):
                    doc_text = state["document_analysis"]
                else:
                    self.logger.info("No document provided for analysis, skipping")
                    return state

            # ë¬¸ì„œ ìœ í˜• íŒë³„ (í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” LLM)
            doc_type = self._detect_document_type(doc_text)
            state["document_type"] = doc_type

            # ë¬¸ì„œ ë¶„ì„ (Prompt Chaining ì‚¬ìš©)
            analysis_result = self._analyze_legal_document_with_chain(doc_text, doc_type)

            # Store analysis in state (summarized)
            if isinstance(state["document_analysis"], dict):
                state["document_analysis"].update({
                    "document_type": doc_type,
                    "summary": analysis_result.get("summary", ""),
                    "analysis_time": time.time() - start_time
                })
            else:
                state["document_analysis"] = {
                    "document_type": doc_type,
                    "summary": analysis_result.get("summary", ""),
                    "analysis_time": time.time() - start_time
                }

            state["key_clauses"] = analysis_result.get("key_clauses", [])
            state["potential_issues"] = analysis_result.get("issues", [])

            # ë¶„ì„ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (pruned)
            doc_summary = self._create_document_summary(analysis_result)
            summary_doc = {
                "content": doc_summary[:MAX_DOCUMENT_CONTENT_LENGTH],  # Summarize
                "source": "Uploaded Document Analysis",
                "type": "document_analysis",
                "relevance_score": 1.0,
                "is_summarized": True
            }

            # Prune retrieved_docs before inserting
            if len(state["retrieved_docs"]) >= MAX_RETRIEVED_DOCS:
                state["retrieved_docs"] = prune_retrieved_docs(
                    state["retrieved_docs"],
                    max_items=MAX_RETRIEVED_DOCS - 1,
                    max_content_per_doc=MAX_DOCUMENT_CONTENT_LENGTH
                )

            state["retrieved_docs"].insert(0, summary_doc)

            self._update_processing_time(state, start_time)
            self._add_step(state, "ë¬¸ì„œ ë¶„ì„", f"{doc_type} ë¶„ì„ ì™„ë£Œ")

        except Exception as e:
            self._handle_error(state, str(e), "ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜")

        return state

    def _detect_document_type(self, text: str) -> str:
        """ë¬¸ì„œ ìœ í˜• ê°ì§€"""
        type_keywords = {
            "contract": ["ê³„ì•½ì„œ", "ê³„ì•½", "ê°‘", "ì„", "ë³¸ ê³„ì•½"],
            "complaint": ["ê³ ì†Œì¥", "í”¼ê³ ì†Œì¸", "ê³ ì†Œì¸", "ê³ ì†Œì·¨ì§€"],
            "agreement": ["í•©ì˜ì„œ", "í•©ì˜", "ìŒë°©"],
            "power_of_attorney": ["ìœ„ì„ì¥", "ìœ„ì„ì¸", "ìˆ˜ì„ì¸"]
        }

        text_lower = text.lower()
        for doc_type, keywords in type_keywords.items():
            if any(k in text_lower for k in keywords):
                return doc_type

        return "general_legal_document"

    def _analyze_legal_document(self, text: str, doc_type: str) -> Dict[str, Any]:
        """ë²•ë¥  ë¬¸ì„œ ë¶„ì„"""
        analysis = {
            "document_type": doc_type,
            "key_clauses": [],
            "issues": [],
            "summary": "",
            "recommendations": []
        }

        # ì£¼ìš” ì¡°í•­ ì¶”ì¶œ
        if doc_type == "contract":
            analysis["key_clauses"] = self._extract_contract_clauses(text)
            analysis["issues"] = self._identify_contract_issues(text, analysis["key_clauses"])
        elif doc_type == "complaint":
            analysis["key_clauses"] = self._extract_complaint_elements(text)
            analysis["issues"] = self._identify_complaint_issues(text)

        # ìš”ì•½ ìƒì„±
        analysis["summary"] = self._generate_document_summary(text, doc_type, analysis)

        return analysis

    def _extract_contract_clauses(self, text: str) -> List[Dict[str, Any]]:
        """ê³„ì•½ì„œ ì£¼ìš” ì¡°í•­ ì¶”ì¶œ"""
        return DocumentExtractor.extract_contract_clauses(text)

    def _identify_contract_issues(self, text: str, clauses: List[Dict]) -> List[Dict[str, Any]]:
        """ê³„ì•½ì„œ ì ì¬ ë¬¸ì œì  ì‹ë³„"""
        issues = []

        # í•„ìˆ˜ ì¡°í•­ í™•ì¸
        required_clauses = ["payment", "period", "termination"]
        # hashable íƒ€ì…ë§Œ í•„í„°ë§ (ìŠ¬ë¼ì´ìŠ¤ ê°ì²´ ë“± unhashable íƒ€ì… ë°©ì§€)
        found_types = set()
        for c in clauses:
            clause_type = c.get("type")
            if clause_type is not None and isinstance(clause_type, (str, int, float, tuple)):
                found_types.add(clause_type)
            elif clause_type is not None:
                # unhashable íƒ€ì…ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
                try:
                    found_types.add(str(clause_type))
                except Exception:
                    pass

        for req_type in required_clauses:
            if req_type not in found_types:
                issues.append({
                    "severity": "high",
                    "type": "missing_clause",
                    "description": f"í•„ìˆ˜ ì¡°í•­ ëˆ„ë½: {req_type}",
                    "recommendation": f"{req_type} ì¡°í•­ì„ ì¶”ê°€í•˜ì‹­ì‹œì˜¤"
                })

        # ë¶ˆëª…í™•í•œ í‘œí˜„ í™•ì¸
        vague_terms = ["ê¸°íƒ€", "ë“±ë“±", "ì ì ˆí•œ", "í•©ë‹¹í•œ"]
        for term in vague_terms:
            if term in text:
                issues.append({
                    "severity": "medium",
                    "type": "vague_term",
                    "description": f"ë¶ˆëª…í™•í•œ ìš©ì–´ ì‚¬ìš©: {term}",
                    "recommendation": "êµ¬ì²´ì ì¸ ìš©ì–´ë¡œ ëŒ€ì²´í•˜ì‹­ì‹œì˜¤"
                })

        return issues[:5]  # ìƒìœ„ 5ê°œë§Œ

    def _extract_complaint_elements(self, text: str) -> List[Dict[str, Any]]:
        """ê³ ì†Œì¥ ìš”ê±´ ì¶”ì¶œ"""
        return DocumentExtractor.extract_complaint_elements(text)

    def _identify_complaint_issues(self, text: str) -> List[Dict[str, Any]]:
        """ê³ ì†Œì¥ ë¬¸ì œì  ì‹ë³„"""
        issues = []

        # í•„ìˆ˜ ìš”ì†Œ í™•ì¸
        required_elements = ["í”¼ê³ ì†Œì¸", "ì‚¬ì‹¤ê´€ê³„", "ì²­êµ¬"]
        for elem in required_elements:
            if elem not in text:
                issues.append({
                    "severity": "high",
                    "type": "missing_element",
                    "description": f"í•„ìˆ˜ ìš”ì†Œ ëˆ„ë½: {elem}",
                    "recommendation": f"{elem} ì •ë³´ë¥¼ ì¶”ê°€í•˜ì‹­ì‹œì˜¤"
                })

        return issues

    def _analyze_legal_document_with_chain(self, text: str, doc_type: str) -> Dict[str, Any]:
        """
        Prompt Chainingì„ ì‚¬ìš©í•œ ë²•ë¥  ë¬¸ì„œ ë¶„ì„ (ë‹¤ë‹¨ê³„ ì²´ì¸)

        Step 1: ë¬¸ì„œ ìœ í˜• í™•ì¸ (í‚¤ì›Œë“œ ê¸°ë°˜ ê²°ê³¼ ê²€ì¦)
        Step 2: ì£¼ìš” ì¡°í•­ ì¶”ì¶œ (ë¬¸ì„œ ìœ í˜• ê¸°ë°˜)
        Step 3: ë¬¸ì œì  ì‹ë³„ (ì¡°í•­ ê¸°ë°˜)
        Step 4: ìš”ì•½ ìƒì„± (ì¡°í•­ + ë¬¸ì œì  ê¸°ë°˜)
        Step 5: ê°œì„  ê¶Œê³  ìƒì„± (ë¬¸ì œì  ê¸°ë°˜)

        Returns:
            Dict[str, Any]: ë¶„ì„ ê²°ê³¼
        """
        try:
            # PromptChainExecutor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            chain_executor = PromptChainExecutor(self.llm, self.logger)

            # ì²´ì¸ ìŠ¤í… ì •ì˜
            chain_steps = []

            # Step 1: ë¬¸ì„œ ìœ í˜• í™•ì¸ (LLM ê²€ì¦)
            def build_document_type_verification_prompt(prev_output, initial_input):
                doc_text = initial_input.get("text") if isinstance(initial_input, dict) else text[:2000]  # ì²˜ìŒ 2000ìë§Œ
                detected_type = initial_input.get("doc_type") if isinstance(initial_input, dict) else doc_type

                return f"""ë‹¤ìŒ ë¬¸ì„œì˜ ìœ í˜•ì„ í™•ì¸í•˜ê³  ê²€ì¦í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš© (ì¼ë¶€):
{doc_text}

í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì§€ ê²°ê³¼: {detected_type}

ë‹¤ìŒ ë¬¸ì„œ ìœ í˜• ì¤‘ í•˜ë‚˜ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”:
- contract (ê³„ì•½ì„œ): ê³„ì•½ì„œ, ê°‘/ì„, ê³„ì•½ ì¡°ê±´ ë“±
- complaint (ê³ ì†Œì¥): ê³ ì†Œì¥, í”¼ê³ ì†Œì¸, ê³ ì†Œì¸ ë“±
- agreement (í•©ì˜ì„œ): í•©ì˜ì„œ, í•©ì˜, ìŒë°© í•©ì˜ ë“±
- power_of_attorney (ìœ„ì„ì¥): ìœ„ì„ì¥, ìœ„ì„ì¸, ìˆ˜ì„ì¸ ë“±
- general_legal_document (ì¼ë°˜ ë²•ë¥  ë¬¸ì„œ): ìœ„ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "document_type": "contract" | "complaint" | "agreement" | "power_of_attorney" | "general_legal_document",
    "confidence": 0.0-1.0,
    "reasoning": "íŒë‹¨ ê·¼ê±° (í•œêµ­ì–´)"
}}
"""

            chain_steps.append({
                "name": "document_type_verification",
                "prompt_builder": build_document_type_verification_prompt,
                "input_extractor": lambda prev: prev,
                "output_parser": lambda response, prev: DocumentParser.parse_document_type_response(response),
                "validator": lambda output: output and isinstance(output, dict) and "document_type" in output,
                "required": True
            })

            # Step 2: ì£¼ìš” ì¡°í•­ ì¶”ì¶œ (ë¬¸ì„œ ìœ í˜• ê¸°ë°˜)
            def build_clause_extraction_prompt(prev_output, initial_input):
                # prev_outputì€ Step 1ì˜ ê²°ê³¼ (document_type í¬í•¨)
                if not isinstance(prev_output, dict):
                    prev_output = {}

                verified_doc_type = prev_output.get("document_type", doc_type)
                doc_text = initial_input.get("text") if isinstance(initial_input, dict) else text[:3000]  # ì²˜ìŒ 3000ìë§Œ

                if verified_doc_type == "contract":
                    return f"""ë‹¤ìŒ ê³„ì•½ì„œ ë¬¸ì„œì—ì„œ ì£¼ìš” ì¡°í•­ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{doc_text[:3000]}

ë‹¤ìŒ ìœ í˜•ì˜ ì¡°í•­ì„ ì°¾ì•„ì£¼ì„¸ìš”:
- payment (ëŒ€ê¸ˆ/ì§€ê¸‰): ëŒ€ê¸ˆ, ê¸ˆì•¡, ì§€ê¸‰, ê²°ì œ ê´€ë ¨ ì¡°í•­
- period (ê¸°ê°„/ê¸°í•œ): ê¸°ê°„, ê¸°í•œ, ë§Œë£Œ ê´€ë ¨ ì¡°í•­
- termination (í•´ì§€/í•´ì œ): í•´ì§€, í•´ì œ, ì¢…ë£Œ ê´€ë ¨ ì¡°í•­
- liability (ì±…ì„): ì±…ì„, ì†í•´ë°°ìƒ, ìœ„ì•½ê¸ˆ ê´€ë ¨ ì¡°í•­
- confidentiality (ë¹„ë°€/ê¸°ë°€): ë¹„ë°€, ê¸°ë°€, ë³´ì•ˆ ê´€ë ¨ ì¡°í•­

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "key_clauses": [
        {{
            "type": "payment",
            "text": "ì œ1ì¡° ëŒ€ê¸ˆì€...",
            "article_number": "ì œ1ì¡°"
        }},
        ...
    ],
    "clause_count": 5
}}
"""
                elif verified_doc_type == "complaint":
                    return f"""ë‹¤ìŒ ê³ ì†Œì¥ ë¬¸ì„œì—ì„œ ì£¼ìš” ìš”ì†Œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{doc_text[:3000]}

ë‹¤ìŒ ìš”ì†Œë¥¼ ì°¾ì•„ì£¼ì„¸ìš”:
- parties (ë‹¹ì‚¬ì): í”¼ê³ ì†Œì¸, ê³ ì†Œì¸, í”¼í•´ì, ê°€í•´ì ë“±
- facts (ì‚¬ì‹¤ê´€ê³„): ì‚¬ì‹¤ê´€ê³„, ê²½ìœ„, ë‚´ìš© ë“±
- claims (ì²­êµ¬ì‚¬í•­): ì²­êµ¬, ìš”êµ¬, ì£¼ì¥ ë“±

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "key_clauses": [
        {{
            "type": "parties",
            "text": "í”¼ê³ ì†Œì¸: ...",
            "found": true
        }},
        ...
    ],
    "clause_count": 3
}}
"""
                else:
                    # ì¼ë°˜ ë¬¸ì„œ
                    return f"""ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œì—ì„œ ì£¼ìš” ë‚´ìš©ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{doc_text[:3000]}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "key_clauses": [
        {{
            "type": "general",
            "text": "ì£¼ìš” ë‚´ìš© 1...",
            "summary": "ìš”ì•½"
        }},
        ...
    ],
    "clause_count": 3
}}
"""

            chain_steps.append({
                "name": "clause_extraction",
                "prompt_builder": build_clause_extraction_prompt,
                "input_extractor": lambda prev: prev,  # Step 1ì˜ ì¶œë ¥ ì‚¬ìš©
                "output_parser": lambda response, prev: DocumentParser.parse_clause_extraction_response(response),
                "validator": lambda output: output and isinstance(output, dict) and "key_clauses" in output,
                "required": True
            })

            # Step 3: ë¬¸ì œì  ì‹ë³„ (ì¡°í•­ ê¸°ë°˜)
            def build_issue_identification_prompt(prev_output, initial_input):
                # prev_outputì€ Step 2ì˜ ê²°ê³¼ (key_clauses í¬í•¨)
                if not isinstance(prev_output, dict):
                    prev_output = {}

                key_clauses = prev_output.get("key_clauses", [])
                verified_doc_type = initial_input.get("verified_doc_type") or doc_type
                doc_text = initial_input.get("text") if isinstance(initial_input, dict) else text[:2000]

                # Step 1ì—ì„œ ë¬¸ì„œ ìœ í˜• ê°€ì ¸ì˜¤ê¸°
                if hasattr(chain_executor, 'chain_history'):
                    for step in chain_executor.chain_history:
                        if step.get("step_name") == "document_type_verification" and step.get("success"):
                            step_output = step.get("output", {})
                            if isinstance(step_output, dict):
                                verified_doc_type = step_output.get("document_type", doc_type)
                                break

                if verified_doc_type == "contract":
                    return f"""ë‹¤ìŒ ê³„ì•½ì„œì˜ ì¡°í•­ì„ ë¶„ì„í•˜ì—¬ ì ì¬ì  ë¬¸ì œì ì„ ì‹ë³„í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš© (ì¼ë¶€):
{doc_text[:2000]}

ì¶”ì¶œëœ ì£¼ìš” ì¡°í•­:
{chr(10).join([f"- {clause.get('type', 'unknown')}: {clause.get('text', '')[:100]}..." for clause in key_clauses[:5]])}

ë‹¤ìŒ ê´€ì ì—ì„œ ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”:
1. í•„ìˆ˜ ì¡°í•­ ëˆ„ë½: ëŒ€ê¸ˆ, ê¸°ê°„, í•´ì§€ ì¡°í•­ ë“±
2. ë¶ˆëª…í™•í•œ í‘œí˜„: "ê¸°íƒ€", "ë“±ë“±", "ì ì ˆí•œ" ë“±
3. ë¶ˆê³µì •í•œ ì¡°í•­: ì¼ë°©ì  ë¶ˆë¦¬í•œ ì¡°ê±´
4. ë²•ì  ë¬¸ì œ: ë²•ë ¹ ìœ„ë°˜ ê°€ëŠ¥ì„±

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "issues": [
        {{
            "severity": "high" | "medium" | "low",
            "type": "missing_clause" | "vague_term" | "unfair_clause" | "legal_issue",
            "description": "ë¬¸ì œì  ì„¤ëª…",
            "recommendation": "ê°œì„  ê¶Œê³ ì‚¬í•­"
        }},
        ...
    ],
    "issue_count": 3
}}
"""
                elif verified_doc_type == "complaint":
                    return f"""ë‹¤ìŒ ê³ ì†Œì¥ì˜ ìš”ì†Œë¥¼ ë¶„ì„í•˜ì—¬ ë¬¸ì œì ì„ ì‹ë³„í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš© (ì¼ë¶€):
{doc_text[:2000]}

ì¶”ì¶œëœ ì£¼ìš” ìš”ì†Œ:
{chr(10).join([f"- {clause.get('type', 'unknown')}" for clause in key_clauses[:5]])}

ë‹¤ìŒ ê´€ì ì—ì„œ ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”:
1. í•„ìˆ˜ ìš”ì†Œ ëˆ„ë½: í”¼ê³ ì†Œì¸, ì‚¬ì‹¤ê´€ê³„, ì²­êµ¬ì‚¬í•­ ë“±
2. ë¶ˆëª…í™•í•œ ì‚¬ì‹¤: ëª¨í˜¸í•œ ì„œìˆ , ë¶ˆì¶©ë¶„í•œ ì¦ê±° ì œì‹œ
3. ë²•ì  ìš”ê±´ ë¯¸ë¹„: ê³ ì†Œ ìš”ê±´ ë¶ˆì¶©ì¡± ê°€ëŠ¥ì„±

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "issues": [
        {{
            "severity": "high" | "medium" | "low",
            "type": "missing_element" | "vague_facts" | "insufficient_evidence",
            "description": "ë¬¸ì œì  ì„¤ëª…",
            "recommendation": "ê°œì„  ê¶Œê³ ì‚¬í•­"
        }},
        ...
    ],
    "issue_count": 2
}}
"""
                else:
                    # ì¼ë°˜ ë¬¸ì„œëŠ” ë¬¸ì œì  ì‹ë³„ ìƒëµ ê°€ëŠ¥
                    return None

            chain_steps.append({
                "name": "issue_identification",
                "prompt_builder": build_issue_identification_prompt,
                "input_extractor": lambda prev: prev,  # Step 2ì˜ ì¶œë ¥ ì‚¬ìš©
                "output_parser": lambda response, prev: DocumentParser.parse_issue_identification_response_with_context(response, prev),
                "validator": lambda output: output is None or (isinstance(output, dict) and "issues" in output),
                "required": False,  # ì„ íƒ ë‹¨ê³„ (ì¼ë°˜ ë¬¸ì„œëŠ” ìƒëµ ê°€ëŠ¥)
                "skip_if": lambda prev: not isinstance(prev, dict) or not prev.get("key_clauses")
            })

            # Step 4: ìš”ì•½ ìƒì„± (ì¡°í•­ + ë¬¸ì œì  ê¸°ë°˜)
            def build_summary_generation_prompt(prev_output, initial_input):
                # prev_outputì€ Step 3ì˜ ê²°ê³¼ (issues í¬í•¨) ë˜ëŠ” Step 2ì˜ ê²°ê³¼ (key_clauses í¬í•¨)
                if not isinstance(prev_output, dict):
                    prev_output = {}

                # Step 3 ê²°ê³¼ì—ì„œ key_clausesì™€ issues ëª¨ë‘ ê°€ì ¸ì˜¤ê¸° (í†µí•©ëœ ê²°ê³¼)
                key_clauses = prev_output.get("key_clauses", [])
                issues = prev_output.get("issues", [])
                verified_doc_type = prev_output.get("document_type") or initial_input.get("verified_doc_type") or doc_type

                return f"""ë‹¤ìŒ ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ìœ í˜•: {verified_doc_type}
ì£¼ìš” ì¡°í•­ ìˆ˜: {len(key_clauses)}
ë°œê²¬ëœ ë¬¸ì œì  ìˆ˜: {len(issues)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ìœ í˜•: {verified_doc_type}
ë¶„ì„ëœ ì¡°í•­ ìˆ˜: {len(key_clauses)}
ë°œê²¬ëœ ë¬¸ì œì : {len(issues)}

ì£¼ìš” ë¬¸ì œì :
{chr(10).join([f"- {issue.get('description', '')}" for issue in issues[:3]]) if issues else "ì—†ìŒ"}

ìœ„ í˜•ì‹ì— ë§ì¶° ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""

            # Step 4ì˜ input_extractor: ì´ì „ ë‹¨ê³„ ê²°ê³¼ í†µí•©
            def extract_summary_input(prev_output):
                # prev_outputì€ Step 3ì˜ ê²°ê³¼ ë˜ëŠ” Step 2ì˜ ê²°ê³¼
                if not isinstance(prev_output, dict):
                    return prev_output

                # Step 3 ê²°ê³¼ì—ì„œ key_clausesì™€ issues í†µí•© (Step 3ê°€ key_clausesë„ í¬í•¨í•˜ë„ë¡ í•¨)
                result = {
                    "key_clauses": prev_output.get("key_clauses", []),
                    "issues": prev_output.get("issues", []),
                    "document_type": prev_output.get("document_type", doc_type)
                }
                return result

            chain_steps.append({
                "name": "summary_generation",
                "prompt_builder": build_summary_generation_prompt,
                "input_extractor": extract_summary_input,  # ì´ì „ ë‹¨ê³„ì˜ í†µí•© ê²°ê³¼ ì‚¬ìš©
                "output_parser": lambda response, prev: self._normalize_answer(response),
                "validator": lambda output: output and len(output.strip()) > 10,
                "required": True
            })

            # Step 5ì˜ input_extractor: Step 3 ê²°ê³¼ ì°¾ê¸°
            def extract_improvement_input(prev_output):
                # prev_outputì€ Step 4ì˜ ê²°ê³¼ (summary ë¬¸ìì—´)
                # Step 3 ê²°ê³¼ë¥¼ ì°¾ê¸° ìœ„í•´ ì´ì „ ë‹¨ê³„ ì¶œë ¥ë“¤ì„ í™•ì¸í•´ì•¼ í•˜ì§€ë§Œ,
                # prompt_builder ì‹œì ì—ëŠ” ì²´ì¸ íˆìŠ¤í† ë¦¬ê°€ ì—†ìœ¼ë¯€ë¡œ
                # Noneì„ ë°˜í™˜í•˜ë©´ ê±´ë„ˆë›°ê¸° (ì²´ì¸ ì‹¤í–‰ í›„ì— issues í™•ì¸)
                return prev_output

            # Step 5: ê°œì„  ê¶Œê³  ìƒì„± (ë¬¸ì œì  ê¸°ë°˜)
            def build_improvement_recommendations_prompt(prev_output, initial_input):
                # prev_outputì€ Step 4ì˜ ê²°ê³¼ (summary ë¬¸ìì—´)
                # Step 3ì˜ issuesë¥¼ ì°¾ê¸° ìœ„í•´ì„œëŠ” ì²´ì¸ íˆìŠ¤í† ë¦¬ê°€ í•„ìš”í•œë°,
                # prompt_builder ì‹œì ì—ëŠ” ì•„ì§ ì²´ì¸ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŒ
                # ë”°ë¼ì„œ Step 5ëŠ” ì²´ì¸ ì‹¤í–‰ í›„ì— issuesë¥¼ í™•ì¸í•˜ì—¬ ì¡°ê±´ë¶€ë¡œ ì‹¤í–‰
                # ì—¬ê¸°ì„œëŠ” í•­ìƒ None ë°˜í™˜í•˜ì—¬ prompt_builderì—ì„œ ê±´ë„ˆë›°ê³ ,
                # ì²´ì¸ ì‹¤í–‰ í›„ì— issuesê°€ ìˆìœ¼ë©´ ë³„ë„ë¡œ ì‹¤í–‰
                return None  # í•­ìƒ ê±´ë„ˆë›°ê¸° (ì²´ì¸ ì‹¤í–‰ í›„ ì²˜ë¦¬)

                verified_doc_type = initial_input.get("verified_doc_type") or doc_type
                doc_text = initial_input.get("text") if isinstance(initial_input, dict) else text[:1500]

                return f"""ë‹¤ìŒ ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ê¶Œê³ ì‚¬í•­ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ìœ í˜•: {verified_doc_type}
ë¬¸ì„œ ë‚´ìš© (ì¼ë¶€):
{doc_text[:1500]}

ë°œê²¬ëœ ë¬¸ì œì :
{chr(10).join([f"{idx+1}. [{issue.get('severity', 'medium')}] {issue.get('description', '')}" for idx, issue in enumerate(issues[:5])])}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°œì„  ê¶Œê³ ì‚¬í•­ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
{{
    "recommendations": [
        {{
            "priority": "high" | "medium" | "low",
            "issue_type": "ë¬¸ì œì  ìœ í˜•",
            "recommendation": "êµ¬ì²´ì ì¸ ê°œì„  ê¶Œê³ ì‚¬í•­",
            "rationale": "ê¶Œê³  ê·¼ê±°"
        }},
        ...
    ],
    "recommendation_count": 3
}}
"""

            # Step 5ëŠ” prompt_builderì—ì„œ Noneì„ ë°˜í™˜í•˜ì—¬ ê±´ë„ˆë›°ê³ ,
            # ì²´ì¸ ì‹¤í–‰ í›„ì— issuesê°€ ìˆìœ¼ë©´ ë³„ë„ë¡œ ì‹¤í–‰
            # ì—¬ê¸°ì„œëŠ” ì²´ì¸ì— ì¶”ê°€í•˜ì§€ ì•Šê³ , ì²´ì¸ ì‹¤í–‰ í›„ì— ì¡°ê±´ë¶€ë¡œ ì²˜ë¦¬

            # ì²´ì¸ ì‹¤í–‰
            initial_input_dict = {
                "text": text,
                "doc_type": doc_type
            }

            chain_result = chain_executor.execute_chain(
                chain_steps=chain_steps,
                initial_input=initial_input_dict,
                max_iterations=2,
                stop_on_failure=False
            )

            # ê²°ê³¼ ì¶”ì¶œ ë° í†µí•©
            chain_history = chain_result.get("chain_history", [])

            # Step 1: ë¬¸ì„œ ìœ í˜• í™•ì¸
            verified_doc_type = doc_type
            for step in chain_history:
                if step.get("step_name") == "document_type_verification" and step.get("success"):
                    step_output = step.get("output", {})
                    if isinstance(step_output, dict):
                        verified_doc_type = step_output.get("document_type", doc_type)
                        break

            # Step 2: ì£¼ìš” ì¡°í•­ ì¶”ì¶œ
            key_clauses = []
            for step in chain_history:
                if step.get("step_name") == "clause_extraction" and step.get("success"):
                    step_output = step.get("output", {})
                    if isinstance(step_output, dict):
                        key_clauses = step_output.get("key_clauses", [])
                        break

            # Step 3: ë¬¸ì œì  ì‹ë³„
            issues = []
            for step in chain_history:
                if step.get("step_name") == "issue_identification" and step.get("success"):
                    step_output = step.get("output", {})
                    if isinstance(step_output, dict):
                        issues = step_output.get("issues", [])
                        break

            # Step 4: ìš”ì•½ ìƒì„±
            summary = ""
            for step in chain_history:
                if step.get("step_name") == "summary_generation" and step.get("success"):
                    summary = step.get("output", "")
                    if isinstance(summary, str):
                        break
                    elif isinstance(summary, dict):
                        summary = summary.get("summary", "")
                        break

            # Step 5: ê°œì„  ê¶Œê³  (ì¡°ê±´ë¶€ ì‹¤í–‰ - issuesê°€ ìˆëŠ” ê²½ìš°)
            recommendations = []
            if issues and len(issues) > 0:
                try:
                    # issuesê°€ ìˆìœ¼ë©´ ê°œì„  ê¶Œê³  ìƒì„±
                    improvement_prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ê¶Œê³ ì‚¬í•­ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ìœ í˜•: {verified_doc_type}
ë¬¸ì„œ ë‚´ìš© (ì¼ë¶€):
{text[:1500]}

ë°œê²¬ëœ ë¬¸ì œì :
{chr(10).join([f"{idx+1}. [{issue.get('severity', 'medium')}] {issue.get('description', '')}" for idx, issue in enumerate(issues[:5])])}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°œì„  ê¶Œê³ ì‚¬í•­ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
{{
    "recommendations": [
        {{
            "priority": "high" | "medium" | "low",
            "issue_type": "ë¬¸ì œì  ìœ í˜•",
            "recommendation": "êµ¬ì²´ì ì¸ ê°œì„  ê¶Œê³ ì‚¬í•­",
            "rationale": "ê¶Œê³  ê·¼ê±°"
        }},
        ...
    ],
    "recommendation_count": 3
}}
"""
                    llm = self.llm_fast if hasattr(self, 'llm_fast') and self.llm_fast else self.llm
                    improvement_response = llm.invoke(improvement_prompt)
                    improvement_content = self._extract_response_content(improvement_response)
                    improvement_result = DocumentParser.parse_improvement_recommendations_response(improvement_content)
                    if improvement_result and isinstance(improvement_result, dict):
                        recommendations = improvement_result.get("recommendations", [])
                except Exception as e:
                    self.logger.warning(f"Failed to generate improvement recommendations: {e}")

            # ê²°ê³¼ í†µí•©
            analysis_result = {
                "document_type": verified_doc_type,
                "key_clauses": key_clauses,
                "issues": issues,
                "summary": summary if summary else self._generate_document_summary_fallback(text, verified_doc_type, key_clauses, issues),
                "recommendations": recommendations
            }

            # ì²´ì¸ ì‹¤í–‰ ê²°ê³¼ ë¡œê¹…
            chain_summary = chain_executor.get_chain_summary()
            self.logger.info(
                f"âœ… [DOCUMENT CHAIN] Executed {chain_summary['total_steps']} steps, "
                f"{chain_summary['successful_steps']} successful, "
                f"{chain_summary['failed_steps']} failed, "
                f"Total time: {chain_summary['total_time']:.2f}s"
            )

            return analysis_result

        except Exception as e:
            self.logger.error(f"âŒ [DOCUMENT CHAIN ERROR] Prompt chain failed: {e}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            return self._analyze_legal_document(text, doc_type)

    def _generate_document_summary(self, text: str, doc_type: str, analysis: Dict[str, Any]) -> str:
        """ë¬¸ì„œ ìš”ì•½ ìƒì„±"""
        summary_parts = [f"ë¬¸ì„œ ìœ í˜•: {doc_type}"]
        summary_parts.append(f"ë¶„ì„ëœ ì¡°í•­ ìˆ˜: {len(analysis.get('key_clauses', []))}")
        summary_parts.append(f"ë°œê²¬ëœ ë¬¸ì œì : {len(analysis.get('issues', []))}")

        if analysis.get("issues"):
            summary_parts.append("\nì£¼ìš” ë¬¸ì œì :")
            for issue in analysis["issues"][:3]:
                summary_parts.append(f"- {issue['description']}")

        return "\n".join(summary_parts)

    def _generate_document_summary_fallback(self, text: str, doc_type: str, key_clauses: List[Dict], issues: List[Dict]) -> str:
        """ë¬¸ì„œ ìš”ì•½ ìƒì„± (í´ë°±)"""
        summary_parts = [f"ë¬¸ì„œ ìœ í˜•: {doc_type}"]
        summary_parts.append(f"ë¶„ì„ëœ ì¡°í•­ ìˆ˜: {len(key_clauses)}")
        summary_parts.append(f"ë°œê²¬ëœ ë¬¸ì œì : {len(issues)}")

        if issues:
            summary_parts.append("\nì£¼ìš” ë¬¸ì œì :")
            for issue in issues[:3]:
                if isinstance(issue, dict):
                    summary_parts.append(f"- {issue.get('description', '')}")
                else:
                    summary_parts.append(f"- {str(issue)}")

        return "\n".join(summary_parts)

    def _parse_document_type_response(self, response: str) -> Dict[str, Any]:
        """ClassificationHandler.parse_document_type_response ë˜í¼"""
        return self.classification_handler.parse_document_type_response(response)

    def _parse_clause_extraction_response(self, response: str) -> Dict[str, Any]:
        """ì¡°í•­ ì¶”ì¶œ ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            import re

            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                parsed = json.loads(json_str)
                # key_clausesê°€ ìˆëŠ”ì§€ í™•ì¸
                if "key_clauses" in parsed:
                    return parsed

            # ê¸°ë³¸ê°’
            return {
                "key_clauses": [],
                "clause_count": 0
            }
        except Exception as e:
            self.logger.warning(f"Failed to parse clause extraction response: {e}")
            return {
                "key_clauses": [],
                "clause_count": 0
            }

    def _parse_issue_identification_response(self, response: str) -> Optional[Dict[str, Any]]:
        """ë¬¸ì œì  ì‹ë³„ ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            import re

            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                parsed = json.loads(json_str)
                # issuesê°€ ìˆëŠ”ì§€ í™•ì¸
                if "issues" in parsed:
                    return parsed

            return None
        except Exception as e:
            self.logger.warning(f"Failed to parse issue identification response: {e}")
            return None

    def _parse_improvement_recommendations_response(self, response: str) -> Optional[Dict[str, Any]]:
        """ê°œì„  ê¶Œê³  ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            import re

            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                parsed = json.loads(json_str)
                # recommendationsê°€ ìˆëŠ”ì§€ í™•ì¸
                if "recommendations" in parsed:
                    return parsed

            return None
        except Exception as e:
            self.logger.warning(f"Failed to parse improvement recommendations response: {e}")
            return None

    def _parse_issue_identification_response_with_context(self, response: str, prev_output: Any) -> Optional[Dict[str, Any]]:
        """ë¬¸ì œì  ì‹ë³„ ì‘ë‹µ íŒŒì‹± (ì´ì „ ë‹¨ê³„ ì¶œë ¥ í†µí•©)"""
        try:
            import json
            import re

            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                parsed = json.loads(json_str)
                # issuesê°€ ìˆëŠ”ì§€ í™•ì¸
                if "issues" in parsed:
                    # ì´ì „ ë‹¨ê³„ ê²°ê³¼(key_clauses)ë„ í¬í•¨
                    if isinstance(prev_output, dict):
                        parsed["key_clauses"] = prev_output.get("key_clauses", [])
                        parsed["document_type"] = prev_output.get("document_type", "")
                    return parsed

            return None
        except Exception as e:
            self.logger.warning(f"Failed to parse issue identification response: {e}")
            return None

    def _create_document_summary(self, analysis: Dict[str, Any]) -> str:
        """ë¬¸ì„œ ë¶„ì„ ìš”ì•½ ìƒì„±"""
        summary_parts = [f"## ì—…ë¡œë“œ ë¬¸ì„œ ë¶„ì„ ({analysis['document_type']})"]

        if analysis.get("key_clauses"):
            summary_parts.append("\n### ì£¼ìš” ì¡°í•­")
            for clause in analysis["key_clauses"][:3]:
                summary_parts.append(f"- {clause['type']}: {clause['text'][:100]}...")

        if analysis.get("issues"):
            summary_parts.append("\n### ì ì¬ ë¬¸ì œì ")
            for issue in analysis["issues"]:
                summary_parts.append(f"- [{issue['severity']}] {issue['description']}")

        return "\n".join(summary_parts)

    @observe(name="route_expert")
    @with_state_optimization("route_expert", enable_reduction=True)
    def route_expert(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì „ë¬¸ê°€ ì„œë¸Œê·¸ë˜í”„ë¡œ ë¼ìš°íŒ… (Phase 9 ë¦¬íŒ©í† ë§: WorkflowRoutes ì‚¬ìš©)"""
        try:
            start_time = time.time()

            # WorkflowRoutes.route_expert í˜¸ì¶œ
            state = self.workflow_routes.route_expert(state)

            complexity = state.get("complexity_level", "simple")
            requires_expert = state.get("requires_expert", False)

            self._update_processing_time(state, start_time)
            self._add_step(state, "ì „ë¬¸ê°€ ë¼ìš°íŒ…", f"ë³µì¡ë„: {complexity}, ì „ë¬¸ê°€: {requires_expert}")

        except Exception as e:
            self._handle_error(state, str(e), "ì „ë¬¸ê°€ ë¼ìš°íŒ… ì¤‘ ì˜¤ë¥˜")
            state["complexity_level"] = "simple"
            state["requires_expert"] = False
            state["expert_subgraph"] = None

        return state

    # Phase 9 ë¦¬íŒ©í† ë§: ë¼ìš°íŒ… ê´€ë ¨ ë©”ì„œë“œëŠ” WorkflowRoutesë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _assess_complexity(self, state: LegalWorkflowState) -> str:
        """WorkflowRoutes.assess_complexity ë˜í¼"""
        return self.workflow_routes.assess_complexity(state)

    def _should_analyze_document(self, state: LegalWorkflowState) -> str:
        """WorkflowRoutes.should_analyze_document ë˜í¼"""
        return self.workflow_routes.should_analyze_document(state)

    # ============================================================================
    # ê°œì„ ëœ ê²€ìƒ‰ ë…¸ë“œë“¤ (ë…¸ë“œ ë¶„ë¦¬ ë° ë³‘ë ¬ ì‹¤í–‰ ì§€ì›)
    # ============================================================================

    @observe(name="prepare_search_query")
    @with_state_optimization("prepare_search_query", enable_reduction=False)
    def prepare_search_query(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ ë° ìµœì í™” ì „ìš© ë…¸ë“œ (Part 2)"""
        try:
            start_time = time.time()

            # metadata ë³´ì¡´
            preserved_complexity = state.get("metadata", {}).get("query_complexity") if isinstance(state.get("metadata"), dict) else None
            preserved_needs_search = state.get("metadata", {}).get("needs_search") if isinstance(state.get("metadata"), dict) else None

            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            state["metadata"] = dict(state["metadata"])
            if preserved_complexity:
                state["metadata"]["query_complexity"] = preserved_complexity
            if preserved_needs_search is not None:
                state["metadata"]["needs_search"] = preserved_needs_search
            state["metadata"]["_last_executed_node"] = "prepare_search_query"

            if "common" not in state or not isinstance(state.get("common"), dict):
                state["common"] = {}
            if "metadata" not in state["common"]:
                state["common"]["metadata"] = {}
            state["common"]["metadata"]["_last_executed_node"] = "prepare_search_query"

            # ì¤‘ìš”: ë…¸ë“œ ì‹œì‘ ì‹œ input ê·¸ë£¹ ë³´ì¥
            # LangGraphê°€ ì´ì „ ë…¸ë“œì˜ ê²°ê³¼ë§Œ ì „ë‹¬í•˜ëŠ” ê²½ìš°, inputì´ ì‚¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
            # input ê·¸ë£¹ í™•ì¸ ë° ìƒì„±
            if "input" not in state or not isinstance(state.get("input"), dict):
                state["input"] = {}

            # queryê°€ ì—†ìœ¼ë©´ ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ ì°¾ê¸°
            current_query = state["input"].get("query", "")
            if not current_query:
                # 1. ìµœìƒìœ„ ë ˆë²¨ì—ì„œ ì°¾ê¸°
                query_from_top = state.get("query", "")
                session_id_from_top = state.get("session_id", "")
                if query_from_top:
                    state["input"]["query"] = query_from_top
                    if session_id_from_top:
                        state["input"]["session_id"] = session_id_from_top
                # 2. search ê·¸ë£¹ì—ì„œ ì°¾ê¸°
                elif "search" in state and isinstance(state["search"], dict):
                    search_query = state["search"].get("search_query", "")
                    if search_query:
                        state["input"]["query"] = search_query

            # ì¬ì‹œë„ ì¹´ìš´í„° ê´€ë¦¬
            metadata = state.get("metadata", {}) if isinstance(state.get("metadata"), dict) else {}
            if not isinstance(metadata, dict):
                metadata = {}

            # ì¤‘ìš”: state.get("common")ì´ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            common_state = state.get("common")
            if common_state and isinstance(common_state, dict):
                common_metadata = common_state.get("metadata", {})
                if isinstance(common_metadata, dict):
                    metadata = {**metadata, **common_metadata}

            last_executed_node = metadata.get("_last_executed_node", "")
            is_retry_from_generation = (last_executed_node == "generate_answer_enhanced")
            is_retry_from_validation = (last_executed_node == "validate_answer_quality")

            # ì¬ì‹œë„ ì¹´ìš´í„° ì¦ê°€
            if is_retry_from_generation:
                if self.retry_manager.should_allow_retry(state, "generation"):
                    self.retry_manager.increment_retry_count(state, "generation")

            if is_retry_from_validation:
                if self.retry_manager.should_allow_retry(state, "validation"):
                    self.retry_manager.increment_retry_count(state, "validation")

            # ì¬ì‹œë„ íšŸìˆ˜ ì²´í¬
            retry_counts = self.retry_manager.get_retry_counts(state)
            if retry_counts["total"] >= RetryConfig.MAX_TOTAL_RETRIES:
                self.logger.error("Maximum total retry count reached")
                if not self._get_state_value(state, "answer", ""):
                    query = self._get_state_value(state, "query", "")
                    self._set_state_value(state, "answer",
                        f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ '{query}'ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤.")
                return state

            # ì¿¼ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ë° ê²€ì¦ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
            # ì¤‘ìš”: stateì˜ input ê·¸ë£¹ì—ì„œ query ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ì‹œì‘ ë¶€ë¶„ì—ì„œ ë³µì›í–ˆì–´ì•¼ í•¨)
            query = None

            # ìš°ì„ ìˆœìœ„ 1: state["input"]["query"]
            if "input" in state and isinstance(state["input"], dict):
                query = state["input"].get("query", "")

            # ìš°ì„ ìˆœìœ„ 2: _get_state_value ì‚¬ìš©
            if not query or not str(query).strip():
                query = self._get_state_value(state, "query", "")

            # ìš°ì„ ìˆœìœ„ 3: stateì—ì„œ ì§ì ‘ ì½ê¸°
            if not query or not str(query).strip():
                if isinstance(state, dict) and "query" in state:
                    query = state["query"]

            search_query = self._get_state_value(state, "search_query") or query

            # queryê°€ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì—ëŸ¬
            if not query or not str(query).strip():
                self.logger.error(f"prepare_search_query: query is empty! State keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")
                if "input" in state:
                    self.logger.error(f"prepare_search_query: state['input'] = {state['input']}")
                self._set_state_value(state, "answer", "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")
                return state

            # query_type ì •ê·œí™”
            query_type_raw = self._get_state_value(state, "query_type", "")
            query_type_str = self._get_query_type_str(query_type_raw)
            # í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í•œë²ˆ ë” ì •ê·œí™”
            query_type_str = self._normalize_query_type_for_prompt(query_type_str)

            # extracted_keywords ê²€ì¦
            extracted_keywords_raw = self._get_state_value(state, "extracted_keywords", [])
            if not isinstance(extracted_keywords_raw, list):
                self.logger.warning(f"extracted_keywords is not a list: {type(extracted_keywords_raw)}, converting to empty list")
                extracted_keywords = []
            else:
                # ìœ íš¨í•œ í‚¤ì›Œë“œë§Œ í•„í„°ë§
                extracted_keywords = [kw for kw in extracted_keywords_raw if kw and isinstance(kw, str) and len(str(kw).strip()) > 0]

            # legal_field ê²€ì¦
            legal_field_raw = self._get_state_value(state, "legal_field", "")
            legal_field = str(legal_field_raw).strip() if legal_field_raw else ""

            # ë¡œê¹…: ì „ë‹¬ë˜ëŠ” ë°ì´í„° í™•ì¸
            self.logger.debug(
                f"ğŸ“‹ [PREPARE SEARCH QUERY] Data for query optimization:\n"
                f"   query: '{query[:50]}{'...' if len(query) > 50 else ''}'\n"
                f"   search_query: '{search_query[:50]}{'...' if len(search_query) > 50 else ''}'\n"
                f"   query_type (raw): '{query_type_raw}' â†’ (normalized): '{query_type_str}'\n"
                f"   extracted_keywords: {len(extracted_keywords)} items {extracted_keywords[:5] if extracted_keywords else '[]'}\n"
                f"   legal_field: '{legal_field}'"
            )

            is_retry = (last_executed_node == "validate_answer_quality")

            # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
            optimized_queries = self._optimize_search_query(
                query=search_query,
                query_type=query_type_str,
                extracted_keywords=extracted_keywords,
                legal_field=legal_field
            )

            # ì¬ì‹œë„ ì‹œ ì¶”ê°€ ê°œì„ 
            if is_retry:
                quality_feedback = self.answer_generator.get_quality_feedback_for_retry(state)
                improved_query = self._improve_search_query_for_retry(
                    optimized_queries["semantic_query"],
                    quality_feedback,
                    state
                )
                if improved_query != optimized_queries["semantic_query"]:
                    self.logger.info(
                        f"ğŸ” [SEARCH RETRY] Improved query: '{optimized_queries['semantic_query']}' â†’ '{improved_query}'"
                    )
                    optimized_queries["semantic_query"] = improved_query
                    optimized_queries["keyword_queries"][0] = improved_query

            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ê²°ì •
            search_params = self._determine_search_parameters(
                query_type=query_type_str,
                query_complexity=len(query),
                keyword_count=len(extracted_keywords),
                is_retry=is_retry
            )

            # ìµœì í™”ëœ ì¿¼ë¦¬ì™€ íŒŒë¼ë¯¸í„°ë¥¼ stateì— ì €ì¥
            self._set_state_value(state, "optimized_queries", optimized_queries)
            self._set_state_value(state, "search_params", search_params)
            self._set_state_value(state, "is_retry_search", is_retry)
            self._set_state_value(state, "search_start_time", start_time)

            # semantic_query ê²€ì¦ ë° ìˆ˜ì • (ë¹ˆ ë¬¸ìì—´ì´ë©´ ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©)
            semantic_query_created = optimized_queries.get("semantic_query", "")
            if not semantic_query_created or not str(semantic_query_created).strip():
                self.logger.warning(f"semantic_query is empty, using base query: '{query[:50]}...'")
                optimized_queries["semantic_query"] = query
                semantic_query_created = query
                # ìˆ˜ì •ëœ ê°’ì„ ë‹¤ì‹œ ì €ì¥
                self._set_state_value(state, "optimized_queries", optimized_queries)

            # keyword_queriesë„ í™•ì¸ ë° ìˆ˜ì •
            keyword_queries_created = optimized_queries.get("keyword_queries", [])
            if not keyword_queries_created or len(keyword_queries_created) == 0:
                self.logger.warning("keyword_queries is empty, using base query")
                optimized_queries["keyword_queries"] = [query]
                keyword_queries_created = [query]
                # ìˆ˜ì •ëœ ê°’ì„ ë‹¤ì‹œ ì €ì¥
                self._set_state_value(state, "optimized_queries", optimized_queries)

            self._set_state_value(state, "search_query", semantic_query_created)

            # ìºì‹œ í™•ì¸ (ì¬ì‹œë„ ì‹œì—ëŠ” ìºì‹œ ìš°íšŒ)
            cache_hit = False
            if not is_retry:
                cached_documents = self.performance_optimizer.cache.get_cached_documents(
                    optimized_queries["semantic_query"],
                    query_type_str
                )
                if cached_documents:
                    self._set_state_value(state, "retrieved_docs", cached_documents)
                    self._set_state_value(state, "search_cache_hit", True)
                    cache_hit = True
                    self._add_step(state, "ìºì‹œ íˆíŠ¸", f"ìºì‹œ íˆíŠ¸: {len(cached_documents)}ê°œ ë¬¸ì„œ")

            self._set_state_value(state, "search_cache_hit", cache_hit)
            self._save_metadata_safely(state, "_last_executed_node", "prepare_search_query")
            self._update_processing_time(state, start_time)
            self._add_step(state, "ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„", f"ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ ì™„ë£Œ: {semantic_query_created[:50]}...")

            if cache_hit:
                self.logger.info(f"âœ… [CACHE HIT] ìºì‹œ íˆíŠ¸: {len(cached_documents)}ê°œ ë¬¸ì„œ, ê²€ìƒ‰ ìŠ¤í‚µ")
            else:
                self.logger.info(
                    f"âœ… [PREPARE SEARCH QUERY] "
                    f"semantic_query: '{semantic_query_created[:50]}...', "
                    f"keyword_queries: {len(keyword_queries_created)}ê°œ, "
                    f"search_params: k={search_params.get('semantic_k', 'N/A')}"
                )

        except Exception as e:
            self._handle_error(state, str(e), "ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

        # ì¤‘ìš”: ë°˜í™˜ ì „ì— input ê·¸ë£¹ ë³´ì¥ (LangGraph state ë³‘í•© ì‹œ ë³´ì¡´)
        if "input" not in state or not isinstance(state.get("input"), dict):
            state["input"] = {}
        if not state["input"].get("query"):
            query_value = self._get_state_value(state, "query", "")
            session_id_value = self._get_state_value(state, "session_id", "")
            if query_value:
                state["input"]["query"] = query_value
                if session_id_value:
                    state["input"]["session_id"] = session_id_value
                self.logger.debug(f"Ensured input group in state after prepare_search_query: query length={len(query_value)}")

        return state

    # Phase 9 ë¦¬íŒ©í† ë§: ë¼ìš°íŒ… ê´€ë ¨ ë©”ì„œë“œëŠ” WorkflowRoutesë¡œ ì´ë™ë¨
    # í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œ
    def _should_skip_search(self, state: LegalWorkflowState) -> str:
        """WorkflowRoutes.should_skip_search ë˜í¼"""
        return self.workflow_routes.should_skip_search(state)

    def _should_skip_search_adaptive(self, state: LegalWorkflowState) -> str:
        """WorkflowRoutes.should_skip_search_adaptive ë˜í¼"""
        return self.workflow_routes.should_skip_search_adaptive(state)

    def _route_by_complexity(self, state: LegalWorkflowState) -> str:
        """WorkflowRoutes.route_by_complexity ë˜í¼"""
        return self.workflow_routes.route_by_complexity(state)

    @observe(name="process_search_results_combined")
    @with_state_optimization("process_search_results_combined", enable_reduction=True)
    def process_search_results_combined(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ í†µí•© ë…¸ë“œ (6ê°œ ë…¸ë“œë¥¼ 1ê°œë¡œ ë³‘í•©)"""
        try:
            start_time = time.time()

            # ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            semantic_results = self._get_state_value(state, "semantic_results", [])
            keyword_results = self._get_state_value(state, "keyword_results", [])
            semantic_count = self._get_state_value(state, "semantic_count", 0)
            keyword_count = self._get_state_value(state, "keyword_count", 0)
            query = self._get_state_value(state, "query", "")
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            search_params = self._get_state_value(state, "search_params", {})
            extracted_keywords = self._get_state_value(state, "extracted_keywords", [])

            # 1. í’ˆì§ˆ í‰ê°€ (ê¸°ì¡´ evaluate_search_quality ë¡œì§)
            semantic_quality = self._evaluate_semantic_search_quality(
                semantic_results=semantic_results,
                query=query,
                query_type=query_type_str,
                min_results=search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K) // 2
            )

            keyword_quality = self._evaluate_keyword_search_quality(
                keyword_results=keyword_results,
                query=query,
                query_type=query_type_str,
                min_results=search_params.get("keyword_limit", WorkflowConstants.CATEGORY_SEARCH_LIMIT) // 2
            )

            overall_quality = (semantic_quality["score"] + keyword_quality["score"]) / 2.0
            needs_retry = semantic_quality["needs_retry"] or keyword_quality["needs_retry"]

            quality_evaluation = {
                "semantic_quality": semantic_quality,
                "keyword_quality": keyword_quality,
                "overall_quality": overall_quality,
                "needs_retry": needs_retry
            }

            self._set_state_value(state, "search_quality_evaluation", quality_evaluation)

            # 2. ì¡°ê±´ë¶€ ì¬ê²€ìƒ‰ (ê¸°ì¡´ conditional_retry_search ë¡œì§)
            if needs_retry and overall_quality < 0.6 and semantic_count + keyword_count < 10:
                self.logger.info(f"ê²€ìƒ‰ í’ˆì§ˆ ë‚®ìŒ (ì ìˆ˜: {overall_quality:.2f}), ì¬ê²€ìƒ‰ ìˆ˜í–‰...")
                try:
                    # ì¬ê²€ìƒ‰ ë¡œì§ (ê°„ë‹¨í•œ ë²„ì „)
                    retry_semantic = []
                    retry_keyword = []

                    if semantic_quality["needs_retry"]:
                        # ì˜ë¯¸ ê²€ìƒ‰ ì¬ì‹œë„
                        optimized_queries = self._get_state_value(state, "optimized_queries", {})
                        retry_semantic = self._execute_semantic_search_internal(
                            optimized_queries, search_params, query
                        )[0][:5]  # ìµœëŒ€ 5ê°œ

                    if keyword_quality["needs_retry"]:
                        # í‚¤ì›Œë“œ ê²€ìƒ‰ ì¬ì‹œë„
                        optimized_queries = self._get_state_value(state, "optimized_queries", {})
                        retry_keyword = self._execute_keyword_search_internal(
                            optimized_queries, search_params, query_type_str,
                            self._get_state_value(state, "legal_field", ""),
                            extracted_keywords, query
                        )[0][:5]  # ìµœëŒ€ 5ê°œ

                    # ê¸°ì¡´ ê²°ê³¼ì— ì¶”ê°€
                    semantic_results.extend(retry_semantic)
                    keyword_results.extend(retry_keyword)
                    semantic_count += len(retry_semantic)
                    keyword_count += len(retry_keyword)
                except Exception as e:
                    self.logger.warning(f"ì¬ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

            # 3. ë³‘í•© ë° ì¬ìˆœìœ„ (ê¸°ì¡´ merge_and_rerank ë¡œì§)
            # result_merger.merge_resultsëŠ” Dict[str, List[Dict]] í˜•íƒœì˜ exact_resultsë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ
            # keyword_resultsë¥¼ dict í˜•íƒœë¡œ ë³€í™˜
            exact_results_dict = {
                "keyword": keyword_results if isinstance(keyword_results, list) else []
            } if keyword_results else {}

            merged_results = self.result_merger.merge_results(
                exact_results=exact_results_dict,
                semantic_results=semantic_results if isinstance(semantic_results, list) else [],
                weights={"exact": 0.7, "semantic": 0.3}
            )

            # MergedResult ê°ì²´ë¥¼ dictë¡œ ë³€í™˜
            merged_docs = []
            for merged_result in merged_results:
                if hasattr(merged_result, 'text'):
                    merged_docs.append({
                        "content": merged_result.text,
                        "text": merged_result.text,
                        "relevance_score": merged_result.score,
                        "source": merged_result.source,
                        "metadata": merged_result.metadata if hasattr(merged_result, 'metadata') else {}
                    })
                elif isinstance(merged_result, dict):
                    merged_docs.append(merged_result)

            # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš© (merge_and_rerank_with_keyword_weightsì™€ ë™ì¼í•œ ë¡œì§)
            # í‚¤ì›Œë“œ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°
            keyword_weights = self._calculate_keyword_weights(
                extracted_keywords=extracted_keywords,
                query=query,
                query_type=query_type_str,
                legal_field=self._get_state_value(state, "legal_field", "")
            )

            # ê° ë¬¸ì„œì— í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° ë° ê°€ì¤‘ì¹˜ ì ìš©
            weighted_docs = []
            for doc in merged_docs:
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                keyword_scores = self._calculate_keyword_match_score(
                    document=doc,
                    keyword_weights=keyword_weights,
                    query=query
                )

                # ê°€ì¤‘ì¹˜ ì ìš© ìµœì¢… ì ìˆ˜ ê³„ì‚°
                final_score = self._calculate_weighted_final_score(
                    document=doc,
                    keyword_scores=keyword_scores,
                    search_params=search_params,
                    query_type=query_type_str
                )

                # ë¬¸ì„œì— ì ìˆ˜ ì •ë³´ ì¶”ê°€
                doc["keyword_match_score"] = keyword_scores.get("keyword_match_score", 0.0)
                doc["keyword_coverage"] = keyword_scores.get("keyword_coverage", 0.0)
                doc["matched_keywords"] = keyword_scores.get("matched_keywords", [])
                doc["weighted_keyword_score"] = keyword_scores.get("weighted_keyword_score", 0.0)
                doc["final_weighted_score"] = final_score

                weighted_docs.append(doc)

            # Reranking ìˆ˜í–‰ (ì ìˆ˜ìˆœ ì •ë ¬)
            weighted_docs.sort(key=lambda x: x.get("final_weighted_score", x.get("relevance_score", 0.0)), reverse=True)

            # 4. í•„í„°ë§ ë° ê²€ì¦ (ê¸°ì¡´ filter_and_validate_results ë¡œì§)
            filtered_docs = []
            for doc in weighted_docs:
                # ë¹ˆ ë¬¸ì„œ ì œì™¸
                content = doc.get("content", "") or doc.get("text", "")
                if not content or len(content.strip()) < 10:
                    continue

                # ê´€ë ¨ì„± ì ìˆ˜ í™•ì¸
                score = doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)
                if score < 0.1:  # ë„ˆë¬´ ë‚®ì€ ì ìˆ˜ ì œì™¸
                    continue

                filtered_docs.append(doc)

            # ìµœëŒ€ ë¬¸ì„œ ìˆ˜ ì œí•œ
            max_docs = self.config.max_retrieved_docs or 20
            final_docs = filtered_docs[:max_docs]

            # 5. ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ê¸°ì¡´ update_search_metadata ë¡œì§)
            search_metadata = {
                "total_results": len(merged_docs),
                "filtered_results": len(filtered_docs),
                "final_results": len(final_docs),
                "quality_score": overall_quality,
                "semantic_count": semantic_count,
                "keyword_count": keyword_count,
                "retry_performed": needs_retry,
                "timestamp": time.time()
            }
            self._set_state_value(state, "search_metadata", search_metadata)

            # 6. State ì €ì¥
            self._set_state_value(state, "retrieved_docs", final_docs)
            self._set_state_value(state, "merged_documents", final_docs)

            processing_time = self._update_processing_time(state, start_time)
            self._add_step(
                state,
                "ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬",
                f"ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ: {len(final_docs)}ê°œ ë¬¸ì„œ (í’ˆì§ˆ ì ìˆ˜: {overall_quality:.2f}, ì‹œê°„: {processing_time:.3f}s)"
            )

            self.logger.info(
                f"âœ… ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ: {len(final_docs)}ê°œ ë¬¸ì„œ "
                f"(í’ˆì§ˆ: {overall_quality:.2f}, ì¬ê²€ìƒ‰: {needs_retry}, ì‹œê°„: {processing_time:.3f}s)"
            )

        except Exception as e:
            self._handle_error(state, str(e), "ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # í´ë°±: ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ë¼ë„ ì‚¬ìš©
            self._set_state_value(state, "retrieved_docs", [])
            self._set_state_value(state, "merged_documents", [])

        return state

    @observe(name="execute_searches_parallel")
    @with_state_optimization("execute_searches_parallel", enable_reduction=True)
    def execute_searches_parallel(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
        try:
            from concurrent.futures import ThreadPoolExecutor

            start_time = time.time()

            optimized_queries = self._get_state_value(state, "optimized_queries", {})
            search_params = self._get_state_value(state, "search_params", {})
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            legal_field = self._get_state_value(state, "legal_field", "")

            # ë””ë²„ê¹…: stateì—ì„œ ì§ì ‘ í™•ì¸
            from .state_helpers import get_field
            optimized_queries_raw = get_field(state, "optimized_queries")
            search_params_raw = get_field(state, "search_params")

            print(f"[DEBUG] execute_searches_parallel: START")
            print(f"[DEBUG]   - optimized_queries (via _get_state_value): {type(optimized_queries).__name__}, exists={bool(optimized_queries)}")
            print(f"[DEBUG]   - optimized_queries (via get_field): {type(optimized_queries_raw).__name__}, is None={optimized_queries_raw is None}")
            print(f"[DEBUG]   - search_params (via _get_state_value): {type(search_params).__name__}, exists={bool(search_params)}")
            print(f"[DEBUG]   - search_params (via get_field): {type(search_params_raw).__name__}, is None={search_params_raw is None}")

            # state êµ¬ì¡° í™•ì¸
            if "search" in state:
                print(f"[DEBUG]   - state has 'search' key: {type(state['search']).__name__}")
                if isinstance(state.get("search"), dict):
                    print(f"[DEBUG]   - search keys: {list(state['search'].keys())}")

            # state["search"]ì—ì„œ ì§ì ‘ ì½ê¸° (get_fieldê°€ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš° ëŒ€ë¹„)
            if "search" in state and isinstance(state["search"], dict):
                direct_optimized = state["search"].get("optimized_queries")
                if direct_optimized and isinstance(direct_optimized, dict) and len(direct_optimized) > 0:
                    optimized_queries = direct_optimized
                    extracted_keywords = optimized_queries.get("expanded_keywords", [])
                    print(f"[DEBUG]   - Using direct state['search']['optimized_queries'], keys: {list(optimized_queries.keys())}")
                    print(f"[DEBUG]   - semantic_query: '{optimized_queries.get('semantic_query', 'N/A')[:50]}...'")
                    print(f"[DEBUG]   - keyword_queries: {len(optimized_queries.get('keyword_queries', []))} queries")
                elif optimized_queries_raw is not None and len(optimized_queries_raw) > 0:
                    optimized_queries = optimized_queries_raw
                    extracted_keywords = optimized_queries.get("expanded_keywords", [])
                    print(f"[DEBUG]   - Using optimized_queries_raw (direct was empty), keys: {list(optimized_queries.keys())}")
                else:
                    optimized_queries = {}
                    extracted_keywords = []
                    print(f"[DEBUG]   - Both direct and raw are empty/None")
            elif optimized_queries_raw is not None and len(optimized_queries_raw) > 0:
                optimized_queries = optimized_queries_raw
                extracted_keywords = optimized_queries.get("expanded_keywords", [])
                print(f"[DEBUG]   - Using optimized_queries_raw (no direct access), keys: {list(optimized_queries.keys())}")
            else:
                optimized_queries = {}
                extracted_keywords = []
                print(f"[DEBUG]   - optimized_queries not found anywhere")

            # search_params_rawë„ í™•ì¸
            if search_params_raw is not None and len(search_params_raw) > 0:
                search_params = search_params_raw
                print(f"[DEBUG]   - Using search_params_raw, keys: {list(search_params.keys())}")
            elif search_params_raw is not None:
                # ë¹ˆ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° - ì§ì ‘ state["search"]ì—ì„œ í™•ì¸
                print(f"[DEBUG]   - search_params_raw is empty dict, checking state['search'] directly")
                if "search" in state and isinstance(state["search"], dict):
                    direct_params = state["search"].get("search_params")
                    if direct_params and len(direct_params) > 0:
                        search_params = direct_params
                        print(f"[DEBUG]   - Found in state['search'], keys: {list(search_params.keys())}")
                    else:
                        search_params = {}
                else:
                    search_params = {}
            else:
                search_params = {}

            # ê²€ì¦: optimized_queriesì™€ search_paramsê°€ Noneì´ ì•„ë‹ˆê³ , í•„ìˆ˜ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
            semantic_query_value = optimized_queries.get("semantic_query", "") if optimized_queries else ""

            # semantic_queryê°€ ë¹ˆ ë¬¸ìì—´ì´ë©´ ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
            if not semantic_query_value or not str(semantic_query_value).strip():
                query = self._get_state_value(state, "query", "")
                if query:
                    self.logger.warning(f"semantic_query is empty in execute_searches_parallel, using base query: '{query[:50]}...'")
                    optimized_queries["semantic_query"] = query
                    semantic_query_value = query

            has_semantic_query = optimized_queries and semantic_query_value and len(str(semantic_query_value).strip()) > 0
            keyword_queries_value = optimized_queries.get("keyword_queries", []) if optimized_queries else []

            # keyword_queriesê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
            if not keyword_queries_value or len(keyword_queries_value) == 0:
                query = self._get_state_value(state, "query", "")
                if query:
                    self.logger.warning(f"keyword_queries is empty in execute_searches_parallel, using base query")
                    optimized_queries["keyword_queries"] = [query]
                    keyword_queries_value = [query]

            has_keyword_queries = optimized_queries and keyword_queries_value and len(keyword_queries_value) > 0

            print(f"[DEBUG]   - Validation: semantic_query='{semantic_query_value[:50] if semantic_query_value else 'EMPTY'}...', has_semantic_query={has_semantic_query}")
            print(f"[DEBUG]   - Validation: keyword_queries={len(keyword_queries_value) if keyword_queries_value else 0}, has_keyword_queries={has_keyword_queries}")
            print(f"[DEBUG]   - Validation: search_params is None={search_params is None}, is empty={search_params == {}}, keys={list(search_params.keys()) if search_params else []}")

            if optimized_queries_raw is None or search_params_raw is None or not has_semantic_query:
                self.logger.warning("Optimized queries or search params not found")
                print(f"[DEBUG] PARALLEL SEARCH SKIP: optimized_queries={optimized_queries is not None}, search_params={search_params is not None}")
                self._set_state_value(state, "semantic_results", [])
                self._set_state_value(state, "keyword_results", [])
                self._set_state_value(state, "semantic_count", 0)
                self._set_state_value(state, "keyword_count", 0)
                return state

            semantic_results = []
            semantic_count = 0
            keyword_results = []
            keyword_count = 0

            # ì›ë³¸ query ê°€ì ¸ì˜¤ê¸° (ì¶”ê°€ ê²€ìƒ‰ìš©)
            original_query = self._get_state_value(state, "query", "")
            if not original_query:
                # input ê·¸ë£¹ì—ì„œë„ í™•ì¸
                if "input" in state and isinstance(state.get("input"), dict):
                    original_query = state["input"].get("query", "")

            print(f"[DEBUG] PARALLEL SEARCH START: semantic_query={optimized_queries.get('semantic_query', 'N/A')[:50]}, keyword_queries={len(optimized_queries.get('keyword_queries', []))}, original_query={original_query[:50] if original_query else 'N/A'}...")

            with ThreadPoolExecutor(max_workers=2) as executor:
                # ì˜ë¯¸ì  ê²€ìƒ‰ ì‘ì—… ì œì¶œ (ì›ë³¸ query í¬í•¨)
                semantic_future = executor.submit(
                    self._execute_semantic_search_internal,
                    optimized_queries,
                    search_params,
                    original_query  # ì›ë³¸ query ì¶”ê°€
                )

                # í‚¤ì›Œë“œ ê²€ìƒ‰ ì‘ì—… ì œì¶œ (ì›ë³¸ query í¬í•¨)
                keyword_future = executor.submit(
                    self._execute_keyword_search_internal,
                    optimized_queries,
                    search_params,
                    query_type_str,
                    legal_field,
                    extracted_keywords,
                    original_query  # ì›ë³¸ query ì¶”ê°€
                )

                # ë‘ ì‘ì—…ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                try:
                    semantic_results, semantic_count = semantic_future.result(timeout=30)
                    print(f"[DEBUG] Semantic future completed: {semantic_count} results")
                except Exception as e:
                    self.logger.error(f"Semantic search failed: {e}")
                    print(f"[DEBUG] Semantic search exception: {e}")
                    semantic_results, semantic_count = [], 0

                try:
                    keyword_results, keyword_count = keyword_future.result(timeout=30)
                    print(f"[DEBUG] Keyword future completed: {keyword_count} results")
                except Exception as e:
                    self.logger.error(f"Keyword search failed: {e}")
                    print(f"[DEBUG] Keyword search exception: {e}")
                    keyword_results, keyword_count = [], 0

            # ê²°ê³¼ ì €ì¥
            # ì¤‘ìš”: search ê·¸ë£¹ì´ í™•ì‹¤íˆ ì¡´ì¬í•˜ë„ë¡ ensure_state_group í˜¸ì¶œ
            from .state_helpers import ensure_state_group
            ensure_state_group(state, "search")

            print(f"[DEBUG] PARALLEL SEARCH: Before save - semantic_results={len(semantic_results)}, keyword_results={len(keyword_results)}")

            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "semantic_count", semantic_count)
            self._set_state_value(state, "keyword_count", keyword_count)

            # ì €ì¥ í™•ì¸ ë¡œê·¸
            stored_semantic = self._get_state_value(state, "semantic_results", [])
            stored_keyword = self._get_state_value(state, "keyword_results", [])
            print(f"[DEBUG] PARALLEL SEARCH: After save - semantic_results={len(stored_semantic)}, keyword_results={len(stored_keyword)}")

            # state["search"]ì—ì„œ ì§ì ‘ í™•ì¸ (ë””ë²„ê¹…)
            if "search" in state and isinstance(state.get("search"), dict):
                direct_semantic = state["search"].get("semantic_results", [])
                direct_keyword = state["search"].get("keyword_results", [])
                print(f"[DEBUG] PARALLEL SEARCH: Direct state['search'] check - semantic={len(direct_semantic)}, keyword={len(direct_keyword)}")
            else:
                print(f"[DEBUG] PARALLEL SEARCH: state['search'] not found or not dict, state keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")

            self._save_metadata_safely(state, "_last_executed_node", "execute_searches_parallel")
            self._update_processing_time(state, start_time)

            elapsed_time = time.time() - start_time

            # ìƒì„¸ ë””ë²„ê¹… ë¡œê·¸
            self.logger.info(
                f"âœ… [PARALLEL SEARCH] Completed in {elapsed_time:.3f}s - "
                f"Semantic: {semantic_count} results, Keyword: {keyword_count} results"
            )
            print(f"[DEBUG] PARALLEL SEARCH: Semantic={semantic_count}, Keyword={keyword_count}")

            # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì •ë³´ ë¡œê¹…
            if semantic_results:
                semantic_scores = [doc.get("relevance_score", 0.0) for doc in semantic_results[:5]]
                self.logger.info(
                    f"ğŸ” [DEBUG] Semantic search details: "
                    f"Top scores: {semantic_scores}, "
                    f"Sample sources: {[doc.get('source', 'Unknown')[:30] for doc in semantic_results[:3]]}"
                )
            else:
                self.logger.warning("âš ï¸ [DEBUG] Semantic search returned 0 results")

            if keyword_results:
                keyword_scores = [doc.get("relevance_score", doc.get("score", 0.0)) for doc in keyword_results[:5]]
                self.logger.info(
                    f"ğŸ” [DEBUG] Keyword search details: "
                    f"Top scores: {keyword_scores}, "
                    f"Sample sources: {[doc.get('source', 'Unknown')[:30] for doc in keyword_results[:3]]}"
                )
            else:
                self.logger.warning("âš ï¸ [DEBUG] Keyword search returned 0 results")

        except Exception as e:
            self._handle_error(state, str(e), "ë³‘ë ¬ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # í´ë°±: ìˆœì°¨ ì‹¤í–‰
            return self._fallback_sequential_search(state)

        # ë°˜í™˜ ì „ì— search ê·¸ë£¹ í™•ì¸ ë° ë¡œê¹…
        if "search" in state and isinstance(state.get("search"), dict):
            final_search = state["search"]
            final_semantic = len(final_search.get("semantic_results", []))
            final_keyword = len(final_search.get("keyword_results", []))
            print(f"[DEBUG] execute_searches_parallel: Returning state with search group - semantic_results={final_semantic}, keyword_results={final_keyword}")
            print(f"[DEBUG] execute_searches_parallel: Returning state keys={list(state.keys()) if isinstance(state, dict) else 'N/A'}")
        else:
            print(f"[DEBUG] execute_searches_parallel: WARNING - Returning state WITHOUT search group!")
            print(f"[DEBUG] execute_searches_parallel: Returning state keys={list(state.keys()) if isinstance(state, dict) else 'N/A'}")

        return state

    def _execute_semantic_search_internal(
        self,
        optimized_queries: Dict[str, Any],
        search_params: Dict[str, Any],
        original_query: str = ""
    ) -> Tuple[List[Dict[str, Any]], int]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤í–‰ (ë‚´ë¶€ í—¬í¼)"""
        semantic_results = []
        semantic_count = 0

        semantic_query = optimized_queries.get("semantic_query", "")
        semantic_k = search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K)

        self.logger.info(
            f"ğŸ” [DEBUG] Executing semantic search: query='{semantic_query[:50]}...', k={semantic_k}, original_query='{original_query[:50] if original_query else 'N/A'}...'"
        )

        # ë©”ì¸ ì¿¼ë¦¬ë¡œ ì˜ë¯¸ì  ê²€ìƒ‰
        main_semantic, main_count = self._semantic_search(
            semantic_query,
            k=semantic_k
        )
        semantic_results.extend(main_semantic)
        semantic_count += main_count

        self.logger.info(
            f"ğŸ” [DEBUG] Main semantic search: {main_count} results (query: '{semantic_query[:50]}...')"
        )

        # ì›ë³¸ queryë¡œë„ ì˜ë¯¸ì  ê²€ìƒ‰ ìˆ˜í–‰ (í•­ìƒ í¬í•¨)
        # ì¤‘ìš”: ì›ë³¸ queryëŠ” ì‚¬ìš©ìì˜ ì§ì ‘ì ì¸ ì˜ë„ì´ë¯€ë¡œ semantic_queryì™€ ê°™ì•„ë„ ë³„ë„ë¡œ ê²€ìƒ‰
        if original_query and original_query.strip():
            original_semantic, original_count = self._semantic_search(
                original_query,
                k=semantic_k // 2
            )
            semantic_results.extend(original_semantic)
            semantic_count += original_count
            self.logger.info(
                f"ğŸ” [DEBUG] Original query semantic search: {original_count} results (query: '{original_query[:50]}...')"
            )
            print(f"[DEBUG] _execute_semantic_search_internal: Added {original_count} results from original query search")

        # í‚¤ì›Œë“œ ì¿¼ë¦¬ë¡œ ì¶”ê°€ ì˜ë¯¸ì  ê²€ìƒ‰
        keyword_queries = optimized_queries.get("keyword_queries", [])[:2]
        for i, kw_query in enumerate(keyword_queries, 1):
            # semantic_queryì™€ëŠ” ë‹¤ë¥´ì§€ë§Œ, original_queryì™€ëŠ” ì¤‘ë³µ í—ˆìš© ê°€ëŠ¥
            # (í‚¤ì›Œë“œ ì¿¼ë¦¬ê°€ ì›ë³¸ queryë¥¼ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
            if kw_query and kw_query.strip() and kw_query != semantic_query:
                kw_semantic, kw_count = self._semantic_search(
                    kw_query,
                    k=semantic_k // 2
                )
                semantic_results.extend(kw_semantic)
                semantic_count += kw_count
                self.logger.info(
                    f"ğŸ” [DEBUG] Keyword-based semantic search #{i}: {kw_count} results (query: '{kw_query[:50]}...')"
                )
                print(f"[DEBUG] _execute_semantic_search_internal: Added {kw_count} results from keyword query #{i}")

        self.logger.info(
            f"ğŸ” [DEBUG] Total semantic search results: {semantic_count} (unique: {len(semantic_results)})"
        )
        print(f"[DEBUG] SEMANTIC SEARCH INTERNAL: Total={semantic_count}, Unique={len(semantic_results)}")

        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± ìš”ì•½ ë¡œê·¸
        search_queries_used = []
        if semantic_query:
            search_queries_used.append(f"semantic_query({len(semantic_query)} chars)")
        if original_query:
            search_queries_used.append(f"original_query({len(original_query)} chars)")
        keyword_queries_used = optimized_queries.get("keyword_queries", [])[:2]
        if keyword_queries_used:
            search_queries_used.append(f"keyword_queries({len(keyword_queries_used)} queries)")
        print(f"[DEBUG] SEMANTIC SEARCH INTERNAL: Queries used: {', '.join(search_queries_used)}")

        return semantic_results, semantic_count

    def _execute_keyword_search_internal(
        self,
        optimized_queries: Dict[str, Any],
        search_params: Dict[str, Any],
        query_type_str: str,
        legal_field: str,
        extracted_keywords: List[str],
        original_query: str = ""
    ) -> Tuple[List[Dict[str, Any]], int]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰ (ë‚´ë¶€ í—¬í¼)"""
        keyword_results = []
        keyword_count = 0

        keyword_queries = optimized_queries.get("keyword_queries", [])
        keyword_limit = search_params.get("keyword_limit", WorkflowConstants.CATEGORY_SEARCH_LIMIT)

        self.logger.info(
            f"ğŸ” [DEBUG] Executing keyword search: {len(keyword_queries)} queries, "
            f"limit={keyword_limit}, field={legal_field}, "
            f"keywords={extracted_keywords[:5] if extracted_keywords else []}, "
            f"original_query='{original_query[:50] if original_query else 'N/A'}...'"
        )

        # ì›ë³¸ queryë¡œë„ í‚¤ì›Œë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°)
        if original_query and original_query.strip():
            original_kw_results, original_kw_count = self._keyword_search(
                query=original_query,
                query_type_str=query_type_str,
                limit=keyword_limit,
                legal_field=legal_field,
                extracted_keywords=extracted_keywords
            )
            keyword_results.extend(original_kw_results)
            keyword_count += original_kw_count
            self.logger.info(
                f"ğŸ” [DEBUG] Original query keyword search: {original_kw_count} results (query: '{original_query[:50]}...')"
            )

        # ìµœì í™”ëœ í‚¤ì›Œë“œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        for i, kw_query in enumerate(keyword_queries, 1):
            # ì›ë³¸ queryì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ê²€ìƒ‰
            if kw_query and kw_query.strip() and kw_query != original_query:
                kw_results, kw_count = self._keyword_search(
                    query=kw_query,
                    query_type_str=query_type_str,
                    limit=keyword_limit,
                    legal_field=legal_field,
                    extracted_keywords=extracted_keywords
                )
                keyword_results.extend(kw_results)
                keyword_count += kw_count
                self.logger.info(
                    f"ğŸ” [DEBUG] Keyword search #{i}: {kw_count} results (query: '{kw_query[:50]}...')"
                )

        self.logger.info(
            f"ğŸ” [DEBUG] Total keyword search results: {keyword_count} (unique: {len(keyword_results)})"
        )
        print(f"[DEBUG] KEYWORD SEARCH INTERNAL: Total={keyword_count}, Unique={len(keyword_results)}")

        return keyword_results, keyword_count

    def _fallback_sequential_search(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ìˆœì°¨ ê²€ìƒ‰ ì‹¤í–‰ (í´ë°±)"""
        try:
            self.logger.warning("Falling back to sequential search")

            optimized_queries = self._get_state_value(state, "optimized_queries", {})
            search_params = self._get_state_value(state, "search_params", {})
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            legal_field = self._get_state_value(state, "legal_field", "")
            extracted_keywords = optimized_queries.get("expanded_keywords", [])

            # ì›ë³¸ query ê°€ì ¸ì˜¤ê¸°
            original_query = self._get_state_value(state, "query", "")
            if not original_query and "input" in state and isinstance(state.get("input"), dict):
                original_query = state["input"].get("query", "")

            # ì˜ë¯¸ì  ê²€ìƒ‰ (ìˆœì°¨)
            semantic_results, semantic_count = self._execute_semantic_search_internal(
                optimized_queries, search_params, original_query
            )

            # í‚¤ì›Œë“œ ê²€ìƒ‰ (ìˆœì°¨)
            keyword_results, keyword_count = self._execute_keyword_search_internal(
                optimized_queries, search_params, query_type_str, legal_field, extracted_keywords, original_query
            )

            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "semantic_count", semantic_count)
            self._set_state_value(state, "keyword_count", keyword_count)

            self.logger.info(f"Sequential search completed: {semantic_count} semantic, {keyword_count} keyword")

        except Exception as e:
            self._handle_error(state, str(e), "ìˆœì°¨ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

        return state

    @observe(name="evaluate_search_quality")
    @with_state_optimization("evaluate_search_quality", enable_reduction=True)
    def evaluate_search_quality(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ê° ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆ í‰ê°€"""
        try:
            start_time = time.time()

            semantic_results = self._get_state_value(state, "semantic_results", [])
            keyword_results = self._get_state_value(state, "keyword_results", [])
            query = self._get_state_value(state, "query", "")
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            search_params = self._get_state_value(state, "search_params", {})

            # ì˜ë¯¸ì  ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€
            semantic_quality = self._evaluate_semantic_search_quality(
                semantic_results=semantic_results,
                query=query,
                query_type=query_type_str,
                min_results=search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K) // 2
            )

            # í‚¤ì›Œë“œ ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€
            keyword_quality = self._evaluate_keyword_search_quality(
                keyword_results=keyword_results,
                query=query,
                query_type=query_type_str,
                min_results=search_params.get("keyword_limit", WorkflowConstants.CATEGORY_SEARCH_LIMIT) // 2
            )

            # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ì €ì¥
            quality_evaluation = {
                "semantic_quality": semantic_quality,
                "keyword_quality": keyword_quality,
                "overall_quality": (semantic_quality["score"] + keyword_quality["score"]) / 2.0,
                "needs_retry": semantic_quality["needs_retry"] or keyword_quality["needs_retry"]
            }

            self._set_state_value(state, "search_quality_evaluation", quality_evaluation)
            self._save_metadata_safely(state, "_last_executed_node", "evaluate_search_quality")
            self._update_processing_time(state, start_time)

            self.logger.info(
                f"ğŸ“Š [SEARCH QUALITY] Semantic: {semantic_quality['score']:.2f} "
                f"(needs_retry: {semantic_quality['needs_retry']}), "
                f"Keyword: {keyword_quality['score']:.2f} (needs_retry: {keyword_quality['needs_retry']})"
            )

        except Exception as e:
            self._handle_error(state, str(e), "ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # í´ë°±: ê¸°ë³¸ í’ˆì§ˆ í‰ê°€
            quality_evaluation = {
                "semantic_quality": {"score": 0.5, "needs_retry": False},
                "keyword_quality": {"score": 0.5, "needs_retry": False},
                "overall_quality": 0.5,
                "needs_retry": False
            }
            self._set_state_value(state, "search_quality_evaluation", quality_evaluation)

        return state

    def _evaluate_semantic_search_quality(
        self,
        semantic_results: List[Dict[str, Any]],
        query: str,
        query_type: str,
        min_results: int = 5
    ) -> Dict[str, Any]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€"""
        quality = {
            "score": 0.0,
            "result_count": len(semantic_results),
            "avg_relevance": 0.0,
            "diversity_ratio": 0.0,
            "query_match": 0.0,
            "needs_retry": False,
            "issues": []
        }

        if not semantic_results:
            quality["needs_retry"] = True
            quality["issues"].append("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŒ")
            return quality

        # ê²°ê³¼ ìˆ˜ í‰ê°€
        result_count_score = min(1.0, len(semantic_results) / max(1, min_results))
        quality["result_count"] = len(semantic_results)

        # í‰ê·  ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        relevance_scores = [
            doc.get("relevance_score", doc.get("score", 0.0))
            for doc in semantic_results
            if doc.get("relevance_score") or doc.get("score")
        ]
        avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.0
        quality["avg_relevance"] = avg_relevance

        # ë‹¤ì–‘ì„± í‰ê°€ (ì¤‘ë³µ ì œê±° ë¹„ìœ¨)
        seen_contents = set()
        unique_count = 0
        for doc in semantic_results:
            content_preview = doc.get("content", "")[:100]
            if content_preview and content_preview not in seen_contents:
                seen_contents.add(content_preview)
                unique_count += 1

        diversity_ratio = unique_count / len(semantic_results) if semantic_results else 0.0
        quality["diversity_ratio"] = diversity_ratio

        # ì¿¼ë¦¬ ì¼ì¹˜ë„ í‰ê°€
        query_words = set(query.lower().split())
        match_count = 0
        for doc in semantic_results:
            doc_content = doc.get("content", "").lower()
            doc_words = set(doc_content.split())
            if query_words and doc_words:
                overlap = len(query_words.intersection(doc_words))
                if overlap > 0:
                    match_count += 1

        query_match = match_count / len(semantic_results) if semantic_results else 0.0
        quality["query_match"] = query_match

        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = (
            result_count_score * 0.25 +
            avg_relevance * 0.30 +
            diversity_ratio * 0.20 +
            query_match * 0.25
        )
        quality["score"] = quality_score

        # ì¬ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨
        needs_retry = (
            result_count_score < 0.5 or
            avg_relevance < 0.6 or
            query_match < 0.3
        )
        quality["needs_retry"] = needs_retry

        if needs_retry:
            if result_count_score < 0.5:
                quality["issues"].append(f"ê²°ê³¼ ìˆ˜ ë¶€ì¡±: {len(semantic_results)}ê°œ")
            if avg_relevance < 0.6:
                quality["issues"].append(f"í‰ê·  ê´€ë ¨ì„± ì ìˆ˜ ë‚®ìŒ: {avg_relevance:.2f}")
            if query_match < 0.3:
                quality["issues"].append(f"ì¿¼ë¦¬ ì¼ì¹˜ë„ ë‚®ìŒ: {query_match:.2f}")

        return quality

    def _evaluate_keyword_search_quality(
        self,
        keyword_results: List[Dict[str, Any]],
        query: str,
        query_type: str,
        min_results: int = 3
    ) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€"""
        quality = {
            "score": 0.0,
            "result_count": len(keyword_results),
            "avg_relevance": 0.0,
            "category_match": 0.0,
            "legal_citation_ratio": 0.0,
            "needs_retry": False,
            "issues": []
        }

        if not keyword_results:
            quality["needs_retry"] = True
            quality["issues"].append("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŒ")
            return quality

        # ê²°ê³¼ ìˆ˜ í‰ê°€
        result_count_score = min(1.0, len(keyword_results) / max(1, min_results))
        quality["result_count"] = len(keyword_results)

        # í‰ê·  ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        relevance_scores = [
            doc.get("relevance_score", doc.get("score", 0.0))
            for doc in keyword_results
            if doc.get("relevance_score") or doc.get("score")
        ]
        avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.0
        quality["avg_relevance"] = avg_relevance

        # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ë„ í‰ê°€
        category_match_count = sum(1 for doc in keyword_results if doc.get("category_boost", 1.0) > 1.0)
        category_match = category_match_count / len(keyword_results) if keyword_results else 0.0
        quality["category_match"] = category_match

        # ë²•ë¥  ì¡°í•­ í¬í•¨ë„ í‰ê°€
        legal_citation_count = 0
        for doc in keyword_results:
            content = doc.get("content", "")
            # ë²•ë¥  ì¡°í•­ íŒ¨í„´ í™•ì¸
            if re.search(r'[ê°€-í£]+ë²•\s*ì œ?\s*\d+\s*ì¡°', content):
                legal_citation_count += 1

        legal_citation_ratio = legal_citation_count / len(keyword_results) if keyword_results else 0.0
        quality["legal_citation_ratio"] = legal_citation_ratio

        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = (
            result_count_score * 0.25 +
            avg_relevance * 0.25 +
            category_match * 0.25 +
            legal_citation_ratio * 0.25
        )
        quality["score"] = quality_score

        # ì¬ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨
        needs_retry = (
            result_count_score < 0.5 or
            avg_relevance < 0.6 or
            legal_citation_ratio < 0.2
        )
        quality["needs_retry"] = needs_retry

        if needs_retry:
            if result_count_score < 0.5:
                quality["issues"].append(f"ê²°ê³¼ ìˆ˜ ë¶€ì¡±: {len(keyword_results)}ê°œ")
            if avg_relevance < 0.6:
                quality["issues"].append(f"í‰ê·  ê´€ë ¨ì„± ì ìˆ˜ ë‚®ìŒ: {avg_relevance:.2f}")
            if legal_citation_ratio < 0.2:
                quality["issues"].append(f"ë²•ë¥  ì¡°í•­ í¬í•¨ë„ ë‚®ìŒ: {legal_citation_ratio:.2f}")

        return quality

    @observe(name="conditional_retry_search")
    @with_state_optimization("conditional_retry_search", enable_reduction=True)
    def conditional_retry_search(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """í’ˆì§ˆì´ ë‚®ì€ ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ì¡°ê±´ë¶€ ì¬ê²€ìƒ‰"""
        try:
            start_time = time.time()

            quality_evaluation = self._get_state_value(state, "search_quality_evaluation", {})
            semantic_results = self._get_state_value(state, "semantic_results", [])
            keyword_results = self._get_state_value(state, "keyword_results", [])
            optimized_queries = self._get_state_value(state, "optimized_queries", {})
            search_params = self._get_state_value(state, "search_params", {})

            semantic_quality = quality_evaluation.get("semantic_quality", {})
            keyword_quality = quality_evaluation.get("keyword_quality", {})

            semantic_needs_retry = semantic_quality.get("needs_retry", False)
            keyword_needs_retry = keyword_quality.get("needs_retry", False)

            # ì¬ê²€ìƒ‰ ì¹´ìš´í„° í™•ì¸ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            retry_metadata = self._get_state_value(state, "search_retry_metadata", {})
            semantic_retry_count = retry_metadata.get("semantic_retry_count", 0)
            keyword_retry_count = retry_metadata.get("keyword_retry_count", 0)

            max_retry_per_type = 1  # ê° ê²€ìƒ‰ íƒ€ì…ë‹¹ ìµœëŒ€ 1íšŒ ì¬ê²€ìƒ‰

            # ì˜ë¯¸ì  ê²€ìƒ‰ ì¬ê²€ìƒ‰
            if semantic_needs_retry and semantic_retry_count < max_retry_per_type:
                self.logger.info(f"ğŸ”„ [RETRY SEMANTIC] Retrying semantic search (count: {semantic_retry_count})")

                # ì¿¼ë¦¬ ê°œì„ 
                improved_semantic_query = self._improve_search_query_for_retry(
                    optimized_queries.get("semantic_query", ""),
                    {"failed_checks": semantic_quality.get("issues", [])},
                    state
                )

                # ì˜ë¯¸ì  ê²€ìƒ‰ ì¬ì‹¤í–‰
                retry_semantic, retry_count = self._semantic_search(
                    improved_semantic_query,
                    k=search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K) + 5  # ë” ë§ì€ ê²°ê³¼
                )

                semantic_results = retry_semantic
                semantic_retry_count += 1
                retry_metadata["semantic_retry_count"] = semantic_retry_count

            # í‚¤ì›Œë“œ ê²€ìƒ‰ ì¬ê²€ìƒ‰
            if keyword_needs_retry and keyword_retry_count < max_retry_per_type:
                self.logger.info(f"ğŸ”„ [RETRY KEYWORD] Retrying keyword search (count: {keyword_retry_count})")

                query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
                legal_field = self._get_state_value(state, "legal_field", "")
                extracted_keywords = optimized_queries.get("expanded_keywords", [])

                # í‚¤ì›Œë“œ ê²€ìƒ‰ ì¬ì‹¤í–‰
                retry_keyword, retry_count = self._keyword_search(
                    query=optimized_queries.get("keyword_queries", [""])[0],
                    query_type_str=query_type_str,
                    limit=search_params.get("keyword_limit", WorkflowConstants.CATEGORY_SEARCH_LIMIT) + 3,
                    legal_field=legal_field,
                    extracted_keywords=extracted_keywords
                )

                keyword_results = retry_keyword
                keyword_retry_count += 1
                retry_metadata["keyword_retry_count"] = keyword_retry_count

            # ì¬ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "search_retry_metadata", retry_metadata)

            self._save_metadata_safely(state, "_last_executed_node", "conditional_retry_search")
            self._update_processing_time(state, start_time)

            self.logger.info(
                f"âœ… [CONDITIONAL RETRY] Semantic retry: {semantic_retry_count}, "
                f"Keyword retry: {keyword_retry_count}"
            )

        except Exception as e:
            self._handle_error(state, str(e), "ì¡°ê±´ë¶€ ì¬ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

        return state

    @observe(name="merge_and_rerank_with_keyword_weights")
    @with_state_optimization("merge_and_rerank_with_keyword_weights", enable_reduction=True)
    def merge_and_rerank_with_keyword_weights(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """í‚¤ì›Œë“œë³„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ê²°ê³¼ ë³‘í•© ë° Reranking"""
        try:
            start_time = time.time()

            print(f"[DEBUG] MERGE: START - merge_and_rerank_with_keyword_weights")

            semantic_results = self._get_state_value(state, "semantic_results", [])
            keyword_results = self._get_state_value(state, "keyword_results", [])
            optimized_queries = self._get_state_value(state, "optimized_queries", {})
            search_params = self._get_state_value(state, "search_params", {})
            query = self._get_state_value(state, "query", "")
            extracted_keywords = self._get_state_value(state, "extracted_keywords", [])
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            legal_field = self._get_state_value(state, "legal_field", "")

            print(f"[DEBUG] MERGE: Input - semantic_results={len(semantic_results)}, keyword_results={len(keyword_results)}")

            # stateì—ì„œ ì§ì ‘ í™•ì¸ (ë””ë²„ê¹…)
            # _get_state_valueê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì§ì ‘ state["search"]ì—ì„œ ì½ê¸°
            if len(semantic_results) == 0 and len(keyword_results) == 0:
                print(f"[DEBUG] MERGE: _get_state_value returned empty, checking state['search'] directly...")
                # state["search"]ì—ì„œ ì§ì ‘ ì½ê¸° ì‹œë„
                if "search" in state and isinstance(state.get("search"), dict):
                    direct_semantic = state["search"].get("semantic_results", [])
                    direct_keyword = state["search"].get("keyword_results", [])
                    print(f"[DEBUG] MERGE: Direct state['search'] check - semantic={len(direct_semantic)}, keyword={len(direct_keyword)}")
                    if direct_semantic or direct_keyword:
                        print(f"[DEBUG] MERGE: Found results in state['search'] - semantic={len(direct_semantic)}, keyword={len(direct_keyword)}")
                        semantic_results = direct_semantic
                        keyword_results = direct_keyword
                else:
                    print(f"[DEBUG] MERGE: state['search'] not found or not dict, state keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")

            # ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ state ì „ì²´ì—ì„œ ì°¾ê¸°
            if len(semantic_results) == 0 and len(keyword_results) == 0:
                print(f"[DEBUG] MERGE: Still empty, checking all state keys...")
                if isinstance(state, dict):
                    # flat êµ¬ì¡°ì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ í™•ì¸
                    if "semantic_results" in state:
                        flat_semantic = state.get("semantic_results", [])
                        if isinstance(flat_semantic, list) and len(flat_semantic) > 0:
                            print(f"[DEBUG] MERGE: Found semantic_results in flat state: {len(flat_semantic)}")
                            semantic_results = flat_semantic
                    if "keyword_results" in state:
                        flat_keyword = state.get("keyword_results", [])
                        if isinstance(flat_keyword, list) and len(flat_keyword) > 0:
                            print(f"[DEBUG] MERGE: Found keyword_results in flat state: {len(flat_keyword)}")
                            keyword_results = flat_keyword

            # í‚¤ì›Œë“œ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°
            keyword_weights = self._calculate_keyword_weights(
                extracted_keywords=extracted_keywords,
                query=query,
                query_type=query_type_str,
                legal_field=legal_field
            )

            # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ ë° ê²€ìƒ‰ íƒ€ì… ì •ë³´ ë³´ì¡´
            all_results = []

            # ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ì— ê²€ìƒ‰ íƒ€ì… ì •ë³´ ì¶”ê°€
            for doc in semantic_results:
                if not doc.get("search_type"):
                    doc["search_type"] = "semantic"
                    doc["search_method"] = "vector_search"
                all_results.append(doc)

            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì— ê²€ìƒ‰ íƒ€ì… ì •ë³´ ì¶”ê°€
            for doc in keyword_results:
                if not doc.get("search_type"):
                    doc["search_type"] = "keyword"
                    doc["search_method"] = "keyword_search"
                all_results.append(doc)

            # ì¤‘ë³µ ì œê±° (ê²€ìƒ‰ íƒ€ì… ì •ë³´ ë³´ì¡´)
            unique_results = self._remove_duplicate_results_for_merge(all_results)

            # ê° ë¬¸ì„œì— í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° ë° ê°€ì¤‘ì¹˜ ì ìš©
            weighted_results = []
            for doc in unique_results:
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                keyword_scores = self._calculate_keyword_match_score(
                    document=doc,
                    keyword_weights=keyword_weights,
                    query=query
                )

                # ê°€ì¤‘ì¹˜ ì ìš© ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°•í™”: query_type ì¶”ê°€)
                final_score = self._calculate_weighted_final_score(
                    document=doc,
                    keyword_scores=keyword_scores,
                    search_params=search_params,
                    query_type=query_type_str
                )

                # ë¬¸ì„œì— ì ìˆ˜ ì •ë³´ ì¶”ê°€
                doc["keyword_match_score"] = keyword_scores["keyword_match_score"]
                doc["keyword_coverage"] = keyword_scores["keyword_coverage"]
                doc["matched_keywords"] = keyword_scores["matched_keywords"]
                doc["weighted_keyword_score"] = keyword_scores["weighted_keyword_score"]
                doc["final_weighted_score"] = final_score

                # ê²€ìƒ‰ íƒ€ì… ì •ë³´ ìœ ì§€ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •)
                if not doc.get("search_type"):
                    doc["search_type"] = "hybrid"  # ì¤‘ë³µ ì œê±°ë¡œ ì¸í•œ ë³‘í•© ê²°ê³¼
                    doc["search_method"] = "hybrid_search"

                weighted_results.append(doc)

            # Reranking ìˆ˜í–‰
            reranked_results = self._rerank_with_keyword_weights(
                results=weighted_results,
                keyword_weights=keyword_weights,
                rerank_params=search_params.get("rerank", {})
            )

            # ê²€ìƒ‰ í’ˆì§ˆ ê²€ì¦ ì¶”ê°€ (4ë‹¨ê³„)
            quality_valid, quality_message = self._validate_search_quality(
                results=reranked_results,
                query=query,
                query_type=query_type_str
            )

            if not quality_valid:
                self.logger.warning(f"âš ï¸ [SEARCH QUALITY] Validation failed: {quality_message}")
                print(f"[DEBUG] MERGE: Search quality validation failed - {quality_message}")
                # í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìƒìœ„ ì ìˆ˜ ë¬¸ì„œë§Œ ìœ ì§€ (ìµœì†Œ 5ê°œ)
                if reranked_results:
                    min_score = 0.4  # ìµœì†Œ ì ìˆ˜ ê¸°ì¤€ ì™„í™” (0.5 â†’ 0.4)
                    filtered_reranked = [
                        doc for doc in reranked_results
                        if doc.get("final_weighted_score", doc.get("relevance_score", 0.0)) >= min_score
                    ]
                    if len(filtered_reranked) >= 5:  # ìµœì†Œ 5ê°œ ë³´ì¥
                        reranked_results = filtered_reranked[:10]  # ìƒìœ„ 10ê°œë§Œ
                        self.logger.info(f"ğŸ”§ [SEARCH QUALITY] Filtered to {len(reranked_results)} high-quality documents")
                        print(f"[DEBUG] MERGE: Filtered to {len(reranked_results)} high-quality documents")
                    elif len(filtered_reranked) >= 3:
                        reranked_results = filtered_reranked  # ìµœì†Œ 3ê°œ ì´ìƒì´ë©´ ëª¨ë‘ ìœ ì§€
                        self.logger.warning(f"âš ï¸ [SEARCH QUALITY] Low quality results, keeping {len(reranked_results)} documents")
                        print(f"[DEBUG] MERGE: Low quality, keeping {len(reranked_results)} documents")
                    else:
                        # ìµœì†Œ 3ê°œ ë¯¸ë§Œì´ë©´ ìƒìœ„ 5ê°œë§Œ ìœ ì§€ (ì ìˆ˜ ìƒê´€ì—†ì´, 3ê°œ â†’ 5ê°œë¡œ ì¦ê°€)
                        reranked_results = reranked_results[:5]
                        self.logger.warning(f"âš ï¸ [SEARCH QUALITY] Very low quality results, keeping top 5 only")
                        print(f"[DEBUG] MERGE: Very low quality, keeping top 5 only")
            else:
                self.logger.info(f"âœ… [SEARCH QUALITY] Validation passed: {quality_message}")
                print(f"[DEBUG] MERGE: Search quality validation passed - {quality_message}")

            # ê²°ê³¼ ì €ì¥
            self._set_state_value(state, "merged_documents", reranked_results)
            self._set_state_value(state, "keyword_weights", keyword_weights)

            # ì¤‘ìš”: ë³‘í•©ëœ ê²°ê³¼ë¥¼ retrieved_docsì—ë„ ì €ì¥ (ë‹¤ìŒ ë…¸ë“œì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
            # ëª¨ë“  ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ ê²°ê³¼(semantic_query, original_query, keyword_queries)ê°€ í¬í•¨ë¨
            self._set_state_value(state, "retrieved_docs", reranked_results)
            print(f"[DEBUG] MERGE: Saved {len(reranked_results)} documents to retrieved_docs")

            # ì €ì¥ í™•ì¸
            stored_merged = self._get_state_value(state, "merged_documents", [])
            stored_retrieved = self._get_state_value(state, "retrieved_docs", [])
            print(f"[DEBUG] MERGE: After save - merged_documents={len(stored_merged)}, retrieved_docs={len(stored_retrieved)}")

            self._save_metadata_safely(state, "_last_executed_node", "merge_and_rerank_with_keyword_weights")
            self._update_processing_time(state, start_time)

            self.logger.info(
                f"âœ… [KEYWORD-WEIGHTED RERANKING] Merged {len(unique_results)} results, "
                f"reranked to {len(reranked_results)} with {len(keyword_weights)} weighted keywords"
            )
            print(f"[DEBUG] MERGE: Semantic input={len(semantic_results)}, Keyword input={len(keyword_results)}, Unique={len(unique_results)}, Reranked={len(reranked_results)}")

            # ë³‘í•© ê²°ê³¼ ìƒì„¸ ë””ë²„ê¹… ë¡œê·¸
            if reranked_results:
                top_scores = [doc.get("final_weighted_score", doc.get("relevance_score", 0.0)) for doc in reranked_results[:5]]
                search_type_counts = {
                    "semantic": sum(1 for doc in reranked_results if doc.get("search_type") == "semantic"),
                    "keyword": sum(1 for doc in reranked_results if doc.get("search_type") == "keyword"),
                    "hybrid": sum(1 for doc in reranked_results if doc.get("search_type") not in ["semantic", "keyword"])
                }
                self.logger.info(
                    f"ğŸ” [DEBUG] Merge & Rerank details: "
                    f"Total merged: {len(unique_results)}, "
                    f"After rerank: {len(reranked_results)}, "
                    f"Top scores: {top_scores}, "
                    f"Search types: {search_type_counts}"
                )
            else:
                self.logger.warning(
                    f"âš ï¸ [DEBUG] Merge & Rerank resulted in 0 documents. "
                    f"Input: semantic={len(semantic_results)}, keyword={len(keyword_results)}, "
                    f"unique={len(unique_results)}, weighted={len(weighted_results)}"
                )

        except Exception as e:
            self._handle_error(state, str(e), "í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë³‘í•© ë° Reranking ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # í´ë°±: ê°„ë‹¨í•œ ë³‘í•©
            semantic_results = self._get_state_value(state, "semantic_results", [])
            keyword_results = self._get_state_value(state, "keyword_results", [])
            all_results = semantic_results + keyword_results
            fallback_docs = all_results[:20]
            self._set_state_value(state, "merged_documents", fallback_docs)
            # í´ë°± ê²°ê³¼ë„ retrieved_docsì— ì €ì¥
            self._set_state_value(state, "retrieved_docs", fallback_docs)
            print(f"[DEBUG] MERGE: Fallback - Saved {len(fallback_docs)} documents to retrieved_docs")

        return state

    def _remove_duplicate_results_for_merge(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë³‘í•©ì„ ìœ„í•œ ì¤‘ë³µ ì œê±°"""
        seen_texts = set()
        unique_results = []

        for doc in results:
            doc_content = doc.get("content", "")
            if not doc_content:
                continue

            content_preview = doc_content[:100]
            content_hash = hash(content_preview)

            if content_hash not in seen_texts:
                seen_texts.add(content_hash)
                unique_results.append(doc)

        return unique_results

    def _calculate_keyword_weights(
        self,
        extracted_keywords: List[str],
        query: str,
        query_type: str,
        legal_field: str
    ) -> Dict[str, float]:
        """í‚¤ì›Œë“œë³„ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        keyword_weights = {}

        if not extracted_keywords:
            return keyword_weights

        query_lower = query.lower()

        # ë²•ë¥  ìš©ì–´ íŒ¨í„´
        legal_term_patterns = [
            r'[ê°€-í£]+ë²•', r'[ê°€-í£]+ê·œì •', r'[ê°€-í£]+ì¡°í•­',
            r'íŒë¡€', r'ëŒ€ë²•ì›', r'ë²•ì›', r'íŒê²°',
            r'ê³„ì•½', r'ì†í•´ë°°ìƒ', r'ì†Œì†¡', r'ì²­êµ¬'
        ]

        # ì§ˆë¬¸ ìœ í˜•ë³„ ì¤‘ìš” í‚¤ì›Œë“œ
        query_type_keywords = {
            "precedent_search": ["íŒë¡€", "ì‚¬ê±´", "íŒê²°", "ëŒ€ë²•ì›"],
            "law_inquiry": ["ë²•ë¥ ", "ì¡°ë¬¸", "ë²•ë ¹", "ê·œì •", "ì¡°í•­"],
            "legal_advice": ["ì¡°ì–¸", "í•´ì„", "ê¶Œë¦¬", "ì˜ë¬´", "ì±…ì„"],
            "procedure_guide": ["ì ˆì°¨", "ë°©ë²•", "ëŒ€ì‘", "ì†Œì†¡"],
            "term_explanation": ["ì˜ë¯¸", "ì •ì˜", "ê°œë…", "í•´ì„"]
        }

        # ë²•ë¥  ë¶„ì•¼ë³„ ê´€ë ¨ í‚¤ì›Œë“œ
        field_keywords = {
            "family": ["ê°€ì¡±", "ì´í˜¼", "ì–‘ìœ¡", "ìƒì†", "ë¶€ë¶€"],
            "civil": ["ë¯¼ì‚¬", "ê³„ì•½", "ì†í•´ë°°ìƒ", "ì±„ê¶Œ", "ì±„ë¬´"],
            "criminal": ["í˜•ì‚¬", "ë²”ì£„", "ì²˜ë²Œ", "í˜•ëŸ‰"],
            "labor": ["ë…¸ë™", "ê·¼ë¡œ", "í•´ê³ ", "ì„ê¸ˆ", "ê·¼ë¡œì"],
            "corporate": ["ê¸°ì—…", "íšŒì‚¬", "ì£¼ì£¼", "ë²•ì¸"]
        }

        important_keywords_for_type = query_type_keywords.get(query_type, [])
        important_keywords_for_field = field_keywords.get(legal_field, [])

        for keyword in extracted_keywords:
            if not keyword or not isinstance(keyword, str):
                continue

            keyword_lower = keyword.lower()
            weight = 0.0

            # 1. ì¿¼ë¦¬ ì¶œí˜„ ë¹ˆë„ (30%)
            query_frequency = query_lower.count(keyword_lower)
            query_weight = min(0.3, (query_frequency / max(1, len(query.split()))) * 0.3)
            weight += query_weight

            # 2. ë²•ë¥  ìš©ì–´ ì—¬ë¶€ (30%)
            is_legal_term = any(
                re.search(pattern, keyword, re.IGNORECASE)
                for pattern in legal_term_patterns
            )
            if is_legal_term:
                weight += 0.3

            # 3. ì§ˆë¬¸ ìœ í˜•ë³„ ì¤‘ìš”ë„ (20%)
            if any(imp_kw in keyword_lower for imp_kw in important_keywords_for_type):
                weight += 0.2

            # 4. ë²•ë¥  ë¶„ì•¼ë³„ ê´€ë ¨ì„± (20%)
            if any(imp_kw in keyword_lower for imp_kw in important_keywords_for_field):
                weight += 0.2

            # ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ìµœì†Œê°’)
            if weight == 0.0:
                weight = 0.1

            # ì •ê·œí™” (0.0-1.0)
            keyword_weights[keyword] = min(1.0, weight)

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(keyword_weights.values())
        if total_weight > 0:
            max_weight = max(keyword_weights.values()) if keyword_weights else 1.0
            if max_weight > 0:
                for kw in keyword_weights:
                    keyword_weights[kw] = keyword_weights[kw] / max_weight

        return keyword_weights

    def _calculate_keyword_match_score(
        self,
        document: Dict[str, Any],
        keyword_weights: Dict[str, float],
        query: str
    ) -> Dict[str, float]:
        """ë¬¸ì„œì— ëŒ€í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        doc_content = document.get("content", "")
        if not doc_content:
            return {
                "keyword_match_score": 0.0,
                "keyword_coverage": 0.0,
                "matched_keywords": [],
                "weighted_keyword_score": 0.0
            }

        doc_content_lower = doc_content.lower()

        matched_keywords = []
        total_weight = 0.0
        matched_weight = 0.0

        for keyword, weight in keyword_weights.items():
            if not keyword:
                continue

            total_weight += weight
            keyword_lower = keyword.lower()

            if keyword_lower in doc_content_lower:
                matched_keywords.append(keyword)
                matched_weight += weight

                # ì¶”ê°€ ë³´ë„ˆìŠ¤: í‚¤ì›Œë“œê°€ ì—¬ëŸ¬ ë²ˆ ì¶œí˜„
                keyword_count = doc_content_lower.count(keyword_lower)
                if keyword_count > 1:
                    matched_weight += weight * 0.1 * min(2, keyword_count - 1)

        keyword_coverage = len(matched_keywords) / max(1, len(keyword_weights))
        keyword_match_score = matched_weight / max(0.1, total_weight) if total_weight > 0 else 0.0
        weighted_keyword_score = min(1.0, matched_weight / max(1, len(keyword_weights)))

        return {
            "keyword_match_score": keyword_match_score,
            "keyword_coverage": keyword_coverage,
            "matched_keywords": matched_keywords,
            "weighted_keyword_score": weighted_keyword_score
        }

    def _calculate_weighted_final_score(
        self,
        document: Dict[str, Any],
        keyword_scores: Dict[str, float],
        search_params: Dict[str, Any],
        query_type: Optional[str] = None
    ) -> float:
        """ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°•í™”ëœ ë²„ì „)"""
        base_relevance = (
            document.get("relevance_score", 0.0) or
            document.get("combined_score", 0.0) or
            document.get("score", 0.0)
        )

        keyword_match = keyword_scores.get("weighted_keyword_score", 0.0)

        search_type = document.get("search_type", "")
        type_weight = 1.2 if search_type == "semantic" else 1.0

        # ë¬¸ì„œ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ (ê°•í™”)
        doc_type = document.get("type", "").lower() if document.get("type") else ""
        doc_type_weight = 1.0
        if "ë²•ë ¹" in doc_type or "law" in doc_type:
            doc_type_weight = 1.2  # ë²•ë ¹ ìš°ì„ 
        elif "íŒë¡€" in doc_type or "precedent" in doc_type:
            doc_type_weight = 1.1  # íŒë¡€ ì°¨ìˆœ
        else:
            doc_type_weight = 0.9

        # ì§ˆë¬¸ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜ (ê°•í™”)
        query_type_weight = 1.0
        if query_type:
            if query_type == "precedent_search" and ("íŒë¡€" in doc_type or "precedent" in doc_type):
                query_type_weight = 1.3  # íŒë¡€ ê²€ìƒ‰ì—ì„œ íŒë¡€ ë¬¸ì„œ
            elif query_type == "law_inquiry" and ("ë²•ë ¹" in doc_type or "law" in doc_type):
                query_type_weight = 1.3  # ë²•ë ¹ ë¬¸ì˜ì—ì„œ ë²•ë ¹ ë¬¸ì„œ

        category_boost = document.get("category_boost", 1.0)
        field_match_score = document.get("field_match_score", 0.5)
        category_bonus = (category_boost * 0.7 + field_match_score * 0.3)

        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°•í™”ëœ ê°€ì¤‘ì¹˜ ì ìš©)
        final_score = (
            base_relevance * 0.50 +  # ê¸°ë³¸ ì ìˆ˜ ë¹„ì¤‘ ì¦ê°€ (40% â†’ 50%)
            keyword_match * 0.30 +  # í‚¤ì›Œë“œ ì ìˆ˜ ë¹„ì¤‘ ê°ì†Œ (35% â†’ 30%)
            (base_relevance * doc_type_weight * query_type_weight) * 0.10 +  # ë¬¸ì„œ/ì§ˆë¬¸ íƒ€ì… ê°€ì¤‘ì¹˜ ì¶”ê°€
            type_weight * 0.05 +  # ê²€ìƒ‰ íƒ€ì… ê°€ì¤‘ì¹˜ ê°ì†Œ (15% â†’ 5%)
            category_bonus * 0.05  # ì¹´í…Œê³ ë¦¬ ë³´ë„ˆìŠ¤ ê°ì†Œ (10% â†’ 5%)
        )

        return min(1.0, max(0.0, final_score))

    def _validate_search_quality(
        self,
        results: List[Dict[str, Any]],
        query: str,
        query_type: str
    ) -> Tuple[bool, str]:
        """
        ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ (quality_validators ëª¨ë“ˆ ì‚¬ìš©)

        Args:
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            query: ì›ë³¸ ì¿¼ë¦¬
            query_type: ì§ˆë¬¸ ìœ í˜•

        Returns:
            (is_valid, message): ê²€ì¦ í†µê³¼ ì—¬ë¶€ì™€ ë©”ì‹œì§€
        """
        validation_result = SearchValidator.validate_search_quality(
            search_results=results,
            query=query,
            query_type=query_type
        )

        is_valid = validation_result.get("is_valid", False)
        quality_score = validation_result.get("quality_score", 0.0)
        avg_relevance = validation_result.get("avg_relevance", 0.0)
        issues = validation_result.get("issues", [])

        if is_valid:
            message = f"ê²€ìƒ‰ í’ˆì§ˆ ì–‘í˜¸ (í‰ê·  ì ìˆ˜: {avg_relevance:.2f}, í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f})"
        else:
            message = "; ".join(issues) if issues else f"ê²€ìƒ‰ í’ˆì§ˆ ë¶€ì¡± (í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f})"

        return is_valid, message

    def _rerank_with_keyword_weights(
        self,
        results: List[Dict[str, Any]],
        keyword_weights: Dict[str, float],
        rerank_params: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ Reranking"""
        try:
            # ìµœì¢… ê°€ì¤‘ì¹˜ ì ìˆ˜ë¡œ ì •ë ¬
            sorted_results = sorted(
                results,
                key=lambda x: (
                    x.get("final_weighted_score", 0.0),
                    x.get("keyword_match_score", 0.0),
                    x.get("keyword_coverage", 0.0)
                ),
                reverse=True
            )

            # í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ë³´ë„ˆìŠ¤ ì ìš©
            for doc in sorted_results:
                coverage = doc.get("keyword_coverage", 0.0)
                if coverage > 0.7:
                    doc["final_weighted_score"] *= 1.1
                elif coverage > 0.5:
                    doc["final_weighted_score"] *= 1.05

            # ë‹¤ì‹œ ì •ë ¬
            sorted_results = sorted(
                sorted_results,
                key=lambda x: x.get("final_weighted_score", 0.0),
                reverse=True
            )

            # Reranker ì‚¬ìš© (ìˆëŠ” ê²½ìš°)
            top_k = rerank_params.get("top_k", 20)
            if self.result_ranker and len(sorted_results) > 0:
                try:
                    reranked_results = self.result_ranker.rank_results(
                        sorted_results[:top_k * 2],
                        top_k=top_k
                    )
                    # ResultRanker ê²°ê³¼ë¥¼ Dict í˜•íƒœë¡œ ë³€í™˜
                    if reranked_results and hasattr(reranked_results[0], 'score'):
                        reranked_dicts = []
                        for result in reranked_results:
                            doc = {
                                "content": result.text,
                                "relevance_score": result.score,
                                "source": result.source,
                                "id": f"{result.source}_{hash(result.text)}",
                                "final_weighted_score": result.score
                            }
                            if isinstance(result.metadata, dict):
                                doc.update(result.metadata)
                            reranked_dicts.append(doc)
                        sorted_results = reranked_dicts[:top_k]
                    else:
                        sorted_results = sorted_results[:top_k]
                except Exception as e:
                    self.logger.warning(f"Reranker failed, using keyword-weighted scores: {e}")
                    sorted_results = sorted_results[:top_k]
            else:
                sorted_results = sorted_results[:top_k]

            # ë‹¤ì–‘ì„± í•„í„° ì ìš©
            try:
                if self.result_ranker and hasattr(self.result_ranker, 'apply_diversity_filter'):
                    diverse_results = self.result_ranker.apply_diversity_filter(
                        sorted_results,
                        max_per_type=5,
                        diversity_weight=rerank_params.get("diversity_weight", 0.3)
                    )
                else:
                    diverse_results = sorted_results
            except Exception as e:
                self.logger.warning(f"Diversity filter failed: {e}")
                diverse_results = sorted_results

            return diverse_results

        except Exception as e:
            self.logger.warning(f"Reranking with keyword weights failed: {e}")
            return sorted(
                results,
                key=lambda x: x.get("final_weighted_score", 0.0),
                reverse=True
            )[:rerank_params.get("top_k", 20)]

    @observe(name="filter_and_validate_results")
    @with_state_optimization("filter_and_validate_results", enable_reduction=True)
    def filter_and_validate_results(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ ë° í’ˆì§ˆ ê²€ì¦"""
        try:
            start_time = time.time()

            documents = self._get_state_value(state, "merged_documents", [])

            # merged_documentsê°€ ë¹„ì–´ìˆìœ¼ë©´ retrieved_docsì—ì„œ ê°€ì ¸ì˜¤ê¸° (fallback)
            if not documents or len(documents) == 0:
                documents = self._get_state_value(state, "retrieved_docs", [])
                if documents:
                    print(f"[DEBUG] FILTER: merged_documents is empty, using retrieved_docs ({len(documents)} documents)")
                    self.logger.warning(f"filter_and_validate_results: merged_documents is empty, using retrieved_docs")

            search_params = self._get_state_value(state, "search_params", {})
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            legal_field = self._get_state_value(state, "legal_field", "")

            # ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©
            documents = self._apply_metadata_filters(
                documents,
                query_type_str,
                legal_field
            )

            # ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ ë° í•„í„°ë§
            filtered_docs = self._filter_low_quality_results(
                documents,
                min_relevance=search_params.get("min_relevance", self.config.similarity_threshold),
                max_diversity=search_params.get("max_results", WorkflowConstants.MAX_DOCUMENTS)
            )

            # Pruning ë° ìµœì¢… ì •ë¦¬
            pruned_docs = prune_retrieved_docs(
                filtered_docs[:WorkflowConstants.MAX_DOCUMENTS],
                max_items=MAX_RETRIEVED_DOCS,
                max_content_per_doc=MAX_DOCUMENT_CONTENT_LENGTH
            )

            self._set_state_value(state, "retrieved_docs", pruned_docs)
            self._save_metadata_safely(state, "_last_executed_node", "filter_and_validate_results")
            self._update_processing_time(state, start_time)

            # ìƒì„¸ ë¡œê¹…: ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
            self.logger.info(
                f"âœ… [FILTER & VALIDATE] Results filtered and validated: "
                f"{len(pruned_docs)} final documents "
                f"(from {len(documents)} input documents)"
            )

            # í•„í„°ë§ ìƒì„¸ ë””ë²„ê¹… ë¡œê·¸
            min_relevance = search_params.get("min_relevance", self.config.similarity_threshold)
            self.logger.info(
                f"ğŸ” [DEBUG] Filter & Validate details: "
                f"Input documents: {len(documents)}, "
                f"After metadata filter: {len(documents)}, "
                f"After quality filter: {len(filtered_docs)}, "
                f"After pruning: {len(pruned_docs)}, "
                f"Min relevance threshold: {min_relevance:.3f}"
            )
            print(f"[DEBUG] FILTER: Input={len(documents)}, After quality={len(filtered_docs)}, After prune={len(pruned_docs)}, Min relevance={min_relevance:.3f}")

            if documents:
                # ì ìˆ˜ ë²”ìœ„ ë¶„ì„
                all_scores = []
                for doc in documents:
                    score = doc.get("combined_score") or doc.get("relevance_score") or doc.get("final_weighted_score", 0.0)
                    all_scores.append(score)

                if all_scores:
                    min_score = min(all_scores)
                    max_score = max(all_scores)
                    avg_score = sum(all_scores) / len(all_scores)
                    below_threshold = sum(1 for s in all_scores if s < min_relevance)

                    self.logger.info(
                        f"ğŸ” [DEBUG] Score statistics: "
                        f"min={min_score:.3f}, max={max_score:.3f}, avg={avg_score:.3f}, "
                        f"below threshold ({min_relevance:.3f}): {below_threshold}/{len(all_scores)}"
                    )

                    # í•„í„°ë§ìœ¼ë¡œ ì œê±°ëœ ë¬¸ì„œ ìˆ˜
                    removed_by_score = len(documents) - len(filtered_docs)
                    if removed_by_score > 0:
                        self.logger.warning(
                            f"âš ï¸ [DEBUG] {removed_by_score} documents removed by relevance score filter "
                            f"(score < {min_relevance:.3f})"
                        )

            # ë¬¸ì„œ ìƒ˜í”Œ ë¡œê¹… (ìƒìœ„ 3ê°œ)
            if pruned_docs:
                self.logger.info("ğŸ“„ [FILTER & VALIDATE] Document samples:")
                for i, doc in enumerate(pruned_docs[:3], 1):
                    source = doc.get("source", "Unknown")
                    content = doc.get("content") or doc.get("text", "")
                    content_length = len(content) if content else 0
                    score = doc.get("relevance_score", doc.get("combined_score", doc.get("final_weighted_score", doc.get("score", 0.0))))
                    search_type = doc.get("search_type", "unknown")
                    self.logger.info(
                        f"  [{i}] {source} | score={score:.3f} | "
                        f"type={search_type} | "
                        f"content={content_length}chars | "
                        f"preview={content[:50] if content else 'N/A'}..."
                    )
            else:
                self.logger.warning(
                    f"âš ï¸ [FILTER & VALIDATE] No documents after filtering and validation! "
                    f"Input: {len(documents)}, After metadata filter: {len(documents)}, "
                    f"After quality filter: {len(filtered_docs)}, After pruning: {len(pruned_docs)}"
                )

        except Exception as e:
            self._handle_error(state, str(e), "ê²°ê³¼ í•„í„°ë§ ë° ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            self._fallback_search(state)

        return state

    @observe(name="update_search_metadata")
    @with_state_optimization("update_search_metadata", enable_reduction=True)
    def update_search_metadata(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        try:
            start_time = time.time()

            semantic_count = self._get_state_value(state, "semantic_count", 0)
            keyword_count = self._get_state_value(state, "keyword_count", 0)
            filtered_docs = self._get_state_value(state, "retrieved_docs", [])
            optimized_queries = self._get_state_value(state, "optimized_queries", {})
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            search_start_time = self._get_state_value(state, "search_start_time", time.time())

            # ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self._update_search_metadata(
                state,
                semantic_count,
                keyword_count,
                filtered_docs,
                query_type_str,
                search_start_time,
                optimized_queries
            )

            # ìºì‹œ ì €ì¥
            if optimized_queries:
                self.performance_optimizer.cache.cache_documents(
                    optimized_queries.get("semantic_query", ""),
                    query_type_str,
                    filtered_docs
                )

            self._save_metadata_safely(state, "_last_executed_node", "update_search_metadata")
            self._update_processing_time(state, start_time)

            retrieved_docs = self._get_state_value(state, "retrieved_docs", [])
            self.logger.info(f"Search metadata updated: {len(retrieved_docs)} documents retrieved")

        except Exception as e:
            self._handle_error(state, str(e), "ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

        return state

    @observe(name="prepare_document_context_for_prompt")
    @with_state_optimization("prepare_document_context_for_prompt", enable_reduction=True)
    def prepare_document_context_for_prompt(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """í”„ë¡¬í”„íŠ¸ì— ìµœëŒ€í•œ ë°˜ì˜ë˜ë„ë¡ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„"""
        try:
            start_time = time.time()

            retrieved_docs = self._get_state_value(state, "retrieved_docs", [])
            query = self._get_state_value(state, "query", "")
            extracted_keywords = self._get_state_value(state, "extracted_keywords", [])
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            legal_field = self._get_state_value(state, "legal_field", "")

            # retrieved_docs ê²€ì¦
            if not retrieved_docs:
                self.logger.warning(
                    f"âš ï¸ [PREPARE CONTEXT] No retrieved_docs to prepare for prompt. "
                    f"Query: '{query[:50]}...', Query type: {query_type_str}"
                )
                self._set_state_value(state, "prompt_optimized_context", {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                })
                return state

            # retrieved_docs íƒ€ì… ê²€ì¦
            if not isinstance(retrieved_docs, list):
                self.logger.error(
                    f"âš ï¸ [PREPARE CONTEXT] retrieved_docs is not a list: {type(retrieved_docs).__name__}"
                )
                self._set_state_value(state, "prompt_optimized_context", {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                })
                return state

            # ë¬¸ì„œ ë‚´ìš© ê²€ì¦
            valid_docs_count = 0
            docs_without_content = 0
            total_content_length = 0

            for doc in retrieved_docs:
                if not isinstance(doc, dict):
                    docs_without_content += 1
                    continue

                # content í•„ë“œ í™•ì¸ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ í•„ë“œëª… ì§€ì›)
                content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                if content and len(content.strip()) >= 10:  # ìµœì†Œ 10ì ì´ìƒ
                    valid_docs_count += 1
                    total_content_length += len(content)
                else:
                    docs_without_content += 1
                    source = doc.get("source", "Unknown")
                    self.logger.debug(
                        f"[PREPARE CONTEXT] Document filtered: content missing or too short "
                        f"(source: {source}, content_length: {len(content) if content else 0})"
                    )

            # ê²€ì¦ ê²°ê³¼ ë¡œê¹…
            if docs_without_content > 0:
                self.logger.warning(
                    f"âš ï¸ [PREPARE CONTEXT] Found {docs_without_content} documents without valid content "
                    f"out of {len(retrieved_docs)} total documents. "
                    f"Valid docs: {valid_docs_count}, Total content: {total_content_length} chars"
                )

            # ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì—ëŸ¬
            if valid_docs_count == 0:
                self.logger.error(
                    f"âŒ [PREPARE CONTEXT] No valid documents with content found! "
                    f"Total docs: {len(retrieved_docs)}, "
                    f"Query: '{query[:50]}...'"
                )
                self._set_state_value(state, "prompt_optimized_context", {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                })
                return state

            self.logger.info(
                f"âœ… [PREPARE CONTEXT] Preparing context from {valid_docs_count} valid documents "
                f"(total: {len(retrieved_docs)}, content: {total_content_length} chars)"
            )

            # í”„ë¡¬í”„íŠ¸ ìµœì í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•
            prompt_optimized_context = self._build_prompt_optimized_context(
                retrieved_docs=retrieved_docs,
                query=query,
                extracted_keywords=extracted_keywords,
                query_type=query_type_str,
                legal_field=legal_field
            )

            # Stateì— ì €ì¥
            self._set_state_value(state, "prompt_optimized_context", prompt_optimized_context)

            self._save_metadata_safely(state, "_last_executed_node", "prepare_document_context_for_prompt")
            self._update_processing_time(state, start_time)

            # ìƒì„¸ ë¡œê¹…: í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ê²°ê³¼
            doc_count = prompt_optimized_context.get("document_count", 0)
            context_length = prompt_optimized_context.get("total_context_length", 0)
            content_validation = prompt_optimized_context.get("content_validation", {})

            self.logger.info(
                f"âœ… [DOCUMENT PREPARATION] Prepared prompt context: "
                f"{doc_count} documents, "
                f"{context_length} chars, "
                f"input docs: {len(retrieved_docs)}"
            )

            if content_validation:
                has_content = content_validation.get("has_document_content", False)
                docs_with_content = content_validation.get("documents_with_content", 0)
                self.logger.info(
                    f"ğŸ“Š [DOCUMENT PREPARATION] Content validation: "
                    f"has_content={has_content}, "
                    f"docs_with_content={docs_with_content}"
                )

            # í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ ë¡œê¹… (ì²˜ìŒ 200ì)
            prompt_text = prompt_optimized_context.get("prompt_optimized_text", "")
            if prompt_text:
                self.logger.debug(
                    f"ğŸ“ [DOCUMENT PREPARATION] Prompt text preview (first 200 chars):\n"
                    f"{prompt_text[:200]}..."
                )
            else:
                self.logger.warning(
                    "âš ï¸ [DOCUMENT PREPARATION] prompt_optimized_text is empty!"
                )

        except Exception as e:
            self._handle_error(state, str(e), "ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # í´ë°±: ë¹ˆ ì»¨í…ìŠ¤íŠ¸
            self._set_state_value(state, "prompt_optimized_context", {
                "prompt_optimized_text": "",
                "structured_documents": {},
                "document_count": 0,
                "total_context_length": 0
            })

        return state

    @observe(name="prepare_documents_and_terms")
    @with_state_optimization("prepare_documents_and_terms", enable_reduction=True)
    def prepare_documents_and_terms(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """í†µí•©ëœ ë¬¸ì„œ ì¤€ë¹„ ë° ìš©ì–´ ì²˜ë¦¬ (prepare_document_context_for_prompt + process_legal_terms)"""
        try:
            overall_start_time = time.time()

            # ========== Part 1: prepare_document_context_for_prompt ë¡œì§ ==========
            context_start_time = time.time()

            retrieved_docs = self._get_state_value(state, "retrieved_docs", [])
            query = self._get_state_value(state, "query", "")
            extracted_keywords = self._get_state_value(state, "extracted_keywords", [])
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            legal_field = self._get_state_value(state, "legal_field", "")

            # retrieved_docs ê²€ì¦
            if not retrieved_docs:
                self.logger.warning(
                    f"âš ï¸ [PREPARE CONTEXT] No retrieved_docs to prepare for prompt. "
                    f"Query: '{query[:50]}...', Query type: {query_type_str}"
                )
                self._set_state_value(state, "prompt_optimized_context", {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                })
            elif not isinstance(retrieved_docs, list):
                self.logger.error(
                    f"âš ï¸ [PREPARE CONTEXT] retrieved_docs is not a list: {type(retrieved_docs).__name__}"
                )
                self._set_state_value(state, "prompt_optimized_context", {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                })
            else:
                # ë¬¸ì„œ ë‚´ìš© ê²€ì¦
                valid_docs_count = 0
                docs_without_content = 0
                total_content_length = 0

                for doc in retrieved_docs:
                    if not isinstance(doc, dict):
                        docs_without_content += 1
                        continue

                    content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                    if content and len(content.strip()) >= 10:
                        valid_docs_count += 1
                        total_content_length += len(content)
                    else:
                        docs_without_content += 1

                if docs_without_content > 0:
                    self.logger.warning(
                        f"âš ï¸ [PREPARE CONTEXT] Found {docs_without_content} documents without valid content "
                        f"out of {len(retrieved_docs)} total documents. "
                        f"Valid docs: {valid_docs_count}, Total content: {total_content_length} chars"
                    )

                if valid_docs_count == 0:
                    self.logger.error(
                        f"âŒ [PREPARE CONTEXT] No valid documents with content found! "
                        f"Total docs: {len(retrieved_docs)}, "
                        f"Query: '{query[:50]}...'"
                    )
                    self._set_state_value(state, "prompt_optimized_context", {
                        "prompt_optimized_text": "",
                        "structured_documents": {},
                        "document_count": 0,
                        "total_context_length": 0
                    })
                else:
                    self.logger.info(
                        f"âœ… [PREPARE CONTEXT] Preparing context from {valid_docs_count} valid documents "
                        f"(total: {len(retrieved_docs)}, content: {total_content_length} chars)"
                    )

                    # í”„ë¡¬í”„íŠ¸ ìµœì í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•
                    prompt_optimized_context = self._build_prompt_optimized_context(
                        retrieved_docs=retrieved_docs,
                        query=query,
                        extracted_keywords=extracted_keywords,
                        query_type=query_type_str,
                        legal_field=legal_field
                    )

                    # Stateì— ì €ì¥
                    self._set_state_value(state, "prompt_optimized_context", prompt_optimized_context)

                    # ìƒì„¸ ë¡œê¹…
                    doc_count = prompt_optimized_context.get("document_count", 0)
                    context_length = prompt_optimized_context.get("total_context_length", 0)
                    self.logger.info(
                        f"âœ… [DOCUMENT PREPARATION] Prepared prompt context: "
                        f"{doc_count} documents, "
                        f"{context_length} chars, "
                        f"input docs: {len(retrieved_docs)}"
                    )

            self._save_metadata_safely(state, "_last_executed_node", "prepare_documents_and_terms")
            self._update_processing_time(state, context_start_time)
            self._add_step(state, "ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„", "í”„ë¡¬í”„íŠ¸ ìµœì í™” ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

            # ========== Part 2: process_legal_terms ë¡œì§ ==========
            terms_start_time = time.time()

            # ë©”íƒ€ë°ì´í„° ì„¤ì • (process_legal_terms ë¡œì§)
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            state["metadata"] = dict(state["metadata"])
            state["metadata"]["_last_executed_node"] = "prepare_documents_and_terms"

            if "common" not in state or not isinstance(state.get("common"), dict):
                state["common"] = {}
            if "metadata" not in state["common"]:
                state["common"]["metadata"] = {}
            state["common"]["metadata"]["_last_executed_node"] = "prepare_documents_and_terms"

            # ê¸°ì¡´ ì¬ì‹œë„ ì¹´ìš´í„° ë³´ì¡´
            existing_metadata_direct = state.get("metadata", {})
            existing_metadata_common = state.get("common", {}).get("metadata", {}) if isinstance(state.get("common"), dict) else {}
            existing_metadata = existing_metadata_direct if isinstance(existing_metadata_direct, dict) else {}
            if isinstance(existing_metadata_common, dict):
                existing_metadata = {**existing_metadata, **existing_metadata_common}

            existing_top_level = state.get("retry_count", 0)
            saved_gen_retry = max(
                existing_metadata.get("generation_retry_count", 0),
                existing_top_level
            )
            saved_val_retry = existing_metadata.get("validation_retry_count", 0)

            # ë²•ë¥  ìš©ì–´ ì¶”ì¶œ ë° í†µí•©
            retrieved_docs = self._get_state_value(state, "retrieved_docs", [])
            all_terms = self._extract_terms_from_documents(retrieved_docs)
            self.logger.info(f"ì¶”ì¶œëœ ìš©ì–´ ìˆ˜: {len(all_terms)}")

            if all_terms:
                representative_terms = self._integrate_and_process_terms(all_terms)
                metadata = dict(existing_metadata)
                metadata["extracted_terms"] = representative_terms
                metadata["total_terms_extracted"] = len(all_terms)
                metadata["unique_terms"] = len(representative_terms)
                # ì¬ì‹œë„ ì¹´ìš´í„° ë³´ì¡´
                metadata["generation_retry_count"] = saved_gen_retry
                metadata["validation_retry_count"] = saved_val_retry
                metadata["_last_executed_node"] = "prepare_documents_and_terms"
                state["metadata"] = metadata

                # common ê·¸ë£¹ì—ë„ ë™ê¸°í™”
                if "common" not in state:
                    state["common"] = {}
                if "metadata" not in state["common"]:
                    state["common"]["metadata"] = {}
                state["common"]["metadata"].update(metadata)
                state["retry_count"] = saved_gen_retry

                self._set_state_value(state, "metadata", metadata)
                self._add_step(state, "ìš©ì–´ í†µí•© ì™„ë£Œ", f"ìš©ì–´ í†µí•© ì™„ë£Œ: {len(representative_terms)}ê°œ")
                self.logger.info(f"í†µí•©ëœ ìš©ì–´ ìˆ˜: {len(representative_terms)}")
            else:
                metadata = dict(existing_metadata)
                metadata["extracted_terms"] = []
                # ì¬ì‹œë„ ì¹´ìš´í„° ë³´ì¡´
                metadata["generation_retry_count"] = saved_gen_retry
                metadata["validation_retry_count"] = saved_val_retry
                metadata["_last_executed_node"] = "prepare_documents_and_terms"
                state["metadata"] = metadata

                # common ê·¸ë£¹ì—ë„ ë™ê¸°í™”
                if "common" not in state:
                    state["common"] = {}
                if "metadata" not in state["common"]:
                    state["common"]["metadata"] = {}
                state["common"]["metadata"].update(metadata)
                state["retry_count"] = saved_gen_retry

                self._set_state_value(state, "metadata", metadata)
                self._add_step(state, "ìš©ì–´ ì¶”ì¶œ ì—†ìŒ", "ìš©ì–´ ì¶”ì¶œ ì—†ìŒ (ë¬¸ì„œ ë‚´ìš© ë¶€ì¡±)")

            self._update_processing_time(state, terms_start_time)

        except Exception as e:
            self._handle_error(state, str(e), "ë¬¸ì„œ ì¤€ë¹„ ë° ìš©ì–´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # í´ë°±: ë¹ˆ ì»¨í…ìŠ¤íŠ¸ ë° ë¹ˆ ìš©ì–´
            self._set_state_value(state, "prompt_optimized_context", {
                "prompt_optimized_text": "",
                "structured_documents": {},
                "document_count": 0,
                "total_context_length": 0
            })
            metadata = self._get_state_value(state, "metadata", {})
            if not isinstance(metadata, dict):
                metadata = {}
            metadata["extracted_terms"] = []
            self._set_state_value(state, "metadata", metadata)

        self._update_processing_time(state, overall_start_time)
        return state

    def _build_prompt_optimized_context(
        self,
        retrieved_docs: List[Dict[str, Any]],
        query: str,
        extracted_keywords: List[str],
        query_type: str,
        legal_field: str
    ) -> Dict[str, Any]:
        """í”„ë¡¬í”„íŠ¸ì— ìµœëŒ€í•œ ë°˜ì˜ë˜ë„ë¡ ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not retrieved_docs:
                self.logger.warning("_build_prompt_optimized_context: retrieved_docs is empty")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }

            # ë¬¸ì„œ ê²€ì¦: content í•„ë“œì™€ ê´€ë ¨ë„ ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§ (ê²€ìƒ‰ íƒ€ì… ê³ ë ¤)
            valid_docs = []
            invalid_docs_count = 0
            # ê²€ìƒ‰ íƒ€ì…ë³„ ë‹¤ë¥¸ ê´€ë ¨ë„ ê¸°ì¤€ ì ìš©
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ëŠ” ê´€ë ¨ë„ê°€ ë‚®ì•„ë„ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ìˆìœ¼ë©´ í¬í•¨
            min_relevance_score_semantic = 0.3  # ì˜ë¯¸ì  ê²€ìƒ‰: 0.3 ì´ìƒ
            min_relevance_score_keyword = 0.15  # í‚¤ì›Œë“œ ê²€ìƒ‰: 0.15 ì´ìƒ (í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œ)

            for doc in retrieved_docs:
                if not isinstance(doc, dict):
                    invalid_docs_count += 1
                    continue

                # content í•„ë“œ í™•ì¸ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ í•„ë“œëª… ì§€ì›)
                content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                if not content or len(content.strip()) < 10:  # ìµœì†Œ 10ì ì´ìƒ
                    invalid_docs_count += 1
                    self.logger.debug(f"Document filtered: content too short or empty (source: {doc.get('source', 'Unknown')})")
                    continue

                # ê´€ë ¨ë„ ì ìˆ˜ í™•ì¸ (ê²€ìƒ‰ íƒ€ì…ë³„ ê¸°ì¤€ ì ìš©)
                search_type = doc.get("search_type", "semantic")  # ê¸°ë³¸ê°’ì€ semantic
                relevance_score = doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)
                keyword_match_score = doc.get("keyword_match_score", 0.0)
                has_keyword_match = keyword_match_score > 0.0 or len(doc.get("matched_keywords", [])) > 0

                # ê²€ìƒ‰ íƒ€ì…ë³„ í•„í„°ë§ ê¸°ì¤€
                min_score = min_relevance_score_keyword if search_type == "keyword" else min_relevance_score_semantic

                # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ëŠ” í‚¤ì›Œë“œ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ê´€ë ¨ë„ ê¸°ì¤€ ì™„í™”
                if search_type == "keyword" and has_keyword_match:
                    min_score = min_relevance_score_keyword  # 0.15 ì´ìƒ
                elif search_type == "semantic":
                    min_score = min_relevance_score_semantic  # 0.3 ì´ìƒ

                if relevance_score < min_score:
                    invalid_docs_count += 1
                    self.logger.debug(
                        f"Document filtered: relevance score too low ({relevance_score:.3f} < {min_score}) "
                        f"(source: {doc.get('source', 'Unknown')}, type: {search_type})"
                    )
                    continue

                valid_docs.append(doc)

            if invalid_docs_count > 0:
                self.logger.warning(
                    f"_build_prompt_optimized_context: Filtered {invalid_docs_count} invalid documents "
                    f"(no content, content too short, or relevance < 0.3). Valid docs: {len(valid_docs)}"
                )

            # ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°˜í™˜
            if not valid_docs:
                self.logger.error("_build_prompt_optimized_context: No valid documents with content found")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }

            # ìµœì¢… ê°€ì¤‘ì¹˜ ì ìˆ˜ë¡œ ì •ë ¬ (ì´ë¯¸ rerankedë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
            sorted_docs = sorted(
                valid_docs,
                key=lambda x: (
                    x.get("final_weighted_score", x.get("relevance_score", 0.0)),
                    x.get("keyword_match_score", 0.0)
                ),
                reverse=True
            )

            # ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì˜ ê· í˜•ì„ ë§ì¶°ì„œ ì„ íƒ
            balanced_docs = self._select_balanced_documents(sorted_docs, max_docs=10)

            # ìµœì†Œ 1ê°œ ë¬¸ì„œ ë³´ì¥
            if not balanced_docs and sorted_docs:
                balanced_docs = sorted_docs[:min(8, len(sorted_docs))]

            sorted_docs = balanced_docs

            # ìµœì†Œ 1ê°œ ë¬¸ì„œ ë³´ì¥
            if not sorted_docs:
                self.logger.error("_build_prompt_optimized_context: sorted_docs is empty after filtering")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }

            # ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ì§€ì‹œì‚¬í•­ ìƒì„±
            document_instructions = self._generate_document_based_instructions(
                documents=sorted_docs,
                query=query,
                query_type=query_type
            )

            # ê²€ìƒ‰ ê²°ê³¼ í†µê³„ ê³„ì‚°
            semantic_count = sum(1 for doc in sorted_docs if doc.get("search_type") == "semantic")
            keyword_count = sum(1 for doc in sorted_docs if doc.get("search_type") == "keyword")
            hybrid_count = len(sorted_docs) - semantic_count - keyword_count

            # í”„ë¡¬í”„íŠ¸ ì„¹ì…˜ êµ¬ì¶• (ê²€ìƒ‰ ê²°ê³¼ í†µê³„ í¬í•¨)
            prompt_section = f"""## ë‹µë³€ ìƒì„± ì§€ì‹œì‚¬í•­

{document_instructions}

## ì°¸ê³  ë¬¸ì„œ ëª©ë¡

ë‹¤ìŒ {len(sorted_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
ê° ë¬¸ì„œëŠ” ê´€ë ¨ì„± ì ìˆ˜ì™€ í•µì‹¬ ë‚´ìš©ì´ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ê²€ìƒ‰ ê²°ê³¼ í†µê³„:**
- ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼: {semantic_count}ê°œ
- í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {keyword_count}ê°œ
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼: {hybrid_count}ê°œ
- ì´ ë¬¸ì„œ ìˆ˜: {len(sorted_docs)}ê°œ

**ì°¸ê³ :** ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ëŠ” ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼, í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ëŠ” í‚¤ì›Œë“œ ë§¤ì¹­ ì •ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
ë‘ ê²€ìƒ‰ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

"""

            # ìš°ì„ ìˆœìœ„ ë†’ì€ ë¬¸ì„œë¶€í„° êµ¬ì¡°í™”í•˜ì—¬ ì œê³µ
            for idx, doc in enumerate(sorted_docs, 1):
                relevance_score = doc.get("final_weighted_score") or doc.get("relevance_score", 0.0)
                source = doc.get("source", "Unknown")
                content = doc.get("content", "")

                # ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë¬¸ì¥ ì¶”ì¶œ
                relevant_sentences = self._extract_query_relevant_sentences(
                    doc_content=content,
                    query=query,
                    extracted_keywords=extracted_keywords
                )

                # ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                search_type = doc.get("search_type", "hybrid")
                search_method = doc.get("search_method", "hybrid_search")
                keyword_match_score = doc.get("keyword_match_score", 0.0)
                matched_keywords = doc.get("matched_keywords", [])

                # ë¬¸ì„œ ì„¹ì…˜ êµ¬ì„± (ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° í¬í•¨)
                doc_section = f"""
### ë¬¸ì„œ {idx}: {source} (ê´€ë ¨ì„± ì ìˆ˜: {relevance_score:.2f})

**ê²€ìƒ‰ ì •ë³´:**
- ê²€ìƒ‰ ë°©ì‹: {search_type} ({search_method})
- í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜: {keyword_match_score:.2f}
- ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(matched_keywords[:5]) if matched_keywords else 'ì—†ìŒ'}

**í•µì‹¬ ë‚´ìš©:**
"""

                # ê´€ë ¨ ë¬¸ì¥ ê°•ì¡° í‘œì‹œ
                if relevant_sentences:
                    doc_section += "\n".join([
                        f"- [ì¤‘ìš”] {sent['sentence']}"
                        for sent in relevant_sentences[:3]
                    ])
                    doc_section += "\n\n"

                # ì „ì²´ ë¬¸ì„œ ë‚´ìš© (ì ì ˆí•œ ê¸¸ì´ë¡œ ì œí•œ)
                max_content_length = 1500
                if len(content) > max_content_length:
                    content = content[:max_content_length] + "..."

                doc_section += f"""**ì „ì²´ ë‚´ìš©:**
{content}

---
"""

                prompt_section += doc_section

            # ë¬¸ì„œ ì¸ìš© í˜•ì‹ ì§€ì •
            prompt_section += """
## ë¬¸ì„œ ì¸ìš© ê·œì¹™

ë‹µë³€ì—ì„œ ìœ„ ë¬¸ì„œë¥¼ ì¸ìš©í•  ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ ëª…ì‹œí•˜ì„¸ìš”:
- "ë¬¸ì„œ {0}ì— ë”°ë¥´ë©´..." ë˜ëŠ” "[{0}] ì¸ìš© ë‚´ìš©"
- ê° ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œ

## ì¤‘ìš” ì‚¬í•­

- ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”
- ë¬¸ì„œì—ì„œ ì¶”ë¡ í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì¼ê´€ëœ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”
""".format("n")

            # í”„ë¡¬í”„íŠ¸ ì„¹ì…˜ ê²€ì¦: ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ì´ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            content_validation = {
                "has_document_content": False,
                "total_content_length": 0,
                "documents_with_content": 0
            }

            # í”„ë¡¬í”„íŠ¸ì— ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ì´ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            for doc in sorted_docs:
                content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                if content and len(content.strip()) >= 10:
                    # í”„ë¡¬í”„íŠ¸ ì„¹ì…˜ì— ë¬¸ì„œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    content_preview = content[:100]  # ì²˜ìŒ 100ìë§Œ í™•ì¸
                    if content_preview in prompt_section:
                        content_validation["has_document_content"] = True
                        content_validation["total_content_length"] += len(content)
                        content_validation["documents_with_content"] += 1

            # ê²€ì¦ ê²°ê³¼ ë¡œê¹…
            if not content_validation["has_document_content"]:
                self.logger.error(
                    f"_build_prompt_optimized_context: WARNING - prompt_section does not contain actual document content! "
                    f"Documents processed: {len(sorted_docs)}, "
                    f"Prompt length: {len(prompt_section)}"
                )
            else:
                self.logger.info(
                    f"_build_prompt_optimized_context: Successfully included content from {content_validation['documents_with_content']} documents "
                    f"(total content length: {content_validation['total_content_length']} chars, "
                    f"prompt length: {len(prompt_section)} chars)"
                )

            # ìµœì†Œ ê²€ì¦: í”„ë¡¬í”„íŠ¸ì— ë¬¸ì„œ ë‚´ìš©ì´ ì‹¤ì œë¡œ í¬í•¨ë˜ì–´ì•¼ í•¨
            if not content_validation["has_document_content"] and len(sorted_docs) > 0:
                # ë¬¸ì„œê°€ ìˆëŠ”ë° ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ (í•˜ì§€ë§Œ ë¹ˆ ê²°ê³¼ëŠ” ë°˜í™˜í•˜ì§€ ì•Šê³  ê²½ê³ ë§Œ)
                self.logger.warning(
                    f"_build_prompt_optimized_context: Content validation failed, but returning prompt anyway "
                    f"(may contain instructions only without actual document content)"
                )

            return {
                "prompt_optimized_text": prompt_section,
                "structured_documents": {
                    "total_count": len(sorted_docs),
                    "documents": [{
                        "document_id": idx,
                        "source": doc.get("source", "Unknown"),
                        "relevance_score": doc.get("final_weighted_score") or doc.get("relevance_score", 0.0),
                        "content": (doc.get("content") or doc.get("text") or doc.get("content_text", ""))[:2000]
                    } for idx, doc in enumerate(sorted_docs, 1)]
                },
                "document_count": len(sorted_docs),
                "total_context_length": len(prompt_section),
                "content_validation": content_validation  # ê²€ì¦ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            }

        except Exception as e:
            self.logger.error(f"Prompt optimized context building failed: {e}")
            return {
                "prompt_optimized_text": "",
                "structured_documents": {},
                "document_count": 0,
                "total_context_length": 0
            }

    def _extract_query_relevant_sentences(
        self,
        doc_content: str,
        query: str,
        extracted_keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """QueryEnhancer.extract_query_relevant_sentences ë˜í¼"""
        return self.query_enhancer.extract_query_relevant_sentences(doc_content, query, extracted_keywords)

    def _generate_document_based_instructions(
        self,
        documents: List[Dict[str, Any]],
        query: str,
        query_type: str
    ) -> str:
        """ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±í•˜ë¼ëŠ” ëª…ì‹œì  ì§€ì‹œì‚¬í•­ ìƒì„±"""
        instructions = f"""ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ë¬¸ì„œë“¤ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

**ì§ˆë¬¸**: {query}
**ì§ˆë¬¸ ìœ í˜•**: {query_type}

**ë‹µë³€ ìƒì„± ê·œì¹™**:
1. **ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€**: ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
2. **ë¬¸ì„œ ì¸ìš© í•„ìˆ˜**: ë‹µë³€ì—ì„œ ë¬¸ì„œë¥¼ ì¸ìš©í•  ë•ŒëŠ” "ë¬¸ì„œ [ë²ˆí˜¸]ì— ë”°ë¥´ë©´..." í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
3. **ì •í™•ì„±**: ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ë¡ í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
4. **êµ¬ì¡°í™”**: ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”:
   - í•µì‹¬ ë‹µë³€
   - ê´€ë ¨ ë²•ë ¹ ë° ì¡°í•­
   - ì‹¤ë¬´ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­
   - ì°¸ê³ í•  ë§Œí•œ íŒë¡€ (ìˆëŠ” ê²½ìš°)
5. **ì¶œì²˜ ëª…ì‹œ**: ê° ì¸ìš©ë¬¸ì— ëŒ€í•´ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
"""

        return instructions

    def _select_balanced_documents(
        self,
        sorted_docs: List[Dict[str, Any]],
        max_docs: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì˜ ê· í˜•ì„ ë§ì¶°ì„œ ë¬¸ì„œ ì„ íƒ

        Args:
            sorted_docs: ì ìˆ˜ë¡œ ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            max_docs: ì„ íƒí•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜

        Returns:
            ê· í˜•ì¡íŒ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not sorted_docs:
            return []

        # ê²€ìƒ‰ íƒ€ì…ë³„ë¡œ ë¶„ë¥˜
        semantic_docs = [doc for doc in sorted_docs if doc.get("search_type") == "semantic"]
        keyword_docs = [doc for doc in sorted_docs if doc.get("search_type") == "keyword"]
        hybrid_docs = [doc for doc in sorted_docs if doc.get("search_type") not in ["semantic", "keyword"]]

        # ê· í˜•ì¡íŒ ì„ íƒ ì „ëµ
        selected_docs = []

        # 1. ìƒìœ„ ë¬¸ì„œëŠ” ë¬´ì¡°ê±´ í¬í•¨ (ìµœëŒ€ ì ˆë°˜)
        top_count = max(1, max_docs // 2)
        selected_docs.extend(sorted_docs[:top_count])

        # 2. ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê· í˜•ìˆê²Œ í¬í•¨
        remaining_slots = max_docs - len(selected_docs)

        if remaining_slots > 0:
            # ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì„ íƒ
            semantic_to_add = []
            for doc in semantic_docs:
                if doc not in selected_docs:
                    semantic_to_add.append(doc)

            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì„ íƒ
            keyword_to_add = []
            for doc in keyword_docs:
                if doc not in selected_docs:
                    keyword_to_add.append(doc)

            # êµëŒ€ë¡œ ì¶”ê°€í•˜ì—¬ ê· í˜• ìœ ì§€
            max_alternate = remaining_slots // 2
            for i in range(min(max_alternate, max(len(semantic_to_add), len(keyword_to_add)))):
                if i < len(semantic_to_add) and len(selected_docs) < max_docs:
                    if semantic_to_add[i] not in selected_docs:
                        selected_docs.append(semantic_to_add[i])
                if i < len(keyword_to_add) and len(selected_docs) < max_docs:
                    if keyword_to_add[i] not in selected_docs:
                        selected_docs.append(keyword_to_add[i])

            # ë‚¨ì€ ìŠ¬ë¡¯ì´ ìˆìœ¼ë©´ hybrid ë¬¸ì„œ ì¶”ê°€
            if len(selected_docs) < max_docs:
                for doc in hybrid_docs:
                    if doc not in selected_docs and len(selected_docs) < max_docs:
                        selected_docs.append(doc)

            # ì•„ì§ ìŠ¬ë¡¯ì´ ë‚¨ìœ¼ë©´ ì ìˆ˜ ìˆœìœ¼ë¡œ ì¶”ê°€
            if len(selected_docs) < max_docs:
                for doc in sorted_docs:
                    if doc not in selected_docs and len(selected_docs) < max_docs:
                        selected_docs.append(doc)

        # ì›ë˜ ì ìˆ˜ ìˆœì„œ ìœ ì§€
        selected_docs = sorted(
            selected_docs,
            key=lambda x: (
                x.get("final_weighted_score", x.get("relevance_score", 0.0)),
                x.get("keyword_match_score", 0.0)
            ),
            reverse=True
        )

        return selected_docs[:max_docs]

    def _extract_legal_references_from_docs(self, documents: List[Dict[str, Any]]) -> List[str]:
        """ë¬¸ì„œì—ì„œ ë²•ë¥  ì°¸ì¡° ì •ë³´ ì¶”ì¶œ"""
        return DocumentExtractor.extract_legal_references_from_docs(documents)
