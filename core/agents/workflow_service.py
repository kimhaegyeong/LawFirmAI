# -*- coding: utf-8 -*-
"""
LangGraph Workflow Service
ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ í†µí•© í´ë˜ìŠ¤
"""

import logging
import os
import sys
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# .env íŒŒì¼ ë¡œë“œ (python-dotenv íŒ¨í‚¤ì§€ í•„ìš”)
try:
    from dotenv import load_dotenv
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ .env íŒŒì¼ ë¡œë“œ
    load_dotenv(dotenv_path=str(project_root / ".env"))
except ImportError:
    # python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ê²½ê³ ë§Œ ì¶œë ¥í•˜ê³  ê³„ì† ì§„í–‰
    logging.warning("python-dotenv not installed. .env file will not be loaded.")

from core.agents.legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
from infrastructure.utils.langgraph_config import LangGraphConfig

# Langfuse í´ë¼ì´ì–¸íŠ¸ í†µí•©
try:
    from langfuse import Langfuse, trace

    from source.services.langfuse_client import LangfuseClient
    LANGFUSE_CLIENT_AVAILABLE = True
    LANGFUSE_TRACE_AVAILABLE = True
except ImportError:
    LANGFUSE_CLIENT_AVAILABLE = False
    LANGFUSE_TRACE_AVAILABLE = False

logger = logging.getLogger(__name__)

if not LANGFUSE_CLIENT_AVAILABLE:
    logger.warning("LangfuseClient not available for LangGraph workflow tracking")

# from core.agents.checkpoint_manager import CheckpointManager
from core.agents.state_definitions import create_initial_legal_state


class LangGraphWorkflowService:
    """LangGraph ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤"""

    def __init__(self, config: Optional[LangGraphConfig] = None):
        """
        ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”

        Args:
            config: LangGraph ì„¤ì • ê°ì²´
        """
        self.config = config or LangGraphConfig.from_env()
        self.logger = logging.getLogger(__name__)

        # ì´ˆê¸° input ë³´ì¡´ì„ ìœ„í•œ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜
        self._initial_input: Optional[Dict[str, str]] = None

        # ê²€ìƒ‰ ê²°ê³¼ ë³´ì¡´ì„ ìœ„í•œ ìºì‹œ (LangGraph reducer ë¬¸ì œ ìš°íšŒ)
        self._search_results_cache: Optional[Dict[str, Any]] = None

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ì²´í¬í¬ì¸íŠ¸ ì œê±°)
        # self.checkpoint_manager = CheckpointManager(self.config.checkpoint_db_path)
        self.checkpoint_manager = None  # Checkpoint manager is disabled
        self.legal_workflow = EnhancedLegalQuestionWorkflow(self.config)

        # LangSmith í™œì„±í™” ì—¬ë¶€ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥)
        import os
        enable_langsmith = os.environ.get("ENABLE_LANGSMITH", "false").lower() == "true"

        if not enable_langsmith:
            # LangSmith ë¹„í™œì„±í™” ëª¨ë“œ (ê¸°ë³¸ê°’) - State Reductionìœ¼ë¡œ ìµœì í™”ëœ í›„ì—ë„ ê¸°ë³¸ì€ ë¹„í™œì„±í™”
            # LangSmith íŠ¸ë ˆì´ì‹± ë¹„í™œì„±í™” (ê¸´ê¸‰) - ëŒ€ìš©ëŸ‰ ìƒíƒœ ë¡œê¹… ë°©ì§€
            original_tracing = os.environ.get("LANGCHAIN_TRACING_V2")
            original_api_key = os.environ.get("LANGCHAIN_API_KEY")

            # ì„ì‹œë¡œ LangSmith ë¹„í™œì„±í™”
            os.environ["LANGCHAIN_TRACING_V2"] = "false"
            if "LANGCHAIN_API_KEY" in os.environ:
                del os.environ["LANGCHAIN_API_KEY"]

            try:
                # ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼
                self.app = self.legal_workflow.graph.compile(
                    checkpointer=None,
                    interrupt_before=None,
                    interrupt_after=None,
                    debug=False,
                )
                self.logger.info("ì›Œí¬í”Œë¡œìš°ê°€ ì²´í¬í¬ì¸íŠ¸ ì—†ì´ ì»´íŒŒì¼ë˜ì—ˆìŠµë‹ˆë‹¤ (LangSmith ë¹„í™œì„±í™”ë¨)")
            finally:
                # í™˜ê²½ ë³€ìˆ˜ ë³µì›
                if original_tracing:
                    os.environ["LANGCHAIN_TRACING_V2"] = original_tracing
                elif "LANGCHAIN_TRACING_V2" in os.environ:
                    del os.environ["LANGCHAIN_TRACING_V2"]

                if original_api_key:
                    os.environ["LANGCHAIN_API_KEY"] = original_api_key
        else:
            # LangSmith í™œì„±í™” ëª¨ë“œ (ENABLE_LANGSMITH=trueë¡œ ì„¤ì •ëœ ê²½ìš°)
            self.app = self.legal_workflow.graph.compile(
                checkpointer=None,
                interrupt_before=None,
                interrupt_after=None,
                debug=False,
            )
            self.logger.info("ì›Œí¬í”Œë¡œìš°ê°€ LangSmith ì¶”ì ìœ¼ë¡œ ì»´íŒŒì¼ë˜ì—ˆìŠµë‹ˆë‹¤ (State Reduction ì ìš©ë¨)")

        if self.app is None:
            self.logger.error("Failed to compile workflow")
            raise RuntimeError("ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

        # LangfuseClient ì´ˆê¸°í™” (ë‹µë³€ í’ˆì§ˆ ì¶”ì )
        self.langfuse_client_service = None
        if LANGFUSE_CLIENT_AVAILABLE and self.config.langfuse_enabled:
            try:
                from source.services.langfuse_client import LangfuseClient
                self.langfuse_client_service = LangfuseClient(self.config)
                if self.langfuse_client_service and self.langfuse_client_service.is_enabled():
                    self.logger.info("LangfuseClient initialized for answer quality tracking")
            except Exception as e:
                self.logger.warning(f"Failed to initialize LangfuseClient: {e}")
                self.langfuse_client_service = None

        self.logger.info("LangGraphWorkflowService initialized successfully")

    async def process_query(
        self,
        query: str,
        session_id: Optional[str] = None,
        enable_checkpoint: bool = True
    ) -> Dict[str, Any]:
        # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì´ˆê¸°í™” (ê° ì¿¼ë¦¬ë§ˆë‹¤ ìƒˆë¡œ ì‹œì‘)
        self._search_results_cache = None

        """
        ì§ˆë¬¸ ì²˜ë¦¬

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            enable_checkpoint: ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© ì—¬ë¶€

        Returns:
            Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            start_time = time.time()

            # ì„¸ì…˜ ID ìƒì„±
            if not session_id:
                session_id = str(uuid.uuid4())

            self.logger.info(f"Processing query: {query[:100]}... (session: {session_id})")
            self.logger.debug(f"process_query: query length={len(query)}, query='{query[:50]}...'")

            # ì´ˆê¸° ìƒíƒœ ì„¤ì • (flat êµ¬ì¡° ì‚¬ìš©)
            initial_state = create_initial_legal_state(query, session_id)

            # ì¤‘ìš”: initial_stateì— queryê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ë„ë¡ ê°•ì œ
            # LangGraphì— ì „ë‹¬í•˜ê¸° ì „ì— input ê·¸ë£¹ì— queryê°€ ìˆì–´ì•¼ í•¨
            if "input" not in initial_state:
                initial_state["input"] = {}
            if not initial_state["input"].get("query"):
                initial_state["input"]["query"] = query
            if not initial_state["input"].get("session_id"):
                initial_state["input"]["session_id"] = session_id

            # ìµœìƒìœ„ ë ˆë²¨ì—ë„ query í¬í•¨ (ì´ì¤‘ ë³´ì¥)
            if not initial_state.get("query"):
                initial_state["query"] = query
            if not initial_state.get("session_id"):
                initial_state["session_id"] = session_id

            # ì´ˆê¸° state ê²€ì¦
            initial_query = initial_state.get("input", {}).get("query", "") if initial_state.get("input") else initial_state.get("query", "")
            self.logger.debug(f"process_query: initial_state query length={len(initial_query)}, query='{initial_query[:50] if initial_query else 'EMPTY'}...'")
            if not initial_query or not str(initial_query).strip():
                self.logger.error(f"Initial state query is empty! Input query was: '{query[:50]}...'")
                self.logger.debug(f"process_query: ERROR - initial_state query is empty!")
                self.logger.debug(f"process_query: initial_state keys: {list(initial_state.keys())}")
                self.logger.debug(f"process_query: initial_state['input']: {initial_state.get('input')}")
            else:
                self.logger.debug(f"process_query: SUCCESS - initial_state has query with length={len(initial_query)}")

            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì„¤ì • (ì²´í¬í¬ì¸íŠ¸ ë¹„í™œì„±í™”)
            config = {}
            # if enable_checkpoint:
            #     config = {"configurable": {"thread_id": session_id}}

            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì§„í–‰ìƒí™© í‘œì‹œ)
            if self.app:
                # Recursion limit ì¦ê°€ (ì¬ì‹œë„ ë¡œì§ ê°œì„ ìœ¼ë¡œ ì¸í•´ ë” ë†’ê²Œ ì„¤ì •)
                # ì¬ì‹œë„ ìµœëŒ€ 3íšŒ + ê° ë‹¨ê³„ë³„ ë…¸ë“œ ì‹¤í–‰ì„ ê³ ë ¤í•˜ì—¬ ì—¬ìœ ìˆê²Œ ì„¤ì •
                enhanced_config = {"recursion_limit": 200}
                enhanced_config.update(config)

                # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰í•˜ì—¬ ì§„í–‰ìƒí™© í‘œì‹œ
                flat_result = None
                node_count = 0
                executed_nodes = []
                last_node_time = time.time()
                # processing_steps ì¶”ì  (state reductionìœ¼ë¡œ ì¸í•´ ì†ì‹¤ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
                tracked_processing_steps = []

                self.logger.info("ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘...")
                print("ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘...", flush=True)

                # ì´ˆê¸° state ê²€ì¦: input ê·¸ë£¹ê³¼ query í™•ì¸
                initial_query_check = initial_state.get("input", {}).get("query", "") if initial_state.get("input") else initial_state.get("query", "")
                self.logger.debug(f"astream: initial_state before astream - query='{initial_query_check[:50] if initial_query_check else 'EMPTY'}...', keys={list(initial_state.keys())}")

                # ì¤‘ìš”: initial_stateì— inputì´ ì—†ê±°ë‚˜ queryê°€ ë¹„ì–´ìˆìœ¼ë©´ ë³µì›
                # LangGraphì— ì „ë‹¬í•˜ê¸° ì „ì— ë°˜ë“œì‹œ queryê°€ ìˆì–´ì•¼ í•¨
                if not initial_query_check or not str(initial_query_check).strip():
                    self.logger.error(f"Initial state query is empty before astream! Initial state keys: {list(initial_state.keys())}")
                    if initial_state.get("input"):
                        self.logger.error(f"Initial state input: {initial_state['input']}")

                    # query íŒŒë¼ë¯¸í„°ì—ì„œ ì§ì ‘ ë³µì› (process_queryì˜ query íŒŒë¼ë¯¸í„°)
                    # í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì´ë¯¸ initial_stateë¥¼ ë°›ì•˜ìœ¼ë¯€ë¡œ, queryë¥¼ ë‹¤ì‹œ ì°¾ì•„ì•¼ í•¨
                    # ëŒ€ì‹  initial_stateë¥¼ ìˆ˜ì •í•˜ì—¬ query í¬í•¨ ë³´ì¥
                    if "input" not in initial_state:
                        initial_state["input"] = {}
                    # query íŒŒë¼ë¯¸í„°ëŠ” í•¨ìˆ˜ ì¸ìì— ìˆìœ¼ë¯€ë¡œ ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥
                    # í•˜ì§€ë§Œ ì´ë¯¸ initial_stateë¥¼ ìƒì„±í–ˆìœ¼ë¯€ë¡œ, ì›ë³¸ queryë¥¼ ì‚¬ìš©
                    # create_initial_legal_stateì—ì„œ ì´ë¯¸ ì„¤ì •í–ˆì„ ê²ƒì´ë¯€ë¡œ ë¬¸ì œ ì—†ì–´ì•¼ í•¨
                    # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë‹¤ì‹œ í™•ì¸
                    if not initial_state["input"].get("query"):
                        # ìµœìƒìœ„ ë ˆë²¨ í™•ì¸
                        if initial_state.get("query"):
                            initial_state["input"]["query"] = initial_state["query"]
                        else:
                            self.logger.error(f"CRITICAL: Cannot find query anywhere in initial_state!")

                # ì¤‘ìš”: ì´ˆê¸° input ë³´ì¡´ (ëª¨ë“  ë…¸ë“œì—ì„œ ë³µì› ê°€ëŠ¥í•˜ë„ë¡)
                if initial_state.get("input") and isinstance(initial_state["input"], dict):
                    self._initial_input = initial_state["input"].copy()
                    # queryê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
                    if not self._initial_input.get("query"):
                        # ìµœìƒìœ„ì—ì„œ ì°¾ê¸°
                        if initial_state.get("query"):
                            self._initial_input["query"] = initial_state["query"]
                elif initial_state.get("query"):
                    # nested êµ¬ì¡°ê°€ ì•„ë‹ˆë©´ flatì—ì„œ ì¶”ì¶œ
                    self._initial_input = {
                        "query": initial_state["query"],
                        "session_id": initial_state.get("session_id", "") if initial_state.get("session_id") else (initial_state.get("input", {}).get("session_id", "") if initial_state.get("input") else "")
                    }
                else:
                    self._initial_input = {"query": "", "session_id": ""}

                # ìµœì¢… í™•ì¸: initial_inputì— queryê°€ ìˆì–´ì•¼ í•¨
                if not self._initial_input.get("query"):
                    self.logger.error(f"CRITICAL: _initial_input has no query! This should never happen.")
                else:
                    self.logger.debug(f"Preserved initial input: query length={len(self._initial_input.get('query', ''))}")

                async for event in self.app.astream(initial_state, enhanced_config):
                    # ê° ì´ë²¤íŠ¸ëŠ” {node_name: updated_state} í˜•íƒœ
                    for node_name, node_state in event.items():
                        # ìƒˆë¡œ ì‹¤í–‰ëœ ë…¸ë“œì¸ ê²½ìš°ì—ë§Œ ì¹´ìš´íŠ¸
                        if node_name not in executed_nodes:
                            node_count += 1
                            executed_nodes.append(node_name)

                            # ë…¸ë“œ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
                            node_duration = time.time() - last_node_time if node_count > 1 else 0
                            last_node_time = time.time()

                            # ì§„í–‰ìƒí™© í‘œì‹œ
                            if node_count == 1:
                                progress_msg = f"  [{node_count}] ğŸ”„ ì‹¤í–‰ ì¤‘: {node_name}"
                            else:
                                progress_msg = f"  [{node_count}] ğŸ”„ ì‹¤í–‰ ì¤‘: {node_name} (ì´ì „ ë…¸ë“œ ì™„ë£Œ: {node_duration:.2f}ì´ˆ)"

                            self.logger.info(progress_msg)
                            print(progress_msg, flush=True)

                            # ë…¸ë“œ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë³€í™˜í•˜ì—¬ ë” ëª…í™•í•˜ê²Œ í‘œì‹œ
                            node_display_name = self._get_node_display_name(node_name)
                            if node_display_name != node_name:
                                detail_msg = f"      â†’ {node_display_name}"
                                self.logger.info(detail_msg)
                                print(detail_msg, flush=True)

                            # ë””ë²„ê¹…: node_stateì˜ query í™•ì¸
                            if node_name == "classify_query":
                                # ì¤‘ìš”: node_state.get("input")ì´ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                                node_input = node_state.get("input") if isinstance(node_state, dict) else None
                                node_query = ""
                                if node_input and isinstance(node_input, dict):
                                    node_query = node_input.get("query", "")
                                elif isinstance(node_state, dict):
                                    node_query = node_state.get("query", "")
                                self.logger.debug(f"astream: event[{node_name}] query='{node_query[:50] if node_query else 'EMPTY'}...', keys={list(node_state.keys()) if isinstance(node_state, dict) else 'N/A'}")

                        # processing_steps ì¶”ì  (state reductionìœ¼ë¡œ ì†ì‹¤ ë°©ì§€, ê°œì„ )
                        if isinstance(node_state, dict):
                            # 1. common ê·¸ë£¹ì—ì„œ processing_steps í™•ì¸
                            node_common = node_state.get("common", {})
                            if isinstance(node_common, dict):
                                common_steps = node_common.get("processing_steps", [])
                                if isinstance(common_steps, list) and len(common_steps) > 0:
                                    for step in common_steps:
                                        if isinstance(step, str) and step not in tracked_processing_steps:
                                            tracked_processing_steps.append(step)

                            # 2. ìµœìƒìœ„ ë ˆë²¨ì—ì„œë„ í™•ì¸
                            top_steps = node_state.get("processing_steps", [])
                            if isinstance(top_steps, list) and len(top_steps) > 0:
                                for step in top_steps:
                                    if isinstance(step, str) and step not in tracked_processing_steps:
                                        tracked_processing_steps.append(step)

                            # 3. metadataì—ì„œë„ í™•ì¸ (ê°œì„ )
                            metadata = node_state.get("metadata", {})
                            if isinstance(metadata, dict):
                                metadata_steps = metadata.get("processing_steps", [])
                                if isinstance(metadata_steps, list) and len(metadata_steps) > 0:
                                    for step in metadata_steps:
                                        if isinstance(step, str) and step not in tracked_processing_steps:
                                            tracked_processing_steps.append(step)

                            # 4. ë…¸ë“œ ì‹¤í–‰ ì •ë³´ ì¶”ê°€ (ì¶”ì  ë³´ê°•)
                            if node_name and len(node_name) < 50:
                                node_info = f"ë…¸ë“œ ì‹¤í–‰: {node_name}"
                                if node_info not in tracked_processing_steps:
                                    tracked_processing_steps.append(node_info)

                        # ìµœì¢… ê²°ê³¼ ì—…ë°ì´íŠ¸ (ê° ì´ë²¤íŠ¸ëŠ” ì—…ë°ì´íŠ¸ëœ ìƒíƒœë¥¼ ë°˜í™˜)
                        # ì¤‘ìš”: ëª¨ë“  ë…¸ë“œì˜ ê²°ê³¼ì— input ê·¸ë£¹ì´ ìˆë„ë¡ ë³´ì¥
                        # LangGraphëŠ” stateë¥¼ ë³‘í•©í•  ë•Œ TypedDictì˜ ê° í•„ë“œë¥¼ ë³‘í•©í•˜ëŠ”ë°,
                        # input í•„ë“œê°€ ì—†ìœ¼ë©´ ì´ì „ ê°’ì´ ì‚¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
                        # í•´ê²°ì±…: ë³´ì¡´ëœ ì´ˆê¸° inputì„ í•­ìƒ ë³µì›

                        # ë””ë²„ê¹…: node_stateì˜ query í™•ì¸ (ëª¨ë“  ë…¸ë“œì— ëŒ€í•´)
                        if node_name in ["classify_query", "prepare_search_query", "execute_searches_parallel"]:
                            # ì¤‘ìš”: node_state.get("input")ì´ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                            node_input = node_state.get("input") if isinstance(node_state, dict) else None
                            node_query = ""
                            if node_input and isinstance(node_input, dict):
                                node_query = node_input.get("query", "")
                            elif isinstance(node_state, dict):
                                node_query = node_state.get("query", "")
                            self.logger.debug(f"astream: event[{node_name}] - node_state query='{node_query[:50] if node_query else 'EMPTY'}...'")
                            self.logger.debug(f"astream: event[{node_name}] - node_state keys={list(node_state.keys()) if isinstance(node_state, dict) else 'N/A'}")

                            # execute_searches_parallelì˜ ê²½ìš° search ê·¸ë£¹ í™•ì¸ ë° ìºì‹œ
                            # semantic_resultsë¥¼ retrieved_docsë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                            if node_name == "execute_searches_parallel" and isinstance(node_state, dict):
                                # semantic_resultsë¥¼ retrieved_docsë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                                search_group_for_cache = node_state.get("search", {}) if isinstance(node_state.get("search"), dict) else {}
                                semantic_results_for_cache = search_group_for_cache.get("semantic_results", [])
                                keyword_results_for_cache = search_group_for_cache.get("keyword_results", [])

                                # ìµœìƒìœ„ ë ˆë²¨ì—ì„œë„ í™•ì¸
                                if not semantic_results_for_cache:
                                    semantic_results_for_cache = node_state.get("semantic_results", [])
                                if not keyword_results_for_cache:
                                    keyword_results_for_cache = node_state.get("keyword_results", [])

                                # semantic_resultsì™€ keyword_resultsë¥¼ retrieved_docsë¡œ ë³€í™˜
                                combined_docs = []
                                if isinstance(semantic_results_for_cache, list):
                                    combined_docs.extend(semantic_results_for_cache)
                                if isinstance(keyword_results_for_cache, list):
                                    combined_docs.extend(keyword_results_for_cache)

                                # ì¤‘ë³µ ì œê±° (id ê¸°ë°˜)
                                seen_ids = set()
                                unique_docs = []
                                for doc in combined_docs:
                                    doc_id = doc.get("id") or doc.get("content_id") or str(doc.get("content", ""))[:100]
                                    if doc_id not in seen_ids:
                                        seen_ids.add(doc_id)
                                        unique_docs.append(doc)

                                # retrieved_docsë¥¼ ìºì‹œì— ì €ì¥
                                if unique_docs:
                                    if not self._search_results_cache:
                                        self._search_results_cache = {}
                                    self._search_results_cache["retrieved_docs"] = unique_docs
                                    self._search_results_cache["merged_documents"] = unique_docs
                                    if "search" not in self._search_results_cache:
                                        self._search_results_cache["search"] = {}
                                    self._search_results_cache["search"]["retrieved_docs"] = unique_docs
                                    self._search_results_cache["search"]["merged_documents"] = unique_docs
                                    self.logger.debug(f"astream: Converted semantic_results to retrieved_docs: {len(unique_docs)} docs")
                                search_group = node_state.get("search", {}) if isinstance(node_state.get("search"), dict) else {}
                                semantic_count = len(search_group.get("semantic_results", []))
                                keyword_count = len(search_group.get("keyword_results", []))

                                # ìµœìƒìœ„ ë ˆë²¨ì—ì„œë„ í™•ì¸ (node_wrappersì—ì„œ ì¶”ê°€í–ˆì„ ìˆ˜ ìˆìŒ)
                                top_semantic = node_state.get("semantic_results", [])
                                top_keyword = node_state.get("keyword_results", [])
                                if isinstance(top_semantic, list):
                                    semantic_count = max(semantic_count, len(top_semantic))
                                if isinstance(top_keyword, list):
                                    keyword_count = max(keyword_count, len(top_keyword))

                                self.logger.debug(f"astream: event[{node_name}] - search group: semantic_results={semantic_count}, keyword_results={keyword_count}, top_level_semantic={len(top_semantic) if isinstance(top_semantic, list) else 0}")

                                # search ê·¸ë£¹ ë˜ëŠ” ìµœìƒìœ„ ë ˆë²¨ì— ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìºì‹œì— ì €ì¥
                                if (semantic_count > 0 or keyword_count > 0):
                                    # search ê·¸ë£¹ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„ , ì—†ìœ¼ë©´ ìµœìƒìœ„ ë ˆë²¨ ê°’ìœ¼ë¡œ êµ¬ì„±
                                    if search_group and (len(search_group.get("semantic_results", [])) > 0 or len(search_group.get("keyword_results", [])) > 0):
                                        self._search_results_cache = search_group.copy()
                                    elif isinstance(top_semantic, list) or isinstance(top_keyword, list):
                                        # ìµœìƒìœ„ ë ˆë²¨ì—ì„œ ìºì‹œ êµ¬ì„±
                                        self._search_results_cache = {
                                            "semantic_results": top_semantic if isinstance(top_semantic, list) else [],
                                            "keyword_results": top_keyword if isinstance(top_keyword, list) else [],
                                            "semantic_count": len(top_semantic) if isinstance(top_semantic, list) else 0,
                                            "keyword_count": len(top_keyword) if isinstance(top_keyword, list) else 0
                                        }
                                    self.logger.debug(f"astream: Cached search results - semantic={semantic_count}, keyword={keyword_count}")
                                # search ê·¸ë£¹ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ìºì‹œì—ì„œ ë³µì› ì‹œë„
                                elif self._search_results_cache:
                                    self.logger.debug(f"astream: Restoring search results from cache")
                                    if "search" not in node_state:
                                        node_state["search"] = {}
                                    node_state["search"].update(self._search_results_cache)
                                    # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì¶”ê°€
                                    if "semantic_results" not in node_state:
                                        node_state["semantic_results"] = self._search_results_cache.get("semantic_results", [])
                                    if "keyword_results" not in node_state:
                                        node_state["keyword_results"] = self._search_results_cache.get("keyword_results", [])
                                    semantic_restored = len(node_state["search"].get("semantic_results", []))
                                    keyword_restored = len(node_state["search"].get("keyword_results", []))
                                    self.logger.debug(f"astream: Restored search results - semantic={semantic_restored}, keyword={keyword_restored}")

                        if isinstance(node_state, dict) and self._initial_input:
                            # ì¤‘ìš”: node_state.get("input")ì´ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                            node_input = node_state.get("input")
                            node_has_input = node_input is not None and isinstance(node_input, dict)
                            node_has_query = node_has_input and bool(node_input.get("query"))

                            # node_stateì— inputì´ ì—†ê±°ë‚˜ queryê°€ ì—†ìœ¼ë©´ ë³´ì¡´ëœ ì´ˆê¸° inputì—ì„œ ë³µì›
                            if not node_has_input or not node_has_query:
                                if self._initial_input.get("query"):
                                    if "input" not in node_state or not isinstance(node_state.get("input"), dict):
                                        node_state["input"] = {}
                                    node_state["input"]["query"] = self._initial_input["query"]
                                    if self._initial_input.get("session_id"):
                                        node_state["input"]["session_id"] = self._initial_input["session_id"]
                                    if node_name == "classify_query":
                                        self.logger.debug(f"astream: Restored query from preserved initial_input for {node_name}: '{self._initial_input['query'][:50]}...'")

                            # ëª¨ë“  ë…¸ë“œ ê²°ê³¼ì— í•­ìƒ input ê·¸ë£¹ í¬í•¨ (LangGraph ë³‘í•© ë³´ì¥)
                            # ì´ˆê¸° inputì´ ìˆìœ¼ë©´ í•­ìƒ í¬í•¨
                            node_input_check = node_state.get("input")
                            if node_input_check is None or not isinstance(node_input_check, dict):
                                node_state["input"] = self._initial_input.copy()
                            elif not node_input_check.get("query") and self._initial_input.get("query"):
                                # queryê°€ ë¹„ì–´ìˆìœ¼ë©´ ë³µì›
                                node_state["input"]["query"] = self._initial_input["query"]
                                if self._initial_input.get("session_id"):
                                    node_state["input"]["session_id"] = self._initial_input["session_id"]

                        # ì¤‘ìš”: merge_and_rerank_with_keyword_weights ì´í›„ retrieved_docs ìºì‹œ ì—…ë°ì´íŠ¸
                        if node_name == "merge_and_rerank_with_keyword_weights" and isinstance(node_state, dict):
                            search_group = node_state.get("search", {}) if isinstance(node_state.get("search"), dict) else {}
                            retrieved_docs = search_group.get("retrieved_docs", [])
                            merged_documents = search_group.get("merged_documents", [])

                            # ìµœìƒìœ„ ë ˆë²¨ì—ì„œë„ í™•ì¸
                            top_retrieved_docs = node_state.get("retrieved_docs", [])
                            top_merged_docs = node_state.get("merged_documents", [])

                            # retrieved_docs ë˜ëŠ” merged_documentsê°€ ìˆìœ¼ë©´ ìºì‹œ ì—…ë°ì´íŠ¸
                            final_retrieved_docs = (retrieved_docs if isinstance(retrieved_docs, list) and len(retrieved_docs) > 0 else
                                                   top_retrieved_docs if isinstance(top_retrieved_docs, list) and len(top_retrieved_docs) > 0 else
                                                   merged_documents if isinstance(merged_documents, list) and len(merged_documents) > 0 else
                                                   top_merged_docs if isinstance(top_merged_docs, list) and len(top_merged_docs) > 0 else [])

                            if isinstance(final_retrieved_docs, list) and len(final_retrieved_docs) > 0:
                                # ìºì‹œ ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
                                if not self._search_results_cache:
                                    self._search_results_cache = {}

                                # retrieved_docsì™€ merged_documentsë¥¼ ìºì‹œì— ì €ì¥
                                self._search_results_cache["retrieved_docs"] = final_retrieved_docs
                                self._search_results_cache["merged_documents"] = final_retrieved_docs
                                # search ê·¸ë£¹ ì „ì²´ë¥¼ ìºì‹œì— ì €ì¥ (ë‚˜ì¤‘ì— ë³µì›í•  ë•Œ ì‚¬ìš©)
                                if search_group:
                                    self._search_results_cache.update(search_group)
                                else:
                                    # search ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ìƒì„±í•˜ì—¬ ì €ì¥
                                    if "search" not in self._search_results_cache:
                                        self._search_results_cache["search"] = {}
                                    self._search_results_cache["search"]["retrieved_docs"] = final_retrieved_docs
                                    self._search_results_cache["search"]["merged_documents"] = final_retrieved_docs

                                self.logger.debug(f"astream: Updated cache with retrieved_docs={len(final_retrieved_docs)}, cache has search group={bool(self._search_results_cache.get('search'))}")

                        # ì¤‘ìš”: execute_searches_parallel ì´í›„ ë…¸ë“œë“¤ì— ëŒ€í•´ ìºì‹œëœ search ê²°ê³¼ ë³µì›
                        # LangGraph reducerê°€ search ê·¸ë£¹ì„ ì œê±°í•˜ëŠ” ë¬¸ì œë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•´ ìºì‹œì—ì„œ ë³µì›
                        if node_name in ["merge_and_rerank_with_keyword_weights", "filter_and_validate_results", "update_search_metadata", "prepare_document_context_for_prompt"]:
                            if self._search_results_cache and isinstance(node_state, dict):
                                # node_stateì— search ê·¸ë£¹ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ìºì‹œì—ì„œ ë³µì›
                                search_group = node_state.get("search", {}) if isinstance(node_state.get("search"), dict) else {}
                                top_semantic = node_state.get("semantic_results", [])
                                top_keyword = node_state.get("keyword_results", [])
                                has_results = (len(search_group.get("semantic_results", [])) > 0 or
                                             len(search_group.get("keyword_results", [])) > 0 or
                                             (isinstance(top_semantic, list) and len(top_semantic) > 0) or
                                             (isinstance(top_keyword, list) and len(top_keyword) > 0))

                                if not has_results:
                                    self.logger.debug(f"astream: Restoring search results for {node_name} from cache")
                                    if "search" not in node_state:
                                        node_state["search"] = {}
                                    node_state["search"].update(self._search_results_cache)
                                    # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì¶”ê°€ (flat êµ¬ì¡° í˜¸í™˜)
                                    if "semantic_results" not in node_state:
                                        node_state["semantic_results"] = self._search_results_cache.get("semantic_results", [])
                                    if "keyword_results" not in node_state:
                                        node_state["keyword_results"] = self._search_results_cache.get("keyword_results", [])
                                    if "semantic_count" not in node_state:
                                        node_state["semantic_count"] = self._search_results_cache.get("semantic_count", 0)
                                    if "keyword_count" not in node_state:
                                        node_state["keyword_count"] = self._search_results_cache.get("keyword_count", 0)
                                    semantic_restored = len(node_state["search"].get("semantic_results", []))
                                    keyword_restored = len(node_state["search"].get("keyword_results", []))
                                    self.logger.debug(f"astream: Restored for {node_name} - semantic={semantic_restored}, keyword={keyword_restored}, top_level_semantic={len(node_state.get('semantic_results', []))}")

                        # ì¤‘ìš”: merge_and_rerank_with_keyword_weights ë˜ëŠ” process_search_results_combined ì´í›„ retrieved_docs ìºì‹œ ì—…ë°ì´íŠ¸
                        # flat_result ì—…ë°ì´íŠ¸ ì „ì— ìºì‹œ ì—…ë°ì´íŠ¸ (node_stateì—ì„œ ì§ì ‘ ì½ê¸°)
                        if node_name in ["merge_and_rerank_with_keyword_weights", "process_search_results_combined"] and isinstance(node_state, dict):
                            self.logger.debug(f"astream: Checking merge_and_rerank node_state for retrieved_docs")
                            # node_state ì—…ë°ì´íŠ¸ í›„ ë‹¤ì‹œ ì½ê¸°
                            search_group_updated = node_state.get("search", {}) if isinstance(node_state.get("search"), dict) else {}
                            retrieved_docs_updated = search_group_updated.get("retrieved_docs", [])
                            merged_documents_updated = search_group_updated.get("merged_documents", [])

                            # ìµœìƒìœ„ ë ˆë²¨ì—ì„œë„ í™•ì¸
                            top_retrieved_docs_updated = node_state.get("retrieved_docs", [])
                            top_merged_docs_updated = node_state.get("merged_documents", [])

                            self.logger.debug(f"astream: merge_and_rerank - search_group retrieved_docs={len(retrieved_docs_updated) if isinstance(retrieved_docs_updated, list) else 0}, merged_documents={len(merged_documents_updated) if isinstance(merged_documents_updated, list) else 0}, top_retrieved_docs={len(top_retrieved_docs_updated) if isinstance(top_retrieved_docs_updated, list) else 0}, top_merged_docs={len(top_merged_docs_updated) if isinstance(top_merged_docs_updated, list) else 0}")

                            # retrieved_docs ë˜ëŠ” merged_documentsê°€ ìˆìœ¼ë©´ ìºì‹œ ì—…ë°ì´íŠ¸
                            final_retrieved_docs = (retrieved_docs_updated if isinstance(retrieved_docs_updated, list) and len(retrieved_docs_updated) > 0 else
                                                   top_retrieved_docs_updated if isinstance(top_retrieved_docs_updated, list) and len(top_retrieved_docs_updated) > 0 else
                                                   merged_documents_updated if isinstance(merged_documents_updated, list) and len(merged_documents_updated) > 0 else
                                                   top_merged_docs_updated if isinstance(top_merged_docs_updated, list) and len(top_merged_docs_updated) > 0 else [])

                            self.logger.debug(f"astream: merge_and_rerank - final_retrieved_docs={len(final_retrieved_docs) if isinstance(final_retrieved_docs, list) else 0}")

                            if isinstance(final_retrieved_docs, list) and len(final_retrieved_docs) > 0:
                                # ìºì‹œ ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
                                if not self._search_results_cache:
                                    self._search_results_cache = {}

                                # retrieved_docsì™€ merged_documentsë¥¼ ìºì‹œì— ì €ì¥
                                self._search_results_cache["retrieved_docs"] = final_retrieved_docs
                                self._search_results_cache["merged_documents"] = final_retrieved_docs
                                # search ê·¸ë£¹ ì „ì²´ë¥¼ ìºì‹œì— ì €ì¥ (ë‚˜ì¤‘ì— ë³µì›í•  ë•Œ ì‚¬ìš©)
                                if search_group_updated:
                                    self._search_results_cache.update(search_group_updated)
                                else:
                                    # search ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ìƒì„±í•˜ì—¬ ì €ì¥
                                    if "search" not in self._search_results_cache:
                                        self._search_results_cache["search"] = {}
                                    self._search_results_cache["search"]["retrieved_docs"] = final_retrieved_docs
                                    self._search_results_cache["search"]["merged_documents"] = final_retrieved_docs

                                self.logger.debug(f"astream: Updated cache with retrieved_docs={len(final_retrieved_docs)}, cache has search group={bool(self._search_results_cache.get('search'))}, cache keys={list(self._search_results_cache.keys())}")
                            else:
                                self.logger.warning(f"astream: merge_and_rerank node_state has no retrieved_docs or merged_documents")

                        flat_result = node_state

                # ëª¨ë“  ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ í‘œì‹œ
                total_nodes = len(executed_nodes)
                self.logger.info(f"âœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ (ì´ {total_nodes}ê°œ ë…¸ë“œ ì‹¤í–‰)")
                print(f"âœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ (ì´ {total_nodes}ê°œ ë…¸ë“œ ì‹¤í–‰)", flush=True)

                # ì‹¤í–‰ëœ ë…¸ë“œ ëª©ë¡ í‘œì‹œ
                if total_nodes > 0:
                    nodes_list = ", ".join(executed_nodes[:5])
                    if total_nodes > 5:
                        nodes_list += f" ì™¸ {total_nodes - 5}ê°œ"
                    self.logger.info(f"  ì‹¤í–‰ëœ ë…¸ë“œ: {nodes_list}")
                    print(f"  ì‹¤í–‰ëœ ë…¸ë“œ: {nodes_list}", flush=True)

                # ìµœì¢… ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì´ˆê¸° ìƒíƒœ ì‚¬ìš©
                if flat_result is None:
                    flat_result = initial_state

                # processing_stepsë¥¼ flat_resultì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥ (state reduction ì†ì‹¤ ë°©ì§€, ê°œì„ )
                if isinstance(flat_result, dict):
                    if tracked_processing_steps and len(tracked_processing_steps) > 0:
                        # common ê·¸ë£¹ì— ì €ì¥
                        if "common" not in flat_result:
                            flat_result["common"] = {}
                        if not isinstance(flat_result["common"], dict):
                            flat_result["common"] = {}
                        flat_result["common"]["processing_steps"] = tracked_processing_steps
                        # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì €ì¥ (fallback)
                        flat_result["processing_steps"] = tracked_processing_steps
                    else:
                        # tracked_processing_stepsê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì¶”ê°€ (ê°œì„ )
                        default_steps = ["ì›Œí¬í”Œë¡œìš° ì‹œì‘", "ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"]
                        if "common" not in flat_result:
                            flat_result["common"] = {}
                        if not isinstance(flat_result["common"], dict):
                            flat_result["common"] = {}
                        flat_result["common"]["processing_steps"] = default_steps
                        flat_result["processing_steps"] = default_steps

                # ì¤‘ìš”: ìµœì¢… ê²°ê³¼ì— search ê·¸ë£¹ ë³´ì¡´
                # LangGraph reducerê°€ search ê·¸ë£¹ì„ ì œê±°í–ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìºì‹œì—ì„œ ë³µì›
                # ì „ì—­ ìºì‹œì™€ ì¸ìŠ¤í„´ìŠ¤ ìºì‹œ ëª¨ë‘ í™•ì¸
                if isinstance(flat_result, dict):
                    # ì „ì—­ ìºì‹œì—ì„œë„ í™•ì¸ (node_wrappersì—ì„œ ì €ì¥í•œ ê²ƒ)
                    try:
                        import sys
                        node_wrappers_module = sys.modules.get('core.agents.node_wrappers')
                        if node_wrappers_module:
                            global_cache = getattr(node_wrappers_module, '_global_search_results_cache', None)
                        else:
                            global_cache = None
                    except (ImportError, AttributeError) as e:
                        global_cache = None
                        self.logger.debug(f"Failed to import global cache: {e}")

                    self.logger.debug(f"Final result check - has instance cache={self._search_results_cache is not None}, has global cache={global_cache is not None}")
                    if global_cache:
                        self.logger.debug(f"Global cache keys={list(global_cache.keys()) if isinstance(global_cache, dict) else 'N/A'}")
                        if isinstance(global_cache, dict):
                            if "search" in global_cache:
                                search_group_cache = global_cache["search"]
                                if isinstance(search_group_cache, dict):
                                    self.logger.debug(f"Global cache search group has retrieved_docs={len(search_group_cache.get('retrieved_docs', []))}, merged_documents={len(search_group_cache.get('merged_documents', []))}")
                            # ìµœìƒìœ„ ë ˆë²¨ì—ì„œë„ í™•ì¸
                            if "retrieved_docs" in global_cache:
                                self.logger.debug(f"Global cache top-level has retrieved_docs={len(global_cache.get('retrieved_docs', []))}")

                    # ì „ì—­ ìºì‹œ ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ìºì‹œ ì‚¬ìš© (ì „ì—­ ìºì‹œ ìš°ì„ )
                    search_cache = global_cache if global_cache else self._search_results_cache

                    if search_cache:
                        # search ê·¸ë£¹ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ìºì‹œì—ì„œ ë³µì›
                        if "search" not in flat_result or not isinstance(flat_result.get("search"), dict):
                            self.logger.debug(f"Final result has no search group, creating from cache")
                            flat_result["search"] = {}

                        search_group = flat_result["search"]
                        has_results = (len(search_group.get("retrieved_docs", [])) > 0 or
                                     len(search_group.get("merged_documents", [])) > 0 or
                                     len(flat_result.get("retrieved_docs", [])) > 0)

                        if not has_results:
                            self.logger.debug(f"Restoring search group in final result from cache")
                            # ìºì‹œì— search ê·¸ë£¹ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©
                            if "search" in search_cache and isinstance(search_cache["search"], dict):
                                flat_result["search"].update(search_cache["search"])
                            else:
                                # ìºì‹œì˜ ìµœìƒìœ„ retrieved_docs/merged_documents ì‚¬ìš©
                                flat_result["search"].update({
                                    "retrieved_docs": search_cache.get("retrieved_docs", []),
                                    "merged_documents": search_cache.get("merged_documents", [])
                                })

                            # retrieved_docsê°€ ì—†ìœ¼ë©´ merged_documents ì‚¬ìš©
                            if "retrieved_docs" not in flat_result["search"] or len(flat_result["search"].get("retrieved_docs", [])) == 0:
                                if "merged_documents" in flat_result["search"] and len(flat_result["search"].get("merged_documents", [])) > 0:
                                    flat_result["search"]["retrieved_docs"] = flat_result["search"]["merged_documents"]

                            # ìµœìƒìœ„ ë ˆë²¨ì—ë„ ì¶”ê°€
                            if "retrieved_docs" not in flat_result:
                                flat_result["retrieved_docs"] = flat_result["search"].get("retrieved_docs", [])
                            if not flat_result["retrieved_docs"]:
                                flat_result["retrieved_docs"] = flat_result["search"].get("merged_documents", [])

                            restored_count = len(flat_result["search"].get("retrieved_docs", []))
                            merged_count = len(flat_result["search"].get("merged_documents", []))
                            self.logger.debug(f"Restored search group - retrieved_docs={restored_count}, merged_documents={merged_count}, top_level={len(flat_result.get('retrieved_docs', []))}")
                        else:
                            self.logger.debug(f"Final result already has search results - retrieved_docs={len(search_group.get('retrieved_docs', []))}, top_level={len(flat_result.get('retrieved_docs', []))}")
                    else:
                        self.logger.warning(f"No search cache available for final result restoration")

                # ì¤‘ìš”: query_complexityì™€ needs_search ë³µì› (Adaptive RAG ì •ë³´)
                # prepare_final_responseì—ì„œ ë³´ì¡´í–ˆì§€ë§Œ, reducerì— ì˜í•´ ì‚¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¬í™•ì¸
                if isinstance(flat_result, dict):
                    self.logger.debug(f"flat_result keys={list(flat_result.keys())[:20]}")

                    # ëª¨ë“  ê°€ëŠ¥í•œ ê²½ë¡œì—ì„œ í™•ì¸
                    query_complexity_found = None
                    needs_search_found = True

                    # 1. ìµœìƒìœ„ ë ˆë²¨ ì§ì ‘ í™•ì¸
                    query_complexity_found = flat_result.get("query_complexity")
                    if "needs_search" in flat_result:
                        needs_search_found = flat_result.get("needs_search", True)
                    self.logger.debug(f"[1] ìµœìƒìœ„ ë ˆë²¨ - complexity={query_complexity_found}, needs_search={needs_search_found}")

                    # 2. common ê·¸ë£¹ í™•ì¸
                    if not query_complexity_found:
                        has_common = "common" in flat_result
                        self.logger.debug(f"checking common - exists={has_common}")
                        if has_common:
                            common_value = flat_result["common"]
                            self.logger.debug(f"common type={type(common_value).__name__}")
                            if isinstance(common_value, dict):
                                query_complexity_found = common_value.get("query_complexity")
                                if "needs_search" in common_value:
                                    needs_search_found = common_value.get("needs_search", True)
                                self.logger.debug(f"[2] common ê·¸ë£¹ - complexity={query_complexity_found}, needs_search={needs_search_found}")
                            else:
                                self.logger.debug(f"common is not dict: {type(common_value)}")

                    # 3. metadata í™•ì¸ (ì—¬ëŸ¬ í˜•íƒœ ì§€ì›)
                    if not query_complexity_found and "metadata" in flat_result:
                        metadata_value = flat_result["metadata"]
                        self.logger.debug(f"checking metadata - type={type(metadata_value).__name__}")
                        if isinstance(metadata_value, dict):
                            query_complexity_found = metadata_value.get("query_complexity")
                            if "needs_search" in metadata_value:
                                needs_search_found = metadata_value.get("needs_search", True)
                            self.logger.debug(f"[3] metadata (dict) - complexity={query_complexity_found}, needs_search={needs_search_found}")
                        # metadataê°€ ë‹¤ë¥¸ í˜•íƒœì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ í™•ì¸

                    # 4. classification ê·¸ë£¹ í™•ì¸
                    if not query_complexity_found and "classification" in flat_result:
                        classification_value = flat_result["classification"]
                        if isinstance(classification_value, dict):
                            query_complexity_found = classification_value.get("query_complexity")
                            if "needs_search" in classification_value:
                                needs_search_found = classification_value.get("needs_search", True)
                            self.logger.debug(f"[4] classification ê·¸ë£¹ - complexity={query_complexity_found}, needs_search={needs_search_found}")

                    # 5. Global cacheì—ì„œ í™•ì¸ (classify_complexityì—ì„œ ì €ì¥í•œ ê°’)
                    if not query_complexity_found:
                        try:
                            from core.agents import node_wrappers
                            global_cache = getattr(node_wrappers, '_global_search_results_cache', None)
                            self.logger.debug(f"[5] Global cache í™•ì¸ - exists={global_cache is not None}, type={type(global_cache).__name__ if global_cache else 'None'}")
                            if global_cache and isinstance(global_cache, dict):
                                query_complexity_found = global_cache.get("query_complexity")
                                if "needs_search" in global_cache:
                                    needs_search_found = global_cache.get("needs_search", True)
                                self.logger.debug(f"[5] Global cache ë‚´ìš© - complexity={query_complexity_found}, needs_search={needs_search_found}")
                                self.logger.debug(f"[5] Global cache ì „ì²´ keys={list(global_cache.keys())[:10]}")
                                if query_complexity_found:
                                    self.logger.debug(f"[5] âœ… Global cacheì—ì„œ ì°¾ìŒ - complexity={query_complexity_found}, needs_search={needs_search_found}")
                            elif global_cache is None:
                                self.logger.debug(f"[5] Global cache is None")
                            else:
                                self.logger.debug(f"[5] Global cache is not dict: {type(global_cache)}")
                        except Exception as e:
                            self.logger.debug(f"[5] Global cache í™•ì¸ ì‹¤íŒ¨: {e}")
                            import traceback
                            self.logger.debug(f"[5] Exception details: {traceback.format_exc()}")

                    # 6. ì „ì²´ state ì¬ê·€ ê²€ìƒ‰ (ë§ˆì§€ë§‰ ì‹œë„)
                    if not query_complexity_found:
                        self.logger.debug(f"[6] ì¬ê·€ ê²€ìƒ‰ ì‹œì‘...")
                        def find_in_dict(d, depth=0):
                            if depth > 3:  # ìµœëŒ€ ê¹Šì´ ì œí•œ
                                return None, None
                            for k, v in d.items() if isinstance(d, dict) else []:
                                if k == "query_complexity":
                                    return v, d.get("needs_search", True)
                                elif isinstance(v, dict):
                                    found_c, found_n = find_in_dict(v, depth+1)
                                    if found_c:
                                        return found_c, found_n
                            return None, None

                        found_c, found_n = find_in_dict(flat_result)
                        if found_c:
                            query_complexity_found = found_c
                            needs_search_found = found_n if found_n is not None else True
                            self.logger.debug(f"[6] ì¬ê·€ ê²€ìƒ‰ìœ¼ë¡œ ì°¾ìŒ - complexity={query_complexity_found}, needs_search={needs_search_found}")

                    # ì°¾ì€ ê°’ì„ ìµœìƒìœ„ ë ˆë²¨ì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
                    if query_complexity_found:
                        flat_result["query_complexity"] = query_complexity_found
                        flat_result["needs_search"] = needs_search_found if needs_search_found is not None else True
                        # commonê³¼ metadataì—ë„ ì €ì¥ (ë‹¤ìŒ ë…¸ë“œì—ì„œ ì‚¬ìš©)
                        if "common" not in flat_result:
                            flat_result["common"] = {}
                        flat_result["common"]["query_complexity"] = query_complexity_found
                        flat_result["common"]["needs_search"] = needs_search_found
                        if "metadata" not in flat_result:
                            flat_result["metadata"] = {}
                        if isinstance(flat_result["metadata"], dict):
                            flat_result["metadata"]["query_complexity"] = query_complexity_found
                            flat_result["metadata"]["needs_search"] = needs_search_found
                        self.logger.debug(f"âœ… query_complexity ë³µì› ì™„ë£Œ - {query_complexity_found}, needs_search={flat_result.get('needs_search')}")
                    else:
                        self.logger.debug(f"âŒ query_complexityë¥¼ ì°¾ì§€ ëª»í•¨ (ëª¨ë“  ê²½ë¡œ í™•ì¸ ì™„ë£Œ)")

                # ìµœì¢… ê²°ê³¼ì— queryê°€ ì—†ìœ¼ë©´ ë³´ì¡´ëœ ì´ˆê¸° inputì—ì„œ ë³µì›
                if isinstance(flat_result, dict) and self._initial_input:
                    # ì¤‘ìš”: flat_result.get("input")ì´ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    flat_input = flat_result.get("input")
                    final_query = ""
                    if flat_input and isinstance(flat_input, dict):
                        final_query = flat_input.get("query", "")
                    elif not final_query:
                        final_query = flat_result.get("query", "")

                    if not final_query or not str(final_query).strip():
                        if self._initial_input.get("query"):
                            if "input" not in flat_result or not isinstance(flat_result.get("input"), dict):
                                flat_result["input"] = {}
                            flat_result["input"]["query"] = self._initial_input["query"]
                            if self._initial_input.get("session_id"):
                                flat_result["input"]["session_id"] = self._initial_input["session_id"]
                            self.logger.warning(f"Restored query from preserved initial_input in final result")

            else:
                raise RuntimeError("ì›Œí¬í”Œë¡œìš°ê°€ ì»´íŒŒì¼ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time

            # ê²°ê³¼ í¬ë§·íŒ…
            # ì¤‘ìš”: flat_resultê°€ Noneì´ê±°ë‚˜ dictê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            if not isinstance(flat_result, dict):
                self.logger.error(f"flat_result is not a dict: {type(flat_result).__name__}, using empty dict")
                flat_result = {}

            # nested êµ¬ì¡°ì—ì„œ flat êµ¬ì¡°ë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
            # ì˜ˆ: flat_result["input"]["query"] -> flat_result["query"]
            if isinstance(flat_result, dict) and "input" in flat_result and isinstance(flat_result["input"], dict):
                if "query" not in flat_result and flat_result["input"].get("query"):
                    flat_result["query"] = flat_result["input"]["query"]

            # query_complexityì™€ needs_search ì¶”ì¶œ (Adaptive RAG ì •ë³´)
            query_complexity = None
            needs_search = True
            if isinstance(flat_result, dict):
                # ìš°ì„ ìˆœìœ„: ìµœìƒìœ„ ë ˆë²¨ > common ê·¸ë£¹ > metadata > classification ê·¸ë£¹

                # 1. ìµœìƒìœ„ ë ˆë²¨ ì§ì ‘ í™•ì¸
                query_complexity = flat_result.get("query_complexity")
                if "needs_search" in flat_result:
                    needs_search = flat_result.get("needs_search", True)

                # 2. common ê·¸ë£¹ í™•ì¸ (reducerê°€ ë³´ì¡´í•˜ëŠ” ê·¸ë£¹)
                if not query_complexity and "common" in flat_result:
                    if isinstance(flat_result["common"], dict):
                        query_complexity = flat_result["common"].get("query_complexity")
                        if "needs_search" in flat_result["common"]:
                            needs_search = flat_result["common"].get("needs_search", True)

                # 3. metadataì—ì„œ í™•ì¸
                if not query_complexity:
                    metadata = flat_result.get("metadata", {})
                    if isinstance(metadata, dict):
                        query_complexity = metadata.get("query_complexity")
                        if "needs_search" in metadata:
                            needs_search = metadata.get("needs_search", True)

                # 4. classification ê·¸ë£¹ì—ì„œ í™•ì¸
                if not query_complexity and "classification" in flat_result:
                    if isinstance(flat_result["classification"], dict):
                        query_complexity = flat_result["classification"].get("query_complexity")
                        if "needs_search" in flat_result["classification"]:
                            needs_search = flat_result["classification"].get("needs_search", True)

            # processing_steps ì¶”ì¶œ (ìµœìƒìœ„ ë ˆë²¨, common ê·¸ë£¹, metadata, ë˜ëŠ” ë…¸ë“œ ì‹¤í–‰ ìƒíƒœì—ì„œ)
            processing_steps = []
            if isinstance(flat_result, dict):
                # 1. ìµœìƒìœ„ ë ˆë²¨ì—ì„œ í™•ì¸
                processing_steps = flat_result.get("processing_steps", [])

                # 2. common ê·¸ë£¹ì—ì„œ í™•ì¸
                if (not processing_steps or (isinstance(processing_steps, list) and len(processing_steps) == 0)) and "common" in flat_result:
                    if isinstance(flat_result["common"], dict):
                        common_steps = flat_result["common"].get("processing_steps", [])
                        if isinstance(common_steps, list) and len(common_steps) > 0:
                            processing_steps = common_steps

                # 3. metadataì—ì„œ í™•ì¸
                if (not processing_steps or (isinstance(processing_steps, list) and len(processing_steps) == 0)) and "metadata" in flat_result:
                    if isinstance(flat_result["metadata"], dict):
                        metadata_steps = flat_result["metadata"].get("processing_steps", [])
                        if isinstance(metadata_steps, list) and len(metadata_steps) > 0:
                            processing_steps = metadata_steps

                # 4. ì „ì—­ ìºì‹œì—ì„œ processing_steps í™•ì¸ (node_wrappersì—ì„œ ì €ì¥í•œ ê²ƒ)
                if (not processing_steps or (isinstance(processing_steps, list) and len(processing_steps) == 0)):
                    try:
                        from core.agents.node_wrappers import (
                            _global_search_results_cache,
                        )
                        if _global_search_results_cache and "processing_steps" in _global_search_results_cache:
                            cached_steps = _global_search_results_cache["processing_steps"]
                            if isinstance(cached_steps, list) and len(cached_steps) > 0:
                                processing_steps = cached_steps
                                self.logger.debug(f"Restored {len(processing_steps)} processing_steps from global cache")
                    except (ImportError, AttributeError, TypeError):
                        pass

                # 5. ì¶”ì ëœ processing_steps ì‚¬ìš© (ìµœí›„ì˜ ìˆ˜ë‹¨)
                if (not processing_steps or (isinstance(processing_steps, list) and len(processing_steps) == 0)):
                    if tracked_processing_steps:
                        processing_steps = tracked_processing_steps
                        self.logger.debug(f"Using {len(processing_steps)} tracked processing_steps")

                if not isinstance(processing_steps, list):
                    processing_steps = []

            response = {
                "answer": flat_result.get("answer", "") if isinstance(flat_result, dict) else "",
                "sources": flat_result.get("sources", []) if isinstance(flat_result, dict) else [],
                "confidence": flat_result.get("confidence", 0.0) if isinstance(flat_result, dict) else 0.0,
                "legal_references": flat_result.get("legal_references", []) if isinstance(flat_result, dict) else [],
                "processing_steps": processing_steps,
                "session_id": session_id,
                "processing_time": processing_time,
                "query_type": flat_result.get("query_type", "") if isinstance(flat_result, dict) else "",
                "metadata": flat_result.get("metadata", {}) if isinstance(flat_result, dict) else {},
                "errors": flat_result.get("errors", []) if isinstance(flat_result, dict) else [],
                # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œë“¤
                "legal_field": flat_result.get("legal_field", "unknown") if isinstance(flat_result, dict) else "unknown",
                "legal_domain": flat_result.get("legal_domain", "unknown") if isinstance(flat_result, dict) else "unknown",
                "urgency_level": flat_result.get("urgency_level", "unknown") if isinstance(flat_result, dict) else "unknown",
                "urgency_reasoning": flat_result.get("urgency_reasoning", "") if isinstance(flat_result, dict) else "",
                "emergency_type": flat_result.get("emergency_type", None) if isinstance(flat_result, dict) else None,
                "complexity_level": flat_result.get("complexity_level", "unknown") if isinstance(flat_result, dict) else "unknown",
                "requires_expert": flat_result.get("requires_expert", False) if isinstance(flat_result, dict) else False,
                "expert_subgraph": flat_result.get("expert_subgraph", None) if isinstance(flat_result, dict) else None,
                "legal_validity_check": flat_result.get("legal_validity_check", True) if isinstance(flat_result, dict) else True,
                "document_type": flat_result.get("document_type", None) if isinstance(flat_result, dict) else None,
                "document_analysis": flat_result.get("document_analysis", None) if isinstance(flat_result, dict) else None,
                # âœ¨ Adaptive RAG í•„ë“œ ì¶”ê°€
                "query_complexity": query_complexity if query_complexity else "unknown",
                "needs_search": needs_search,
                # retrieved_docsëŠ” search ê·¸ë£¹ ë˜ëŠ” ìµœìƒìœ„ ë ˆë²¨ì— ìˆì„ ìˆ˜ ìˆìŒ
                "retrieved_docs": self._extract_retrieved_docs_from_result(flat_result)
            }

            # Langfuseì— ë‹µë³€ í’ˆì§ˆ ì¶”ì 
            if self.langfuse_client_service and self.langfuse_client_service.is_enabled():
                self._track_answer_quality(query, response, processing_time)

            self.logger.info(f"Query processed successfully in {processing_time:.2f}s")
            return response

        except Exception as e:
            import traceback
            error_msg = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.logger.error(error_msg)
            self.logger.error(traceback.format_exc())

            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0,
                "legal_references": [],
                "processing_steps": [error_msg],
                "session_id": session_id or str(uuid.uuid4()),
                "processing_time": time.time() - start_time if 'start_time' in locals() else 0.0,
                "query_type": "error",
                "metadata": {"error": str(e)},
                "errors": [error_msg]
            }

    def _extract_retrieved_docs_from_result(self, flat_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ìµœì¢… ê²°ê³¼ì—ì„œ retrieved_docs ì¶”ì¶œ

        retrieved_docsëŠ” search ê·¸ë£¹, ìµœìƒìœ„ ë ˆë²¨, ë˜ëŠ” global cacheì— ìˆì„ ìˆ˜ ìˆìŒ

        Args:
            flat_result: ìµœì¢… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

        Returns:
            retrieved_docs ë¦¬ìŠ¤íŠ¸
        """
        if not isinstance(flat_result, dict):
            return []

        # 1. ìµœìƒìœ„ ë ˆë²¨ì—ì„œ í™•ì¸
        if "retrieved_docs" in flat_result:
            docs = flat_result.get("retrieved_docs", [])
            if isinstance(docs, list) and len(docs) > 0:
                self.logger.debug(f"Found retrieved_docs in top level: {len(docs)}")
                return docs

        # 2. search ê·¸ë£¹ì—ì„œ í™•ì¸
        if "search" in flat_result and isinstance(flat_result["search"], dict):
            search_group = flat_result["search"]
            docs = search_group.get("retrieved_docs", [])
            if isinstance(docs, list) and len(docs) > 0:
                self.logger.debug(f"Found retrieved_docs in search group: {len(docs)}")
                return docs

        # 3. search.retrieved_docsê°€ ì—†ìœ¼ë©´ search.merged_documents í™•ì¸
        if "search" in flat_result and isinstance(flat_result["search"], dict):
            search_group = flat_result["search"]
            merged_docs = search_group.get("merged_documents", [])
            if isinstance(merged_docs, list) and len(merged_docs) > 0:
                self.logger.debug(f"Found merged_documents in search group (using as retrieved_docs): {len(merged_docs)}")
                return merged_docs

        # 4. global cacheì—ì„œ í™•ì¸ (ë§ˆì§€ë§‰ ì‹œë„)
        try:
            from core.agents.node_wrappers import _global_search_results_cache
            if _global_search_results_cache:
                # search ê·¸ë£¹ì—ì„œ í™•ì¸
                if "search" in _global_search_results_cache and isinstance(_global_search_results_cache["search"], dict):
                    cached_search = _global_search_results_cache["search"]
                    cached_docs = cached_search.get("retrieved_docs", [])
                    if isinstance(cached_docs, list) and len(cached_docs) > 0:
                        self.logger.debug(f"Found retrieved_docs in global cache search group: {len(cached_docs)}")
                        return cached_docs
                    cached_merged = cached_search.get("merged_documents", [])
                    if isinstance(cached_merged, list) and len(cached_merged) > 0:
                        self.logger.debug(f"Found merged_documents in global cache search group: {len(cached_merged)}")
                        return cached_merged

                    # semantic_resultsë¥¼ retrieved_docsë¡œ ë³€í™˜ (retrieved_docsê°€ ì—†ëŠ” ê²½ìš°)
                    cached_semantic = cached_search.get("semantic_results", [])
                    if isinstance(cached_semantic, list) and len(cached_semantic) > 0:
                        self.logger.debug(f"Converting semantic_results to retrieved_docs: {len(cached_semantic)}")
                        # semantic_resultsëŠ” ì´ë¯¸ ë¬¸ì„œ í˜•íƒœì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        return cached_semantic

                # ìµœìƒìœ„ ë ˆë²¨ì—ì„œ í™•ì¸
                cached_docs = _global_search_results_cache.get("retrieved_docs", [])
                if isinstance(cached_docs, list) and len(cached_docs) > 0:
                    self.logger.debug(f"Found retrieved_docs in global cache top level: {len(cached_docs)}")
                    return cached_docs
                cached_merged = _global_search_results_cache.get("merged_documents", [])
                if isinstance(cached_merged, list) and len(cached_merged) > 0:
                    self.logger.debug(f"Found merged_documents in global cache top level: {len(cached_merged)}")
                    return cached_merged

                # semantic_resultsë¥¼ retrieved_docsë¡œ ë³€í™˜ (ìµœìƒìœ„ ë ˆë²¨)
                cached_semantic = _global_search_results_cache.get("semantic_results", [])
                if isinstance(cached_semantic, list) and len(cached_semantic) > 0:
                    self.logger.debug(f"Converting semantic_results to retrieved_docs (top level): {len(cached_semantic)}")
                    return cached_semantic
        except (ImportError, AttributeError, TypeError):
            pass  # global cacheë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë©´ ë¬´ì‹œ

        # 5. flat_resultì—ì„œ semantic_resultsë¥¼ retrieved_docsë¡œ ë³€í™˜ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        if "search" in flat_result and isinstance(flat_result["search"], dict):
            search_group = flat_result["search"]
            semantic_results = search_group.get("semantic_results", [])
            if isinstance(semantic_results, list) and len(semantic_results) > 0:
                self.logger.debug(f"Converting semantic_results to retrieved_docs from search group: {len(semantic_results)}")
                return semantic_results

        # ìµœìƒìœ„ ë ˆë²¨ì˜ semantic_results í™•ì¸
        if "semantic_results" in flat_result:
            semantic_results = flat_result.get("semantic_results", [])
            if isinstance(semantic_results, list) and len(semantic_results) > 0:
                self.logger.debug(f"Converting semantic_results to retrieved_docs from top level: {len(semantic_results)}")
                return semantic_results

        self.logger.debug(f"No retrieved_docs found - keys={list(flat_result.keys())[:10]}")
        return []

    async def resume_session(self, session_id: str, query: str) -> Dict[str, Any]:
        """
        ì„¸ì…˜ ì¬ê°œ

        Args:
            session_id: ì„¸ì…˜ ID
            query: ìƒˆë¡œìš´ ì§ˆë¬¸

        Returns:
            Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            self.logger.info(f"Resuming session: {session_id}")

            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì „ ìƒíƒœ í™•ì¸ (checkpoint_managerê°€ ìˆëŠ” ê²½ìš°)
            if self.checkpoint_manager:
                checkpoints = self.checkpoint_manager.list_checkpoints(session_id)

                if checkpoints:
                    self.logger.info(f"Found {len(checkpoints)} checkpoints for session {session_id}")
                    # ì´ì „ ìƒíƒœì—ì„œ ìƒˆë¡œìš´ ì§ˆë¬¸ìœ¼ë¡œ ê³„ì†
                    return await self.process_query(query, session_id, enable_checkpoint=True)
                else:
                    self.logger.info(f"No checkpoints found for session {session_id}, starting new session")
                    # ìƒˆë¡œìš´ ì„¸ì…˜ìœ¼ë¡œ ì‹œì‘
                    return await self.process_query(query, session_id, enable_checkpoint=True)
            else:
                # checkpoint_managerê°€ ì—†ìœ¼ë©´ ì¼ë°˜ í”„ë¡œì„¸ìŠ¤
                return await self.process_query(query, session_id, enable_checkpoint=False)

        except Exception as e:
            error_msg = f"ì„¸ì…˜ ì¬ê°œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.logger.error(error_msg)

            # ìƒˆë¡œìš´ ì„¸ì…˜ìœ¼ë¡œ í´ë°±
            return await self.process_query(query, session_id, enable_checkpoint=False)

    def get_session_info(self, session_id: str) -> Dict[str, Any]:
        """
        ì„¸ì…˜ ì •ë³´ ì¡°íšŒ

        Args:
            session_id: ì„¸ì…˜ ID

        Returns:
            Dict[str, Any]: ì„¸ì…˜ ì •ë³´
        """
        try:
            if self.checkpoint_manager:
                checkpoints = self.checkpoint_manager.list_checkpoints(session_id)

                return {
                    "session_id": session_id,
                    "checkpoint_count": len(checkpoints),
                    "latest_checkpoint": checkpoints[-1] if checkpoints else None,
                    "has_checkpoints": len(checkpoints) > 0
                }
            else:
                # checkpoint_managerê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë³´ë§Œ ë°˜í™˜
                return {
                    "session_id": session_id,
                    "checkpoint_count": 0,
                    "has_checkpoints": False,
                    "note": "Checkpoint manager is disabled"
                }

        except Exception as e:
            self.logger.error(f"Failed to get session info: {e}")
            return {
                "session_id": session_id,
                "error": str(e),
                "checkpoint_count": 0,
                "has_checkpoints": False
            }

    def cleanup_old_sessions(self, ttl_hours: int = 24) -> int:
        """
        ì˜¤ë˜ëœ ì„¸ì…˜ ì •ë¦¬

        Args:
            ttl_hours: ìœ ì§€ ì‹œê°„ (ì‹œê°„)

        Returns:
            int: ì •ë¦¬ëœ ì²´í¬í¬ì¸íŠ¸ ìˆ˜
        """
        try:
            if self.checkpoint_manager:
                cleaned_count = self.checkpoint_manager.cleanup_old_checkpoints(ttl_hours)
                self.logger.info(f"Cleaned up {cleaned_count} old checkpoints")
                return cleaned_count
            else:
                self.logger.info("Checkpoint manager is disabled")
                return 0

        except Exception as e:
            self.logger.error(f"Failed to cleanup old sessions: {e}")
            return 0

    def get_service_status(self) -> Dict[str, Any]:
        """
        ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ

        Returns:
            Dict[str, Any]: ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´
        """
        try:
            if self.checkpoint_manager:
                db_info = self.checkpoint_manager.get_database_info()
            else:
                db_info = {"note": "Checkpoint manager is disabled"}

            return {
                "service_name": "LangGraphWorkflowService",
                "status": "running",
                "config": self.config.to_dict(),
                "database_info": db_info,
                "workflow_compiled": self.app is not None,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            self.logger.error(f"Failed to get service status: {e}")
            return {
                "service_name": "LangGraphWorkflowService",
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def validate_config(self) -> List[str]:
        """
        ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬

        Returns:
            List[str]: ì˜¤ë¥˜ ë©”ì‹œì§€ ëª©ë¡
        """
        return self.config.validate()

    async def test_workflow(self, test_query: str = "ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­ì€?") -> Dict[str, Any]:
        """
        ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸

        Args:
            test_query: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸

        Returns:
            Dict[str, Any]: í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        try:
            self.logger.info(f"Testing workflow with query: {test_query}")

            result = await self.process_query(test_query, enable_checkpoint=False)

            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦
            test_passed = (
                "answer" in result and
                result["answer"] and
                len(result["processing_steps"]) > 0 and
                len(result["errors"]) == 0
            )

            return {
                "test_passed": test_passed,
                "test_query": test_query,
                "result": result,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            self.logger.error(f"Workflow test failed: {e}")
            return {
                "test_passed": False,
                "test_query": test_query,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def _track_answer_quality(self, query: str, response: Dict[str, Any], processing_time: float):
        """
        Langfuseì— ë‹µë³€ í’ˆì§ˆ ì¶”ì 

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            response: ì‘ë‹µ ê²°ê³¼
            processing_time: ì²˜ë¦¬ ì‹œê°„
        """
        if not self.langfuse_client_service or not self.langfuse_client_service.is_enabled():
            return

        try:
            # ë‹µë³€ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ì¢…í•© ì ìˆ˜)
            quality_score = self._calculate_quality_score(response)

            # ë‹µë³€ í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ì 
            trace_id = self.langfuse_client_service.track_answer_quality_metrics(
                query=query,
                answer=response.get("answer", ""),
                confidence=response.get("confidence", 0.0),
                sources_count=len(response.get("sources", [])),
                legal_refs_count=len(response.get("legal_references", [])),
                processing_time=processing_time,
                has_errors=len(response.get("errors", [])) > 0,
                overall_quality=quality_score
            )

            self.logger.info(f"Answer quality tracked in Langfuse: quality_score={quality_score:.2f}, trace_id={trace_id}")

        except Exception as e:
            self.logger.error(f"Failed to track answer quality in Langfuse: {e}")

    def _calculate_quality_score(self, response: Dict[str, Any]) -> float:
        """
        ë‹µë³€ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°

        Args:
            response: ì‘ë‹µ ê²°ê³¼

        Returns:
            float: í’ˆì§ˆ ì ìˆ˜ (0.0 ~ 1.0)
        """
        score = 0.0
        max_score = 0.0

        # ë‹µë³€ ê¸¸ì´ (20ì )
        answer = response.get("answer", "")
        if len(answer) >= 50:
            score += 10
        if len(answer) >= 100:
            score += 10
        max_score += 20

        # ì‹ ë¢°ë„ (30ì )
        confidence = response.get("confidence", 0.0)
        score += confidence * 30
        max_score += 30

        # ì†ŒìŠ¤ ì œê³µ (20ì )
        sources_count = len(response.get("sources", []))
        if sources_count > 0:
            score += min(20, sources_count * 5)
        max_score += 20

        # ë²•ë¥  ì°¸ì¡° (20ì )
        legal_refs_count = len(response.get("legal_references", []))
        if legal_refs_count > 0:
            score += min(20, legal_refs_count * 10)
        max_score += 20

        # ì—ëŸ¬ ì—†ìŒ (10ì )
        errors_count = len(response.get("errors", []))
        if errors_count == 0:
            score += 10
        max_score += 10

        # ì •ê·œí™”ëœ ì ìˆ˜
        quality_score = score / max_score if max_score > 0 else 0.0
        return round(quality_score, 2)

    def _get_node_display_name(self, node_name: str) -> str:
        """
        ë…¸ë“œ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ

        Args:
            node_name: ë…¸ë“œ ì´ë¦„

        Returns:
            str: í•œêµ­ì–´ ë…¸ë“œ ì´ë¦„
        """
        node_name_map = {
            # ì›Œí¬í”Œë¡œìš° ì£¼ìš” ë…¸ë“œ
            "classify_query": "ì§ˆë¬¸ ë¶„ë¥˜",
            "assess_urgency": "ê¸´ê¸‰ë„ í‰ê°€",
            "analyze_document": "ë¬¸ì„œ ë¶„ì„",
            "resolve_multi_turn": "ë©€í‹°í„´ ì²˜ë¦¬",
            "route_expert": "ì „ë¬¸ê°€ ë¼ìš°íŒ…",
            "expand_keywords_ai": "AI í‚¤ì›Œë“œ í™•ì¥",
            "retrieve_documents": "ë¬¸ì„œ ê²€ìƒ‰",
            "process_legal_terms": "ë²•ë¥  ìš©ì–´ ì²˜ë¦¬",
            "generate_answer_enhanced": "ë‹µë³€ ìƒì„±",
            "validate_answer_quality": "ë‹µë³€ í’ˆì§ˆ ê²€ì¦",
            "enhance_answer_structure": "ë‹µë³€ êµ¬ì¡° í–¥ìƒ",
            "apply_visual_formatting": "ì‹œê°ì  í¬ë§·íŒ…",
            "prepare_final_response": "ìµœì¢… ì‘ë‹µ ì¤€ë¹„",
            # ê¸°íƒ€ ë…¸ë“œ
            "validate_legal_basis": "ë²•ë ¹ ê²€ì¦",
            "route_to_expert": "ì „ë¬¸ê°€ ë¼ìš°íŒ…",
            "retrieve_context": "ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰",
            "generate_answer": "ë‹µë³€ ìƒì„±",
            "enhance_answer": "ë‹µë³€ í–¥ìƒ",
            "format_answer": "ë‹µë³€ í¬ë§·íŒ…",
            "family_law_expert": "ê°€ì¡±ë²• ì „ë¬¸ê°€",
            "corporate_law_expert": "ê¸°ì—…ë²• ì „ë¬¸ê°€",
            "ip_law_expert": "ì§€ì ì¬ì‚°ê¶Œ ì „ë¬¸ê°€",
            "legal_term_extraction": "ë²•ë¥  ìš©ì–´ ì¶”ì¶œ",
            "legal_domain_classification": "ë²•ë¥  ë¶„ì•¼ ë¶„ë¥˜",
        }

        return node_name_map.get(node_name, node_name.replace("_", " ").title())
