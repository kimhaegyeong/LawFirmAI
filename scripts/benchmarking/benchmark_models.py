#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KoBART vs KoGPT-2 ?±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí‚¹ ?¤í¬ë¦½íŠ¸
LawFirmAI ?„ë¡œ?íŠ¸ - TASK 1.2.1
"""

import os
import sys
import time
import psutil
import torch
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    AutoModelForSeq2SeqLM,
    pipeline
)
from sentence_transformers import SentenceTransformer
import json
from datetime import datetime
import logging

# ë¡œê¹… ?¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelBenchmark:
    """AI ëª¨ë¸ ?±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ?´ë˜??""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.results = {}
        self.test_data = self._load_test_data()
        
    def _load_test_data(self) -> List[Dict[str, str]]:
        """ë²•ë¥  ?„ë©”???ŒìŠ¤???°ì´??ë¡œë“œ"""
        return [
            {
                "question": "ê³„ì•½?œì—??ì£¼ì˜?´ì•¼ ??ì¡°í•­?€ ë¬´ì—‡?¸ê???",
                "context": "ê³„ì•½??ê²€????ì¤‘ìš”???¬í•­??,
                "category": "contract"
            },
            {
                "question": "?í•´ë°°ìƒ ì²?µ¬ê¶Œì˜ ?Œë©¸?œíš¨??ëª??„ì¸ê°€??",
                "context": "ë¯¼ë²•???í•´ë°°ìƒ ê´€??ì¡°í•­",
                "category": "civil_law"
            },
            {
                "question": "ê·¼ë¡œê¸°ì?ë²•ìƒ ?´ê²Œ?œê°„?€ ?´ë–»ê²?ê·œì •?˜ì–´ ?ˆë‚˜??",
                "context": "ê·¼ë¡œê¸°ì?ë²??´ê²Œ?œê°„ ê´€??ì¡°í•­",
                "category": "labor_law"
            },
            {
                "question": "ë¶€?™ì‚° ë§¤ë§¤ê³„ì•½?ì„œ ì¤‘ë„ê¸ˆì? ?¸ì œ ì§€ê¸‰í•´???˜ë‚˜??",
                "context": "ë¶€?™ì‚° ë§¤ë§¤ê³„ì•½ ì¤‘ë„ê¸?ì§€ê¸??œê¸°",
                "category": "real_estate"
            },
            {
                "question": "?´í˜¼ ???¬ì‚°ë¶„í• ?€ ?´ë–»ê²??´ë£¨?´ì??˜ìš”?",
                "context": "ê°€ì¡±ë²•???´í˜¼ ?¬ì‚°ë¶„í• ",
                "category": "family_law"
            }
        ]
    
    def benchmark_kobart(self) -> Dict[str, Any]:
        """KoBART ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹"""
        logger.info("KoBART ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ?œì‘...")
        
        model_name = "skt/kobart-base-v1"
        results = {
            "model_name": model_name,
            "model_type": "seq2seq",
            "benchmark_time": datetime.now().isoformat()
        }
        
        try:
            # ëª¨ë¸ ë¡œë”© ?œê°„ ì¸¡ì •
            start_time = time.time()
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )
            model.to(self.device)
            loading_time = time.time() - start_time
            
            # ë©”ëª¨ë¦??¬ìš©??ì¸¡ì •
            memory_usage = self._get_memory_usage()
            
            # ëª¨ë¸ ?•ë³´ ?˜ì§‘
            model_info = {
                "num_parameters": sum(p.numel() for p in model.parameters()),
                "model_size_mb": self._get_model_size(model),
                "loading_time": loading_time,
                "memory_usage_mb": memory_usage
            }
            
            # ì¶”ë¡  ?±ëŠ¥ ?ŒìŠ¤??
            inference_results = self._test_inference_kobart(model, tokenizer)
            
            results.update({
                "model_info": model_info,
                "inference_results": inference_results
            })
            
            # ëª¨ë¸ ?•ë¦¬
            del model, tokenizer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
        except Exception as e:
            logger.error(f"KoBART ë²¤ì¹˜ë§ˆí‚¹ ?¤íŒ¨: {e}")
            results["error"] = str(e)
            
        return results
    
    def benchmark_kogpt2(self) -> Dict[str, Any]:
        """KoGPT-2 ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹"""
        logger.info("KoGPT-2 ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ?œì‘...")
        
        model_name = "skt/kogpt2-base-v2"
        results = {
            "model_name": model_name,
            "model_type": "causal_lm",
            "benchmark_time": datetime.now().isoformat()
        }
        
        try:
            # ëª¨ë¸ ë¡œë”© ?œê°„ ì¸¡ì •
            start_time = time.time()
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )
            model.to(self.device)
            loading_time = time.time() - start_time
            
            # ë©”ëª¨ë¦??¬ìš©??ì¸¡ì •
            memory_usage = self._get_memory_usage()
            
            # ëª¨ë¸ ?•ë³´ ?˜ì§‘
            model_info = {
                "num_parameters": sum(p.numel() for p in model.parameters()),
                "model_size_mb": self._get_model_size(model),
                "loading_time": loading_time,
                "memory_usage_mb": memory_usage
            }
            
            # ì¶”ë¡  ?±ëŠ¥ ?ŒìŠ¤??
            inference_results = self._test_inference_kogpt2(model, tokenizer)
            
            results.update({
                "model_info": model_info,
                "inference_results": inference_results
            })
            
            # ëª¨ë¸ ?•ë¦¬
            del model, tokenizer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
        except Exception as e:
            logger.error(f"KoGPT-2 ë²¤ì¹˜ë§ˆí‚¹ ?¤íŒ¨: {e}")
            results["error"] = str(e)
            
        return results
    
    def _test_inference_kobart(self, model, tokenizer) -> Dict[str, Any]:
        """KoBART ì¶”ë¡  ?±ëŠ¥ ?ŒìŠ¤??""
        results = {
            "total_inference_time": 0,
            "average_inference_time": 0,
            "responses": [],
            "quality_scores": []
        }
        
        total_time = 0
        responses = []
        
        for i, test_case in enumerate(self.test_data):
            try:
                start_time = time.time()
                
                # ?…ë ¥ ?„ì²˜ë¦?
                input_text = f"ì§ˆë¬¸: {test_case['question']} ë§¥ë½: {test_case['context']}"
                inputs = tokenizer.encode(input_text, return_tensors="pt").to(self.device)
                
                # ì¶”ë¡  ?¤í–‰
                with torch.no_grad():
                    outputs = model.generate(
                        inputs,
                        max_length=512,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                # ?‘ë‹µ ?”ì½”??
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                inference_time = time.time() - start_time
                
                total_time += inference_time
                responses.append({
                    "question": test_case['question'],
                    "response": response,
                    "inference_time": inference_time,
                    "category": test_case['category']
                })
                
                logger.info(f"KoBART ?ŒìŠ¤??{i+1}/{len(self.test_data)} ?„ë£Œ")
                
            except Exception as e:
                logger.error(f"KoBART ì¶”ë¡  ?ŒìŠ¤??{i+1} ?¤íŒ¨: {e}")
                continue
        
        results.update({
            "total_inference_time": total_time,
            "average_inference_time": total_time / len(self.test_data) if self.test_data else 0,
            "responses": responses
        })
        
        return results
    
    def _test_inference_kogpt2(self, model, tokenizer) -> Dict[str, Any]:
        """KoGPT-2 ì¶”ë¡  ?±ëŠ¥ ?ŒìŠ¤??""
        results = {
            "total_inference_time": 0,
            "average_inference_time": 0,
            "responses": [],
            "quality_scores": []
        }
        
        total_time = 0
        responses = []
        
        for i, test_case in enumerate(self.test_data):
            try:
                start_time = time.time()
                
                # ?…ë ¥ ?„ì²˜ë¦?(KoGPT-2???„ë¡¬?„íŠ¸ ?•ì‹)
                prompt = f"ì§ˆë¬¸: {test_case['question']}\n?µë?:"
                inputs = tokenizer.encode(prompt, return_tensors="pt").to(self.device)
                
                # ì¶”ë¡  ?¤í–‰
                with torch.no_grad():
                    outputs = model.generate(
                        inputs,
                        max_length=inputs.shape[1] + 200,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                # ?‘ë‹µ ?”ì½”??
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                # ?„ë¡¬?„íŠ¸ ë¶€ë¶??œê±°
                response = response.replace(prompt, "").strip()
                
                inference_time = time.time() - start_time
                total_time += inference_time
                
                responses.append({
                    "question": test_case['question'],
                    "response": response,
                    "inference_time": inference_time,
                    "category": test_case['category']
                })
                
                logger.info(f"KoGPT-2 ?ŒìŠ¤??{i+1}/{len(self.test_data)} ?„ë£Œ")
                
            except Exception as e:
                logger.error(f"KoGPT-2 ì¶”ë¡  ?ŒìŠ¤??{i+1} ?¤íŒ¨: {e}")
                continue
        
        results.update({
            "total_inference_time": total_time,
            "average_inference_time": total_time / len(self.test_data) if self.test_data else 0,
            "responses": responses
        })
        
        return results
    
    def _get_memory_usage(self) -> float:
        """?„ì¬ ë©”ëª¨ë¦??¬ìš©??ë°˜í™˜ (MB)"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    
    def _get_model_size(self, model) -> float:
        """ëª¨ë¸ ?¬ê¸° ë°˜í™˜ (MB)"""
        param_size = 0
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        return param_size / 1024 / 1024
    
    def run_benchmark(self) -> Dict[str, Any]:
        """?„ì²´ ë²¤ì¹˜ë§ˆí‚¹ ?¤í–‰"""
        logger.info("ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ?œì‘...")
        
        # ?œìŠ¤???•ë³´ ?˜ì§‘
        system_info = {
            "device": self.device,
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
            "python_version": sys.version,
            "torch_version": torch.__version__
        }
        
        # ê°?ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ?¤í–‰
        kobart_results = self.benchmark_kobart()
        kogpt2_results = self.benchmark_kogpt2()
        
        # ê²°ê³¼ ì¢…í•©
        benchmark_results = {
            "system_info": system_info,
            "kobart": kobart_results,
            "kogpt2": kogpt2_results,
            "comparison": self._compare_models(kobart_results, kogpt2_results)
        }
        
        return benchmark_results
    
    def _compare_models(self, kobart_results: Dict, kogpt2_results: Dict) -> Dict[str, Any]:
        """ëª¨ë¸ ?±ëŠ¥ ë¹„êµ"""
        comparison = {
            "model_size_comparison": {},
            "memory_usage_comparison": {},
            "inference_speed_comparison": {},
            "recommendation": ""
        }
        
        try:
            # ëª¨ë¸ ?¬ê¸° ë¹„êµ
            if "model_info" in kobart_results and "model_info" in kogpt2_results:
                kobart_size = kobart_results["model_info"]["model_size_mb"]
                kogpt2_size = kogpt2_results["model_info"]["model_size_mb"]
                
                comparison["model_size_comparison"] = {
                    "kobart_mb": kobart_size,
                    "kogpt2_mb": kogpt2_size,
                    "size_ratio": kobart_size / kogpt2_size if kogpt2_size > 0 else 0
                }
            
            # ë©”ëª¨ë¦??¬ìš©??ë¹„êµ
            if "model_info" in kobart_results and "model_info" in kogpt2_results:
                kobart_memory = kobart_results["model_info"]["memory_usage_mb"]
                kogpt2_memory = kogpt2_results["model_info"]["memory_usage_mb"]
                
                comparison["memory_usage_comparison"] = {
                    "kobart_mb": kobart_memory,
                    "kogpt2_mb": kogpt2_memory,
                    "memory_ratio": kobart_memory / kogpt2_memory if kogpt2_memory > 0 else 0
                }
            
            # ì¶”ë¡  ?ë„ ë¹„êµ
            if "inference_results" in kobart_results and "inference_results" in kogpt2_results:
                kobart_time = kobart_results["inference_results"]["average_inference_time"]
                kogpt2_time = kogpt2_results["inference_results"]["average_inference_time"]
                
                comparison["inference_speed_comparison"] = {
                    "kobart_seconds": kobart_time,
                    "kogpt2_seconds": kogpt2_time,
                    "speed_ratio": kobart_time / kogpt2_time if kogpt2_time > 0 else 0
                }
            
            # ê¶Œì¥?¬í•­ ?ì„±
            comparison["recommendation"] = self._generate_recommendation(comparison)
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¹„êµ ì¤??¤ë¥˜: {e}")
            comparison["error"] = str(e)
        
        return comparison
    
    def _generate_recommendation(self, comparison: Dict) -> str:
        """ëª¨ë¸ ? íƒ ê¶Œì¥?¬í•­ ?ì„±"""
        try:
            size_ratio = comparison.get("model_size_comparison", {}).get("size_ratio", 1)
            memory_ratio = comparison.get("memory_usage_comparison", {}).get("memory_ratio", 1)
            speed_ratio = comparison.get("inference_speed_comparison", {}).get("speed_ratio", 1)
            
            # HuggingFace Spaces ?œì•½?¬í•­ ê³ ë ¤ (16GB ë©”ëª¨ë¦??œí•œ)
            if memory_ratio > 1.5:  # KoBARTê°€ ë©”ëª¨ë¦¬ë? ??ë§ì´ ?¬ìš©
                return "KoGPT-2 ê¶Œì¥: ë©”ëª¨ë¦??¨ìœ¨?±ì´ ?°ìˆ˜?˜ì—¬ HuggingFace Spaces ?˜ê²½???í•©"
            elif speed_ratio > 1.2:  # KoBARTê°€ ???ë¦¼
                return "KoGPT-2 ê¶Œì¥: ì¶”ë¡  ?ë„ê°€ ë¹ ë¦„"
            elif size_ratio > 1.3:  # KoBARTê°€ ????
                return "KoGPT-2 ê¶Œì¥: ëª¨ë¸ ?¬ê¸°ê°€ ?‘ì•„ ë°°í¬??? ë¦¬"
            else:
                return "KoBART ê¶Œì¥: seq2seq ?¹ì„±??ë²•ë¥  ì§ˆë¬¸-?µë????í•©"
                
        except Exception as e:
            return f"ê¶Œì¥?¬í•­ ?ì„± ?¤íŒ¨: {e}"
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ë²¤ì¹˜ë§ˆí‚¹ ê²°ê³¼ ?€??""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"model_benchmark_results_{timestamp}.json"
        
        filepath = os.path.join("benchmark_results", filename)
        os.makedirs("benchmark_results", exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë²¤ì¹˜ë§ˆí‚¹ ê²°ê³¼ ?€?? {filepath}")
        return filepath

def main():
    """ë©”ì¸ ?¤í–‰ ?¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="AI ëª¨ë¸ ?±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹")
    parser.add_argument("--device", default="cpu", choices=["cpu", "cuda"], help="?¤í–‰ ?”ë°”?´ìŠ¤")
    parser.add_argument("--output", help="ê²°ê³¼ ?€???Œì¼ëª?)
    
    args = parser.parse_args()
    
    # ë²¤ì¹˜ë§ˆí‚¹ ?¤í–‰
    benchmark = ModelBenchmark(device=args.device)
    results = benchmark.run_benchmark()
    
    # ê²°ê³¼ ?€??
    output_file = benchmark.save_results(results, args.output)
    
    # ê²°ê³¼ ?”ì•½ ì¶œë ¥
    print("\n" + "="*50)
    print("ë²¤ì¹˜ë§ˆí‚¹ ê²°ê³¼ ?”ì•½")
    print("="*50)
    
    if "kobart" in results and "model_info" in results["kobart"]:
        kobart_info = results["kobart"]["model_info"]
        print(f"KoBART - ëª¨ë¸ ?¬ê¸°: {kobart_info.get('model_size_mb', 0):.1f}MB, "
              f"ë©”ëª¨ë¦??¬ìš©?? {kobart_info.get('memory_usage_mb', 0):.1f}MB")
    
    if "kogpt2" in results and "model_info" in results["kogpt2"]:
        kogpt2_info = results["kogpt2"]["model_info"]
        print(f"KoGPT-2 - ëª¨ë¸ ?¬ê¸°: {kogpt2_info.get('model_size_mb', 0):.1f}MB, "
              f"ë©”ëª¨ë¦??¬ìš©?? {kogpt2_info.get('memory_usage_mb', 0):.1f}MB")
    
    if "comparison" in results and "recommendation" in results["comparison"]:
        print(f"\nê¶Œì¥?¬í•­: {results['comparison']['recommendation']}")
    
    print(f"\n?ì„¸ ê²°ê³¼: {output_file}")

if __name__ == "__main__":
    main()
