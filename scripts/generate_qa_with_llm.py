#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM ê¸°ë°˜ Q&A ë°ì´í„°ì…‹ ìƒì„± ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Ollama Qwen2.5:7b ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²•ë¥  Q&A ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from scripts.llm_qa_generator import LLMQAGenerator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/generate_qa_with_llm.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="LLM ê¸°ë°˜ ë²•ë¥  Q&A ë°ì´í„°ì…‹ ìƒì„±",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‹¤í–‰
  python scripts/generate_qa_with_llm.py

  # íŠ¹ì • ëª¨ë¸ê³¼ ë°ì´í„° íƒ€ì… ì§€ì •
  python scripts/generate_qa_with_llm.py --model qwen2.5:7b --data-type laws precedents

  # ì¶œë ¥ ë””ë ‰í† ë¦¬ì™€ ëª©í‘œ ê°œìˆ˜ ì§€ì •
  python scripts/generate_qa_with_llm.py --output data/qa_dataset/llm_generated --target 3000

  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì†Œê·œëª¨ ë°ì´í„°)
  python scripts/generate_qa_with_llm.py --dry-run --max-items 10
        """
    )
    
    parser.add_argument(
        '--model',
        type=str,
        default='qwen2.5:7b',
        help='ì‚¬ìš©í•  Ollama ëª¨ë¸ëª… (ê¸°ë³¸ê°’: qwen2.5:7b)'
    )
    
    parser.add_argument(
        '--data-type',
        nargs='+',
        choices=['laws', 'precedents', 'constitutional_decisions', 'legal_interpretations'],
        default=['laws', 'precedents'],
        help='ì²˜ë¦¬í•  ë°ì´í„° ìœ í˜• (ê¸°ë³¸ê°’: laws precedents)'
    )
    
    parser.add_argument(
        '--data-dir',
        type=str,
        default='data/processed',
        help='ì…ë ¥ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/processed)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='data/qa_dataset/llm_generated',
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/qa_dataset/llm_generated)'
    )
    
    parser.add_argument(
        '--target',
        type=int,
        default=3000,
        help='ëª©í‘œ Q&A ê°œìˆ˜ (ê¸°ë³¸ê°’: 3000)'
    )
    
    parser.add_argument(
        '--max-items',
        type=int,
        default=100,
        help='ë°ì´í„° íƒ€ì…ë³„ ìµœëŒ€ ì²˜ë¦¬ í•­ëª© ìˆ˜ (ê¸°ë³¸ê°’: 100)'
    )
    
    parser.add_argument(
        '--temperature',
        type=float,
        default=0.7,
        help='LLM ìƒì„± ì˜¨ë„ (0.0-1.0, ê¸°ë³¸ê°’: 0.7)'
    )
    
    parser.add_argument(
        '--max-tokens',
        type=int,
        default=1500,
        help='ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 1500)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 10)'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì‹¤ì œ ìƒì„±í•˜ì§€ ì•Šê³  ì„¤ì •ë§Œ í™•ì¸)'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥'
    )
    
    parser.add_argument(
        '--quality-threshold',
        type=float,
        default=0.6,
        help='í’ˆì§ˆ ì ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.6)'
    )
    
    return parser.parse_args()


def validate_environment():
    """í™˜ê²½ ê²€ì¦"""
    logger.info("í™˜ê²½ ê²€ì¦ ì¤‘...")
    
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ í™•ì¸
    data_dir = Path("data/processed")
    if not data_dir.exists():
        logger.error(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_dir}")
        return False
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    logger.info("í™˜ê²½ ê²€ì¦ ì™„ë£Œ")
    return True


def test_ollama_connection(model: str) -> bool:
    """Ollama ì—°ê²° í…ŒìŠ¤íŠ¸"""
    logger.info(f"Ollama ëª¨ë¸ '{model}' ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        from source.utils.ollama_client import OllamaClient
        
        client = OllamaClient(model=model)
        success = client.test_connection()
        
        if success:
            logger.info("âœ… Ollama ì—°ê²° ì„±ê³µ")
            return True
        else:
            logger.error("âŒ Ollama ì—°ê²° ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def generate_qa_dataset(args):
    """Q&A ë°ì´í„°ì…‹ ìƒì„±"""
    logger.info("LLM ê¸°ë°˜ Q&A ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
    
    try:
        # LLM Q&A ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = LLMQAGenerator(
            model=args.model,
            temperature=args.temperature,
            max_tokens=args.max_tokens
        )
        
        # í’ˆì§ˆ ê¸°ì¤€ ì—…ë°ì´íŠ¸
        generator.quality_criteria['min_quality_score'] = args.quality_threshold
        
        # ë°ì´í„°ì…‹ ìƒì„±
        success = generator.generate_dataset(
            data_dir=args.data_dir,
            output_dir=args.output,
            data_types=args.data_type,
            max_items_per_type=args.max_items
        )
        
        if success:
            logger.info("âœ… Q&A ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            print_summary(args.output)
            return True
        else:
            logger.error("âŒ Q&A ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"Q&A ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def print_summary(output_dir: str):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    try:
        import json
        
        stats_file = Path(output_dir) / "llm_qa_dataset_statistics.json"
        if stats_file.exists():
            with open(stats_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)
            
            print("\n" + "="*60)
            print("ğŸ“Š LLM ê¸°ë°˜ Q&A ë°ì´í„°ì…‹ ìƒì„± ê²°ê³¼")
            print("="*60)
            print(f"ì´ Q&A ìŒ ìˆ˜: {stats['total_pairs']:,}ê°œ")
            print(f"ê³ í’ˆì§ˆ Q&A: {stats['high_quality_pairs']:,}ê°œ (í’ˆì§ˆì ìˆ˜ â‰¥0.8)")
            print(f"ì¤‘í’ˆì§ˆ Q&A: {stats['medium_quality_pairs']:,}ê°œ (í’ˆì§ˆì ìˆ˜ 0.6-0.8)")
            print(f"ì €í’ˆì§ˆ Q&A: {stats['low_quality_pairs']:,}ê°œ (í’ˆì§ˆì ìˆ˜ <0.6)")
            print(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {stats['average_quality_score']:.3f}")
            print(f"ì‚¬ìš© ëª¨ë¸: {stats['model_used']}")
            print(f"ìƒì„± ì˜¨ë„: {stats['temperature']}")
            
            print("\nğŸ“ˆ ì†ŒìŠ¤ë³„ ë¶„í¬:")
            for source, count in stats['source_distribution'].items():
                print(f"  - {source}: {count:,}ê°œ")
            
            print("\nğŸ“ˆ ë‚œì´ë„ë³„ ë¶„í¬:")
            for difficulty, count in stats['difficulty_distribution'].items():
                print(f"  - {difficulty}: {count:,}ê°œ")
            
            print("\nğŸ“ˆ ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬:")
            for q_type, count in stats['question_type_distribution'].items():
                print(f"  - {q_type}: {count:,}ê°œ")
            
            print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
            output_path = Path(output_dir)
            for file_path in output_path.glob("*.json"):
                print(f"  - {file_path.name}")
            
            print("="*60)
            
    except Exception as e:
        logger.error(f"ê²°ê³¼ ìš”ì•½ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì¸ìˆ˜ íŒŒì‹±
    args = parse_arguments()
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    logger.info("LLM ê¸°ë°˜ Q&A ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    logger.info(f"ì„¤ì •: {vars(args)}")
    
    # í™˜ê²½ ê²€ì¦
    if not validate_environment():
        logger.error("í™˜ê²½ ê²€ì¦ ì‹¤íŒ¨")
        return 1
    
    # Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_ollama_connection(args.model):
        logger.error("Ollama ì—°ê²° ì‹¤íŒ¨")
        return 1
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™•ì¸
    if args.dry_run:
        logger.info("ğŸ” í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì‹¤ì œ ìƒì„±í•˜ì§€ ì•ŠìŒ")
        logger.info("ì„¤ì • í™•ì¸ ì™„ë£Œ. --dry-run ì˜µì…˜ì„ ì œê±°í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.")
        return 0
    
    # Q&A ë°ì´í„°ì…‹ ìƒì„±
    success = generate_qa_dataset(args)
    
    if success:
        logger.info("ğŸ‰ LLM ê¸°ë°˜ Q&A ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
        return 0
    else:
        logger.error("âŒ LLM ê¸°ë°˜ Q&A ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
