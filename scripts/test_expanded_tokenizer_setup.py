#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í™•ì¥ëœ í† í¬ë‚˜ì´ì € ì„¤ì • í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

540ê°œ í›ˆë ¨ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœ í† í¬ë‚˜ì´ì € ì„¤ì •ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import sys
import os
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/expanded_tokenizer_test.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ExpandedTokenizerTester:
    """í™•ì¥ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """í† í¬ë‚˜ì´ì € í…ŒìŠ¤í„° ì´ˆê¸°í™”"""
        self.output_dir = Path("data/training")
        self.test_results = {}
        
        # í† í¬ë‚˜ì´ì € ì„¤ì • ë¡œë“œ
        self.tokenizer_config = self._load_tokenizer_config()
        
        logger.info("ExpandedTokenizerTester initialized")
    
    def _load_tokenizer_config(self) -> Dict[str, Any]:
        """í† í¬ë‚˜ì´ì € ì„¤ì • ë¡œë“œ"""
        config_path = self.output_dir / "tokenizer_config.json"
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            logger.warning("í† í¬ë‚˜ì´ì € ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
    
    def test_expanded_tokenizer(self) -> Dict[str, Any]:
        """í™•ì¥ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
        logger.info("í™•ì¥ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # 1. í† í¬ë‚˜ì´ì € ì„¤ì • ê²€ì¦
        config_test = self._test_tokenizer_config()
        
        # 2. íŠ¹ìˆ˜ í† í° í…ŒìŠ¤íŠ¸
        special_token_test = self._test_special_tokens()
        
        # 3. í›ˆë ¨ ë°ì´í„° í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸
        training_data_test = self._test_training_data_tokenization()
        
        # 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸
        template_test = self._test_prompt_template_tokenization()
        
        # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
        memory_test = self._test_memory_usage()
        
        # 6. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        performance_test = self._test_performance()
        
        # ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ í†µí•©
        self.test_results = {
            "config_test": config_test,
            "special_token_test": special_token_test,
            "training_data_test": training_data_test,
            "template_test": template_test,
            "memory_test": memory_test,
            "performance_test": performance_test,
            "overall_success": all([
                config_test["success"],
                special_token_test["success"],
                training_data_test["success"],
                template_test["success"],
                memory_test["success"],
                performance_test["success"]
            ]),
            "tested_at": datetime.now().isoformat()
        }
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        self._save_test_results()
        
        logger.info("í™•ì¥ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return self.test_results
    
    def _test_tokenizer_config(self) -> Dict[str, Any]:
        """í† í¬ë‚˜ì´ì € ì„¤ì • í…ŒìŠ¤íŠ¸"""
        logger.info("í† í¬ë‚˜ì´ì € ì„¤ì • í…ŒìŠ¤íŠ¸...")
        
        test_result = {
            "success": False,
            "config_loaded": False,
            "special_tokens_count": 0,
            "max_length": 0,
            "vocab_size": 0,
            "errors": []
        }
        
        try:
            if self.tokenizer_config:
                test_result["config_loaded"] = True
                test_result["special_tokens_count"] = len(self.tokenizer_config.get("special_tokens", []))
                test_result["max_length"] = self.tokenizer_config.get("max_length", 0)
                test_result["vocab_size"] = self.tokenizer_config.get("vocab_size", 0)
                
                # ì„¤ì • ê²€ì¦
                if test_result["special_tokens_count"] >= 7:  # ìµœì†Œ 7ê°œ íŠ¹ìˆ˜ í† í°
                    test_result["success"] = True
                else:
                    test_result["errors"].append(f"íŠ¹ìˆ˜ í† í° ìˆ˜ ë¶€ì¡±: {test_result['special_tokens_count']}ê°œ")
                
                if test_result["max_length"] >= 512:
                    test_result["success"] = test_result["success"] and True
                else:
                    test_result["errors"].append(f"ìµœëŒ€ ê¸¸ì´ ë¶€ì¡±: {test_result['max_length']}")
                
                if test_result["vocab_size"] >= 50000:
                    test_result["success"] = test_result["success"] and True
                else:
                    test_result["errors"].append(f"ì–´íœ˜ í¬ê¸° ë¶€ì¡±: {test_result['vocab_size']}")
            else:
                test_result["errors"].append("í† í¬ë‚˜ì´ì € ì„¤ì • ë¡œë“œ ì‹¤íŒ¨")
        
        except Exception as e:
            test_result["errors"].append(f"ì„¤ì • í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        logger.info(f"í† í¬ë‚˜ì´ì € ì„¤ì • í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'ì„±ê³µ' if test_result['success'] else 'ì‹¤íŒ¨'}")
        return test_result
    
    def _test_special_tokens(self) -> Dict[str, Any]:
        """íŠ¹ìˆ˜ í† í° í…ŒìŠ¤íŠ¸"""
        logger.info("íŠ¹ìˆ˜ í† í° í…ŒìŠ¤íŠ¸...")
        
        test_result = {
            "success": False,
            "required_tokens": [
                "<|startoftext|>",
                "<|endoftext|>",
                "ì§ˆë¬¸:",
                "ë‹µë³€:",
                "ë¶„ì„:",
                "ì„¤ëª…:",
                "ì¡°ì–¸:"
            ],
            "found_tokens": [],
            "missing_tokens": [],
            "errors": []
        }
        
        try:
            special_tokens = self.tokenizer_config.get("special_tokens", [])
            test_result["found_tokens"] = special_tokens
            
            for required_token in test_result["required_tokens"]:
                if required_token in special_tokens:
                    continue
                else:
                    test_result["missing_tokens"].append(required_token)
            
            if len(test_result["missing_tokens"]) == 0:
                test_result["success"] = True
            else:
                test_result["errors"].append(f"ëˆ„ë½ëœ íŠ¹ìˆ˜ í† í°: {test_result['missing_tokens']}")
        
        except Exception as e:
            test_result["errors"].append(f"íŠ¹ìˆ˜ í† í° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        logger.info(f"íŠ¹ìˆ˜ í† í° í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'ì„±ê³µ' if test_result['success'] else 'ì‹¤íŒ¨'}")
        return test_result
    
    def _test_training_data_tokenization(self) -> Dict[str, Any]:
        """í›ˆë ¨ ë°ì´í„° í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸"""
        logger.info("í›ˆë ¨ ë°ì´í„° í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸...")
        
        test_result = {
            "success": False,
            "samples_tested": 0,
            "average_length": 0,
            "max_length_exceeded": 0,
            "min_length": 0,
            "max_length": 0,
            "errors": []
        }
        
        try:
            # í›ˆë ¨ ë°ì´í„° ë¡œë“œ
            train_data_path = self.output_dir / "train_split.json"
            if train_data_path.exists():
                with open(train_data_path, 'r', encoding='utf-8') as f:
                    train_data = json.load(f)
                
                test_result["samples_tested"] = len(train_data)
                
                lengths = []
                max_allowed = self.tokenizer_config.get("max_length", 512)
                
                for item in train_data[:10]:  # ì²˜ìŒ 10ê°œ ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸
                    text = item.get("text", "")
                    length = len(text.split())  # ë‹¨ì–´ ìˆ˜ë¡œ ê·¼ì‚¬
                    lengths.append(length)
                    
                    if length > max_allowed:
                        test_result["max_length_exceeded"] += 1
                
                if lengths:
                    test_result["average_length"] = sum(lengths) / len(lengths)
                    test_result["min_length"] = min(lengths)
                    test_result["max_length"] = max(lengths)
                    
                    # í‰ê·  ê¸¸ì´ê°€ í•©ë¦¬ì ì¸ ë²”ìœ„ì— ìˆëŠ”ì§€ í™•ì¸
                    if 50 <= test_result["average_length"] <= 400:
                        test_result["success"] = True
                    else:
                        test_result["errors"].append(f"í‰ê·  ê¸¸ì´ê°€ ë¶€ì ì ˆ: {test_result['average_length']:.1f}")
                    
                    # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ìƒ˜í”Œì´ 20% ì´í•˜ì¸ì§€ í™•ì¸
                    if test_result["max_length_exceeded"] <= len(lengths) * 0.2:
                        test_result["success"] = test_result["success"] and True
                    else:
                        test_result["errors"].append(f"ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ìƒ˜í”Œì´ ë§ìŒ: {test_result['max_length_exceeded']}ê°œ")
                else:
                    test_result["errors"].append("í† í¬ë‚˜ì´ì§•í•  ë°ì´í„°ê°€ ì—†ìŒ")
            else:
                test_result["errors"].append("í›ˆë ¨ ë°ì´í„° íŒŒì¼ì´ ì—†ìŒ")
        
        except Exception as e:
            test_result["errors"].append(f"í›ˆë ¨ ë°ì´í„° í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        logger.info(f"í›ˆë ¨ ë°ì´í„° í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'ì„±ê³µ' if test_result['success'] else 'ì‹¤íŒ¨'}")
        return test_result
    
    def _test_prompt_template_tokenization(self) -> Dict[str, Any]:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸"""
        logger.info("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸...")
        
        test_result = {
            "success": False,
            "templates_tested": 0,
            "template_lengths": {},
            "errors": []
        }
        
        try:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í…ŒìŠ¤íŠ¸
            test_templates = [
                "ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n\nì§ˆë¬¸: ë¯¼ë²•ì—ì„œ ê³„ì•½ë²•ì˜ ê³„ì•½ì˜ ì„±ë¦½ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?\në‹µë³€: ë¯¼ë²•ì—ì„œ ê³„ì•½ë²•ì˜ ê³„ì•½ì˜ ì„±ë¦½ ìš”ê±´ì€ ì²­ì•½ê³¼ ìŠ¹ë‚™ì˜ í•©ì¹˜ë¡œ ì„±ë¦½ì…ë‹ˆë‹¤.",
                "ë²•ì¡°ë¬¸ì„ í•´ì„í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n\nì§ˆë¬¸: ë¯¼ë²• ì œ527ì¡°ì˜ ë‚´ìš©ê³¼ ì˜ë¯¸ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.\në‹µë³€: ë¯¼ë²• ì œ527ì¡°ì€ ê³„ì•½ì˜ ì„±ë¦½ ìš”ê±´ì— ê´€í•œ ê·œì •ìœ¼ë¡œ, 'ì²­ì•½ê³¼ ìŠ¹ë‚™ì˜ í•©ì¹˜ë¡œ ì„±ë¦½'ë¼ê³  ê·œì •í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
                "íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n\nì§ˆë¬¸: ëŒ€ë²•ì› 2018ë‹¤22222 ì‚¬ê±´ì˜ ìš”ì•½ì„ í•´ì£¼ì„¸ìš”.\në‹µë³€: ëŒ€ë²•ì› 2018ë‹¤22222 ì‚¬ê±´ì€ ë¶€ë™ì‚° ë§¤ë§¤ ê³„ì•½ í•´ì œ ì‹œ ì›ìƒíšŒë³µì— ê´€í•œ ì‚¬ê±´ìœ¼ë¡œ, ëŒ€ë²•ì›ì—ì„œ 2018.12.13ì— ì„ ê³ ë˜ì—ˆìŠµë‹ˆë‹¤."
            ]
            
            test_result["templates_tested"] = len(test_templates)
            
            for i, template in enumerate(test_templates):
                length = len(template.split())
                test_result["template_lengths"][f"template_{i+1}"] = length
            
            # ëª¨ë“  í…œí”Œë¦¿ì´ ì ì ˆí•œ ê¸¸ì´ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
            avg_length = sum(test_result["template_lengths"].values()) / len(test_result["template_lengths"])
            if 100 <= avg_length <= 300:
                test_result["success"] = True
            else:
                test_result["errors"].append(f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í‰ê·  ê¸¸ì´ê°€ ë¶€ì ì ˆ: {avg_length:.1f}")
        
        except Exception as e:
            test_result["errors"].append(f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        logger.info(f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'ì„±ê³µ' if test_result['success'] else 'ì‹¤íŒ¨'}")
        return test_result
    
    def _test_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        logger.info("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸...")
        
        test_result = {
            "success": False,
            "estimated_memory_mb": 0,
            "memory_efficient": False,
            "errors": []
        }
        
        try:
            # ë°ì´í„°ì…‹ í¬ê¸° ê¸°ë°˜ ë©”ëª¨ë¦¬ ì¶”ì •
            train_data_path = self.output_dir / "train_split.json"
            if train_data_path.exists():
                file_size = train_data_path.stat().st_size
                # JSON íŒŒì¼ í¬ê¸°ì˜ ì•½ 3-5ë°°ê°€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                estimated_memory = (file_size * 4) / (1024 * 1024)  # MB
                test_result["estimated_memory_mb"] = estimated_memory
                
                # 1GB ì´í•˜ë©´ íš¨ìœ¨ì 
                if estimated_memory <= 1024:
                    test_result["memory_efficient"] = True
                    test_result["success"] = True
                else:
                    test_result["errors"].append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŒ: {estimated_memory:.1f}MB")
            else:
                test_result["errors"].append("í›ˆë ¨ ë°ì´í„° íŒŒì¼ì´ ì—†ìŒ")
        
        except Exception as e:
            test_result["errors"].append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'ì„±ê³µ' if test_result['success'] else 'ì‹¤íŒ¨'}")
        return test_result
    
    def _test_performance(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        test_result = {
            "success": False,
            "processing_speed": 0,
            "efficient_processing": False,
            "errors": []
        }
        
        try:
            import time
            
            # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸
            test_text = "ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n\nì§ˆë¬¸: ë¯¼ë²•ì—ì„œ ê³„ì•½ë²•ì˜ ê³„ì•½ì˜ ì„±ë¦½ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?\në‹µë³€: ë¯¼ë²•ì—ì„œ ê³„ì•½ë²•ì˜ ê³„ì•½ì˜ ì„±ë¦½ ìš”ê±´ì€ ì²­ì•½ê³¼ ìŠ¹ë‚™ì˜ í•©ì¹˜ë¡œ ì„±ë¦½ì…ë‹ˆë‹¤."
            
            start_time = time.time()
            
            # ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            words = test_text.split()
            processed_words = [word for word in words if len(word) > 0]
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            test_result["processing_speed"] = len(processed_words) / processing_time if processing_time > 0 else 0
            
            # ì´ˆë‹¹ 1000ë‹¨ì–´ ì´ìƒ ì²˜ë¦¬í•˜ë©´ íš¨ìœ¨ì 
            if test_result["processing_speed"] >= 1000:
                test_result["efficient_processing"] = True
                test_result["success"] = True
            else:
                test_result["errors"].append(f"ì²˜ë¦¬ ì†ë„ê°€ ëŠë¦¼: {test_result['processing_speed']:.1f} ë‹¨ì–´/ì´ˆ")
        
        except Exception as e:
            test_result["errors"].append(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        logger.info(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'ì„±ê³µ' if test_result['success'] else 'ì‹¤íŒ¨'}")
        return test_result
    
    def _save_test_results(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        results_path = self.output_dir / "expanded_tokenizer_test_report.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}")
    
    def print_test_summary(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        if not self.test_results:
            logger.warning("í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*60)
        print("ğŸ“Š í™•ì¥ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        # ì „ì²´ ì„±ê³µ ì—¬ë¶€
        overall_success = self.test_results.get("overall_success", False)
        print(f"ğŸ¯ ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'âœ… ì„±ê³µ' if overall_success else 'âŒ ì‹¤íŒ¨'}")
        
        # ê° í…ŒìŠ¤íŠ¸ë³„ ê²°ê³¼
        tests = [
            ("í† í¬ë‚˜ì´ì € ì„¤ì •", "config_test"),
            ("íŠ¹ìˆ˜ í† í°", "special_token_test"),
            ("í›ˆë ¨ ë°ì´í„° í† í¬ë‚˜ì´ì§•", "training_data_test"),
            ("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿", "template_test"),
            ("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", "memory_test"),
            ("ì„±ëŠ¥", "performance_test")
        ]
        
        print(f"\nğŸ“‹ ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        for test_name, test_key in tests:
            test_result = self.test_results.get(test_key, {})
            success = test_result.get("success", False)
            status = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨"
            print(f"  - {test_name}: {status}")
            
            if not success and test_result.get("errors"):
                for error in test_result["errors"][:2]:  # ìµœëŒ€ 2ê°œ ì˜¤ë¥˜ë§Œ í‘œì‹œ
                    print(f"    âš ï¸ {error}")
        
        # ìƒì„¸ í†µê³„
        config_test = self.test_results.get("config_test", {})
        training_test = self.test_results.get("training_data_test", {})
        memory_test = self.test_results.get("memory_test", {})
        
        print(f"\nğŸ“ˆ ìƒì„¸ í†µê³„:")
        print(f"  - íŠ¹ìˆ˜ í† í° ìˆ˜: {config_test.get('special_tokens_count', 0)}ê°œ")
        print(f"  - ìµœëŒ€ ê¸¸ì´: {config_test.get('max_length', 0)}")
        print(f"  - ì–´íœ˜ í¬ê¸°: {config_test.get('vocab_size', 0):,}")
        print(f"  - í…ŒìŠ¤íŠ¸ëœ ìƒ˜í”Œ ìˆ˜: {training_test.get('samples_tested', 0)}ê°œ")
        print(f"  - í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {training_test.get('average_length', 0):.1f} ë‹¨ì–´")
        print(f"  - ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_test.get('estimated_memory_mb', 0):.1f}MB")
        
        print("="*60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("í™•ì¥ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í† í¬ë‚˜ì´ì € í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = ExpandedTokenizerTester()
    
    # í™•ì¥ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    test_results = tester.test_expanded_tokenizer()
    
    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    tester.print_test_summary()
    
    logger.info("í™•ì¥ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
