#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒë¡€ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

íŒë¡€ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""

import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from scripts.data_processing.auto_data_detector import AutoDataDetector
from scripts.data_processing.precedent_preprocessor import PrecedentPreprocessor
from scripts.data_processing.incremental_precedent_preprocessor import IncrementalPrecedentPreprocessor
from scripts.ml_training.vector_embedding.incremental_precedent_vector_builder import IncrementalPrecedentVectorBuilder
from scripts.data_processing.utilities.import_precedents_to_db import PrecedentDataImporter
from scripts.data_processing.auto_pipeline_orchestrator import AutoPipelineOrchestrator
from source.data.database import DatabaseManager

logger = logging.getLogger(__name__)


def test_precedent_data_detection():
    """íŒë¡€ ë°ì´í„° ê°ì§€ í…ŒìŠ¤íŠ¸"""
    logger.info("Testing precedent data detection...")
    
    try:
        detector = AutoDataDetector()
        
        # civil ì¹´í…Œê³ ë¦¬ ë°ì´í„° ê°ì§€ í…ŒìŠ¤íŠ¸
        detected_files = detector.detect_new_data_sources(
            "data/raw/assembly/precedent", 
            "precedent_civil"
        )
        
        logger.info(f"Detected files: {detected_files}")
        
        if detected_files.get('precedent_civil'):
            logger.info(f"âœ“ Found {len(detected_files['precedent_civil'])} civil precedent files")
            return True
        else:
            logger.warning("âœ— No civil precedent files detected")
            return False
            
    except Exception as e:
        logger.error(f"âœ— Data detection test failed: {e}")
        return False


def test_precedent_preprocessing():
    """íŒë¡€ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    logger.info("Testing precedent preprocessing...")
    
    try:
        # ìƒ˜í”Œ íŒë¡€ ë°ì´í„° ìƒì„±
        sample_data = {
            "metadata": {
                "data_type": "precedent",
                "category": "civil",
                "page_number": 1,
                "collected_at": "2025-10-16T10:00:00"
            },
            "items": [
                {
                    "case_name": "íŠ¹í—ˆì¹¨í•´ê¸ˆì§€ë°ì†í•´ë°°ìƒì²­êµ¬ì˜ì†Œ",
                    "case_number": "2017ë‚˜2684",
                    "decision_date": "2018.8.16",
                    "field": "ë¯¼ì‚¬",
                    "court": "íŠ¹í—ˆë²•ì›",
                    "detail_url": "https://example.com",
                    "structured_content": {
                        "case_info": {
                            "case_title": "íŠ¹í—ˆë²•ì› 2018. 8. 16. ì„ ê³  2017ë‚˜2684 íŒê²°",
                            "decision_date": "2018-08-16",
                            "case_number": "2017ë‚˜2684"
                        },
                        "legal_sections": {
                            "íŒì‹œì‚¬í•­": "íŠ¹í—ˆì¹¨í•´ì— ê´€í•œ íŒì‹œì‚¬í•­",
                            "íŒê²°ìš”ì§€": "íŠ¹í—ˆì¹¨í•´ê°€ ì¸ì •ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” íŒê²°ìš”ì§€",
                            "ì°¸ì¡°ì¡°ë¬¸": "íŠ¹í—ˆë²• ì œ126ì¡°",
                            "ì°¸ì¡°íŒë¡€": "ëŒ€ë²•ì› 2015ë‹¤12345 íŒê²°",
                            "ì£¼ë¬¸": "ì›ê³ ì˜ ì²­êµ¬ë¥¼ ê¸°ê°í•œë‹¤",
                            "ì´ìœ ": "íŠ¹í—ˆì¹¨í•´ê°€ ì¸ì •ë˜ì§€ ì•ŠëŠ” ì´ìœ "
                        },
                        "parties": {
                            "plaintiff": "ì›ê³ ",
                            "defendant": "í”¼ê³ "
                        }
                    }
                }
            ]
        }
        
        preprocessor = PrecedentPreprocessor()
        processed_data = preprocessor.process_precedent_data(sample_data)
        
        if processed_data.get('cases') and len(processed_data['cases']) > 0:
            case = processed_data['cases'][0]
            logger.info(f"âœ“ Preprocessing successful: {case['case_name']}")
            logger.info(f"  - Sections: {len(case.get('sections', []))}")
            logger.info(f"  - Parties: {len(case.get('parties', []))}")
            return True
        else:
            logger.error("âœ— Preprocessing failed: No cases processed")
            return False
            
    except Exception as e:
        logger.error(f"âœ— Preprocessing test failed: {e}")
        return False


def test_database_schema():
    """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ í…ŒìŠ¤íŠ¸"""
    logger.info("Testing database schema...")
    
    try:
        db_manager = DatabaseManager()
        
        # íŒë¡€ í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        tables_to_check = [
            'precedent_cases',
            'precedent_sections', 
            'precedent_parties',
            'fts_precedent_cases',
            'fts_precedent_sections'
        ]
        
        for table in tables_to_check:
            result = db_manager.execute_query(f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table}'")
            if result:
                logger.info(f"âœ“ Table {table} exists")
            else:
                logger.error(f"âœ— Table {table} does not exist")
                return False
        
        logger.info("âœ“ All precedent tables exist")
        return True
        
    except Exception as e:
        logger.error(f"âœ— Database schema test failed: {e}")
        return False


def test_precedent_import():
    """íŒë¡€ DB ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸"""
    logger.info("Testing precedent database import...")
    
    try:
        # ìƒ˜í”Œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ìƒì„±
        sample_processed_data = {
            "metadata": {
                "data_type": "precedent",
                "category": "civil",
                "processed_at": datetime.now().isoformat(),
                "total_cases": 1,
                "processing_version": "1.0"
            },
            "cases": [
                {
                    "case_id": "test_case_001",
                    "category": "civil",
                    "case_name": "í…ŒìŠ¤íŠ¸ ì‚¬ê±´",
                    "case_number": "2025í…Œ001",
                    "decision_date": "2025-10-16",
                    "field": "ë¯¼ì‚¬",
                    "court": "ì„œìš¸ì¤‘ì•™ì§€ë°©ë²•ì›",
                    "detail_url": "https://test.com",
                    "full_text": "í…ŒìŠ¤íŠ¸ ì‚¬ê±´ì˜ ì „ì²´ í…ìŠ¤íŠ¸",
                    "searchable_text": "í…ŒìŠ¤íŠ¸ ì‚¬ê±´",
                    "sections": [
                        {
                            "section_type": "points_at_issue",
                            "section_type_korean": "íŒì‹œì‚¬í•­",
                            "section_content": "í…ŒìŠ¤íŠ¸ íŒì‹œì‚¬í•­",
                            "section_length": 10,
                            "has_content": True
                        }
                    ],
                    "parties": [
                        {
                            "party_type": "plaintiff",
                            "party_type_korean": "ì›ê³ ",
                            "party_content": "í…ŒìŠ¤íŠ¸ ì›ê³ ",
                            "party_length": 5
                        }
                    ],
                    "data_quality": {
                        "parsing_quality_score": 0.9,
                        "content_length": 100,
                        "section_count": 1
                    },
                    "processed_at": datetime.now().isoformat(),
                    "status": "success"
                }
            ]
        }
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        temp_file = Path("temp_test_precedent.json")
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(sample_processed_data, f, ensure_ascii=False, indent=2)
        
        # ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸
        importer = PrecedentDataImporter()
        result = importer.import_file(temp_file, incremental=True)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        temp_file.unlink()
        
        if result.get('imported_cases', 0) > 0 or result.get('updated_cases', 0) > 0:
            logger.info(f"âœ“ Import successful: {result}")
            return True
        else:
            logger.error(f"âœ— Import failed: {result}")
            return False
            
    except Exception as e:
        logger.error(f"âœ— Import test failed: {e}")
        return False


def test_precedent_pipeline():
    """ì „ì²´ íŒë¡€ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    logger.info("Testing full precedent pipeline...")
    
    try:
        orchestrator = AutoPipelineOrchestrator()
        
        # civil ì¹´í…Œê³ ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = orchestrator.run_precedent_pipeline(
            category="civil",
            auto_detect=True
        )
        
        if result.success:
            logger.info(f"âœ“ Pipeline successful: {result}")
            return True
        else:
            logger.error(f"âœ— Pipeline failed: {result.error_messages}")
            return False
            
    except Exception as e:
        logger.error(f"âœ— Pipeline test failed: {e}")
        return False


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info("Starting precedent pipeline tests...")
    
    tests = [
        ("Data Detection", test_precedent_data_detection),
        ("Preprocessing", test_precedent_preprocessing),
        ("Database Schema", test_database_schema),
        ("Database Import", test_precedent_import),
        ("Full Pipeline", test_precedent_pipeline)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        logger.info(f"\n{'='*50}")
        logger.info(f"Running {test_name} test...")
        logger.info(f"{'='*50}")
        
        try:
            success = test_func()
            results[test_name] = success
        except Exception as e:
            logger.error(f"Test {test_name} crashed: {e}")
            results[test_name] = False
    
    # ê²°ê³¼ ìš”ì•½
    logger.info(f"\n{'='*50}")
    logger.info("TEST RESULTS SUMMARY")
    logger.info(f"{'='*50}")
    
    passed = 0
    total = len(results)
    
    for test_name, success in results.items():
        status = "âœ“ PASSED" if success else "âœ— FAILED"
        logger.info(f"{test_name}: {status}")
        if success:
            passed += 1
    
    logger.info(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("ğŸ‰ All tests passed!")
        return True
    else:
        logger.error("âŒ Some tests failed!")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
