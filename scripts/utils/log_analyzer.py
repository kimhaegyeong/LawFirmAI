#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¡œê·¸ ë¶„ì„ ìœ í‹¸ë¦¬í‹°

ì›Œí¬í”Œë¡œìš° ë¡œê·¸ë¥¼ ë¶„ì„í•˜ëŠ” ê³µí†µ í•¨ìˆ˜ë“¤
"""

import re
from typing import Dict, List, Any


def analyze_sources_conversion_logs(log_content: str) -> Dict[str, Any]:
    """Sources ë³€í™˜ ê´€ë ¨ ë¡œê·¸ ë¶„ì„"""
    analysis = {
        "conversion_statistics": [],
        "fallback_usage": [],
        "critical_fallbacks": [],
        "lost_documents": [],
        "total_conversions": 0,
        "total_docs": 0,
        "total_failed": 0,
    }
    
    # Conversion statistics íŒ¨í„´
    pattern = r'\[SOURCES\] ğŸ“Š Conversion statistics: (\d+)/(\d+) docs converted \(([\d.]+)%\), failed: (\d+)'
    matches = re.findall(pattern, log_content)
    for match in matches:
        created, total, rate, failed = match
        analysis["conversion_statistics"].append({
            "created": int(created),
            "total": int(total),
            "rate": float(rate),
            "failed": int(failed)
        })
        analysis["total_conversions"] += int(created)
        analysis["total_docs"] += int(total)
        analysis["total_failed"] += int(failed)
    
    # Fallback source ìƒì„± íŒ¨í„´
    fallback_pattern = r'\[SOURCES\] âœ… Generated fallback source for doc (\d+)/(\d+): (.+)'
    fallback_matches = re.findall(fallback_pattern, log_content)
    for match in fallback_matches:
        analysis["fallback_usage"].append({
            "doc_index": int(match[0]),
            "total_docs": int(match[1]),
            "source": match[2]
        })
    
    # Critical fallback íŒ¨í„´
    critical_pattern = r'\[SOURCES\] âš ï¸ CRITICAL: Using final fallback for doc (\d+)/(\d+): (.+)'
    critical_matches = re.findall(critical_pattern, log_content)
    for match in critical_matches:
        analysis["critical_fallbacks"].append({
            "doc_index": int(match[0]),
            "total_docs": int(match[1]),
            "source": match[2]
        })
    
    # Lost documents íŒ¨í„´
    lost_pattern = r'\[SOURCES\] âš ï¸ Lost document.*?doc_index=(\d+).*?type=([^,]+)'
    lost_matches = re.findall(lost_pattern, log_content)
    for match in lost_matches:
        analysis["lost_documents"].append({
            "doc_index": int(match[0]),
            "type": match[1]
        })
    
    return analysis


def analyze_legal_references_logs(log_content: str) -> Dict[str, Any]:
    """Legal References ê´€ë ¨ ë¡œê·¸ ë¶„ì„"""
    analysis = {
        "extracted_from_sources": 0,
        "extracted_from_content": 0,
        "extracted_from_docs": 0,
        "total_extracted": 0,
        "legal_references": []
    }
    
    # Legal references ì¶”ì¶œ íŒ¨í„´
    pattern = r'\[LEGAL_REFS\] Extracted (\d+) legal references'
    matches = re.findall(pattern, log_content)
    for match in matches:
        analysis["total_extracted"] += int(match)
    
    # Sourcesì—ì„œ ì¶”ì¶œ
    sources_pattern = r'\[LEGAL_REFS\] From sources_detail: (\d+) references'
    sources_matches = re.findall(sources_pattern, log_content)
    for match in sources_matches:
        analysis["extracted_from_sources"] += int(match)
    
    # Contentì—ì„œ ì¶”ì¶œ
    content_pattern = r'\[LEGAL_REFS\] From content: (\d+) references'
    content_matches = re.findall(content_pattern, log_content)
    for match in content_matches:
        analysis["extracted_from_content"] += int(match)
    
    # Docsì—ì„œ ì¶”ì¶œ
    docs_pattern = r'\[LEGAL_REFS\] From retrieved_docs: (\d+) references'
    docs_matches = re.findall(docs_pattern, log_content)
    for match in docs_matches:
        analysis["extracted_from_docs"] += int(match)
    
    return analysis


def analyze_answer_length_logs(log_content: str) -> Dict[str, Any]:
    """ë‹µë³€ ê¸¸ì´ ê´€ë ¨ ë¡œê·¸ ë¶„ì„"""
    analysis = {
        "length_warnings": [],
        "length_adjustments": [],
        "too_short_count": 0,
        "too_long_count": 0,
        "adjusted_count": 0,
    }
    
    # ë„ˆë¬´ ì§§ì€ ê²½ìš°
    short_pattern = r'\[ANSWER LENGTH\] âš ï¸ Too short: (\d+) \(target: (\d+)-(\d+)\)'
    short_matches = re.findall(short_pattern, log_content)
    for match in short_matches:
        analysis["length_warnings"].append({
            "current": int(match[0]),
            "min_target": int(match[1]),
            "max_target": int(match[2])
        })
        analysis["too_short_count"] += 1
    
    # ë„ˆë¬´ ê¸´ ê²½ìš°
    long_pattern = r'\[ANSWER LENGTH\] Too long: (\d+), adjusting to max (\d+)'
    long_matches = re.findall(long_pattern, log_content)
    for match in long_matches:
        analysis["length_adjustments"].append({
            "original": int(match[0]),
            "max": int(match[1])
        })
        analysis["too_long_count"] += 1
        analysis["adjusted_count"] += 1
    
    return analysis


def analyze_context_usage_logs(log_content: str) -> Dict[str, Any]:
    """Context Usage ê´€ë ¨ ë¡œê·¸ ë¶„ì„"""
    analysis = {
        "coverage_scores": [],
        "relevance_scores": [],
        "average_coverage": 0.0,
        "average_relevance": 0.0,
    }
    
    # Coverage ì ìˆ˜ íŒ¨í„´
    coverage_pattern = r'\[COVERAGE\] Coverage score: ([\d.]+)'
    coverage_matches = re.findall(coverage_pattern, log_content)
    for match in coverage_matches:
        score = float(match)
        analysis["coverage_scores"].append(score)
    
    # Relevance ì ìˆ˜ íŒ¨í„´
    relevance_pattern = r'\[RELEVANCE\] Relevance score: ([\d.]+)'
    relevance_matches = re.findall(relevance_pattern, log_content)
    for match in relevance_matches:
        score = float(match)
        analysis["relevance_scores"].append(score)
    
    if analysis["coverage_scores"]:
        analysis["average_coverage"] = sum(analysis["coverage_scores"]) / len(analysis["coverage_scores"])
    
    if analysis["relevance_scores"]:
        analysis["average_relevance"] = sum(analysis["relevance_scores"]) / len(analysis["relevance_scores"])
    
    return analysis


def identify_improvements(analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ì‚¬í•­ ì‹ë³„"""
    improvements = []
    
    # Sources ë³€í™˜ë¥  ê°œì„ 
    sources_analysis = analysis_results.get("sources", {})
    if sources_analysis.get("total_docs", 0) > 0:
        avg_conversion_rate = (sources_analysis.get("total_conversions", 0) / 
                              sources_analysis.get("total_docs", 1)) * 100
        if avg_conversion_rate < 90:
            improvements.append({
                "category": "Sources ë³€í™˜ë¥ ",
                "priority": "HIGH",
                "current": f"{avg_conversion_rate:.1f}%",
                "target": "90% ì´ìƒ",
                "description": f"í˜„ì¬ ë³€í™˜ë¥ ì´ {avg_conversion_rate:.1f}%ë¡œ ëª©í‘œ(90%) ë¯¸ë§Œì…ë‹ˆë‹¤.",
                "recommendation": "fallback ë¡œì§ ê°•í™”, source_type ì¶”ë¡  ê°œì„  í•„ìš”"
            })
        
        if sources_analysis.get("critical_fallbacks"):
            improvements.append({
                "category": "Critical Fallback ì‚¬ìš©",
                "priority": "MEDIUM",
                "current": f"{len(sources_analysis['critical_fallbacks'])}ê±´",
                "target": "0ê±´",
                "description": f"ìµœì¢… fallbackì´ {len(sources_analysis['critical_fallbacks'])}ê±´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "recommendation": "metadata ì¶”ì¶œ ë¡œì§ ê°œì„ , content ê¸°ë°˜ ì¶”ë¡  ê°•í™”"
            })
    
    # Legal References ìƒì„±ë¥  ê°œì„ 
    legal_analysis = analysis_results.get("legal_references", {})
    if legal_analysis.get("total_extracted", 0) == 0:
        improvements.append({
            "category": "Legal References ìƒì„±",
            "priority": "HIGH",
            "current": "0ê°œ",
            "target": "statute_article ë¬¸ì„œ ìˆ˜ë§Œí¼",
            "description": "Legal referencesê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "recommendation": "legal_references ì¶”ì¶œ ë¡œì§ ê²€ì¦ ë° ê°œì„  í•„ìš”"
        })
    
    # ë‹µë³€ ê¸¸ì´ ê°œì„ 
    length_analysis = analysis_results.get("answer_length", {})
    if length_analysis.get("too_short_count", 0) > 0:
        improvements.append({
            "category": "ë‹µë³€ ê¸¸ì´",
            "priority": "MEDIUM",
            "current": f"{length_analysis['too_short_count']}ê±´ ë„ˆë¬´ ì§§ìŒ",
            "target": "ëª¨ë“  ë‹µë³€ì´ ìµœì†Œ ê¸¸ì´ ì´ìƒ",
            "description": f"{length_analysis['too_short_count']}ê±´ì˜ ë‹µë³€ì´ ìµœì†Œ ê¸¸ì´ ë¯¸ë§Œì…ë‹ˆë‹¤.",
            "recommendation": "í”„ë¡¬í”„íŠ¸ ê°œì„ , ì»¨í…ìŠ¤íŠ¸ í™œìš© ê°•í™”"
        })
    
    # Context Usage ê°œì„ 
    context_analysis = analysis_results.get("context_usage", {})
    avg_coverage = context_analysis.get("average_coverage", 0.0)
    if avg_coverage < 0.8:
        improvements.append({
            "category": "Context Usage",
            "priority": "MEDIUM",
            "current": f"{avg_coverage:.2f}",
            "target": "0.80 ì´ìƒ",
            "description": f"í‰ê·  coverageê°€ {avg_coverage:.2f}ë¡œ ëª©í‘œ(0.80) ë¯¸ë§Œì…ë‹ˆë‹¤.",
            "recommendation": "í”„ë¡¬í”„íŠ¸ ê°œì„ , ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í–¥ìƒ"
        })
    
    return improvements

