#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
statutes_articles í…Œì´ë¸” ì¤‘ë³µ ë°ì´í„° ê²€í†  ìŠ¤í¬ë¦½íŠ¸
PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì˜ statutes_articles í…Œì´ë¸”ì—ì„œ ì¤‘ë³µëœ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
"""

import argparse
import logging
import os
import sys
from pathlib import Path
from typing import List, Dict, Any

from sqlalchemy import create_engine, text

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
_CURRENT_FILE = Path(__file__).resolve()
# scripts/ingest/open_law/scripts/check_duplicate_articles.py -> í”„ë¡œì íŠ¸ ë£¨íŠ¸
_PROJECT_ROOT = _CURRENT_FILE.parents[4]
if str(_PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(_PROJECT_ROOT))

# .env íŒŒì¼ ë¡œë“œ (utils/env_loader.py ì‚¬ìš©)
try:
    from utils.env_loader import ensure_env_loaded
    ensure_env_loaded(_PROJECT_ROOT)
except ImportError:
    try:
        from dotenv import load_dotenv
        scripts_env = _PROJECT_ROOT / "scripts" / ".env"
        if scripts_env.exists():
            load_dotenv(dotenv_path=str(scripts_env), override=True)
        root_env = _PROJECT_ROOT / ".env"
        if root_env.exists():
            load_dotenv(dotenv_path=str(root_env), override=False)
    except ImportError:
        pass

# ê³µí†µ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from scripts.ingest.open_law.utils import build_database_url

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def check_duplicate_articles(conn) -> Dict[str, Any]:
    """statutes_articles í…Œì´ë¸”ì˜ ì¤‘ë³µ ë°ì´í„° í™•ì¸"""
    
    results = {
        'total_count': 0,
        'duplicate_groups': [],
        'duplicate_count': 0,
        'unique_count': 0
    }
    
    # ì „ì²´ ë ˆì½”ë“œ ìˆ˜
    result = conn.execute(text("SELECT COUNT(*) FROM statutes_articles"))
    results['total_count'] = result.fetchone()[0]
    
    # ì¤‘ë³µ í™•ì¸ ê¸°ì¤€: statute_id, article_no, article_title, article_content, clause_no
    duplicate_query = text("""
        SELECT 
            statute_id,
            article_no,
            article_title,
            article_content,
            clause_no,
            COUNT(*) as duplicate_count,
            MIN(id) as min_id,
            MAX(id) as max_id,
            MIN(collected_at) as first_collected,
            MAX(collected_at) as last_collected,
            ARRAY_AGG(id ORDER BY id) as all_ids
        FROM statutes_articles
        GROUP BY statute_id, article_no, article_title, article_content, clause_no
        HAVING COUNT(*) > 1
        ORDER BY duplicate_count DESC, statute_id, article_no
    """)
    
    result = conn.execute(duplicate_query)
    duplicate_groups = result.fetchall()
    
    results['duplicate_groups'] = [
        {
            'statute_id': row[0],
            'article_no': row[1],
            'article_title': row[2] or '',
            'article_content': row[3] or '',
            'clause_no': row[4] or '',
            'duplicate_count': row[5],
            'min_id': row[6],
            'max_id': row[7],
            'first_collected': row[8],
            'last_collected': row[9],
            'all_ids': row[10]
        }
        for row in duplicate_groups
    ]
    
    results['duplicate_count'] = sum(group['duplicate_count'] for group in results['duplicate_groups'])
    results['unique_count'] = results['total_count'] - results['duplicate_count'] + len(results['duplicate_groups'])
    
    return results


def get_duplicate_details(conn, statute_id: int, article_no: str, article_title: str = None,
                          article_content: str = None, clause_no: str = None) -> List[Dict[str, Any]]:
    """íŠ¹ì • ì¡°ë¬¸ì˜ ì¤‘ë³µ ë ˆì½”ë“œ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    
    query = text("""
        SELECT 
            sa.id,
            sa.statute_id,
            s.law_name_kr,
            sa.article_no,
            sa.article_title,
            sa.clause_no,
            sa.item_no,
            sa.sub_item_no,
            LEFT(sa.article_content, 100) as content_preview,
            sa.effective_date,
            sa.collected_at
        FROM statutes_articles sa
        JOIN statutes s ON sa.statute_id = s.id
        WHERE sa.statute_id = :statute_id
          AND sa.article_no = :article_no
          AND COALESCE(sa.article_title, '') = COALESCE(:article_title, '')
          AND sa.article_content = :article_content
          AND COALESCE(sa.clause_no, '') = COALESCE(:clause_no, '')
        ORDER BY sa.id
    """)
    
    result = conn.execute(query, {
        'statute_id': statute_id,
        'article_no': article_no,
        'article_title': article_title,
        'article_content': article_content,
        'clause_no': clause_no
    })
    
    return [
        {
            'id': row[0],
            'statute_id': row[1],
            'law_name_kr': row[2],
            'article_no': row[3],
            'article_title': row[4],
            'clause_no': row[5] or '',
            'item_no': row[6] or '',
            'sub_item_no': row[7] or '',
            'content_preview': row[8],
            'effective_date': row[9],
            'collected_at': row[10]
        }
        for row in result.fetchall()
    ]


def generate_cleanup_sql(duplicate_groups: List[Dict[str, Any]], keep_oldest: bool = True) -> str:
    """ì¤‘ë³µ ë°ì´í„° ì œê±°ë¥¼ ìœ„í•œ SQL ìƒì„±"""
    
    sql_lines = ["-- ì¤‘ë³µ ë°ì´í„° ì œê±° SQL", "-- ì£¼ì˜: ì‹¤í–‰ ì „ì— ë°±ì—…ì„ ê¶Œì¥í•©ë‹ˆë‹¤", ""]
    
    for group in duplicate_groups:
        ids = group['all_ids']
        if len(ids) <= 1:
            continue
        
        # ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ(ë˜ëŠ” ê°€ì¥ ìµœì‹  ê²ƒ)ì„ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
        if keep_oldest:
            ids_to_delete = ids[1:]  # ì²« ë²ˆì§¸(ê°€ì¥ ì˜¤ë˜ëœ) IDë¥¼ ì œì™¸
        else:
            ids_to_delete = ids[:-1]  # ë§ˆì§€ë§‰(ê°€ì¥ ìµœì‹ ) IDë¥¼ ì œì™¸
        
        ids_str = ', '.join(map(str, ids_to_delete))
        title_preview = (group['article_title'][:30] + '...') if group['article_title'] and len(group['article_title']) > 30 else (group['article_title'] or '')
        sql_lines.append(f"-- ë²•ë ¹ID: {group['statute_id']}, ì¡°ë¬¸: {group['article_no']}, í•­: {group['clause_no'] or '(ì—†ìŒ)'}, ì¤‘ë³µ {group['duplicate_count']}ê°œ")
        if title_preview:
            sql_lines.append(f"-- ì œëª©: {title_preview}")
        sql_lines.append(f"DELETE FROM statutes_articles WHERE id IN ({ids_str});")
        sql_lines.append("")
    
    return "\n".join(sql_lines)


def main():
    parser = argparse.ArgumentParser(description='statutes_articles í…Œì´ë¸” ì¤‘ë³µ ë°ì´í„° ê²€í† ')
    parser.add_argument(
        '--db',
        default=build_database_url(),
        help='PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ URL (í™˜ê²½ë³€ìˆ˜: DATABASE_URL ë˜ëŠ” ê°œë³„ POSTGRES_* ë³€ìˆ˜)'
    )
    parser.add_argument(
        '--detail',
        action='store_true',
        help='ì¤‘ë³µëœ ë ˆì½”ë“œì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥'
    )
    parser.add_argument(
        '--generate-sql',
        action='store_true',
        help='ì¤‘ë³µ ë°ì´í„° ì œê±°ë¥¼ ìœ„í•œ SQL ìƒì„±'
    )
    parser.add_argument(
        '--keep-oldest',
        action='store_true',
        default=True,
        help='ì œê±° SQL ìƒì„± ì‹œ ê°€ì¥ ì˜¤ë˜ëœ ë ˆì½”ë“œ ìœ ì§€ (ê¸°ë³¸ê°’: True)'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=20,
        help='ìƒì„¸ ì •ë³´ ì¶œë ¥ ì‹œ ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 20)'
    )
    
    args = parser.parse_args()
    
    if not args.db:
        logger.error("--db ì¸ì ë˜ëŠ” DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    engine = create_engine(
        args.db,
        pool_pre_ping=True,
        echo=False
    )
    
    print("=" * 80)
    print("statutes_articles í…Œì´ë¸” ì¤‘ë³µ ë°ì´í„° ê²€í† ")
    print("=" * 80)
    print()
    
    with engine.connect() as conn:
        results = check_duplicate_articles(conn)
        
        # ê¸°ë³¸ í†µê³„ ì¶œë ¥
        print("ğŸ“Š í†µê³„")
        print("-" * 80)
        print(f"ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {results['total_count']:,}ê°œ")
        print(f"ê³ ìœ  ë ˆì½”ë“œ ìˆ˜: {results['unique_count']:,}ê°œ")
        print(f"ì¤‘ë³µ ê·¸ë£¹ ìˆ˜: {len(results['duplicate_groups'])}ê°œ")
        print(f"ì¤‘ë³µ ë ˆì½”ë“œ ìˆ˜: {results['duplicate_count']:,}ê°œ")
        print()
        
        if results['duplicate_groups']:
            print("âš ï¸  ì¤‘ë³µ ë°ì´í„° ë°œê²¬")
            print("-" * 80)
            print(f"ì¤‘ë³µëœ ì¡°ë¬¸ ê·¸ë£¹: {len(results['duplicate_groups'])}ê°œ")
            print()
            
            # ìƒìœ„ ì¤‘ë³µ ê·¸ë£¹ ì¶œë ¥
            print("ğŸ” ìƒìœ„ ì¤‘ë³µ ê·¸ë£¹ (ìƒìœ„ 10ê°œ):")
            print("-" * 80)
            for i, group in enumerate(results['duplicate_groups'][:10], 1):
                title_preview = (group['article_title'][:30] + '...') if group['article_title'] and len(group['article_title']) > 30 else (group['article_title'] or '(ì—†ìŒ)')
                content_preview = (group['article_content'][:50] + '...') if group['article_content'] and len(group['article_content']) > 50 else (group['article_content'] or '(ì—†ìŒ)')
                print(f"{i}. ë²•ë ¹ID: {group['statute_id']}, ì¡°ë¬¸: {group['article_no']}, "
                      f"í•­: {group['clause_no'] or '(ì—†ìŒ)'}")
                print(f"   ì œëª©: {title_preview}")
                print(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content_preview}")
                print(f"   ì¤‘ë³µ íšŸìˆ˜: {group['duplicate_count']}íšŒ")
                print(f"   ë ˆì½”ë“œ ID: {group['all_ids']}")
                print(f"   ìµœì´ˆ ìˆ˜ì§‘: {group['first_collected']}")
                print(f"   ìµœì¢… ìˆ˜ì§‘: {group['last_collected']}")
                print()
            
            if len(results['duplicate_groups']) > 10:
                print(f"... ì™¸ {len(results['duplicate_groups']) - 10}ê°œ ê·¸ë£¹")
                print()
            
            # ìƒì„¸ ì •ë³´ ì¶œë ¥
            if args.detail:
                print("ğŸ“‹ ì¤‘ë³µ ë ˆì½”ë“œ ìƒì„¸ ì •ë³´:")
                print("-" * 80)
                detail_count = 0
                for group in results['duplicate_groups']:
                    if detail_count >= args.limit:
                        break
                    
                    details = get_duplicate_details(
                        conn,
                        group['statute_id'],
                        group['article_no'],
                        group['article_title'],
                        group['article_content'],
                        group['clause_no']
                    )
                    
                    print(f"\në²•ë ¹: {details[0]['law_name_kr'] if details else 'N/A'}")
                    print(f"ì¡°ë¬¸: {group['article_no']}")
                    for detail in details:
                        print(f"  - ID: {detail['id']}, ìˆ˜ì§‘ì¼ì‹œ: {detail['collected_at']}")
                        print(f"    ì œëª©: {detail['article_title'] or '(ì—†ìŒ)'}")
                        print(f"    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {detail['content_preview']}...")
                    detail_count += 1
                    print()
            
            # SQL ìƒì„±
            if args.generate_sql:
                print("ğŸ’¾ ì¤‘ë³µ ë°ì´í„° ì œê±° SQL ìƒì„±:")
                print("-" * 80)
                cleanup_sql = generate_cleanup_sql(results['duplicate_groups'], args.keep_oldest)
                print(cleanup_sql)
                
                # SQL íŒŒì¼ ì €ì¥
                sql_file = Path(_PROJECT_ROOT) / "scripts" / "ingest" / "open_law" / "scripts" / "cleanup_duplicate_articles.sql"
                sql_file.parent.mkdir(parents=True, exist_ok=True)
                with open(sql_file, 'w', encoding='utf-8') as f:
                    f.write(cleanup_sql)
                print(f"\nâœ… SQL íŒŒì¼ ì €ì¥: {sql_file}")
        else:
            print("âœ… ì¤‘ë³µ ë°ì´í„° ì—†ìŒ")
            print()
        
        print("=" * 80)


if __name__ == '__main__':
    main()

