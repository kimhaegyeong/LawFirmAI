#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë²•ë ¹í•´ì„ë¡€ ë‚ ì§œ ê¸°ë°˜ ìˆ˜ì§‘ê¸°

ì´ ëª¨ë“ˆì€ ë‚ ì§œë³„ë¡œ ì²´ê³„ì ì¸ ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- ì—°ë„ë³„, ë¶„ê¸°ë³„, ì›”ë³„ ìˆ˜ì§‘ ì „ëµ
- í•´ì„ì¼ì ë‚´ë¦¼ì°¨ìˆœ ìµœì í™”
- í´ë”ë³„ raw ë°ì´í„° ì €ì¥ êµ¬ì¡°
- ì¤‘ë³µ ë°©ì§€ ë° ì²´í¬í¬ì¸íŠ¸ ì§€ì›
"""

import sys
import os
import json
import time
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum

# source ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'source'))

from data.law_open_api_client import LawOpenAPIClient
import logging

logger = logging.getLogger(__name__)


class DateCollectionStrategy(Enum):
    """ë‚ ì§œ ìˆ˜ì§‘ ì „ëµ"""
    YEARLY = "yearly"
    QUARTERLY = "quarterly"
    MONTHLY = "monthly"


@dataclass
class CollectionConfig:
    """ìˆ˜ì§‘ ì„¤ì • í´ë˜ìŠ¤"""
    base_output_dir: Path = Path("data/raw/legal_interpretations")
    max_retries: int = 3
    retry_delay: int = 5
    api_delay_range: Tuple[float, float] = (1.0, 3.0)
    output_subdir: Optional[str] = None


@dataclass
class LegalInterpretationData:
    """ë²•ë ¹í•´ì„ë¡€ ë°ì´í„° í´ë˜ìŠ¤ - ëª©ë¡ ë°ì´í„° ë‚´ë¶€ì— ë³¸ë¬¸ ë°ì´í„° í¬í•¨"""
    # ëª©ë¡ ì¡°íšŒ API ì‘ë‹µ (ê¸°ë³¸ ì •ë³´)
    id: str  # ê²€ìƒ‰ê²°ê³¼ë²ˆí˜¸
    ë²•ë ¹í•´ì„ë¡€ì¼ë ¨ë²ˆí˜¸: str
    ì•ˆê±´ëª…: str
    ì•ˆê±´ë²ˆí˜¸: str
    ì§ˆì˜ê¸°ê´€ì½”ë“œ: str
    ì§ˆì˜ê¸°ê´€ëª…: str
    íšŒì‹ ê¸°ê´€ì½”ë“œ: str
    íšŒì‹ ê¸°ê´€ëª…: str
    íšŒì‹ ì¼ì: str
    ë²•ë ¹í•´ì„ë¡€ìƒì„¸ë§í¬: str
    
    # ìƒì„¸ ì¡°íšŒ API ì‘ë‹µ (ë³¸ë¬¸ ë°ì´í„°)
    í•´ì„ì¼ì: Optional[str] = None
    í•´ì„ê¸°ê´€ì½”ë“œ: Optional[str] = None
    í•´ì„ê¸°ê´€ëª…: Optional[str] = None
    ê´€ë¦¬ê¸°ê´€ì½”ë“œ: Optional[str] = None
    ë“±ë¡ì¼ì‹œ: Optional[str] = None
    ì§ˆì˜ìš”ì§€: Optional[str] = None
    íšŒë‹µ: Optional[str] = None
    ì´ìœ : Optional[str] = None
    
    # ë©”íƒ€ë°ì´í„°
    document_type: str = "legal_interpretation"
    collected_at: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class CollectionStats:
    """ìˆ˜ì§‘ í†µê³„ í´ë˜ìŠ¤"""
    total_collected: int = 0
    total_duplicates: int = 0
    total_errors: int = 0
    api_requests_made: int = 0
    api_errors: int = 0
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    status: str = "PENDING"
    target_count: int = 0
    retry_delay: int = 5
    collected_decisions: Set[str] = field(default_factory=set)


class DateBasedLegalInterpretationCollector:
    """ë‚ ì§œ ê¸°ë°˜ ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self, config: CollectionConfig = None):
        self.config = config or CollectionConfig()
        # ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        class SimpleConfig:
            def __init__(self):
                self.law_open_api_oc = os.getenv('LAW_OPEN_API_OC', '{OC}')
                self.oc = os.getenv('LAW_OPEN_API_OC', '{OC}')  # API í´ë¼ì´ì–¸íŠ¸ê°€ oc ì†ì„±ì„ ì°¾ìŒ
                self.base_url = 'https://www.law.go.kr/DRF/'
                self.rate_limit = 1000
                self.request_timeout = 30
                self.connect_timeout = 30
                self.timeout = 30
                self.max_retries = 3
                self.retry_delay = 5
                self.retry_delay_base = 1
                self.retry_delay_max = 60
                self.law_firm_ai_api_key = os.getenv('LAW_FIRM_AI_API_KEY', 'your-api-key-here')
                self.api_host = '0.0.0.0'
                self.api_port = 8000
                self.debug = False
                self.database_url = 'sqlite:///./data/lawfirm.db'
                self.database_path = './data/lawfirm.db'
                self.model_path = './models'
                self.device = 'cpu'
                self.model_cache_dir = './model_cache'
                self.chroma_db_path = './data/chroma_db'
                self.embedding_model = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        
        api_config = SimpleConfig()
        self.client = LawOpenAPIClient(api_config)
        self.stats = CollectionStats()
        self.collected_decisions: Set[str] = set()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.config.base_output_dir.mkdir(parents=True, exist_ok=True)
    
    def _create_output_directory(self, strategy: DateCollectionStrategy, 
                               year: int = None, quarter: int = None, 
                               month: int = None) -> Path:
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if strategy == DateCollectionStrategy.YEARLY:
            if year:
                dir_name = f"yearly_{year}_{timestamp}"
            else:
                dir_name = f"yearly_collection_{timestamp}"
        elif strategy == DateCollectionStrategy.QUARTERLY:
            if year and quarter:
                dir_name = f"quarterly_{year}Q{quarter}_{timestamp}"
            else:
                dir_name = f"quarterly_collection_{timestamp}"
        elif strategy == DateCollectionStrategy.MONTHLY:
            if year and month:
                dir_name = f"monthly_{year}{month:02d}_{timestamp}"
            else:
                dir_name = f"monthly_collection_{timestamp}"
        else:
            dir_name = f"daily_collection_{timestamp}"
        
        output_dir = self.config.base_output_dir / dir_name
        output_dir.mkdir(parents=True, exist_ok=True)
        return output_dir
    
    def _save_batch(self, interpretations: List[LegalInterpretationData], 
                   output_dir: Path, page_num: int, 
                   category: str = "legal_interpretation") -> bool:
        """ë°°ì¹˜ ë°ì´í„° ì €ì¥"""
        try:
            if not interpretations:
                return True
            
            # íŒŒì¼ëª… ìƒì„±
            start_id = interpretations[0].ë²•ë ¹í•´ì„ë¡€ì¼ë ¨ë²ˆí˜¸
            end_id = interpretations[-1].ë²•ë ¹í•´ì„ë¡€ì¼ë ¨ë²ˆí˜¸
            count = len(interpretations)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            filename = f"page_{page_num:03d}_{category}_{start_id}-{end_id}_{count}ê±´_{timestamp}.json"
            file_path = output_dir / filename
            
            # ë°ì´í„° êµ¬ì¡°í™”
            batch_data = {
                "metadata": {
                    "category": category,
                    "page": page_num,
                    "count": count,
                    "start_id": start_id,
                    "end_id": end_id,
                    "collected_at": timestamp,
                    "strategy": "date_based"
                },
                "interpretations": [interpretation.__dict__ for interpretation in interpretations]
            }
            
            # JSON íŒŒì¼ ì €ì¥
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(batch_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {filename} ({count}ê±´)")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì§„í–‰ ìƒí™© ê¸°ë¡)
            self._save_checkpoint(output_dir, page_num, collected_count=len(interpretations))
            
            return True
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _save_checkpoint(self, output_dir: Path, page_num: int, collected_count: int):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì§„í–‰ ìƒí™© ê¸°ë¡)"""
        try:
            checkpoint_data = {
                "checkpoint_info": {
                    "last_page": page_num,
                    "collected_count": collected_count,
                    "timestamp": datetime.now().isoformat(),
                    "status": "in_progress"
                }
            }
            
            checkpoint_file = output_dir / "checkpoint.json"
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _load_checkpoint(self, output_dir: Path) -> dict:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ)"""
        try:
            checkpoint_file = output_dir / "checkpoint.json"
            if checkpoint_file.exists():
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                logger.info(f"ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: í˜ì´ì§€ {checkpoint_data['checkpoint_info']['last_page']}, ìˆ˜ì§‘ëœ ê±´ìˆ˜ {checkpoint_data['checkpoint_info']['collected_count']}")
                return checkpoint_data
        except Exception as e:
            logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def _save_summary(self, output_dir: Path, strategy: DateCollectionStrategy, 
                     total_collected: int, total_duplicates: int, 
                     total_errors: int, duration: timedelta):
        """ìˆ˜ì§‘ ìš”ì•½ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_data = {
                "collection_info": {
                    "strategy": strategy.value,
                    "start_time": self.stats.start_time,
                    "end_time": self.stats.end_time,
                    "duration_seconds": duration.total_seconds(),
                    "duration_str": str(duration)
                },
                "statistics": {
                    "total_collected": total_collected,
                    "total_duplicates": total_duplicates,
                    "total_errors": total_errors,
                    "api_requests_made": self.stats.api_requests_made,
                    "api_errors": self.stats.api_errors,
                    "success_rate": (total_collected / (total_collected + total_errors) * 100) if (total_collected + total_errors) > 0 else 0
                },
                "collected_interpretations": list(self.collected_decisions),
                "metadata": {
                    "collected_at": timestamp,
                    "output_directory": str(output_dir),
                    "total_files": len(list(output_dir.glob("page_*.json")))
                }
            }
            
            summary_file = output_dir / f"{strategy.value}_collection_summary_{timestamp}.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ìš”ì•½ ì €ì¥: {summary_file.name}")
            
        except Exception as e:
            logger.error(f"ìˆ˜ì§‘ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def collect_by_year(self, year: int, target_count: Optional[int] = None, 
                       unlimited: bool = False, use_interpretation_date: bool = True) -> bool:
        """íŠ¹ì • ì—°ë„ ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘"""
        try:
            date_type = "í•´ì„ì¼ì" if use_interpretation_date else "íšŒì‹ ì¼ì"
            logger.info(f"ğŸ—“ï¸ {year}ë…„ ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ ì‹œì‘ ({date_type} ê¸°ì¤€)")
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = self._create_output_directory(DateCollectionStrategy.YEARLY, year=year)
            
            # ì²´í¬í¬ì¸íŠ¸ í™•ì¸ (ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ)
            checkpoint = self._load_checkpoint(output_dir)
            start_page = 1
            if checkpoint:
                start_page = checkpoint['checkpoint_info']['last_page'] + 1
                logger.info(f"ğŸ”„ ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ: í˜ì´ì§€ {start_page}ë¶€í„° ì‹œì‘")
            
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (íŠ¹ì • ì—°ë„ë§Œ)
            self._load_existing_data(target_year=year)
            
            # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
            start_date = f"{year}0101"
            end_date = f"{year}1231"
            
            # ëª©í‘œ ê±´ìˆ˜ ì„¤ì •
            if unlimited:
                target_count = 2000  # ë¬´ì œí•œì´ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´ ì œí•œ
            elif target_count is None:
                target_count = 1000  # ê¸°ë³¸ê°’
            
            self.stats.start_time = datetime.now().isoformat()
            self.stats.target_count = target_count
            self.stats.status = "RUNNING"
            
            logger.info(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date} ({date_type} ê¸°ì¤€)")
            logger.info(f"ğŸ¯ ëª©í‘œ ê±´ìˆ˜: {target_count:,}ê±´")
            logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
            
            # ìˆ˜ì§‘ ì‹¤í–‰
            success = self._collect_interpretations_by_date_range(
                start_date=start_date,
                end_date=end_date,
                target_count=target_count,
                output_dir=output_dir,
                category=f"{year}ë…„",
                use_interpretation_date=use_interpretation_date,
                start_page=start_page
            )
            
            # ìˆ˜ì§‘ ì™„ë£Œ ì²˜ë¦¬
            self.stats.end_time = datetime.now().isoformat()
            duration = datetime.fromisoformat(self.stats.end_time) - datetime.fromisoformat(self.stats.start_time)
            
            # ìš”ì•½ ì €ì¥
            self._save_summary(
                output_dir=output_dir,
                strategy=DateCollectionStrategy.YEARLY,
                total_collected=self.stats.total_collected,
                total_duplicates=self.stats.total_duplicates,
                total_errors=self.stats.total_errors,
                duration=duration
            )
            
            logger.info(f"âœ… {year}ë…„ ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {self.stats.total_collected:,}ê±´ ìˆ˜ì§‘, {self.stats.total_duplicates:,}ê±´ ì¤‘ë³µ, {self.stats.total_errors:,}ê±´ ì˜¤ë¥˜")
            logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
            
            return success
            
        except Exception as e:
            logger.error(f"{year}ë…„ ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def collect_by_quarter(self, year: int, quarter: int, target_count: int = 500) -> bool:
        """íŠ¹ì • ë¶„ê¸° ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘"""
        try:
            logger.info(f"ğŸ—“ï¸ {year}ë…„ {quarter}ë¶„ê¸° ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ ì‹œì‘")
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = self._create_output_directory(DateCollectionStrategy.QUARTERLY, year=year, quarter=quarter)
            
            # ë¶„ê¸°ë³„ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
            quarter_months = {
                1: (1, 3),    # 1ë¶„ê¸°: 1-3ì›”
                2: (4, 6),    # 2ë¶„ê¸°: 4-6ì›”
                3: (7, 9),    # 3ë¶„ê¸°: 7-9ì›”
                4: (10, 12)   # 4ë¶„ê¸°: 10-12ì›”
            }
            
            start_month, end_month = quarter_months[quarter]
            start_date = f"{year}{start_month:02d}01"
            end_date = f"{year}{end_month:02d}31"
            
            self.stats.start_time = datetime.now().isoformat()
            self.stats.target_count = target_count
            self.stats.status = "RUNNING"
            
            logger.info(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
            logger.info(f"ğŸ¯ ëª©í‘œ ê±´ìˆ˜: {target_count:,}ê±´")
            logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
            
            # ìˆ˜ì§‘ ì‹¤í–‰
            success = self._collect_interpretations_by_date_range(
                start_date=start_date,
                end_date=end_date,
                target_count=target_count,
                output_dir=output_dir,
                category=f"{year}ë…„{quarter}ë¶„ê¸°",
                start_page=1
            )
            
            # ìˆ˜ì§‘ ì™„ë£Œ ì²˜ë¦¬
            self.stats.end_time = datetime.now().isoformat()
            duration = datetime.fromisoformat(self.stats.end_time) - datetime.fromisoformat(self.stats.start_time)
            
            # ìš”ì•½ ì €ì¥
            self._save_summary(
                output_dir=output_dir,
                strategy=DateCollectionStrategy.QUARTERLY,
                total_collected=self.stats.total_collected,
                total_duplicates=self.stats.total_duplicates,
                total_errors=self.stats.total_errors,
                duration=duration
            )
            
            logger.info(f"âœ… {year}ë…„ {quarter}ë¶„ê¸° ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {self.stats.total_collected:,}ê±´ ìˆ˜ì§‘, {self.stats.total_duplicates:,}ê±´ ì¤‘ë³µ, {self.stats.total_errors:,}ê±´ ì˜¤ë¥˜")
            logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
            
            return success
            
        except Exception as e:
            logger.error(f"{year}ë…„ {quarter}ë¶„ê¸° ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def collect_by_month(self, year: int, month: int, target_count: int = 200) -> bool:
        """íŠ¹ì • ì›” ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘"""
        try:
            logger.info(f"ğŸ—“ï¸ {year}ë…„ {month}ì›” ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ ì‹œì‘")
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = self._create_output_directory(DateCollectionStrategy.MONTHLY, year=year, month=month)
            
            # ì›”ë³„ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
            start_date = f"{year}{month:02d}01"
            # ì›”ë§ ë‚ ì§œ ê³„ì‚°
            if month == 12:
                end_date = f"{year}1231"
            else:
                next_month = month + 1
                end_date = f"{year}{next_month:02d}01"
            
            self.stats.start_time = datetime.now().isoformat()
            self.stats.target_count = target_count
            self.stats.status = "RUNNING"
            
            logger.info(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
            logger.info(f"ğŸ¯ ëª©í‘œ ê±´ìˆ˜: {target_count:,}ê±´")
            logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
            
            # ìˆ˜ì§‘ ì‹¤í–‰
            success = self._collect_interpretations_by_date_range(
                start_date=start_date,
                end_date=end_date,
                target_count=target_count,
                output_dir=output_dir,
                category=f"{year}ë…„{month}ì›”",
                start_page=1
            )
            
            # ìˆ˜ì§‘ ì™„ë£Œ ì²˜ë¦¬
            self.stats.end_time = datetime.now().isoformat()
            duration = datetime.fromisoformat(self.stats.end_time) - datetime.fromisoformat(self.stats.start_time)
            
            # ìš”ì•½ ì €ì¥
            self._save_summary(
                output_dir=output_dir,
                strategy=DateCollectionStrategy.MONTHLY,
                total_collected=self.stats.total_collected,
                total_duplicates=self.stats.total_duplicates,
                total_errors=self.stats.total_errors,
                duration=duration
            )
            
            logger.info(f"âœ… {year}ë…„ {month}ì›” ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {self.stats.total_collected:,}ê±´ ìˆ˜ì§‘, {self.stats.total_duplicates:,}ê±´ ì¤‘ë³µ, {self.stats.total_errors:,}ê±´ ì˜¤ë¥˜")
            logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
            
            return success
            
        except Exception as e:
            logger.error(f"{year}ë…„ {month}ì›” ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def _collect_interpretations_by_date_range(self, start_date: str, end_date: str, 
                                             target_count: int, output_dir: Path, 
                                             category: str, use_interpretation_date: bool = True, 
                                             start_page: int = 1) -> bool:
        """ë‚ ì§œ ë²”ìœ„ë³„ ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ (í•´ì„ì¼ì ë‚´ë¦¼ì°¨ìˆœ)"""
        try:
            page = start_page
            collected_count = 0
            batch_interpretations = []
            date_type = "í•´ì„ì¼ì" if use_interpretation_date else "íšŒì‹ ì¼ì"
            
            while collected_count < target_count:
                try:
                    logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
                    
                    # API í˜¸ì¶œ
                    if use_interpretation_date:
                        # í•´ì„ì¼ì ê¸°ì¤€ ê²€ìƒ‰
                        interpretations = self.client.get_legal_interpretation_list(
                            page=page,
                            display=100,
                            explYd=f"{start_date}~{end_date}",
                            sort="ddes"  # í•´ì„ì¼ì ë‚´ë¦¼ì°¨ìˆœ
                        )
                    else:
                        # íšŒì‹ ì¼ì ê¸°ì¤€ ê²€ìƒ‰
                        interpretations = self.client.get_legal_interpretation_list(
                            page=page,
                            display=100,
                            regYd=f"{start_date}~{end_date}",
                            sort="ddes"  # íšŒì‹ ì¼ì ë‚´ë¦¼ì°¨ìˆœ
                        )
                    
                    if not interpretations:
                        logger.info(f"ğŸ“„ í˜ì´ì§€ {page}: ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    self.stats.api_requests_made += 1
                    new_interpretations = 0
                    
                    for result in interpretations:
                        interpretation_id = result.get('ë²•ë ¹í•´ì„ë¡€ì¼ë ¨ë²ˆí˜¸', '')
                        
                        if not interpretation_id:
                            continue
                        
                        # ì¤‘ë³µ í™•ì¸
                        if interpretation_id in self.collected_decisions:
                            self.stats.total_duplicates += 1
                            continue
                        
                        # ìƒì„¸ ì •ë³´ ì¡°íšŒ
                        try:
                            detail = self.client.get_legal_interpretation_detail(interpretation_id)
                            
                            # LegalInterpretationData ê°ì²´ ìƒì„± (ëª©ë¡ ë°ì´í„° ë‚´ë¶€ì— ë³¸ë¬¸ ë°ì´í„° í¬í•¨)
                            interpretation_data = LegalInterpretationData(
                                # ëª©ë¡ ì¡°íšŒ API ì‘ë‹µ (ê¸°ë³¸ ì •ë³´)
                                id=result.get('id', ''),
                                ë²•ë ¹í•´ì„ë¡€ì¼ë ¨ë²ˆí˜¸=interpretation_id,
                                ì•ˆê±´ëª…=result.get('ì•ˆê±´ëª…', ''),
                                ì•ˆê±´ë²ˆí˜¸=result.get('ì•ˆê±´ë²ˆí˜¸', ''),
                                ì§ˆì˜ê¸°ê´€ì½”ë“œ=result.get('ì§ˆì˜ê¸°ê´€ì½”ë“œ', ''),
                                ì§ˆì˜ê¸°ê´€ëª…=result.get('ì§ˆì˜ê¸°ê´€ëª…', ''),
                                íšŒì‹ ê¸°ê´€ì½”ë“œ=result.get('íšŒì‹ ê¸°ê´€ì½”ë“œ', ''),
                                íšŒì‹ ê¸°ê´€ëª…=result.get('íšŒì‹ ê¸°ê´€ëª…', ''),
                                íšŒì‹ ì¼ì=result.get('íšŒì‹ ì¼ì', ''),
                                ë²•ë ¹í•´ì„ë¡€ìƒì„¸ë§í¬=result.get('ë²•ë ¹í•´ì„ë¡€ìƒì„¸ë§í¬', ''),
                                
                                # ìƒì„¸ ì¡°íšŒ API ì‘ë‹µ (ë³¸ë¬¸ ë°ì´í„°)
                                í•´ì„ì¼ì=detail.get('í•´ì„ì¼ì') if detail else None,
                                í•´ì„ê¸°ê´€ì½”ë“œ=detail.get('í•´ì„ê¸°ê´€ì½”ë“œ') if detail else None,
                                í•´ì„ê¸°ê´€ëª…=detail.get('í•´ì„ê¸°ê´€ëª…') if detail else None,
                                ê´€ë¦¬ê¸°ê´€ì½”ë“œ=detail.get('ê´€ë¦¬ê¸°ê´€ì½”ë“œ') if detail else None,
                                ë“±ë¡ì¼ì‹œ=detail.get('ë“±ë¡ì¼ì‹œ') if detail else None,
                                ì§ˆì˜ìš”ì§€=detail.get('ì§ˆì˜ìš”ì§€') if detail else None,
                                íšŒë‹µ=detail.get('íšŒë‹µ') if detail else None,
                                ì´ìœ =detail.get('ì´ìœ ') if detail else None,
                                
                                # ë©”íƒ€ë°ì´í„°
                                document_type="legal_interpretation",
                                collected_at=datetime.now().isoformat()
                            )
                            
                            batch_interpretations.append(interpretation_data)
                            self.collected_decisions.add(interpretation_id)
                            collected_count += 1
                            new_interpretations += 1
                            
                            logger.info(f"âœ… ìƒˆë¡œìš´ ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘: {interpretation_data.ì•ˆê±´ëª…} (ID: {interpretation_id})")
                            
                            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¤‘ê°„ ì €ì¥ (10ê±´ë§ˆë‹¤)
                            if len(batch_interpretations) >= 10:
                                self._save_batch(batch_interpretations, output_dir, page, category)
                                batch_interpretations = []  # ë°°ì¹˜ ì´ˆê¸°í™”
                            
                        except Exception as e:
                            logger.error(f"ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {interpretation_id} - {e}")
                            # ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì •ë³´ëŠ” ì €ì¥
                            interpretation_data = LegalInterpretationData(
                                # ëª©ë¡ ì¡°íšŒ API ì‘ë‹µ (ê¸°ë³¸ ì •ë³´)
                                id=result.get('id', ''),
                                ë²•ë ¹í•´ì„ë¡€ì¼ë ¨ë²ˆí˜¸=interpretation_id,
                                ì•ˆê±´ëª…=result.get('ì•ˆê±´ëª…', ''),
                                ì•ˆê±´ë²ˆí˜¸=result.get('ì•ˆê±´ë²ˆí˜¸', ''),
                                ì§ˆì˜ê¸°ê´€ì½”ë“œ=result.get('ì§ˆì˜ê¸°ê´€ì½”ë“œ', ''),
                                ì§ˆì˜ê¸°ê´€ëª…=result.get('ì§ˆì˜ê¸°ê´€ëª…', ''),
                                íšŒì‹ ê¸°ê´€ì½”ë“œ=result.get('íšŒì‹ ê¸°ê´€ì½”ë“œ', ''),
                                íšŒì‹ ê¸°ê´€ëª…=result.get('íšŒì‹ ê¸°ê´€ëª…', ''),
                                íšŒì‹ ì¼ì=result.get('íšŒì‹ ì¼ì', ''),
                                ë²•ë ¹í•´ì„ë¡€ìƒì„¸ë§í¬=result.get('ë²•ë ¹í•´ì„ë¡€ìƒì„¸ë§í¬', ''),
                                
                                # ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨ë¡œ ë³¸ë¬¸ ë°ì´í„°ëŠ” None
                                í•´ì„ì¼ì=None,
                                í•´ì„ê¸°ê´€ì½”ë“œ=None,
                                í•´ì„ê¸°ê´€ëª…=None,
                                ê´€ë¦¬ê¸°ê´€ì½”ë“œ=None,
                                ë“±ë¡ì¼ì‹œ=None,
                                ì§ˆì˜ìš”ì§€=None,
                                íšŒë‹µ=None,
                                ì´ìœ =None,
                                
                                # ë©”íƒ€ë°ì´í„°
                                document_type="legal_interpretation",
                                collected_at=datetime.now().isoformat()
                            )
                            
                            batch_interpretations.append(interpretation_data)
                            self.collected_decisions.add(interpretation_id)
                            collected_count += 1
                            new_interpretations += 1
                            
                            logger.warning(f"âš ï¸ ê¸°ë³¸ ì •ë³´ë§Œ ìˆ˜ì§‘: {interpretation_data.ì•ˆê±´ëª…} (ID: {interpretation_id})")
                            
                            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¤‘ê°„ ì €ì¥ (10ê±´ë§ˆë‹¤)
                            if len(batch_interpretations) >= 10:
                                self._save_batch(batch_interpretations, output_dir, page, category)
                                batch_interpretations = []  # ë°°ì¹˜ ì´ˆê¸°í™”
                            self.stats.total_errors += 1
                    
                    # ë°°ì¹˜ ì €ì¥ (100ê±´ë§ˆë‹¤) - ì¶”ê°€ ì•ˆì „ì¥ì¹˜
                    if len(batch_interpretations) >= 100:
                        self._save_batch(batch_interpretations, output_dir, page, category)
                        batch_interpretations = []
                    
                    logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ì™„ë£Œ: {new_interpretations}ê±´ì˜ ìƒˆë¡œìš´ í•´ì„ë¡€ ìˆ˜ì§‘")
                    logger.info(f"   ğŸ“Š ëˆ„ì  ìˆ˜ì§‘: {collected_count:,}/{target_count:,}ê±´ ({collected_count/target_count*100:.1f}%)")
                    
                    page += 1
                    
                    # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
                    time.sleep(1.0)
                    
                except Exception as e:
                    logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    self.stats.total_errors += 1
                    
                    # ì¬ì‹œë„ ë¡œì§
                    if self.stats.total_errors < self.config.max_retries:
                        logger.info(f"ì¬ì‹œë„ ì¤‘... ({self.stats.total_errors}/{self.config.max_retries})")
                        time.sleep(self.stats.retry_delay)
                        continue
                    else:
                        logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        break
            
            # ë‚¨ì€ ë°°ì¹˜ ì €ì¥
            if batch_interpretations:
                self._save_batch(batch_interpretations, output_dir, page, category)
            
            logger.info(f"âœ… ë‚ ì§œ ë²”ìœ„ë³„ ìˆ˜ì§‘ ì™„ë£Œ: {collected_count:,}ê±´ ìˆ˜ì§‘")
            return True
            
        except Exception as e:
            logger.error(f"ë‚ ì§œ ë²”ìœ„ë³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def _load_existing_data(self, target_year: int = None):
        """ê¸°ì¡´ ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)"""
        try:
            # ê¸°ì¡´ ë°ì´í„° ë””ë ‰í† ë¦¬ ìŠ¤ìº”
            for year_dir in self.config.base_output_dir.glob("yearly_*"):
                if target_year and str(target_year) not in str(year_dir):
                    continue
                
                # JSON íŒŒì¼ë“¤ì—ì„œ ì´ë¯¸ ìˆ˜ì§‘ëœ ID ì¶”ì¶œ
                for json_file in year_dir.glob("page_*.json"):
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        if 'interpretations' in data:
                            for interpretation in data['interpretations']:
                                interpretation_id = interpretation.get('ë²•ë ¹í•´ì„ë¡€ì¼ë ¨ë²ˆí˜¸', '')
                                if interpretation_id:
                                    self.collected_decisions.add(interpretation_id)
                    except Exception as e:
                        logger.warning(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {json_file} - {e}")
            
            logger.info(f"ğŸ“‹ ê¸°ì¡´ ìˆ˜ì§‘ëœ ë²•ë ¹í•´ì„ë¡€: {len(self.collected_decisions):,}ê±´")
            
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    collector = DateBasedLegalInterpretationCollector()
    
    # 2025ë…„ ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (10ê±´)
    success = collector.collect_by_year(2025, target_count=10, use_interpretation_date=True)
    
    if success:
        print("âœ… ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    else:
        print("âŒ ë²•ë ¹í•´ì„ë¡€ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
