# -*- coding: utf-8 -*-
"""
?Œí¬?Œë¡œ??ê²€??ì§„ë‹¨ ?¤í¬ë¦½íŠ¸
LangGraph ?Œí¬?Œë¡œ???¤í–‰ ??ê²€??ê´€??ë¬¸ì œ ì§„ë‹¨
"""

import asyncio
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict

# ?„ë¡œ?íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê?
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# core/agents/workflow_service.pyë¥??¬ìš©?˜ë„ë¡?ë³€ê²?
from source.agents.workflow_service import LangGraphWorkflowService
from infrastructure.utils.langgraph_config import LangGraphConfig

# ë¡œê¹… ?¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'logs/workflow_diagnosis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)

logger = logging.getLogger(__name__)


def analyze_search_results(result: Dict[str, Any]) -> Dict[str, Any]:
    """ê²€??ê²°ê³¼ ë¶„ì„"""
    analysis = {
        "has_answer": bool(result.get("answer", "")),
        "answer_length": len(result.get("answer", "")),
        "has_sources": len(result.get("sources", [])) > 0,
        "sources_count": len(result.get("sources", [])),
        "sources_list": result.get("sources", [])[:10],
        "has_retrieved_docs": len(result.get("retrieved_docs", [])) > 0,
        "retrieved_docs_count": len(result.get("retrieved_docs", [])),
        "confidence": result.get("confidence", 0.0),
        "has_errors": len(result.get("errors", [])) > 0,
        "errors": result.get("errors", []),
        "processing_time": result.get("processing_time", 0.0),
    }

    # retrieved_docs ë¶„ì„
    if analysis["has_retrieved_docs"]:
        docs = result.get("retrieved_docs", [])
        doc_types = {}
        doc_sources = {}
        doc_scores = []

        for doc in docs[:10]:  # ?ìœ„ 10ê°œë§Œ ë¶„ì„
            doc_type = doc.get("type", doc.get("doc_type", "unknown"))
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1

            doc_source = doc.get("source", "Unknown")
            doc_sources[doc_source] = doc_sources.get(doc_source, 0) + 1

            score = doc.get("relevance_score", 0.0)
            if score > 0:
                doc_scores.append(score)

        analysis["doc_types"] = doc_types
        analysis["doc_sources"] = doc_sources
        if doc_scores:
            analysis["avg_score"] = sum(doc_scores) / len(doc_scores)
            analysis["min_score"] = min(doc_scores)
            analysis["max_score"] = max(doc_scores)

    return analysis


async def diagnose_workflow_search(query: str):
    """?Œí¬?Œë¡œ??ê²€??ì§„ë‹¨"""
    print("=" * 80)
    print("?Œí¬?Œë¡œ??ê²€??ì§„ë‹¨")
    print("=" * 80)
    print(f"\nì§„ë‹¨ ì¿¼ë¦¬: {query}")
    print(f"?œì‘ ?œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    try:
        # ?¤ì • ë¡œë“œ
        config = LangGraphConfig.from_env()

        # ?Œí¬?Œë¡œ???œë¹„??ì´ˆê¸°??
        logger.info("?Œí¬?Œë¡œ???œë¹„??ì´ˆê¸°??ì¤?..")
        workflow_service = LangGraphWorkflowService(config)

        # ì¿¼ë¦¬ ì²˜ë¦¬
        logger.info(f"ì¿¼ë¦¬ ì²˜ë¦¬ ?œì‘: {query}")
        session_id = f"diagnosis_{int(datetime.now().timestamp())}"

        result = await workflow_service.process_query(
            query=query,
            session_id=session_id,
            enable_checkpoint=False
        )

        # ê²°ê³¼ ë¶„ì„
        analysis = analyze_search_results(result)

        # ì§„ë‹¨ ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ì§„ë‹¨ ê²°ê³¼")
        print("=" * 80)

        print(f"\n[?µë?]")
        print(f"  - ?ì„± ?¬ë?: {'???ˆìŒ' if analysis['has_answer'] else '???†ìŒ'}")
        print(f"  - ê¸¸ì´: {analysis['answer_length']}??)
        print(f"  - ? ë¢°?? {analysis['confidence']:.2%}")

        print(f"\n[ê²€??ê²°ê³¼]")
        print(f"  - retrieved_docs: {'???ˆìŒ' if analysis['has_retrieved_docs'] else '???†ìŒ'} ({analysis['retrieved_docs_count']}ê°?")
        print(f"  - sources: {'???ˆìŒ' if analysis['has_sources'] else '???†ìŒ'} ({analysis['sources_count']}ê°?")

        if analysis['has_retrieved_docs']:
            print(f"\n  [ë¬¸ì„œ ?€??ë¶„í¬]")
            for doc_type, count in analysis.get('doc_types', {}).items():
                print(f"    - {doc_type}: {count}ê°?)

            print(f"\n  [ë¬¸ì„œ ?ŒìŠ¤ ë¶„í¬]")
            for source, count in list(analysis.get('doc_sources', {}).items())[:5]:
                print(f"    - {source}: {count}ê°?)

            if 'avg_score' in analysis:
                print(f"\n  [?ìˆ˜ ?µê³„]")
                print(f"    - ?‰ê· : {analysis['avg_score']:.3f}")
                print(f"    - ìµœì†Œ: {analysis['min_score']:.3f}")
                print(f"    - ìµœë?: {analysis['max_score']:.3f}")

        if analysis['has_sources']:
            print(f"\n  [Sources ëª©ë¡]")
            for i, source in enumerate(analysis['sources_list'][:10], 1):
                print(f"    {i}. {source}")

        print(f"\n[ì²˜ë¦¬ ?•ë³´]")
        print(f"  - ì²˜ë¦¬ ?œê°„: {analysis['processing_time']:.2f}ì´?)
        print(f"  - ?ëŸ¬ ?¬ë?: {'? ï¸ ?ˆìŒ' if analysis['has_errors'] else '???†ìŒ'}")

        if analysis['has_errors']:
            print(f"\n  [?ëŸ¬ ëª©ë¡]")
            for error in analysis['errors']:
                print(f"    - {error}")

        # ë¬¸ì œ ì§„ë‹¨
        print(f"\n" + "=" * 80)
        print("ë¬¸ì œ ì§„ë‹¨")
        print("=" * 80)

        issues = []
        recommendations = []

        if not analysis['has_retrieved_docs']:
            issues.append("??ê²€??ê²°ê³¼ê°€ ?†ìŠµ?ˆë‹¤ (retrieved_docsê°€ ë¹„ì–´?ˆìŒ)")
            recommendations.append("  ??ê²€??ì¿¼ë¦¬ ?•ì¸ ?„ìš”")
            recommendations.append("  ??ê²€??ì»´í¬?ŒíŠ¸ ì§ì ‘ ?ŒìŠ¤???„ìš”")
            recommendations.append("  ???„ê³„ê°??„í„°ë§??•ì¸ ?„ìš”")

        if not analysis['has_sources'] and analysis['has_retrieved_docs']:
            issues.append("? ï¸ Sourcesê°€ ì¶”ì¶œ?˜ì? ?Šì•˜?µë‹ˆ??)
            recommendations.append("  ??retrieved_docs??source ?„ë“œ ?•ì¸ ?„ìš”")
            recommendations.append("  ??prepare_final_response??sources ì¶”ì¶œ ë¡œì§ ?•ì¸ ?„ìš”")

        if analysis['has_retrieved_docs'] and 'avg_score' in analysis:
            if analysis['avg_score'] < 0.3:
                issues.append("? ï¸ ê²€??ê²°ê³¼???‰ê·  ?ìˆ˜ê°€ ??Šµ?ˆë‹¤ (0.3 ë¯¸ë§Œ)")
                recommendations.append("  ??ê²€??ì¿¼ë¦¬ ìµœì ???„ìš”")
                recommendations.append("  ???„ê³„ê°?ì¡°ì • ê²€???„ìš”")

        if not analysis['has_answer']:
            issues.append("???µë????ì„±?˜ì? ?Šì•˜?µë‹ˆ??)
            recommendations.append("  ??ê²€??ê²°ê³¼ ë¶€ì¡?ê°€?¥ì„±")
            recommendations.append("  ??LLM ?¸ì¶œ ?¤íŒ¨ ê°€?¥ì„±")

        if issues:
            print("\në°œê²¬??ë¬¸ì œ:")
            for issue in issues:
                print(f"  {issue}")

            if recommendations:
                print("\nê¶Œì¥ ì¡°ì¹˜:")
                for rec in set(recommendations):  # ì¤‘ë³µ ?œê±°
                    print(f"  {rec}")
        else:
            print("\n???¹ë³„??ë¬¸ì œê°€ ë°œê²¬?˜ì? ?Šì•˜?µë‹ˆ??")

        print("\n" + "=" * 80)
        print("ì§„ë‹¨ ?„ë£Œ")
        print(f"ë¡œê·¸ ?Œì¼: logs/workflow_diagnosis_*.log")
        print("=" * 80)

        return result, analysis

    except Exception as e:
        logger.error(f"ì§„ë‹¨ ì¤??¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        print(f"\n??ì§„ë‹¨ ?¤íŒ¨: {e}")
        return None, None


def main():
    """ë©”ì¸ ?¨ìˆ˜"""
    # ?ŒìŠ¤??ì¿¼ë¦¬
    test_query = "ë¯¼ì‚¬ë²•ì—??ê³„ì•½ ?´ì? ?”ê±´?€ ë¬´ì—‡?¸ê???"

    # ë¡œê·¸ ?”ë ‰? ë¦¬ ?ì„±
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)

    # ë¹„ë™ê¸??¤í–‰
    result, analysis = asyncio.run(diagnose_workflow_search(test_query))

    if result and analysis:
        print(f"\n??ì§„ë‹¨???„ë£Œ?˜ì—ˆ?µë‹ˆ??")
        print(f"?ì„¸ ë¡œê·¸??logs/ ?”ë ‰? ë¦¬ë¥??•ì¸?˜ì„¸??")


if __name__ == "__main__":
    main()
