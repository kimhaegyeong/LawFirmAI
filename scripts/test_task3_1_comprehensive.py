#!/usr/bin/env python3
"""
TASK 3.1 ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  ê°œë°œ ë‚´ì—­ì„ ì²´ê³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import sys
import subprocess
import json
import time
from datetime import datetime
from typing import Dict, List, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

def run_command(command: str, description: str) -> Dict[str, Any]:
    """ëª…ë ¹ì–´ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
    print(f"\nğŸ”§ {description}")
    print(f"ì‹¤í–‰ ëª…ë ¹: {command}")
    print("-" * 50)
    
    start_time = time.time()
    try:
        result = subprocess.run(
            command.split(),
            capture_output=True,
            text=True,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        return {
            "success": result.returncode == 0,
            "returncode": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "execution_time": execution_time,
            "description": description
        }
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "returncode": -1,
            "stdout": "",
            "stderr": "Command timed out after 5 minutes",
            "execution_time": 300,
            "description": description
        }
    except Exception as e:
        return {
            "success": False,
            "returncode": -1,
            "stdout": "",
            "stderr": str(e),
            "execution_time": 0,
            "description": description
        }

def test_environment_setup():
    """1. ê¸°ë³¸ í™˜ê²½ ë° ì˜ì¡´ì„± í™•ì¸"""
    print("\n" + "="*60)
    print("ğŸ§ª 1. ê¸°ë³¸ í™˜ê²½ ë° ì˜ì¡´ì„± í™•ì¸")
    print("="*60)
    
    tests = [
        ("python scripts/setup_lora_environment.py --verbose", "í™˜ê²½ ê²€ì‚¬ ì‹¤í–‰"),
        ("python source/utils/gpu_memory_monitor.py --interval 10", "GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸")
    ]
    
    results = []
    for command, description in tests:
        result = run_command(command, description)
        results.append(result)
        
        if result["success"]:
            print(f"âœ… {description} - ì„±ê³µ ({result['execution_time']:.2f}ì´ˆ)")
        else:
            print(f"âŒ {description} - ì‹¤íŒ¨")
            if result["stderr"]:
                print(f"ì˜¤ë¥˜: {result['stderr']}")
    
    return results

def test_model_loading():
    """2. ëª¨ë¸ ë¡œë”© ë° êµ¬ì¡° í™•ì¸"""
    print("\n" + "="*60)
    print("ğŸ§ª 2. ëª¨ë¸ ë¡œë”© ë° êµ¬ì¡° í™•ì¸")
    print("="*60)
    
    tests = [
        ("python scripts/analyze_kogpt2_structure.py --test-lora", "KoGPT-2 ëª¨ë¸ êµ¬ì¡° ë¶„ì„"),
        ("python source/models/model_manager.py", "ëª¨ë¸ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    ]
    
    results = []
    for command, description in tests:
        result = run_command(command, description)
        results.append(result)
        
        if result["success"]:
            print(f"âœ… {description} - ì„±ê³µ ({result['execution_time']:.2f}ì´ˆ)")
        else:
            print(f"âŒ {description} - ì‹¤íŒ¨")
            if result["stderr"]:
                print(f"ì˜¤ë¥˜: {result['stderr']}")
    
    return results

def test_dataset_preparation():
    """3. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° ì „ì²˜ë¦¬ í™•ì¸"""
    print("\n" + "="*60)
    print("ğŸ§ª 3. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° ì „ì²˜ë¦¬ í™•ì¸")
    print("="*60)
    
    tests = [
        ("python scripts/prepare_training_dataset.py", "ê¸°ë³¸ ë°ì´í„°ì…‹ ì¤€ë¹„"),
        ("python scripts/prepare_expanded_training_dataset.py", "í™•ì¥ëœ ë°ì´í„°ì…‹ ì¤€ë¹„ (342ê°œ ìƒ˜í”Œ)"),
        ("python scripts/test_expanded_tokenizer_setup.py", "í† í¬ë‚˜ì´ì € ì„¤ì • í…ŒìŠ¤íŠ¸")
    ]
    
    results = []
    for command, description in tests:
        result = run_command(command, description)
        results.append(result)
        
        if result["success"]:
            print(f"âœ… {description} - ì„±ê³µ ({result['execution_time']:.2f}ì´ˆ)")
        else:
            print(f"âŒ {description} - ì‹¤íŒ¨")
            if result["stderr"]:
                print(f"ì˜¤ë¥˜: {result['stderr']}")
    
    return results

def test_lora_finetuning():
    """4. LoRA íŒŒì¸íŠœë‹ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ§ª 4. LoRA íŒŒì¸íŠœë‹ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
    test_model_dir = "models/test/kogpt2-legal-lora-test"
    os.makedirs(test_model_dir, exist_ok=True)
    
    tests = [
        (f"python scripts/finetune_legal_model.py --epochs 1 --batch-size 1 --output {test_model_dir}", "LoRA íŒŒì¸íŠœë‹ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)"),
        (f"python scripts/evaluate_legal_model.py --model {test_model_dir} --test-data data/training/test_split.json", "ëª¨ë¸ í‰ê°€")
    ]
    
    results = []
    for command, description in tests:
        result = run_command(command, description)
        results.append(result)
        
        if result["success"]:
            print(f"âœ… {description} - ì„±ê³µ ({result['execution_time']:.2f}ì´ˆ)")
        else:
            print(f"âŒ {description} - ì‹¤íŒ¨")
            if result["stderr"]:
                print(f"ì˜¤ë¥˜: {result['stderr']}")
    
    return results

def test_day4_features():
    """5. Day 4 ê³ ë„í™”ëœ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ§ª 5. Day 4 ê³ ë„í™”ëœ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    test_model_dir = "models/test/kogpt2-legal-lora-test"
    
    tests = [
        ("python scripts/day4_test.py", "Day 4 í†µí•© í…ŒìŠ¤íŠ¸"),
        (f"python scripts/day4_evaluation_optimization.py --test-data data/training/test_split.json --models {test_model_dir} --output results/day4_evaluation", "ê³ ë„í™”ëœ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"),
        (f"python scripts/day4_evaluation_optimization.py --optimize --model-path {test_model_dir} --output results/day4_optimization", "ëª¨ë¸ ìµœì í™” í…ŒìŠ¤íŠ¸"),
        (f"python scripts/day4_evaluation_optimization.py --ab-test --test-data data/training/test_split.json --output results/day4_ab_test", "A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ í…ŒìŠ¤íŠ¸")
    ]
    
    results = []
    for command, description in tests:
        result = run_command(command, description)
        results.append(result)
        
        if result["success"]:
            print(f"âœ… {description} - ì„±ê³µ ({result['execution_time']:.2f}ì´ˆ)")
        else:
            print(f"âŒ {description} - ì‹¤íŒ¨")
            if result["stderr"]:
                print(f"ì˜¤ë¥˜: {result['stderr']}")
    
    return results

def test_integrated_system():
    """6. ì¢…í•© í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ§ª 6. ì¢…í•© í†µí•© í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    test_model_dir = "models/test/kogpt2-legal-lora-test"
    
    command = f"python scripts/day4_evaluation_optimization.py --test-data data/training/test_split.json --models {test_model_dir} --optimize --ab-test --output results/day4_complete"
    description = "ëª¨ë“  ê¸°ëŠ¥ì„ í•œ ë²ˆì— í…ŒìŠ¤íŠ¸"
    
    result = run_command(command, description)
    
    if result["success"]:
        print(f"âœ… {description} - ì„±ê³µ ({result['execution_time']:.2f}ì´ˆ)")
    else:
        print(f"âŒ {description} - ì‹¤íŒ¨")
        if result["stderr"]:
            print(f"ì˜¤ë¥˜: {result['stderr']}")
    
    return [result]

def check_performance_metrics():
    """ì„±ëŠ¥ ì§€í‘œ í™•ì¸"""
    print("\n" + "="*60)
    print("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ í™•ì¸")
    print("="*60)
    
    metrics = {
        "ëª¨ë¸ í¬ê¸°": "2GB ì´í•˜ ì••ì¶• í™•ì¸",
        "ì¶”ë¡  ì†ë„": "50% ì´ìƒ í–¥ìƒ í™•ì¸", 
        "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰": "40% ì´ìƒ ê°ì†Œ í™•ì¸",
        "ë²•ë¥  Q&A ì •í™•ë„": "75% ì´ìƒ ë‹¬ì„± í™•ì¸"
    }
    
    for metric, description in metrics.items():
        print(f"ğŸ“ˆ {metric}: {description}")
    
    return metrics

def check_result_files():
    """ê²°ê³¼ íŒŒì¼ í™•ì¸"""
    print("\n" + "="*60)
    print("ğŸ“ ê²°ê³¼ íŒŒì¼ í™•ì¸")
    print("="*60)
    
    result_dirs = [
        "results/day4_test/",
        "results/ab_tests/",
        "models/test/kogpt2-legal-lora-test/",
        "logs/"
    ]
    
    for dir_path in result_dirs:
        if os.path.exists(dir_path):
            files = os.listdir(dir_path)
            print(f"âœ… {dir_path}: {len(files)}ê°œ íŒŒì¼ ì¡´ì¬")
        else:
            print(f"âŒ {dir_path}: ë””ë ‰í† ë¦¬ ì—†ìŒ")

def generate_test_report(all_results: List[List[Dict]], output_dir: str = "results/task3_1_test"):
    """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ì „ì²´ ê²°ê³¼ í†µí•©
    all_tests = []
    for test_group in all_results:
        all_tests.extend(test_group)
    
    # í†µê³„ ê³„ì‚°
    total_tests = len(all_tests)
    successful_tests = sum(1 for test in all_tests if test["success"])
    failed_tests = total_tests - successful_tests
    success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
    
    # ë³´ê³ ì„œ ìƒì„±
    report = {
        "test_summary": {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "failed_tests": failed_tests,
            "success_rate": success_rate,
            "test_date": datetime.now().isoformat()
        },
        "test_results": all_tests,
        "performance_metrics": check_performance_metrics(),
        "recommendations": []
    }
    
    # ê¶Œì¥ì‚¬í•­ ì¶”ê°€
    if success_rate < 80:
        report["recommendations"].append("ì „ì²´ ì„±ê³µë¥ ì´ 80% ë¯¸ë§Œì…ë‹ˆë‹¤. ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì¬ê²€í† í•˜ì„¸ìš”.")
    
    if failed_tests > 0:
        report["recommendations"].append("ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë“¤ì„ ìˆ˜ì •í•˜ê³  ì¬ì‹¤í–‰í•˜ì„¸ìš”.")
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    report_file = os.path.join(output_dir, "task3_1_test_report.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {report_file}")
    return report

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ TASK 3.1 ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    all_results = []
    
    try:
        # ê° í…ŒìŠ¤íŠ¸ ê·¸ë£¹ ì‹¤í–‰
        all_results.append(test_environment_setup())
        all_results.append(test_model_loading())
        all_results.append(test_dataset_preparation())
        all_results.append(test_lora_finetuning())
        all_results.append(test_day4_features())
        all_results.append(test_integrated_system())
        
        # ì„±ëŠ¥ ì§€í‘œ ë° ê²°ê³¼ íŒŒì¼ í™•ì¸
        check_performance_metrics()
        check_result_files()
        
        # í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±
        report = generate_test_report(all_results)
        
        # ìµœì¢… ìš”ì•½
        print("\n" + "="*60)
        print("ğŸ¯ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ìš”ì•½")
        print("="*60)
        
        total_tests = report["test_summary"]["total_tests"]
        successful_tests = report["test_summary"]["successful_tests"]
        success_rate = report["test_summary"]["success_rate"]
        
        print(f"ì´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
        print(f"ì„±ê³µ: {successful_tests}ê°œ")
        print(f"ì‹¤íŒ¨: {total_tests - successful_tests}ê°œ")
        print(f"ì„±ê³µë¥ : {success_rate:.1f}%")
        
        if success_rate >= 80:
            print("ğŸ‰ TASK 3.1 í…ŒìŠ¤íŠ¸ í†µê³¼!")
        else:
            print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ë³´ê³ ì„œë¥¼ í™•ì¸í•˜ì—¬ ìˆ˜ì •í•˜ì„¸ìš”.")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ í…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    print(f"\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()
