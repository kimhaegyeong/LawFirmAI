#!/usr/bin/env python3
"""
assembly_articles í…Œì´ë¸”ì—ì„œ ì¡°ë¬¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import json
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict, Counter

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from source.data.database import DatabaseManager
from source.services.legal_text_preprocessor import LegalTextPreprocessor

class ArticleKeywordExtractor:
    """ì¡°ë¬¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.db_manager = DatabaseManager()
        self.preprocessor = LegalTextPreprocessor()
        self.extracted_keywords = defaultdict(set)
        
    def extract_keywords_from_articles(self) -> Dict[str, List[str]]:
        """assembly_articles í…Œì´ë¸”ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        print("ğŸ“‹ assembly_articles í…Œì´ë¸”ì—ì„œ ì¡°ë¬¸ ì •ë³´ ì¡°íšŒ ì¤‘...")
        
        # ì¡°ë¬¸ ë°ì´í„° ì¡°íšŒ (ë²•ë¥ ëª…ê³¼ í•¨ê»˜)
        query = """
        SELECT 
            a.article_content,
            a.article_title,
            a.article_number,
            l.law_name,
            l.category,
            l.ministry,
            l.law_type
        FROM assembly_articles a
        JOIN assembly_laws l ON a.law_id = l.law_id
        WHERE a.article_content IS NOT NULL 
        AND a.article_content != ''
        ORDER BY l.law_name, a.article_number
        LIMIT 10000
        """
        
        articles = self.db_manager.execute_query(query)
        print(f"âœ… ì´ {len(articles)}ê°œ ì¡°ë¬¸ ë°œê²¬")
        
        # ë„ë©”ì¸ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
        domain_keywords = defaultdict(set)
        
        for i, article in enumerate(articles):
            if i % 1000 == 0:
                print(f"  ğŸ“„ ì²˜ë¦¬ ì¤‘: {i+1}/{len(articles)}")
            
            law_name = article['law_name']
            category = article['category'] or 'ê¸°íƒ€'
            ministry = article['ministry'] or 'ê¸°íƒ€'
            law_type = article['law_type'] or 'ë²•ë¥ '
            article_content = article['article_content']
            article_title = article['article_title'] or ''
            article_number = article['article_number']
            
            # ë„ë©”ì¸ ë¶„ë¥˜
            domain = self._classify_law_domain(law_name, category, ministry, law_type)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            extracted = self._extract_keywords_from_article(article_content, article_title, law_name, article_number)
            domain_keywords[domain].update(extracted)
        
        # Setì„ Listë¡œ ë³€í™˜í•˜ê³  ì •ë ¬
        result = {}
        for domain, keywords in domain_keywords.items():
            result[domain] = sorted(list(keywords))
            
        return result
    
    def _classify_law_domain(self, law_name: str, category: str, ministry: str, law_type: str) -> str:
        """ë²•ë ¹ì„ ë„ë©”ì¸ë³„ë¡œ ë¶„ë¥˜"""
        law_name_lower = law_name.lower()
        
        # ë¯¼ì‚¬ë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['ë¯¼ë²•', 'ê³„ì•½', 'ì±„ê¶Œ', 'ì±„ë¬´', 'ì†í•´ë°°ìƒ', 'ë¶ˆë²•í–‰ìœ„', 'ë¬¼ê¶Œ']):
            return 'ë¯¼ì‚¬ë²•'
        
        # í˜•ì‚¬ë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['í˜•ë²•', 'í˜•ì‚¬', 'ë²”ì£„', 'ì²˜ë²Œ', 'ìˆ˜ì‚¬', 'í˜•ë²Œ']):
            return 'í˜•ì‚¬ë²•'
        
        # ê°€ì¡±ë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['ê°€ì¡±', 'í˜¼ì¸', 'ì´í˜¼', 'ì–‘ìœ¡', 'ìƒì†', 'ì¹œì¡±', 'ê°€ì¡±ê´€ê³„']):
            return 'ê°€ì¡±ë²•'
        
        # ìƒì‚¬ë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['ìƒë²•', 'íšŒì‚¬', 'ì£¼ì‹', 'ì–´ìŒ', 'ìˆ˜í‘œ', 'ë³´í—˜', 'í•´ìƒ', 'ìƒí–‰ìœ„']):
            return 'ìƒì‚¬ë²•'
        
        # ë…¸ë™ë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['ê·¼ë¡œ', 'ë…¸ë™', 'ì„ê¸ˆ', 'ê³ ìš©', 'í•´ê³ ', 'ì‚°ì—…ì•ˆì „', 'ê·¼ë¡œê¸°ì¤€']):
            return 'ë…¸ë™ë²•'
        
        # ë¶€ë™ì‚°ë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['ë¶€ë™ì‚°', 'í† ì§€', 'ë“±ê¸°', 'ì„ëŒ€ì°¨', 'ì „ì„¸', 'ë§¤ë§¤', 'ë¶€ë™ì‚°ë“±ê¸°']):
            return 'ë¶€ë™ì‚°ë²•'
        
        # ì§€ì ì¬ì‚°ê¶Œë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['íŠ¹í—ˆ', 'ìƒí‘œ', 'ì €ì‘ê¶Œ', 'ë””ìì¸', 'ì˜ì—…ë¹„ë°€', 'ì§€ì ì¬ì‚°', 'íŠ¹í—ˆë²•', 'ìƒí‘œë²•']):
            return 'ì§€ì ì¬ì‚°ê¶Œë²•'
        
        # ì„¸ë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['ì„¸ë²•', 'ì†Œë“ì„¸', 'ë²•ì¸ì„¸', 'ë¶€ê°€ê°€ì¹˜ì„¸', 'ì¡°ì„¸', 'êµ­ì„¸', 'ì§€ë°©ì„¸']):
            return 'ì„¸ë²•'
        
        # ë¯¼ì‚¬ì†Œì†¡ë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['ë¯¼ì‚¬ì†Œì†¡', 'ì†Œì†¡', 'ì§‘í–‰', 'ê°•ì œì§‘í–‰', 'ë¯¼ì‚¬ì§‘í–‰', 'ë¯¼ì‚¬ì†Œì†¡ë²•']):
            return 'ë¯¼ì‚¬ì†Œì†¡ë²•'
        
        # í˜•ì‚¬ì†Œì†¡ë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['í˜•ì‚¬ì†Œì†¡', 'ìˆ˜ì‚¬', 'ê¸°ì†Œ', 'ê³µì†Œ', 'ë³€í˜¸', 'ì¬íŒ', 'í˜•ì‚¬ì†Œì†¡ë²•']):
            return 'í˜•ì‚¬ì†Œì†¡ë²•'
        
        # í–‰ì •ë²• ë„ë©”ì¸
        if any(keyword in law_name_lower for keyword in ['í–‰ì •', 'í—ˆê°€', 'ì¸ê°€', 'ë©´í—ˆ', 'ì‹ ê³ ', 'ì²˜ë¶„', 'í–‰ì •ë²•']):
            return 'í–‰ì •ë²•'
        
        # ê¸°íƒ€
        return 'ê¸°íƒ€/ì¼ë°˜'
    
    def _extract_keywords_from_article(self, content: str, title: str, law_name: str, article_number: int) -> Set[str]:
        """ì¡°ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not content:
            return set()
        
        keywords = set()
        
        # 1. ì¡°ë¬¸ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        if title:
            title_keywords = self._extract_keywords_from_text(title)
            keywords.update(title_keywords)
        
        # 2. ì¡°ë¬¸ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        content_keywords = self._extract_keywords_from_text(content)
        keywords.update(content_keywords)
        
        # 3. ë²•ë¥ ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        law_keywords = self._extract_keywords_from_law_name(law_name)
        keywords.update(law_keywords)
        
        # 4. ì¡°ë¬¸ ë²ˆí˜¸ ê´€ë ¨ í‚¤ì›Œë“œ
        if article_number:
            keywords.add(f"ì œ{article_number}ì¡°")
        
        return keywords
    
    def _extract_keywords_from_text(self, text: str) -> Set[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text:
            return set()
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        cleaned_text = self.preprocessor.clean_text(text)
        
        keywords = set()
        
        # 1. ë²•ë¥  ì¡°ë¬¸ ë²ˆí˜¸ íŒ¨í„´ (ì œ1ì¡°, ì œ2ì¡° ë“±)
        article_pattern = r'ì œ\d+ì¡°'
        articles = re.findall(article_pattern, cleaned_text)
        keywords.update(articles)
        
        # 2. ë²•ë¥  ìš©ì–´ íŒ¨í„´ (ì¡°, í•­, í˜¸, ëª© ë“±)
        legal_terms = re.findall(r'\d+ì¡°|\d+í•­|\d+í˜¸|\d+ëª©', cleaned_text)
        keywords.update(legal_terms)
        
        # 3. í•œê¸€ ë²•ë¥  ìš©ì–´ (2-6ê¸€ì)
        korean_terms = re.findall(r'[ê°€-í£]{2,6}(?=ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì˜|ì—ì„œ|ìœ¼ë¡œ|ë¡œ|ì™€|ê³¼|ì—|ì—ê²Œ|ì—ê²Œì„œ)', cleaned_text)
        keywords.update(korean_terms)
        
        # 4. ì¼ë°˜ì ì¸ ë²•ë¥  ìš©ì–´
        common_legal_terms = [
            'ë²•ë¥ ', 'ë²•ë ¹', 'ê·œì •', 'ì¡°í•­', 'ì¡°ë¬¸', 'í•­ëª©', 'í˜¸ëª©',
            'ì‹œí–‰', 'ê³µí¬', 'ê°œì •', 'íì§€', 'ì œì •', 'ë¶€ì¹™', 'ë³¸ì¹™',
            'ê¶Œë¦¬', 'ì˜ë¬´', 'ì±…ì„', 'ì²˜ë²Œ', 'ë²Œê¸ˆ', 'ì§•ì—­', 'í˜•',
            'ì†Œì†¡', 'ì¬íŒ', 'íŒê²°', 'ì„ ê³ ', 'í™•ì •', 'ìƒê³ ', 'í•­ì†Œ',
            'ì‹ ì²­', 'ì²­êµ¬', 'ì œì¶œ', 'ì œê¸°', 'ê¸°ê°', 'ì¸ìš©', 'ê°í•˜',
            'ê³„ì•½', 'ì†í•´ë°°ìƒ', 'ë¶ˆë²•í–‰ìœ„', 'ì±„ê¶Œ', 'ì±„ë¬´', 'ì†Œìœ ê¶Œ',
            'í˜¼ì¸', 'ì´í˜¼', 'ì–‘ìœ¡', 'ìƒì†', 'ì¹œê¶Œ', 'ì–‘ìœ¡ê¶Œ',
            'íšŒì‚¬', 'ì£¼ì‹', 'ì£¼ì£¼', 'ì´ì‚¬', 'ìƒí–‰ìœ„', 'ë²•ì¸',
            'ê·¼ë¡œ', 'ê³ ìš©', 'ì„ê¸ˆ', 'í•´ê³ ', 'ë¶€ë‹¹í•´ê³ ', 'ê·¼ë¡œê³„ì•½',
            'ë¶€ë™ì‚°', 'í† ì§€', 'ë“±ê¸°', 'ë§¤ë§¤', 'ì„ëŒ€', 'ì†Œìœ ê¶Œì´ì „',
            'íŠ¹í—ˆ', 'ìƒí‘œ', 'ì €ì‘ê¶Œ', 'ë””ìì¸', 'ì˜ì—…ë¹„ë°€',
            'ì„¸ê¸ˆ', 'ì†Œë“ì„¸', 'ë²•ì¸ì„¸', 'ë¶€ê°€ê°€ì¹˜ì„¸', 'ì‹ ê³ ', 'ë‚©ë¶€',
            'ìˆ˜ì‚¬', 'ê¸°ì†Œ', 'ê³µì†Œ', 'ë³€í˜¸', 'ì¬ì‹¬', 'ìë°±'
        ]
        
        for term in common_legal_terms:
            if term in cleaned_text:
                keywords.add(term)
        
        # 5. íŠ¹ìˆ˜ë¬¸ìì™€ ìˆ«ìê°€ í¬í•¨ëœ ìš©ì–´ ì œê±°
        filtered_keywords = set()
        for keyword in keywords:
            if len(keyword) >= 2 and not re.search(r'[^\wê°€-í£]', keyword):
                filtered_keywords.add(keyword)
        
        return filtered_keywords
    
    def _extract_keywords_from_law_name(self, law_name: str) -> Set[str]:
        """ë²•ë¥ ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = set()
        
        # ë²•ë¥ ëª…ì„ ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìë¡œ ë¶„ë¦¬
        words = re.split(r'[^\wê°€-í£]+', law_name)
        
        for word in words:
            if len(word) >= 2:
                keywords.add(word)
        
        return keywords
    
    def save_keywords_to_database(self, domain_keywords: Dict[str, List[str]]):
        """ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        print("ğŸ’¾ ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
        
        # ê¸°ì¡´ í‚¤ì›Œë“œ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
        db_path = "data/legal_terms_database.json"
        if os.path.exists(db_path):
            with open(db_path, 'r', encoding='utf-8') as f:
                existing_db = json.load(f)
        else:
            existing_db = {}
        
        # ìƒˆë¡œìš´ í‚¤ì›Œë“œ ì¶”ê°€
        for domain, keywords in domain_keywords.items():
            if domain not in existing_db:
                existing_db[domain] = {}
            
            for keyword in keywords:
                if keyword not in existing_db[domain]:
                    existing_db[domain][keyword] = {
                        "weight": 0.8,  # ì¡°ë¬¸ ê¸°ë°˜ì´ë¯€ë¡œ ë†’ì€ ê°€ì¤‘ì¹˜
                        "synonyms": [],
                        "related_terms": [],
                        "context_keywords": [],
                        "source": "assembly_articles",
                        "confidence": 0.9,
                        "verified": True,
                        "added_date": "2025-10-19"
                    }
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        with open(db_path, 'w', encoding='utf-8') as f:
            json.dump(existing_db, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… í‚¤ì›Œë“œ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {db_path}")
    
    def print_statistics(self, domain_keywords: Dict[str, List[str]]):
        """ì¶”ì¶œ í†µê³„ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ì¡°ë¬¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ í†µê³„")
        print("="*60)
        
        total_keywords = 0
        for domain, keywords in domain_keywords.items():
            count = len(keywords)
            total_keywords += count
            print(f"  {domain}: {count:,}ê°œ í‚¤ì›Œë“œ")
        
        print(f"\n  ì´ í‚¤ì›Œë“œ ìˆ˜: {total_keywords:,}ê°œ")
        print("="*60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        extractor = ArticleKeywordExtractor()
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        domain_keywords = extractor.extract_keywords_from_articles()
        
        # í†µê³„ ì¶œë ¥
        extractor.print_statistics(domain_keywords)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        extractor.save_keywords_to_database(domain_keywords)
        
        print("\nâœ… ì¡°ë¬¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
