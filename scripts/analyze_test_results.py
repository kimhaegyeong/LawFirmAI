# -*- coding: utf-8 -*-
"""
í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
run_query_test.py ì‹¤í–‰ í›„ ë¡œê·¸ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ë¥¼ í™•ì¸
"""

import os
import re
from pathlib import Path
from datetime import datetime

def find_latest_log_file():
    """ìµœì‹  ë¡œê·¸ íŒŒì¼ ì°¾ê¸°"""
    log_dir = Path("logs/test")
    if not log_dir.exists():
        return None
    
    log_files = list(log_dir.glob("run_query_test_*.log"))
    if not log_files:
        return None
    
    return max(log_files, key=lambda f: f.stat().st_mtime)

def analyze_log_file(log_file_path):
    """ë¡œê·¸ íŒŒì¼ ë¶„ì„"""
    if not log_file_path or not log_file_path.exists():
        print("âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"ğŸ“ ë¡œê·¸ íŒŒì¼ ë¶„ì„: {log_file_path}")
    print("=" * 80)
    
    with open(log_file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    results = {
        'performance': {},
        'keyword_coverage': [],
        'metadata_typos': [],
        'semantic_skipping': []
    }
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì¶œ
    perf_pattern = r'process_search_results_combinedê°€\s+([\d.]+)ì´ˆ|expand_keywordsê°€\s+([\d.]+)ì´ˆ'
    perf_matches = re.findall(perf_pattern, content)
    for match in perf_matches:
        if match[0]:
            results['performance']['process_search_results_combined'] = float(match[0])
        if match[1]:
            results['performance']['expand_keywords'] = float(match[1])
    
    # Keyword Coverage ì¶”ì¶œ
    coverage_pattern = r'Keyword Coverage[:\s]+([\d.]+)'
    coverage_matches = re.findall(coverage_pattern, content)
    results['keyword_coverage'] = [float(c) for c in coverage_matches]
    
    # ë©”íƒ€ë°ì´í„° ì˜¤íƒ€ ì •ê·œí™” í™•ì¸
    typo_pattern = r'(Normalized typo|Fixed typo).*interpretation_id'
    typo_matches = re.findall(typo_pattern, content)
    results['metadata_typos'] = typo_matches
    
    # ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ìƒëµ í™•ì¸
    skip_pattern = r'Skipping semantic matching.*coverage already high.*([\d.]+)'
    skip_matches = re.findall(skip_pattern, content)
    results['semantic_skipping'] = skip_matches
    
    # Missing required fields í™•ì¸
    missing_pattern = r'Missing required fields.*interpretation_id'
    missing_matches = re.findall(missing_pattern, content)
    results['metadata_typos'].extend([f"Missing: {m}" for m in missing_matches])
    
    return results

def print_results(results):
    """ê²°ê³¼ ì¶œë ¥"""
    if not results:
        return
    
    print("\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
    print("-" * 80)
    
    # process_search_results_combined
    if 'process_search_results_combined' in results['performance']:
        time = results['performance']['process_search_results_combined']
        target = 5.0
        improvement = ((15.82 - time) / 15.82) * 100
        status = "âœ…" if time <= target else "âš ï¸"
        print(f"{status} process_search_results_combined: {time:.2f}ì´ˆ (ëª©í‘œ: {target}ì´ˆ ì´í•˜, ê°œì„ : {improvement:.1f}%)")
    else:
        print("âš ï¸  process_search_results_combined ì‹¤í–‰ ì‹œê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # expand_keywords
    if 'expand_keywords' in results['performance']:
        time = results['performance']['expand_keywords']
        target = 5.0
        improvement = ((8.18 - time) / 8.18) * 100
        status = "âœ…" if time <= target else "âš ï¸"
        print(f"{status} expand_keywords: {time:.2f}ì´ˆ (ëª©í‘œ: {target}ì´ˆ ì´í•˜, ê°œì„ : {improvement:.1f}%)")
    else:
        print("âš ï¸  expand_keywords ì‹¤í–‰ ì‹œê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # Keyword Coverage
    print("\nğŸ“ˆ Keyword Coverage:")
    print("-" * 80)
    if results['keyword_coverage']:
        avg_coverage = sum(results['keyword_coverage']) / len(results['keyword_coverage'])
        max_coverage = max(results['keyword_coverage'])
        min_coverage = min(results['keyword_coverage'])
        status = "âœ…" if avg_coverage >= 0.70 else "âš ï¸"
        print(f"{status} í‰ê· : {avg_coverage:.3f}, ìµœëŒ€: {max_coverage:.3f}, ìµœì†Œ: {min_coverage:.3f} (ëª©í‘œ: 0.70 ì´ìƒ)")
        print(f"   ì¸¡ì • íšŸìˆ˜: {len(results['keyword_coverage'])}íšŒ")
    else:
        print("âš ï¸  Keyword Coverage ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë©”íƒ€ë°ì´í„° ì˜¤íƒ€ ì •ê·œí™”
    print("\nğŸ”§ ë©”íƒ€ë°ì´í„° ì˜¤íƒ€ ì •ê·œí™”:")
    print("-" * 80)
    if results['metadata_typos']:
        normalized_count = len([t for t in results['metadata_typos'] if 'Normalized' in t or 'Fixed' in t])
        missing_count = len([t for t in results['metadata_typos'] if 'Missing' in t])
        print(f"âœ… ì •ê·œí™”ëœ ì˜¤íƒ€: {normalized_count}ê±´")
        if missing_count > 0:
            print(f"âš ï¸  ì—¬ì „íˆ ëˆ„ë½ëœ í•„ë“œ: {missing_count}ê±´")
    else:
        print("â„¹ï¸  ë©”íƒ€ë°ì´í„° ì˜¤íƒ€ ê´€ë ¨ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ìƒëµ
    print("\nâš¡ ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ìµœì í™”:")
    print("-" * 80)
    if results['semantic_skipping']:
        print(f"âœ… ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ìƒëµ ë°œìƒ: {len(results['semantic_skipping'])}íšŒ")
        print(f"   Coverage: {', '.join(results['semantic_skipping'])}")
    else:
        print("â„¹ï¸  ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ìƒëµ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\n" + "=" * 80)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì‹œì‘")
    print("=" * 80)
    
    # ìµœì‹  ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
    log_file = find_latest_log_file()
    
    if not log_file:
        print("âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("   python lawfirm_langgraph/tests/scripts/run_query_test.py \"ê³„ì•½ í•´ì§€ ì‚¬ìœ ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”\"")
        return
    
    # ë¡œê·¸ íŒŒì¼ ë¶„ì„
    results = analyze_log_file(log_file)
    
    # ê²°ê³¼ ì¶œë ¥
    print_results(results)
    
    # ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
    print("\nâœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:")
    print("-" * 80)
    
    if results:
        checks = []
        
        # ì„±ëŠ¥ ê²€ì¦
        if 'process_search_results_combined' in results['performance']:
            checks.append(("process_search_results_combined â‰¤ 5ì´ˆ", 
                          results['performance']['process_search_results_combined'] <= 5.0))
        
        if 'expand_keywords' in results['performance']:
            checks.append(("expand_keywords â‰¤ 5ì´ˆ", 
                          results['performance']['expand_keywords'] <= 5.0))
        
        if results['keyword_coverage']:
            avg = sum(results['keyword_coverage']) / len(results['keyword_coverage'])
            checks.append(("Keyword Coverage â‰¥ 0.70", avg >= 0.70))
        
        if results['semantic_skipping']:
            checks.append(("ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ìƒëµ ì‘ë™", True))
        
        if results['metadata_typos']:
            normalized = len([t for t in results['metadata_typos'] if 'Normalized' in t or 'Fixed' in t])
            checks.append(("ë©”íƒ€ë°ì´í„° ì˜¤íƒ€ ì •ê·œí™” ì‘ë™", normalized > 0))
        
        for check_name, passed in checks:
            status = "âœ…" if passed else "âŒ"
            print(f"{status} {check_name}")

if __name__ == "__main__":
    main()

