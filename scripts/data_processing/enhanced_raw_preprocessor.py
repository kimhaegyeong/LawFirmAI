#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í–¥ìƒëœ Raw ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
Raw ë°ì´í„°ë¥¼ ê³ í’ˆì§ˆë¡œ ì „ì²˜ë¦¬í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import re
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import hashlib

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/enhanced_preprocessing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class ProcessingResult:
    """ì „ì²˜ë¦¬ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    processed_files: int = 0
    total_laws: int = 0
    total_articles: int = 0
    quality_scores: List[float] = None
    errors: List[str] = None
    processing_time: float = 0.0
    
    def __post_init__(self):
        if self.quality_scores is None:
            self.quality_scores = []
        if self.errors is None:
            self.errors = []


class DataQualityValidator:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.min_content_length = 10
        self.min_article_count = 1
        self.max_content_length = 100000
        
    def calculate_quality_score(self, law_data: Dict[str, Any]) -> float:
        """
        ë²•ë¥  ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        
        Args:
            law_data (Dict[str, Any]): ë²•ë¥  ë°ì´í„°
            
        Returns:
            float: í’ˆì§ˆ ì ìˆ˜
        """
        score = 0.0
        max_score = 0.0
        
        # ê¸°ë³¸ ì •ë³´ ì™„ì„±ë„ (30%)
        max_score += 30
        basic_info_score = self._check_basic_info(law_data)
        score += basic_info_score * 30
        
        # ì¡°ë¬¸ êµ¬ì¡° ì™„ì„±ë„ (40%)
        max_score += 40
        article_structure_score = self._check_article_structure(law_data)
        score += article_structure_score * 40
        
        # ë‚´ìš© í’ˆì§ˆ (20%)
        max_score += 20
        content_quality_score = self._check_content_quality(law_data)
        score += content_quality_score * 20
        
        # ì¼ê´€ì„± (10%)
        max_score += 10
        consistency_score = self._check_consistency(law_data)
        score += consistency_score * 10
        
        return score / max_score if max_score > 0 else 0.0
    
    def _check_basic_info(self, law_data: Dict[str, Any]) -> float:
        """ê¸°ë³¸ ì •ë³´ ì™„ì„±ë„ ê²€ì‚¬"""
        required_fields = ['law_name', 'law_type', 'promulgation_date']
        present_fields = sum(1 for field in required_fields if law_data.get(field))
        return present_fields / len(required_fields)
    
    def _check_article_structure(self, law_data: Dict[str, Any]) -> float:
        """ì¡°ë¬¸ êµ¬ì¡° ì™„ì„±ë„ ê²€ì‚¬"""
        articles = law_data.get('articles', [])
        if not articles:
            return 0.0
        
        valid_articles = 0
        for article in articles:
            if (article.get('article_number') and 
                article.get('article_content') and 
                len(article.get('article_content', '')) > self.min_content_length):
                valid_articles += 1
        
        return valid_articles / len(articles) if articles else 0.0
    
    def _check_content_quality(self, law_data: Dict[str, Any]) -> float:
        """ë‚´ìš© í’ˆì§ˆ ê²€ì‚¬"""
        full_text = law_data.get('full_text', '')
        if not full_text:
            return 0.0
        
        # ê¸¸ì´ ì ì ˆì„±
        length_score = min(len(full_text) / 1000, 1.0)  # 1000ì ì´ìƒì´ë©´ ë§Œì 
        
        # HTML íƒœê·¸ ì œê±° í™•ì¸
        html_tags = re.findall(r'<[^>]+>', full_text)
        html_score = 1.0 - min(len(html_tags) / 100, 1.0)  # HTML íƒœê·¸ê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
        
        # íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨
        special_chars = re.findall(r'[^\w\sê°€-í£]', full_text)
        special_score = 1.0 - min(len(special_chars) / len(full_text), 0.3)  # íŠ¹ìˆ˜ë¬¸ì 30% ì´í•˜
        
        return (length_score + html_score + special_score) / 3
    
    def _check_consistency(self, law_data: Dict[str, Any]) -> float:
        """ì¼ê´€ì„± ê²€ì‚¬"""
        articles = law_data.get('articles', [])
        if not articles:
            return 0.0
        
        # ì¡°ë¬¸ ë²ˆí˜¸ ì—°ì†ì„±
        article_numbers = []
        for article in articles:
            try:
                num = int(re.findall(r'\d+', article.get('article_number', '0'))[0])
                article_numbers.append(num)
            except (ValueError, IndexError):
                continue
        
        if not article_numbers:
            return 0.0
        
        # ì—°ì†ì„± ì ìˆ˜ ê³„ì‚°
        sorted_numbers = sorted(article_numbers)
        expected_range = list(range(min(sorted_numbers), max(sorted_numbers) + 1))
        continuity_score = len(set(sorted_numbers) & set(expected_range)) / len(expected_range)
        
        return continuity_score


class HybridArticleParser:
    """í•˜ì´ë¸Œë¦¬ë“œ ì¡°ë¬¸ íŒŒì„œ (ê·œì¹™ ê¸°ë°˜ + ML)"""
    
    def __init__(self):
        # ì¡°ë¬¸ íŒ¨í„´
        self.article_patterns = [
            re.compile(r'ì œ(\d+)ì¡°\s*\(([^)]+)\)'),  # ì œ1ì¡°(ëª©ì )
            re.compile(r'ì œ(\d+)ì¡°'),  # ì œ1ì¡°
            re.compile(r'ì œ(\d+)ì¡°\s*ì˜\s*(\d+)'),  # ì œ1ì¡°ì˜2
        ]
        
        # í•­ íŒ¨í„´
        self.paragraph_patterns = [
            re.compile(r'â‘ |â‘¡|â‘¢|â‘£|â‘¤|â‘¥|â‘¦|â‘§|â‘¨|â‘©|â‘ª|â‘«|â‘¬|â‘­|â‘®|â‘¯|â‘°|â‘±|â‘²|â‘³'),
            re.compile(r'(\d+)\s*í•­'),
            re.compile(r'ì œ(\d+)\s*í•­')
        ]
        
        # í˜¸ íŒ¨í„´
        self.subparagraph_patterns = [
            re.compile(r'(\d+)\s*\.'),
            re.compile(r'ì œ(\d+)\s*í˜¸')
        ]
    
    def parse_with_validation(self, law_content: str) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹±ìœ¼ë¡œ ì¡°ë¬¸ ì¶”ì¶œ
        
        Args:
            law_content (str): ë²•ë¥  ë‚´ìš©
            
        Returns:
            Dict[str, Any]: íŒŒì‹± ê²°ê³¼
        """
        try:
            # ê·œì¹™ ê¸°ë°˜ íŒŒì‹±
            rule_based_result = self._rule_based_parsing(law_content)
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_parsing_quality(rule_based_result)
            
            return {
                'articles': rule_based_result['articles'],
                'quality_score': quality_score,
                'parsing_method': 'hybrid',
                'ml_confidence': quality_score,
                'metadata': {
                    'total_articles': len(rule_based_result['articles']),
                    'parsing_time': rule_based_result.get('parsing_time', 0),
                    'errors': rule_based_result.get('errors', [])
                }
            }
            
        except Exception as e:
            logger.error(f"Error in hybrid parsing: {e}")
            return {
                'articles': [],
                'quality_score': 0.0,
                'parsing_method': 'hybrid',
                'ml_confidence': 0.0,
                'metadata': {
                    'total_articles': 0,
                    'parsing_time': 0,
                    'errors': [str(e)]
                }
            }
    
    def _rule_based_parsing(self, content: str) -> Dict[str, Any]:
        """ê·œì¹™ ê¸°ë°˜ íŒŒì‹±"""
        articles = []
        errors = []
        
        try:
            # ì¡°ë¬¸ ì¶”ì¶œ
            for pattern in self.article_patterns:
                matches = pattern.findall(content)
                for match in matches:
                    if len(match) == 2:  # ì œ1ì¡°(ëª©ì ) í˜•íƒœ
                        article_num, title = match
                        article_content = self._extract_article_content(content, f"ì œ{article_num}ì¡°")
                    else:  # ì œ1ì¡° í˜•íƒœ
                        article_num = match[0] if isinstance(match, tuple) else match
                        title = ""
                        article_content = self._extract_article_content(content, f"ì œ{article_num}ì¡°")
                    
                    if article_content:
                        article = {
                            'article_number': f"ì œ{article_num}ì¡°",
                            'article_title': title,
                            'article_content': article_content.strip(),
                            'word_count': len(article_content.split()),
                            'char_count': len(article_content),
                            'is_supplementary': False,
                            'parsing_method': 'rule_based'
                        }
                        articles.append(article)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            articles = self._remove_duplicates_and_sort(articles)
            
            return {
                'articles': articles,
                'parsing_time': 0,
                'errors': errors
            }
            
        except Exception as e:
            errors.append(f"Rule-based parsing error: {e}")
            return {
                'articles': [],
                'parsing_time': 0,
                'errors': errors
            }
    
    def _extract_article_content(self, content: str, article_ref: str) -> str:
        """ì¡°ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        try:
            # ì¡°ë¬¸ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
            start_pattern = re.compile(f'{re.escape(article_ref)}(?:\s*\([^)]+\))?')
            start_match = start_pattern.search(content)
            
            if not start_match:
                return ""
            
            start_pos = start_match.end()
            
            # ë‹¤ìŒ ì¡°ë¬¸ ë˜ëŠ” ëê¹Œì§€ ì¶”ì¶œ
            next_article_pattern = re.compile(r'ì œ\d+ì¡°')
            remaining_content = content[start_pos:]
            
            # ë‹¤ìŒ ì¡°ë¬¸ ì°¾ê¸°
            next_match = next_article_pattern.search(remaining_content)
            if next_match:
                end_pos = start_pos + next_match.start()
                return content[start_pos:end_pos]
            else:
                return content[start_pos:]
                
        except Exception as e:
            logger.error(f"Error extracting article content: {e}")
            return ""
    
    def _remove_duplicates_and_sort(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±° ë° ì •ë ¬"""
        # ì¤‘ë³µ ì œê±° (ì¡°ë¬¸ ë²ˆí˜¸ ê¸°ì¤€)
        seen_numbers = set()
        unique_articles = []
        
        for article in articles:
            article_num = article.get('article_number', '')
            if article_num not in seen_numbers:
                seen_numbers.add(article_num)
                unique_articles.append(article)
        
        # ì¡°ë¬¸ ë²ˆí˜¸ë¡œ ì •ë ¬
        def sort_key(article):
            try:
                num = int(re.findall(r'\d+', article.get('article_number', '0'))[0])
                return num
            except (ValueError, IndexError):
                return 999999
        
        unique_articles.sort(key=sort_key)
        return unique_articles
    
    def _calculate_parsing_quality(self, parsing_result: Dict[str, Any]) -> float:
        """íŒŒì‹± í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        articles = parsing_result.get('articles', [])
        errors = parsing_result.get('errors', [])
        
        if not articles:
            return 0.0
        
        # ê¸°ë³¸ ì ìˆ˜
        base_score = 0.5
        
        # ì¡°ë¬¸ ìˆ˜ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        article_count_score = min(len(articles) / 10, 0.3)  # 10ê°œ ì´ìƒì´ë©´ ë§Œì 
        
        # ì—ëŸ¬ ìˆ˜ì— ë”°ë¥¸ í˜ë„í‹°
        error_penalty = min(len(errors) * 0.1, 0.3)
        
        # ì¡°ë¬¸ ë‚´ìš© í’ˆì§ˆ
        content_quality = 0.0
        for article in articles:
            content = article.get('article_content', '')
            if len(content) > 50:  # 50ì ì´ìƒ
                content_quality += 1
        content_quality = content_quality / len(articles) * 0.2 if articles else 0
        
        final_score = base_score + article_count_score - error_penalty + content_quality
        return max(0.0, min(1.0, final_score))


class EnhancedRawPreprocessor:
    """í–¥ìƒëœ Raw ë°ì´í„° ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, 
                 raw_data_base_path: str = "data/raw",
                 processed_data_base_path: str = "data/processed"):
        self.raw_data_base_path = Path(raw_data_base_path)
        self.processed_data_base_path = Path(processed_data_base_path)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.quality_validator = DataQualityValidator()
        self.hybrid_parser = HybridArticleParser()
        
        # ì²˜ë¦¬ í†µê³„
        self.stats = {
            'processed_files': 0,
            'total_laws': 0,
            'total_articles': 0,
            'quality_scores': [],
            'errors': [],
            'processing_time': 0.0
        }
    
    def process_law_only_data(self, raw_dir: str) -> ProcessingResult:
        """
        ë²•ë¥  ì „ìš© ë°ì´í„° ì „ì²˜ë¦¬
        
        Args:
            raw_dir (str): Raw ë°ì´í„° ë””ë ‰í† ë¦¬
            
        Returns:
            ProcessingResult: ì²˜ë¦¬ ê²°ê³¼
        """
        logger.info(f"ğŸ”„ Processing law-only data from: {raw_dir}")
        start_time = datetime.now()
        
        result = ProcessingResult()
        raw_path = Path(raw_dir)
        
        if not raw_path.exists():
            error_msg = f"Raw directory not found: {raw_dir}"
            logger.error(error_msg)
            result.errors.append(error_msg)
            return result
        
        # JSON íŒŒì¼ ì²˜ë¦¬
        json_files = list(raw_path.glob("**/*.json"))
        logger.info(f"Found {len(json_files)} JSON files to process")
        
        for file_path in json_files:
            try:
                file_result = self._process_single_law_file(file_path)
                result.processed_files += 1
                result.total_laws += file_result['total_laws']
                result.total_articles += file_result['total_articles']
                result.quality_scores.extend(file_result['quality_scores'])
                result.errors.extend(file_result['errors'])
                
                # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
                self._save_processed_data(file_result['processed_data'], file_path)
                
            except Exception as e:
                error_msg = f"Error processing {file_path}: {e}"
                logger.error(error_msg)
                result.errors.append(error_msg)
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        end_time = datetime.now()
        result.processing_time = (end_time - start_time).total_seconds()
        
        logger.info(f"âœ… Processing completed:")
        logger.info(f"  - Processed files: {result.processed_files}")
        logger.info(f"  - Total laws: {result.total_laws}")
        logger.info(f"  - Total articles: {result.total_articles}")
        logger.info(f"  - Average quality: {sum(result.quality_scores)/len(result.quality_scores):.3f}" if result.quality_scores else "N/A")
        logger.info(f"  - Processing time: {result.processing_time:.2f} seconds")
        
        return result
    
    def _process_single_law_file(self, file_path: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ ë²•ë¥  íŒŒì¼ ì²˜ë¦¬"""
        try:
            # Raw ë°ì´í„° ë¡œë“œ
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            processed_laws = []
            quality_scores = []
            errors = []
            
            # ë²•ë¥  ë°ì´í„° ì²˜ë¦¬
            laws = raw_data.get('laws', [])
            for law_item in laws:
                try:
                    # í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± ì ìš©
                    parsed_result = self.hybrid_parser.parse_with_validation(
                        law_item.get('law_content', '')
                    )
                    
                    # ë²•ë¥  ë°ì´í„° êµ¬ì„±
                    law_data = {
                        'law_id': law_item.get('law_id', ''),
                        'law_name': law_item.get('law_name', ''),
                        'law_type': law_item.get('law_type', ''),
                        'promulgation_date': law_item.get('promulgation_date', ''),
                        'enforcement_date': law_item.get('enforcement_date', ''),
                        'ministry': law_item.get('ministry', ''),
                        'full_text': law_item.get('law_content', ''),
                        'articles': parsed_result.get('articles', []),
                        'parsing_method': parsed_result.get('parsing_method', 'hybrid'),
                        'parsing_quality_score': parsed_result.get('quality_score', 0.0),
                        'ml_confidence_score': parsed_result.get('ml_confidence', 0.0),
                        'article_count': len(parsed_result.get('articles', [])),
                        'created_at': datetime.now().isoformat()
                    }
                    
                    # í’ˆì§ˆ ê²€ì¦
                    quality_score = self.quality_validator.calculate_quality_score(law_data)
                    law_data['parsing_quality_score'] = quality_score
                    quality_scores.append(quality_score)
                    
                    processed_laws.append(law_data)
                    
                except Exception as e:
                    error_msg = f"Error processing law {law_item.get('law_name', 'Unknown')}: {e}"
                    logger.error(error_msg)
                    errors.append(error_msg)
            
            return {
                'processed_data': {'laws': processed_laws},
                'total_laws': len(processed_laws),
                'total_articles': sum(len(law.get('articles', [])) for law in processed_laws),
                'quality_scores': quality_scores,
                'errors': errors
            }
            
        except Exception as e:
            error_msg = f"Error loading file {file_path}: {e}"
            logger.error(error_msg)
            return {
                'processed_data': {'laws': []},
                'total_laws': 0,
                'total_articles': 0,
                'quality_scores': [],
                'errors': [error_msg]
            }
    
    def _save_processed_data(self, processed_data: Dict[str, Any], original_file_path: Path):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        try:
            # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
            relative_path = original_file_path.relative_to(self.raw_data_base_path)
            output_subdir = self.processed_data_base_path / relative_path.parent
            output_subdir.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„±
            output_file_name = f"enhanced_{original_file_path.stem}.json"
            output_file_path = output_subdir / output_file_name
            
            # ë°ì´í„° ì €ì¥
            with open(output_file_path, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"Saved processed data to: {output_file_path}")
            
        except Exception as e:
            logger.error(f"Error saving processed data: {e}")
    
    def process_precedent_data(self, raw_dir: str) -> ProcessingResult:
        """
        íŒë¡€ ë°ì´í„° ì „ì²˜ë¦¬
        
        Args:
            raw_dir (str): Raw ë°ì´í„° ë””ë ‰í† ë¦¬
            
        Returns:
            ProcessingResult: ì²˜ë¦¬ ê²°ê³¼
        """
        logger.info(f"ğŸ”„ Processing precedent data from: {raw_dir}")
        start_time = datetime.now()
        
        result = ProcessingResult()
        raw_path = Path(raw_dir)
        
        if not raw_path.exists():
            error_msg = f"Raw directory not found: {raw_dir}"
            logger.error(error_msg)
            result.errors.append(error_msg)
            return result
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
        categories = ['civil', 'criminal', 'family', 'administrative']
        
        for category in categories:
            category_path = raw_path / category
            if category_path.exists():
                category_result = self._process_precedent_category(category_path, category)
                result.processed_files += category_result.processed_files
                result.total_laws += category_result.total_laws  # íŒë¡€ëŠ” casesë¡œ ì²˜ë¦¬
                result.errors.extend(category_result.errors)
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        end_time = datetime.now()
        result.processing_time = (end_time - start_time).total_seconds()
        
        logger.info(f"âœ… Precedent processing completed:")
        logger.info(f"  - Processed files: {result.processed_files}")
        logger.info(f"  - Total cases: {result.total_laws}")
        logger.info(f"  - Processing time: {result.processing_time:.2f} seconds")
        
        return result
    
    def _process_precedent_category(self, category_path: Path, category: str) -> ProcessingResult:
        """ì¹´í…Œê³ ë¦¬ë³„ íŒë¡€ ì²˜ë¦¬"""
        result = ProcessingResult()
        
        json_files = list(category_path.glob("**/*.json"))
        logger.info(f"Processing {len(json_files)} files for category: {category}")
        
        for file_path in json_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    raw_data = json.load(f)
                
                # íŒë¡€ ë°ì´í„° ì²˜ë¦¬ ë¡œì§ (ê°„ë‹¨í•œ êµ¬í˜„)
                cases = raw_data.get('cases', [])
                processed_cases = []
                
                for case in cases:
                    processed_case = {
                        'case_id': case.get('case_id', ''),
                        'case_name': case.get('case_name', ''),
                        'case_number': case.get('case_number', ''),
                        'field': category,
                        'court': case.get('court', ''),
                        'decision_date': case.get('decision_date', ''),
                        'full_text': case.get('full_text', ''),
                        'created_at': datetime.now().isoformat()
                    }
                    processed_cases.append(processed_case)
                
                # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
                self._save_processed_precedent_data(processed_cases, file_path, category)
                
                result.processed_files += 1
                result.total_laws += len(processed_cases)
                
            except Exception as e:
                error_msg = f"Error processing precedent file {file_path}: {e}"
                logger.error(error_msg)
                result.errors.append(error_msg)
        
        return result
    
    def _save_processed_precedent_data(self, processed_cases: List[Dict[str, Any]], 
                                     original_file_path: Path, category: str):
        """ì „ì²˜ë¦¬ëœ íŒë¡€ ë°ì´í„° ì €ì¥"""
        try:
            # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
            relative_path = original_file_path.relative_to(self.raw_data_base_path)
            output_subdir = self.processed_data_base_path / "precedent" / category / relative_path.parent
            output_subdir.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„±
            output_file_name = f"enhanced_{original_file_path.stem}.json"
            output_file_path = output_subdir / output_file_name
            
            # ë°ì´í„° ì €ì¥
            processed_data = {'cases': processed_cases}
            with open(output_file_path, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"Saved processed precedent data to: {output_file_path}")
            
        except Exception as e:
            logger.error(f"Error saving processed precedent data: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting enhanced raw data preprocessing...")
    
    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = EnhancedRawPreprocessor()
    
    # ë²•ë¥  ë°ì´í„° ì „ì²˜ë¦¬
    logger.info("\nğŸ“‹ Phase 1: Processing law-only data...")
    law_result = preprocessor.process_law_only_data("data/raw/assembly/law_only")
    
    # íŒë¡€ ë°ì´í„° ì „ì²˜ë¦¬
    logger.info("\nğŸ“‹ Phase 2: Processing precedent data...")
    precedent_result = preprocessor.process_precedent_data("data/raw/assembly/precedent")
    
    # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
    total_result = ProcessingResult(
        processed_files=law_result.processed_files + precedent_result.processed_files,
        total_laws=law_result.total_laws + precedent_result.total_laws,
        total_articles=law_result.total_articles,
        quality_scores=law_result.quality_scores,
        errors=law_result.errors + precedent_result.errors,
        processing_time=law_result.processing_time + precedent_result.processing_time
    )
    
    # ê²°ê³¼ ì €ì¥
    result_data = {
        'law_processing': {
            'processed_files': law_result.processed_files,
            'total_laws': law_result.total_laws,
            'total_articles': law_result.total_articles,
            'average_quality': sum(law_result.quality_scores)/len(law_result.quality_scores) if law_result.quality_scores else 0,
            'errors': law_result.errors
        },
        'precedent_processing': {
            'processed_files': precedent_result.processed_files,
            'total_cases': precedent_result.total_laws,
            'errors': precedent_result.errors
        },
        'total_processing': {
            'processed_files': total_result.processed_files,
            'total_laws': total_result.total_laws,
            'total_articles': total_result.total_articles,
            'processing_time': total_result.processing_time,
            'total_errors': len(total_result.errors)
        }
    }
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    with open("data/preprocessing_report.json", "w", encoding="utf-8") as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nğŸ“Š Detailed report saved to: data/preprocessing_report.json")
    logger.info("âœ… Enhanced preprocessing completed successfully!")
    
    return total_result


if __name__ == "__main__":
    result = main()
    if result.errors:
        print(f"\nâš ï¸ Processing completed with {len(result.errors)} errors")
        print("Check logs for details.")
    else:
        print("\nğŸ‰ Preprocessing completed successfully!")
        print("You can now proceed with database import.")
