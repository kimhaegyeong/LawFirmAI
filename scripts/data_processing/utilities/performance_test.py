#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Assembly Collection Performance Test
êµ?šŒ ?˜ì§‘ ?¤í¬ë¦½íŠ¸ ?±ëŠ¥ ?ŒìŠ¤??ë°?ë²¤ì¹˜ë§ˆí¬

???¤í¬ë¦½íŠ¸???˜ì§‘ ?¤í¬ë¦½íŠ¸?¤ì˜ ?±ëŠ¥???ŒìŠ¤?¸í•˜ê³?ë²¤ì¹˜ë§ˆí¬ë¥??œê³µ?©ë‹ˆ??
"""

import time
import psutil
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import json

# ?„ë¡œ?íŠ¸ ë£¨íŠ¸ë¥?Python ê²½ë¡œ??ì¶”ê?
project_root = Path(__file__).parent.parent.parent
import sys
sys.path.append(str(project_root))

from scripts.assembly.common_utils import (
    MemoryManager, CollectionConfig, CollectionLogger,
    get_system_memory_info, check_system_requirements
)


class PerformanceMonitor:
    """?±ëŠ¥ ëª¨ë‹ˆ?°ë§ ?´ë˜??""
    
    def __init__(self):
        """?±ëŠ¥ ëª¨ë‹ˆ??ì´ˆê¸°??""
        self.start_time = None
        self.end_time = None
        self.memory_samples = []
        self.cpu_samples = []
        self.logger = CollectionLogger.setup_logging("performance_test")
    
    def start_monitoring(self):
        """ëª¨ë‹ˆ?°ë§ ?œì‘"""
        self.start_time = time.time()
        self.logger.info("Performance monitoring started")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆ?°ë§ ì¤‘ì?"""
        self.end_time = time.time()
        self.logger.info("Performance monitoring stopped")
    
    def sample_system_metrics(self):
        """?œìŠ¤??ë©”íŠ¸ë¦??˜í”Œë§?""
        try:
            # ë©”ëª¨ë¦??¬ìš©??
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.memory_samples.append(memory_mb)
            
            # CPU ?¬ìš©ë¥?
            cpu_percent = psutil.cpu_percent()
            self.cpu_samples.append(cpu_percent)
            
        except Exception as e:
            self.logger.error(f"Failed to sample metrics: {e}")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """?±ëŠ¥ ë¦¬í¬???ì„±"""
        if not self.start_time or not self.end_time:
            return {"error": "Monitoring not completed"}
        
        duration = self.end_time - self.start_time
        
        # ë©”ëª¨ë¦??µê³„
        memory_stats = {}
        if self.memory_samples:
            memory_stats = {
                "min_mb": min(self.memory_samples),
                "max_mb": max(self.memory_samples),
                "avg_mb": sum(self.memory_samples) / len(self.memory_samples),
                "samples": len(self.memory_samples)
            }
        
        # CPU ?µê³„
        cpu_stats = {}
        if self.cpu_samples:
            cpu_stats = {
                "min_percent": min(self.cpu_samples),
                "max_percent": max(self.cpu_samples),
                "avg_percent": sum(self.cpu_samples) / len(self.cpu_samples),
                "samples": len(self.cpu_samples)
            }
        
        return {
            "duration_seconds": duration,
            "memory_stats": memory_stats,
            "cpu_stats": cpu_stats,
            "system_info": get_system_memory_info(),
            "timestamp": datetime.now().isoformat()
        }


class CollectionBenchmark:
    """?˜ì§‘ ?±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ?´ë˜??""
    
    def __init__(self):
        """ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°??""
        self.logger = CollectionLogger.setup_logging("collection_benchmark")
        self.results = []
    
    def benchmark_memory_manager(self, iterations: int = 100) -> Dict[str, Any]:
        """ë©”ëª¨ë¦?ë§¤ë‹ˆ?€ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info(f"Benchmarking MemoryManager with {iterations} iterations")
        
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # ë©”ëª¨ë¦?ë§¤ë‹ˆ?€ ?ŒìŠ¤??
        memory_manager = MemoryManager(memory_limit_mb=600)
        
        for i in range(iterations):
            # ë©”ëª¨ë¦??¬ìš©??ì²´í¬
            memory_manager.get_memory_usage()
            
            # ì£¼ê¸°?ìœ¼ë¡??˜í”Œë§?
            if i % 10 == 0:
                monitor.sample_system_metrics()
        
        monitor.stop_monitoring()
        
        result = {
            "test_name": "MemoryManager",
            "iterations": iterations,
            "performance": monitor.get_performance_report()
        }
        
        self.results.append(result)
        return result
    
    def benchmark_data_optimizer(self, test_data_size: int = 1000) -> Dict[str, Any]:
        """?°ì´??ìµœì ??ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info(f"Benchmarking DataOptimizer with {test_data_size} items")
        
        from scripts.assembly.common_utils import DataOptimizer
        
        # ?ŒìŠ¤???°ì´???ì„±
        test_items = []
        for i in range(test_data_size):
            test_items.append({
                'content_html': 'x' * 2000000,  # 2MB HTML
                'precedent_content': 'y' * 1500000,  # 1.5MB content
                'structured_content': {
                    'full_text': 'z' * 1000000,  # 1MB text
                    'case_info': 'a' * 100000
                }
            })
        
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # ?°ì´??ìµœì ???ŒìŠ¤??
        optimized_items = []
        for i, item in enumerate(test_items):
            optimized_item = DataOptimizer.optimize_item(item)
            optimized_items.append(optimized_item)
            
            # ì£¼ê¸°?ìœ¼ë¡??˜í”Œë§?
            if i % 100 == 0:
                monitor.sample_system_metrics()
        
        monitor.stop_monitoring()
        
        # ?¬ê¸° ë¹„êµ
        original_size = sum(len(str(item)) for item in test_items)
        optimized_size = sum(len(str(item)) for item in optimized_items)
        compression_ratio = optimized_size / original_size if original_size > 0 else 0
        
        result = {
            "test_name": "DataOptimizer",
            "items_processed": test_data_size,
            "original_size_bytes": original_size,
            "optimized_size_bytes": optimized_size,
            "compression_ratio": compression_ratio,
            "performance": monitor.get_performance_report()
        }
        
        self.results.append(result)
        return result
    
    def benchmark_collection_config(self) -> Dict[str, Any]:
        """?˜ì§‘ ?¤ì • ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info("Benchmarking CollectionConfig")
        
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # ?¤ì • ?ì„± ë°?ì¡°íšŒ ?ŒìŠ¤??
        configs = []
        for i in range(1000):
            config = CollectionConfig(
                memory_limit_mb=600 + i,
                batch_size=20 + (i % 10),
                max_retries=3 + (i % 3)
            )
            configs.append(config)
        
        # ?¤ì • ì¡°íšŒ ?ŒìŠ¤??
        for config in configs:
            _ = config.get('memory_limit_mb')
            _ = config.get('batch_size')
            _ = config.get('max_retries')
        
        monitor.stop_monitoring()
        
        result = {
            "test_name": "CollectionConfig",
            "configs_created": len(configs),
            "performance": monitor.get_performance_report()
        }
        
        self.results.append(result)
        return result
    
    def run_all_benchmarks(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ?¤í–‰"""
        self.logger.info("Starting comprehensive benchmark suite")
        
        # ?œìŠ¤???”êµ¬?¬í•­ ?•ì¸
        if not check_system_requirements(min_memory_gb=2.0):
            self.logger.warning("System requirements not met, proceeding with caution")
        
        # ë²¤ì¹˜ë§ˆí¬ ?¤í–‰
        self.benchmark_memory_manager(iterations=100)
        self.benchmark_data_optimizer(test_data_size=500)
        self.benchmark_collection_config()
        
        self.logger.info(f"Completed {len(self.results)} benchmarks")
        return self.results
    
    def save_results(self, output_file: Path):
        """ê²°ê³¼ ?€??""
        results_data = {
            "benchmark_info": {
                "timestamp": datetime.now().isoformat(),
                "total_tests": len(self.results),
                "system_info": get_system_memory_info()
            },
            "results": self.results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"Benchmark results saved to {output_file}")
    
    def print_summary(self):
        """ê²°ê³¼ ?”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("?“Š BENCHMARK RESULTS SUMMARY")
        print("="*60)
        
        for result in self.results:
            test_name = result["test_name"]
            performance = result.get("performance", {})
            duration = performance.get("duration_seconds", 0)
            
            print(f"\n?”¬ {test_name}:")
            print(f"   Duration: {duration:.3f} seconds")
            
            if "memory_stats" in performance:
                mem_stats = performance["memory_stats"]
                print(f"   Memory: {mem_stats.get('avg_mb', 0):.1f}MB avg "
                      f"({mem_stats.get('min_mb', 0):.1f}-{mem_stats.get('max_mb', 0):.1f}MB)")
            
            if "compression_ratio" in result:
                ratio = result["compression_ratio"]
                print(f"   Compression: {ratio:.2%} of original size")
        
        print("\n" + "="*60)


def main():
    """ë©”ì¸ ?¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Assembly Collection Performance Test',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python performance_test.py --all                    # ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ?¤í–‰
  python performance_test.py --memory-manager         # ë©”ëª¨ë¦?ë§¤ë‹ˆ?€ë§??ŒìŠ¤??
  python performance_test.py --data-optimizer         # ?°ì´??ìµœì ?”ë§Œ ?ŒìŠ¤??
  python performance_test.py --config                # ?¤ì • ê´€ë¦¬ë§Œ ?ŒìŠ¤??
        """
    )
    
    parser.add_argument('--all', action='store_true',
                        help='ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ?¤í–‰')
    parser.add_argument('--memory-manager', action='store_true',
                        help='ë©”ëª¨ë¦?ë§¤ë‹ˆ?€ ë²¤ì¹˜ë§ˆí¬')
    parser.add_argument('--data-optimizer', action='store_true',
                        help='?°ì´??ìµœì ??ë²¤ì¹˜ë§ˆí¬')
    parser.add_argument('--config', action='store_true',
                        help='?¤ì • ê´€ë¦?ë²¤ì¹˜ë§ˆí¬')
    parser.add_argument('--iterations', type=int, default=100,
                        help='ë°˜ë³µ ?Ÿìˆ˜ (ê¸°ë³¸: 100)')
    parser.add_argument('--data-size', type=int, default=500,
                        help='?ŒìŠ¤???°ì´???¬ê¸° (ê¸°ë³¸: 500)')
    parser.add_argument('--output', type=str, default='benchmark_results.json',
                        help='ê²°ê³¼ ?€???Œì¼ (ê¸°ë³¸: benchmark_results.json)')
    
    args = parser.parse_args()
    
    # ë²¤ì¹˜ë§ˆí¬ ?¤í–‰
    benchmark = CollectionBenchmark()
    
    if args.all or not any([args.memory_manager, args.data_optimizer, args.config]):
        # ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ?¤í–‰
        benchmark.run_all_benchmarks()
    else:
        # ? íƒ??ë²¤ì¹˜ë§ˆí¬ ?¤í–‰
        if args.memory_manager:
            benchmark.benchmark_memory_manager(args.iterations)
        if args.data_optimizer:
            benchmark.benchmark_data_optimizer(args.data_size)
        if args.config:
            benchmark.benchmark_collection_config()
    
    # ê²°ê³¼ ?€??ë°?ì¶œë ¥
    output_file = Path(args.output)
    benchmark.save_results(output_file)
    benchmark.print_summary()


if __name__ == "__main__":
    main()
