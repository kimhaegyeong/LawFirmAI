#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í’ˆì§ˆ ê²€ì¦ ë° ìµœì í™” ì‹œìŠ¤í…œ
ë°ì´í„°ë² ì´ìŠ¤ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ìµœì í™”í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import sqlite3
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from contextlib import contextmanager

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/quality_validation.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    total_laws: int = 0
    total_articles: int = 0
    total_cases: int = 0
    quality_distribution: Dict[str, int] = None
    low_quality_items: List[Dict[str, Any]] = None
    recommendations: List[str] = None
    errors: List[str] = None
    validation_time: float = 0.0
    
    def __post_init__(self):
        if self.quality_distribution is None:
            self.quality_distribution = {}
        if self.low_quality_items is None:
            self.low_quality_items = []
        if self.recommendations is None:
            self.recommendations = []
        if self.errors is None:
            self.errors = []


class QualityValidationSystem:
    """í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_path: str = "data/lawfirm.db"):
        self.db_path = db_path
        self.quality_thresholds = {
            'excellent': 0.9,
            'good': 0.7,
            'fair': 0.5,
            'poor': 0.0
        }
    
    @contextmanager
    def get_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        except Exception as e:
            logger.error(f"Database error: {e}")
            raise
        finally:
            conn.close()
    
    def validate_imported_data(self) -> ValidationResult:
        """
        ì„í¬íŠ¸ëœ ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        
        Returns:
            ValidationResult: ê²€ì¦ ê²°ê³¼
        """
        logger.info("ğŸ” Starting data quality validation...")
        start_time = datetime.now()
        
        result = ValidationResult()
        
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                # ê¸°ë³¸ í†µê³„ ìˆ˜ì§‘
                result = self._collect_basic_statistics(cursor, result)
                
                # í’ˆì§ˆ ë¶„í¬ ë¶„ì„
                result = self._analyze_quality_distribution(cursor, result)
                
                # ì €í’ˆì§ˆ í•­ëª© ì‹ë³„
                result = self._identify_low_quality_items(cursor, result)
                
                # ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±
                result = self._generate_recommendations(result)
                
        except Exception as e:
            error_msg = f"Error during validation: {e}"
            logger.error(error_msg)
            result.errors.append(error_msg)
        
        # ê²€ì¦ ì‹œê°„ ê³„ì‚°
        end_time = datetime.now()
        result.validation_time = (end_time - start_time).total_seconds()
        
        logger.info(f"âœ… Validation completed:")
        logger.info(f"  - Total laws: {result.total_laws}")
        logger.info(f"  - Total articles: {result.total_articles}")
        logger.info(f"  - Total cases: {result.total_cases}")
        logger.info(f"  - Quality distribution: {result.quality_distribution}")
        logger.info(f"  - Low quality items: {len(result.low_quality_items)}")
        logger.info(f"  - Validation time: {result.validation_time:.2f} seconds")
        
        return result
    
    def _collect_basic_statistics(self, cursor: sqlite3.Cursor, result: ValidationResult) -> ValidationResult:
        """ê¸°ë³¸ í†µê³„ ìˆ˜ì§‘"""
        try:
            # ë²•ë¥  í†µê³„
            cursor.execute("SELECT COUNT(*) FROM assembly_laws")
            result.total_laws = cursor.fetchone()[0]
            
            # ì¡°ë¬¸ í†µê³„
            cursor.execute("SELECT COUNT(*) FROM assembly_articles")
            result.total_articles = cursor.fetchone()[0]
            
            # íŒë¡€ í†µê³„
            cursor.execute("SELECT COUNT(*) FROM precedent_cases")
            result.total_cases = cursor.fetchone()[0]
            
            logger.info(f"Basic statistics collected: {result.total_laws} laws, {result.total_articles} articles, {result.total_cases} cases")
            
        except Exception as e:
            error_msg = f"Error collecting basic statistics: {e}"
            logger.error(error_msg)
            result.errors.append(error_msg)
        
        return result
    
    def _analyze_quality_distribution(self, cursor: sqlite3.Cursor, result: ValidationResult) -> ValidationResult:
        """í’ˆì§ˆ ë¶„í¬ ë¶„ì„"""
        try:
            # ë²•ë¥  í’ˆì§ˆ ë¶„í¬
            cursor.execute("""
                SELECT 
                    CASE 
                        WHEN parsing_quality_score >= 0.9 THEN 'excellent'
                        WHEN parsing_quality_score >= 0.7 THEN 'good'
                        WHEN parsing_quality_score >= 0.5 THEN 'fair'
                        ELSE 'poor'
                    END as quality_level,
                    COUNT(*) as count
                FROM assembly_laws
                GROUP BY quality_level
            """)
            
            quality_dist = {}
            for row in cursor.fetchall():
                quality_dist[row['quality_level']] = row['count']
            
            result.quality_distribution = quality_dist
            
            # í‰ê·  í’ˆì§ˆ ì ìˆ˜
            cursor.execute("SELECT AVG(parsing_quality_score) FROM assembly_laws")
            avg_quality = cursor.fetchone()[0]
            logger.info(f"Average law quality score: {avg_quality:.3f}")
            
            # ì¡°ë¬¸ í’ˆì§ˆ ë¶„í¬
            cursor.execute("""
                SELECT 
                    CASE 
                        WHEN parsing_quality_score >= 0.9 THEN 'excellent'
                        WHEN parsing_quality_score >= 0.7 THEN 'good'
                        WHEN parsing_quality_score >= 0.5 THEN 'fair'
                        ELSE 'poor'
                    END as quality_level,
                    COUNT(*) as count
                FROM assembly_articles
                GROUP BY quality_level
            """)
            
            article_quality_dist = {}
            for row in cursor.fetchall():
                article_quality_dist[row['quality_level']] = row['count']
            
            logger.info(f"Article quality distribution: {article_quality_dist}")
            
        except Exception as e:
            error_msg = f"Error analyzing quality distribution: {e}"
            logger.error(error_msg)
            result.errors.append(error_msg)
        
        return result
    
    def _identify_low_quality_items(self, cursor: sqlite3.Cursor, result: ValidationResult) -> ValidationResult:
        """ì €í’ˆì§ˆ í•­ëª© ì‹ë³„"""
        try:
            # ì €í’ˆì§ˆ ë²•ë¥  ì‹ë³„
            cursor.execute("""
                SELECT 
                    law_name,
                    parsing_quality_score,
                    parsing_method,
                    article_count,
                    created_at
                FROM assembly_laws
                WHERE parsing_quality_score < 0.5
                ORDER BY parsing_quality_score ASC
                LIMIT 20
            """)
            
            low_quality_laws = []
            for row in cursor.fetchall():
                low_quality_laws.append({
                    'type': 'law',
                    'name': row['law_name'],
                    'quality_score': row['parsing_quality_score'],
                    'parsing_method': row['parsing_method'],
                    'article_count': row['article_count'],
                    'created_at': row['created_at']
                })
            
            # ì €í’ˆì§ˆ ì¡°ë¬¸ ì‹ë³„
            cursor.execute("""
                SELECT 
                    al.law_name,
                    aa.article_number,
                    aa.parsing_quality_score,
                    aa.parsing_method,
                    aa.word_count,
                    aa.char_count
                FROM assembly_articles aa
                JOIN assembly_laws al ON aa.law_id = al.law_id
                WHERE aa.parsing_quality_score < 0.5
                ORDER BY aa.parsing_quality_score ASC
                LIMIT 20
            """)
            
            low_quality_articles = []
            for row in cursor.fetchall():
                low_quality_articles.append({
                    'type': 'article',
                    'law_name': row['law_name'],
                    'article_number': row['article_number'],
                    'quality_score': row['parsing_quality_score'],
                    'parsing_method': row['parsing_method'],
                    'word_count': row['word_count'],
                    'char_count': row['char_count']
                })
            
            result.low_quality_items = low_quality_laws + low_quality_articles
            
            logger.info(f"Identified {len(low_quality_laws)} low-quality laws and {len(low_quality_articles)} low-quality articles")
            
        except Exception as e:
            error_msg = f"Error identifying low quality items: {e}"
            logger.error(error_msg)
            result.errors.append(error_msg)
        
        return result
    
    def _generate_recommendations(self, result: ValidationResult) -> ValidationResult:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # í’ˆì§ˆ ë¶„í¬ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        total_items = sum(result.quality_distribution.values())
        if total_items > 0:
            poor_ratio = result.quality_distribution.get('poor', 0) / total_items
            fair_ratio = result.quality_distribution.get('fair', 0) / total_items
            
            if poor_ratio > 0.1:  # 10% ì´ìƒì´ poor
                recommendations.append("10% ì´ìƒì˜ ë²•ë¥ ì´ ì €í’ˆì§ˆì…ë‹ˆë‹¤. íŒŒì‹± ì•Œê³ ë¦¬ì¦˜ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            if fair_ratio > 0.3:  # 30% ì´ìƒì´ fair
                recommendations.append("30% ì´ìƒì˜ ë²•ë¥ ì´ ë³´í†µ í’ˆì§ˆì…ë‹ˆë‹¤. í’ˆì§ˆ í–¥ìƒ ì‘ì—…ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            
            excellent_ratio = result.quality_distribution.get('excellent', 0) / total_items
            if excellent_ratio < 0.2:  # 20% ë¯¸ë§Œì´ excellent
                recommendations.append("ìš°ìˆ˜ í’ˆì§ˆ ë²•ë¥ ì´ 20% ë¯¸ë§Œì…ë‹ˆë‹¤. ì „ì²´ì ì¸ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì €í’ˆì§ˆ í•­ëª© ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if len(result.low_quality_items) > 0:
            recommendations.append(f"{len(result.low_quality_items)}ê°œì˜ ì €í’ˆì§ˆ í•­ëª©ì´ ì‹ë³„ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # íŒŒì‹± ë°©ë²•ë³„ ë¶„ì„
            parsing_methods = {}
            for item in result.low_quality_items:
                method = item.get('parsing_method', 'unknown')
                parsing_methods[method] = parsing_methods.get(method, 0) + 1
            
            for method, count in parsing_methods.items():
                if count > 5:
                    recommendations.append(f"{method} íŒŒì‹± ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬ëœ í•­ëª© ì¤‘ {count}ê°œê°€ ì €í’ˆì§ˆì…ë‹ˆë‹¤. í•´ë‹¹ ë°©ë²•ì˜ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ë°ì´í„° ì™„ì„±ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if result.total_laws == 0:
            recommendations.append("ë²•ë¥  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ì„í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        if result.total_articles == 0:
            recommendations.append("ì¡°ë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¡°ë¬¸ íŒŒì‹±ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        if result.total_cases == 0:
            recommendations.append("íŒë¡€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒë¡€ ì„í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        result.recommendations = recommendations
        logger.info(f"Generated {len(recommendations)} recommendations")
        
        return result
    
    def generate_quality_report(self, validation_result: ValidationResult) -> Dict[str, Any]:
        """í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            'validation_summary': {
                'total_laws': validation_result.total_laws,
                'total_articles': validation_result.total_articles,
                'total_cases': validation_result.total_cases,
                'validation_time': validation_result.validation_time,
                'total_errors': len(validation_result.errors)
            },
            'quality_distribution': validation_result.quality_distribution,
            'low_quality_items': validation_result.low_quality_items[:10],  # ìƒìœ„ 10ê°œë§Œ
            'recommendations': validation_result.recommendations,
            'errors': validation_result.errors,
            'generated_at': datetime.now().isoformat()
        }
        
        return report
    
    def optimize_database_performance(self) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”"""
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                logger.info("âš¡ Optimizing database performance...")
                
                # ì¸ë±ìŠ¤ ìƒì„± ë° ìµœì í™”
                indexes_to_create = [
                    "CREATE INDEX IF NOT EXISTS idx_assembly_laws_quality ON assembly_laws(parsing_quality_score)",
                    "CREATE INDEX IF NOT EXISTS idx_assembly_laws_method ON assembly_laws(parsing_method)",
                    "CREATE INDEX IF NOT EXISTS idx_assembly_laws_type ON assembly_laws(law_type)",
                    "CREATE INDEX IF NOT EXISTS idx_assembly_articles_quality ON assembly_articles(parsing_quality_score)",
                    "CREATE INDEX IF NOT EXISTS idx_assembly_articles_method ON assembly_articles(parsing_method)",
                    "CREATE INDEX IF NOT EXISTS idx_precedent_cases_field ON precedent_cases(field)",
                    "CREATE INDEX IF NOT EXISTS idx_precedent_cases_court ON precedent_cases(court)"
                ]
                
                for index_sql in indexes_to_create:
                    try:
                        cursor.execute(index_sql)
                        logger.debug(f"Created index: {index_sql}")
                    except sqlite3.OperationalError as e:
                        logger.warning(f"Index creation failed: {e}")
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                cursor.execute("ANALYZE")
                logger.info("Database statistics updated")
                
                # VACUUM ì‹¤í–‰ (ì„ íƒì )
                cursor.execute("VACUUM")
                logger.info("Database vacuumed")
                
                conn.commit()
                logger.info("âœ… Database performance optimization completed")
                return True
                
        except Exception as e:
            logger.error(f"Error optimizing database performance: {e}")
            return False
    
    def validate_data_integrity(self) -> Dict[str, Any]:
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""
        integrity_results = {
            'foreign_key_violations': [],
            'orphaned_records': [],
            'duplicate_records': [],
            'missing_required_fields': [],
            'data_type_violations': []
        }
        
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                
                logger.info("ğŸ” Validating data integrity...")
                
                # ì™¸ë˜í‚¤ ìœ„ë°˜ ê²€ì‚¬
                cursor.execute("""
                    SELECT aa.law_id, COUNT(*) as count
                    FROM assembly_articles aa
                    LEFT JOIN assembly_laws al ON aa.law_id = al.law_id
                    WHERE al.law_id IS NULL
                    GROUP BY aa.law_id
                """)
                
                orphaned_articles = cursor.fetchall()
                if orphaned_articles:
                    integrity_results['orphaned_records'].append({
                        'type': 'orphaned_articles',
                        'count': len(orphaned_articles),
                        'details': [dict(row) for row in orphaned_articles]
                    })
                
                # ì¤‘ë³µ ë ˆì½”ë“œ ê²€ì‚¬
                cursor.execute("""
                    SELECT law_id, COUNT(*) as count
                    FROM assembly_laws
                    GROUP BY law_id
                    HAVING COUNT(*) > 1
                """)
                
                duplicate_laws = cursor.fetchall()
                if duplicate_laws:
                    integrity_results['duplicate_records'].append({
                        'type': 'duplicate_laws',
                        'count': len(duplicate_laws),
                        'details': [dict(row) for row in duplicate_laws]
                    })
                
                # í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ê²€ì‚¬
                cursor.execute("""
                    SELECT COUNT(*) as count
                    FROM assembly_laws
                    WHERE law_name IS NULL OR law_name = ''
                """)
                
                missing_law_names = cursor.fetchone()[0]
                if missing_law_names > 0:
                    integrity_results['missing_required_fields'].append({
                        'type': 'missing_law_names',
                        'count': missing_law_names
                    })
                
                logger.info("âœ… Data integrity validation completed")
                
        except Exception as e:
            logger.error(f"Error validating data integrity: {e}")
            integrity_results['errors'] = [str(e)]
        
        return integrity_results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting quality validation and optimization...")
    
    # ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    validator = QualityValidationSystem()
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    logger.info("\nğŸ“‹ Phase 1: Validating data quality...")
    validation_result = validator.validate_imported_data()
    
    # ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
    logger.info("\nğŸ“‹ Phase 2: Validating data integrity...")
    integrity_result = validator.validate_data_integrity()
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”
    logger.info("\nğŸ“‹ Phase 3: Optimizing database performance...")
    optimization_success = validator.optimize_database_performance()
    
    # í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
    logger.info("\nğŸ“‹ Phase 4: Generating quality report...")
    quality_report = validator.generate_quality_report(validation_result)
    
    # í†µí•© ê²°ê³¼ ìƒì„±
    final_result = {
        'quality_validation': quality_report,
        'data_integrity': integrity_result,
        'performance_optimization': {
            'optimization_success': optimization_success,
            'optimization_time': datetime.now().isoformat()
        },
        'summary': {
            'total_laws': validation_result.total_laws,
            'total_articles': validation_result.total_articles,
            'total_cases': validation_result.total_cases,
            'quality_distribution': validation_result.quality_distribution,
            'low_quality_count': len(validation_result.low_quality_items),
            'recommendations_count': len(validation_result.recommendations),
            'total_errors': len(validation_result.errors)
        }
    }
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    with open("data/quality_validation_report.json", "w", encoding="utf-8") as f:
        json.dump(final_result, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nğŸ“Š Detailed report saved to: data/quality_validation_report.json")
    logger.info("âœ… Quality validation and optimization completed successfully!")
    
    # ì£¼ìš” ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š Quality Validation Summary:")
    print(f"  - Total Laws: {validation_result.total_laws:,}")
    print(f"  - Total Articles: {validation_result.total_articles:,}")
    print(f"  - Total Cases: {validation_result.total_cases:,}")
    print(f"  - Quality Distribution: {validation_result.quality_distribution}")
    print(f"  - Low Quality Items: {len(validation_result.low_quality_items)}")
    print(f"  - Recommendations: {len(validation_result.recommendations)}")
    
    if validation_result.errors:
        print(f"\nâš ï¸ Validation completed with {len(validation_result.errors)} errors")
        print("Check logs for details.")
    else:
        print("\nğŸ‰ Quality validation completed successfully!")
    
    return final_result


if __name__ == "__main__":
    result = main()
    print("\nâœ… Quality validation and optimization process completed!")
