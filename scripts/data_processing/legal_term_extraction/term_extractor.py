
# -*- coding: utf-8 -*-
"""
ë²•ë¥  ?©ì–´ ì¶”ì¶œ ?œìŠ¤??
ê¸°ì¡´ ?°ì´?°ì—??ë²•ë¥  ?©ì–´ë¥??ë™?¼ë¡œ ì¶”ì¶œ?˜ëŠ” ?œìŠ¤??
"""

import json
import re
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Set
from collections import Counter, defaultdict
import glob

logger = logging.getLogger(__name__)


class LegalTermExtractor:
    """ë²•ë¥  ?©ì–´ ì¶”ì¶œê¸?""
    
    def __init__(self):
        """ì´ˆê¸°??""
        self.logger = logging.getLogger(__name__)
        
        # ë²•ë¥  ?©ì–´ ?¨í„´ ?•ì˜
        self.legal_term_patterns = {
            'legal_concepts': r'[ê°€-??{2,6}(?:ë²?ê¶?ì±…ì„|?í•´|ê³„ì•½|?Œì†¡|ì²˜ë²Œ|?œì¬)',
            'legal_actions': r'[ê°€-??{2,4}(?:ë°°ìƒ|ë³´ìƒ|ì²?µ¬|?œê¸°|?´ì?|?„ë°˜|ì¹¨í•´|ì²˜ë¦¬)',
            'legal_procedures': r'[ê°€-??{2,5}(?:?ˆì°¨|? ì²­|?¬ë¦¬|?ê²°|??†Œ|?ê³ |ì¡°ì •|?”í•´)',
            'legal_entities': r'[ê°€-??{2,4}(?:ë²•ì›|ê²€??ë³€?¸ì‚¬|?¼ê³ ???ê³ |?¼ê³ |ì¦ì¸|ê°ì •??',
            'legal_documents': r'[ê°€-??{2,5}(?:?Œì¥|?µë???ì¦ê±°|?ê²°??ì¡°ì„œ|ì§„ìˆ ??ì§„ìˆ ì¡°ì„œ)',
            'legal_penalties': r'[ê°€-??{2,4}(?:?•ë²Œ|ë²Œê¸ˆ|ì§•ì—­|ê¸ˆê³ |ì§‘í–‰? ì˜ˆ|? ê³ ? ì˜ˆ|ë³´í˜¸ê´€ì°?',
            'legal_rights': r'[ê°€-??{2,5}(?:?Œìœ ê¶??ìœ ê¶?ì§€?ê¶Œ|?„ì„¸ê¶??€?¹ê¶Œ|ì§ˆê¶Œ|? ì¹˜ê¶?',
            'legal_relationships': r'[ê°€-??{2,4}(?:?¼ì¸|?´í˜¼|?ì†|?‘ìœ¡|ì¹œê¶Œ|?‘ìœ¡ê¶?ë©´ì ‘êµì„­ê¶?',
            'legal_obligations': r'[ê°€-??{2,4}(?:ì±„ë¬´|ì±„ê¶Œ|?´í–‰|ë¶ˆì´???„ì•½|?í•´|ê³¼ì‹¤|ê³ ì˜)',
            'legal_crimes': r'[ê°€-??{2,5}(?:?´ì¸|ê°•ë„|?ˆë„|?¬ê¸°|?¡ë ¹|ë°°ì„|?Œë¬¼|ê°•ê°„|?±í­??'
        }
        
        # ë²•ë¥  ì¡°ë¬¸ ?¨í„´
        self.law_article_patterns = [
            r'??d+ì¡?,  # ??50ì¡?
            r'??d+??,  # ????
            r'??d+??,  # ????
            r'ë²•ë¥ \s*??d+??,  # ë²•ë¥  ??0883??
            r'?œí–‰??s*??d+ì¡?,  # ?œí–‰????ì¡?
            r'?œí–‰ê·œì¹™\s*??d+ì¡?  # ?œí–‰ê·œì¹™ ??ì¡?
        ]
        
        # ?ë? ?¨í„´
        self.precedent_patterns = [
            r'\d{4}[ê°€?˜ë‹¤?¼ë§ˆë°”ì‚¬?„ìì°¨ì¹´?€?Œí•˜]\d+',  # 2023??2345
            r'?€ë²•ì›\s*\d{4}\.\d+\.\d+',  # ?€ë²•ì› 2023.1.1
            r'ê³ ë“±ë²•ì›\s*\d{4}\.\d+\.\d+',  # ê³ ë“±ë²•ì› 2023.1.1
            r'ì§€ë°©ë²•??s*\d{4}\.\d+\.\d+'  # ì§€ë°©ë²•??2023.1.1
        ]
        
        # ì¶”ì¶œ???©ì–´ ?€??
        self.extracted_terms = defaultdict(list)
        self.term_frequencies = Counter()
        self.term_contexts = defaultdict(list)
        
    def extract_text_from_json(self, data: Dict) -> str:
        """JSON ?°ì´?°ì—???ìŠ¤??ì¶”ì¶œ"""
        text_parts = []
        
        if isinstance(data, dict):
            # ë²•ë ¹ ?°ì´??ì²˜ë¦¬
            if 'laws' in data:
                for law in data['laws']:
                    if 'articles' in law:
                        for article in law['articles']:
                            if 'article_content' in article:
                                text_parts.append(article['article_content'])
                            if 'article_title' in article:
                                text_parts.append(article['article_title'])
            
            # ?ë? ?°ì´??ì²˜ë¦¬
            elif 'cases' in data:
                for case in data['cases']:
                    if 'full_text' in case:
                        text_parts.append(case['full_text'])
                    if 'searchable_text' in case:
                        text_parts.append(case['searchable_text'])
                    if 'case_name' in case:
                        text_parts.append(case['case_name'])
            
            # ê¸°í? ?ìŠ¤???„ë“œ??
            text_fields = ['content', 'text', 'title', 'description', 'summary']
            for field in text_fields:
                if field in data and isinstance(data[field], str):
                    text_parts.append(data[field])
        
        return ' '.join(text_parts)
    
    def extract_terms_from_text(self, text: str, file_path: str = "") -> Dict[str, List[str]]:
        """?ìŠ¤?¸ì—??ë²•ë¥  ?©ì–´ ì¶”ì¶œ"""
        extracted = defaultdict(list)
        
        for pattern_name, pattern in self.legal_term_patterns.items():
            matches = re.findall(pattern, text)
            extracted[pattern_name].extend(matches)
            
            # ?©ì–´ ë¹ˆë„ ë°?ì»¨í…?¤íŠ¸ ?€??
            for match in matches:
                self.term_frequencies[match] += 1
                if file_path:
                    self.term_contexts[match].append(file_path)
        
        return extracted
    
    def extract_law_articles(self, text: str) -> List[str]:
        """ë²•ë¥  ì¡°ë¬¸ ì¶”ì¶œ"""
        articles = []
        for pattern in self.law_article_patterns:
            matches = re.findall(pattern, text)
            articles.extend(matches)
        return articles
    
    def extract_precedents(self, text: str) -> List[str]:
        """?ë? ì¶”ì¶œ"""
        precedents = []
        for pattern in self.precedent_patterns:
            matches = re.findall(pattern, text)
            precedents.extend(matches)
        return precedents
    
    def process_file(self, file_path: str) -> Dict:
        """?¨ì¼ ?Œì¼ ì²˜ë¦¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ?ìŠ¤??ì¶”ì¶œ
            text = self.extract_text_from_json(data)
            
            if not text.strip():
                return {}
            
            # ?©ì–´ ì¶”ì¶œ
            extracted_terms = self.extract_terms_from_text(text, file_path)
            
            # ë²•ë¥  ì¡°ë¬¸ ë°??ë? ì¶”ì¶œ
            law_articles = self.extract_law_articles(text)
            precedents = self.extract_precedents(text)
            
            result = {
                'file_path': file_path,
                'extracted_terms': extracted_terms,
                'law_articles': law_articles,
                'precedents': precedents,
                'text_length': len(text),
                'term_count': sum(len(terms) for terms in extracted_terms.values())
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error processing file {file_path}: {e}")
            return {}
    
    def process_directory(self, directory_path: str, min_frequency: int = 5) -> Dict:
        """?”ë ‰? ë¦¬ ?„ì²´ ì²˜ë¦¬"""
        self.logger.info(f"Processing directory: {directory_path}")
        
        # JSON ?Œì¼ ì°¾ê¸°
        json_files = glob.glob(f"{directory_path}/**/*.json", recursive=True)
        self.logger.info(f"Found {len(json_files)} JSON files")
        
        processed_files = 0
        total_terms = 0
        
        for file_path in json_files:
            result = self.process_file(file_path)
            if result:
                processed_files += 1
                total_terms += result['term_count']
                
                # ê²°ê³¼ ?µí•©
                for pattern_name, terms in result['extracted_terms'].items():
                    self.extracted_terms[pattern_name].extend(terms)
        
        # ë¹ˆë„ ê¸°ë°˜ ?„í„°ë§?
        filtered_terms = self._filter_terms_by_frequency(min_frequency)
        
        # ê²°ê³¼ ?•ë¦¬
        final_result = {
            'processed_files': processed_files,
            'total_files': len(json_files),
            'total_terms_extracted': total_terms,
            'filtered_terms': filtered_terms,
            'term_frequencies': dict(self.term_frequencies.most_common(100)),
            'extraction_summary': self._generate_summary()
        }
        
        self.logger.info(f"Processing completed: {processed_files}/{len(json_files)} files processed")
        self.logger.info(f"Total terms extracted: {total_terms}")
        self.logger.info(f"Terms after filtering: {sum(len(terms) for terms in filtered_terms.values())}")
        
        return final_result
    
    def _filter_terms_by_frequency(self, min_frequency: int) -> Dict[str, List[str]]:
        """ë¹ˆë„ ê¸°ë°˜ ?©ì–´ ?„í„°ë§?""
        filtered_terms = defaultdict(list)
        
        for pattern_name, terms in self.extracted_terms.items():
            # ë¹ˆë„ê°€ ?’ì? ?©ì–´ë§?? íƒ
            frequent_terms = [term for term in terms 
                            if self.term_frequencies[term] >= min_frequency]
            
            # ì¤‘ë³µ ?œê±° ë°??•ë ¬
            unique_terms = sorted(list(set(frequent_terms)))
            filtered_terms[pattern_name] = unique_terms
        
        return filtered_terms
    
    def _generate_summary(self) -> Dict:
        """ì¶”ì¶œ ê²°ê³¼ ?”ì•½ ?ì„±"""
        summary = {}
        
        for pattern_name, terms in self.extracted_terms.items():
            unique_terms = list(set(terms))
            summary[pattern_name] = {
                'total_count': len(terms),
                'unique_count': len(unique_terms),
                'top_terms': sorted(unique_terms, 
                                   key=lambda x: self.term_frequencies[x], 
                                   reverse=True)[:10]
            }
        
        return summary
    
    def save_results(self, output_path: str, results: Dict):
        """ê²°ê³¼ ?€??""
        try:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"Results saved to: {output_file}")
            
        except Exception as e:
            self.logger.error(f"Error saving results: {e}")


def main():
    """ë©”ì¸ ?¤í–‰ ?¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë²•ë¥  ?©ì–´ ì¶”ì¶œê¸?)
    parser.add_argument('--input_dir', type=str, required=True,
                       help='?…ë ¥ ?”ë ‰? ë¦¬ ê²½ë¡œ')
    parser.add_argument('--output_file', type=str, 
                       default='data/extracted_terms/raw_extracted_terms.json',
                       help='ì¶œë ¥ ?Œì¼ ê²½ë¡œ')
    parser.add_argument('--min_frequency', type=int, default=5,
                       help='ìµœì†Œ ë¹ˆë„ (ê¸°ë³¸ê°? 5)')
    parser.add_argument('--log_level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='ë¡œê·¸ ?ˆë²¨')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ?¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ?©ì–´ ì¶”ì¶œ ?¤í–‰
    extractor = LegalTermExtractor()
    results = extractor.process_directory(args.input_dir, args.min_frequency)
    
    # ê²°ê³¼ ?€??
    extractor.save_results(args.output_file, results)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== ë²•ë¥  ?©ì–´ ì¶”ì¶œ ê²°ê³¼ ===")
    print(f"ì²˜ë¦¬???Œì¼: {results['processed_files']}/{results['total_files']}")
    print(f"ì¶”ì¶œ??ì´??©ì–´ ?? {results['total_terms_extracted']}")
    print(f"?„í„°ë§????©ì–´ ?? {sum(len(terms) for terms in results['filtered_terms'].values())}")
    
    print("\n=== ?¨í„´ë³??©ì–´ ??===")
    for pattern_name, terms in results['filtered_terms'].items():
        print(f"{pattern_name}: {len(terms)}ê°?)
    
    print("\n=== ?ìœ„ ë¹ˆë„ ?©ì–´ (?ìœ„ 10ê°? ===")
    for term, freq in list(results['term_frequencies'].items())[:10]:
        print(f"{term}: {freq}??)


if __name__ == "__main__":
    main()
