# -*- coding: utf-8 -*-
"""
ë¡œê·¸ ë¶„ì„ ë° ê°œì„  ì‚¬í•­ ì‹ë³„ ìŠ¤í¬ë¦½íŠ¸
ê° ë‹¨ê³„ë³„ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ê°œì„  ì‚¬í•­ì„ ì‹ë³„
"""

import json
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import defaultdict
from datetime import datetime

from scripts.utils.log_analyzer import (
    analyze_sources_conversion_logs,
    analyze_legal_references_logs,
    analyze_answer_length_logs,
    analyze_context_usage_logs,
    identify_improvements
)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
    from scripts.utils.path_utils import setup_project_path, get_project_root
    from scripts.utils.file_utils import load_json_file, save_json_file
    
    project_root = setup_project_path()
    
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
    log_file = project_root / "logs" / "lawfirm_ai.log"
    
    if not log_file.exists():
        print(f"âš ï¸  ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_file}")
        print("ì‹¤ì œ ë¡œê·¸ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ë¡œê·¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    # ë¡œê·¸ íŒŒì¼ ì½ê¸°
    try:
        with open(log_file, "r", encoding="utf-8") as f:
            log_content = f.read()
    except Exception as e:
        print(f"âš ï¸  ë¡œê·¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return
    
    # ê° ì˜ì—­ë³„ ë¶„ì„
    analysis_results = {
        "sources": analyze_sources_conversion_logs(log_content),
        "legal_references": analyze_legal_references_logs(log_content),
        "answer_length": analyze_answer_length_logs(log_content),
        "context_usage": analyze_context_usage_logs(log_content),
    }
    
    # ê°œì„  ì‚¬í•­ ì‹ë³„
    improvements = identify_improvements(analysis_results)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*80)
    print("ë¡œê·¸ ë¶„ì„ ê²°ê³¼")
    print("="*80)
    
    print("\nğŸ“Š Sources ë³€í™˜ ë¶„ì„:")
    sources = analysis_results["sources"]
    if sources["total_docs"] > 0:
        avg_rate = (sources["total_conversions"] / sources["total_docs"]) * 100
        print(f"  - í‰ê·  ë³€í™˜ë¥ : {avg_rate:.1f}%")
        print(f"  - ì´ ë³€í™˜: {sources['total_conversions']}/{sources['total_docs']}")
        print(f"  - ì‹¤íŒ¨: {sources['total_failed']}")
        print(f"  - Fallback ì‚¬ìš©: {len(sources['fallback_usage'])}ê±´")
        print(f"  - Critical Fallback: {len(sources['critical_fallbacks'])}ê±´")
    
    print("\nâš–ï¸  Legal References ë¶„ì„:")
    legal = analysis_results["legal_references"]
    print(f"  - ì´ ì¶”ì¶œ: {legal['total_extracted']}ê°œ")
    print(f"  - Sourcesì—ì„œ: {legal['extracted_from_sources']}ê°œ")
    print(f"  - Contentì—ì„œ: {legal['extracted_from_content']}ê°œ")
    print(f"  - Docsì—ì„œ: {legal['extracted_from_docs']}ê°œ")
    
    print("\nğŸ“ ë‹µë³€ ê¸¸ì´ ë¶„ì„:")
    length = analysis_results["answer_length"]
    print(f"  - ë„ˆë¬´ ì§§ìŒ: {length['too_short_count']}ê±´")
    print(f"  - ë„ˆë¬´ ê¹€: {length['too_long_count']}ê±´")
    print(f"  - ì¡°ì •ë¨: {length['adjusted_count']}ê±´")
    
    print("\nğŸ“š Context Usage ë¶„ì„:")
    context = analysis_results["context_usage"]
    print(f"  - í‰ê·  Coverage: {context['average_coverage']:.2f}")
    print(f"  - í‰ê·  Relevance: {context['average_relevance']:.2f}")
    
    print("\n" + "="*80)
    print("ê°œì„  ì‚¬í•­")
    print("="*80)
    
    if improvements:
        for i, improvement in enumerate(improvements, 1):
            print(f"\n{i}. [{improvement['priority']}] {improvement['category']}")
            print(f"   í˜„ì¬: {improvement['current']}")
            print(f"   ëª©í‘œ: {improvement['target']}")
            print(f"   ì„¤ëª…: {improvement['description']}")
            print(f"   ê¶Œì¥ì‚¬í•­: {improvement['recommendation']}")
    else:
        print("\nâœ… ì¶”ê°€ ê°œì„  ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    # ê²°ê³¼ ì €ì¥
    from scripts.utils.file_utils import save_json_file
    
    output = {
        "analysis_results": analysis_results,
        "improvements": improvements,
        "timestamp": datetime.now().isoformat()
    }
    
    output_file = project_root / "data" / "ml_metrics" / "log_analysis_results.json"
    save_json_file(output, output_file)
    
    print(f"\nâœ… ë¶„ì„ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()

