#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëª¨ë¸ ?¬ê¸° ë°?ë©”ëª¨ë¦??¬ìš©??ë¶„ì„ ë°?ìµœì ???¤í¬ë¦½íŠ¸
LawFirmAI ?„ë¡œ?íŠ¸ - TASK 1.2.3
"""

import os
import sys
import time
import psutil
import torch
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple
import json
from datetime import datetime
import logging
from pathlib import Path

# ëª¨ë¸ ìµœì ???¼ì´ë¸ŒëŸ¬ë¦?
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    AutoModelForSeq2SeqLM,
    BitsAndBytesConfig
)
import onnx
from onnxruntime import InferenceSession
import onnxruntime as ort

# ë¡œê¹… ?¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelOptimizationAnalyzer:
    """ëª¨ë¸ ìµœì ??ë¶„ì„ ?´ë˜??""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.results = {}
        self.test_data = self._load_test_data()
        
    def _load_test_data(self) -> List[str]:
        """?ŒìŠ¤???°ì´??ë¡œë“œ"""
        return [
            "ê³„ì•½?œì—??ì£¼ì˜?´ì•¼ ??ì¡°í•­?€ ë¬´ì—‡?¸ê???",
            "?í•´ë°°ìƒ ì²?µ¬ê¶Œì˜ ?Œë©¸?œíš¨??ëª??„ì¸ê°€??",
            "ê·¼ë¡œê¸°ì?ë²•ìƒ ?´ê²Œ?œê°„?€ ?´ë–»ê²?ê·œì •?˜ì–´ ?ˆë‚˜??",
            "ë¶€?™ì‚° ë§¤ë§¤ê³„ì•½?ì„œ ì¤‘ë„ê¸ˆì? ?¸ì œ ì§€ê¸‰í•´???˜ë‚˜??",
            "?´í˜¼ ???¬ì‚°ë¶„í• ?€ ?´ë–»ê²??´ë£¨?´ì??˜ìš”?"
        ]
    
    def analyze_kobart_optimization(self) -> Dict[str, Any]:
        """KoBART ëª¨ë¸ ìµœì ??ë¶„ì„"""
        logger.info("KoBART ëª¨ë¸ ìµœì ??ë¶„ì„ ?œì‘...")
        
        model_name = "skt/kobart-base-v1"
        results = {
            "model_name": model_name,
            "optimization_analysis": {},
            "benchmark_time": datetime.now().isoformat()
        }
        
        try:
            # ?ë³¸ ëª¨ë¸ ë¶„ì„
            original_analysis = self._analyze_original_model(model_name, "seq2seq")
            
            # ?‘ì??ë¶„ì„
            quantization_analysis = self._analyze_quantization(model_name, "seq2seq")
            
            # ONNX ë³€??ë¶„ì„
            onnx_analysis = self._analyze_onnx_conversion(model_name, "seq2seq")
            
            # ?„ë£¨??ë¶„ì„
            pruning_analysis = self._analyze_pruning(model_name, "seq2seq")
            
            results["optimization_analysis"] = {
                "original": original_analysis,
                "quantization": quantization_analysis,
                "onnx": onnx_analysis,
                "pruning": pruning_analysis,
                "recommendations": self._generate_optimization_recommendations(
                    original_analysis, quantization_analysis, onnx_analysis, pruning_analysis
                )
            }
            
        except Exception as e:
            logger.error(f"KoBART ìµœì ??ë¶„ì„ ?¤íŒ¨: {e}")
            results["error"] = str(e)
            
        return results
    
    def analyze_kogpt2_optimization(self) -> Dict[str, Any]:
        """KoGPT-2 ëª¨ë¸ ìµœì ??ë¶„ì„"""
        logger.info("KoGPT-2 ëª¨ë¸ ìµœì ??ë¶„ì„ ?œì‘...")
        
        model_name = "skt/kogpt2-base-v2"
        results = {
            "model_name": model_name,
            "optimization_analysis": {},
            "benchmark_time": datetime.now().isoformat()
        }
        
        try:
            # ?ë³¸ ëª¨ë¸ ë¶„ì„
            original_analysis = self._analyze_original_model(model_name, "causal_lm")
            
            # ?‘ì??ë¶„ì„
            quantization_analysis = self._analyze_quantization(model_name, "causal_lm")
            
            # ONNX ë³€??ë¶„ì„
            onnx_analysis = self._analyze_onnx_conversion(model_name, "causal_lm")
            
            # ?„ë£¨??ë¶„ì„
            pruning_analysis = self._analyze_pruning(model_name, "causal_lm")
            
            results["optimization_analysis"] = {
                "original": original_analysis,
                "quantization": quantization_analysis,
                "onnx": onnx_analysis,
                "pruning": pruning_analysis,
                "recommendations": self._generate_optimization_recommendations(
                    original_analysis, quantization_analysis, onnx_analysis, pruning_analysis
                )
            }
            
        except Exception as e:
            logger.error(f"KoGPT-2 ìµœì ??ë¶„ì„ ?¤íŒ¨: {e}")
            results["error"] = str(e)
            
        return results
    
    def _analyze_original_model(self, model_name: str, model_type: str) -> Dict[str, Any]:
        """?ë³¸ ëª¨ë¸ ë¶„ì„"""
        logger.info(f"?ë³¸ {model_name} ëª¨ë¸ ë¶„ì„...")
        
        try:
            # ëª¨ë¸ ë¡œë”©
            start_time = time.time()
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            if model_type == "seq2seq":
                model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            else:
                model = AutoModelForCausalLM.from_pretrained(model_name)
            
            model.to(self.device)
            loading_time = time.time() - start_time
            
            # ëª¨ë¸ ?•ë³´ ?˜ì§‘
            num_parameters = sum(p.numel() for p in model.parameters())
            trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            # ëª¨ë¸ ?¬ê¸° ê³„ì‚°
            model_size = self._calculate_model_size(model)
            
            # ë©”ëª¨ë¦??¬ìš©??ì¸¡ì •
            memory_usage = self._get_memory_usage()
            
            # ì¶”ë¡  ?±ëŠ¥ ?ŒìŠ¤??
            inference_performance = self._test_inference_performance(model, tokenizer, model_type)
            
            # ëª¨ë¸ ?•ë¦¬
            del model, tokenizer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            return {
                "loading_time": loading_time,
                "num_parameters": num_parameters,
                "trainable_parameters": trainable_parameters,
                "model_size_mb": model_size,
                "memory_usage_mb": memory_usage,
                "inference_performance": inference_performance
            }
            
        except Exception as e:
            logger.error(f"?ë³¸ ëª¨ë¸ ë¶„ì„ ?¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _analyze_quantization(self, model_name: str, model_type: str) -> Dict[str, Any]:
        """?‘ì??ë¶„ì„"""
        logger.info(f"{model_name} ?‘ì??ë¶„ì„...")
        
        try:
            # INT8 ?‘ì??
            int8_analysis = self._test_int8_quantization(model_name, model_type)
            
            # INT4 ?‘ì??(BitsAndBytesConfig ?¬ìš©)
            int4_analysis = self._test_int4_quantization(model_name, model_type)
            
            return {
                "int8": int8_analysis,
                "int4": int4_analysis
            }
            
        except Exception as e:
            logger.error(f"?‘ì??ë¶„ì„ ?¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _test_int8_quantization(self, model_name: str, model_type: str) -> Dict[str, Any]:
        """INT8 ?‘ì???ŒìŠ¤??""
        try:
            # ëª¨ë¸ ë¡œë”©
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            if model_type == "seq2seq":
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
                )
            
            # INT8 ?‘ì??
            quantized_model = torch.quantization.quantize_dynamic(
                model, {torch.nn.Linear}, dtype=torch.qint8
            )
            
            # ?±ëŠ¥ ì¸¡ì •
            model_size = self._calculate_model_size(quantized_model)
            memory_usage = self._get_memory_usage()
            inference_performance = self._test_inference_performance(quantized_model, tokenizer, model_type)
            
            # ?•ë¦¬
            del model, quantized_model, tokenizer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            return {
                "model_size_mb": model_size,
                "memory_usage_mb": memory_usage,
                "inference_performance": inference_performance,
                "compression_ratio": 0.5  # INT8?€ ?€??50% ?•ì¶•
            }
            
        except Exception as e:
            logger.error(f"INT8 ?‘ì???ŒìŠ¤???¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _test_int4_quantization(self, model_name: str, model_type: str) -> Dict[str, Any]:
        """INT4 ?‘ì???ŒìŠ¤??(BitsAndBytesConfig)"""
        try:
            # BitsAndBytesConfig ?¤ì •
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            # ëª¨ë¸ ë¡œë”©
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            if model_type == "seq2seq":
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name,
                    quantization_config=quantization_config
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    quantization_config=quantization_config
                )
            
            # ?±ëŠ¥ ì¸¡ì •
            model_size = self._calculate_model_size(model)
            memory_usage = self._get_memory_usage()
            inference_performance = self._test_inference_performance(model, tokenizer, model_type)
            
            # ?•ë¦¬
            del model, tokenizer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            return {
                "model_size_mb": model_size,
                "memory_usage_mb": memory_usage,
                "inference_performance": inference_performance,
                "compression_ratio": 0.25  # INT4???€??75% ?•ì¶•
            }
            
        except Exception as e:
            logger.error(f"INT4 ?‘ì???ŒìŠ¤???¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _analyze_onnx_conversion(self, model_name: str, model_type: str) -> Dict[str, Any]:
        """ONNX ë³€??ë¶„ì„"""
        logger.info(f"{model_name} ONNX ë³€??ë¶„ì„...")
        
        try:
            # PyTorch ëª¨ë¸ ë¡œë”©
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            if model_type == "seq2seq":
                model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            else:
                model = AutoModelForCausalLM.from_pretrained(model_name)
            
            model.eval()
            
            # ONNX ë³€??
            onnx_path = f"{model_name.replace('/', '_')}.onnx"
            dummy_input = torch.randint(0, 1000, (1, 10))  # ?”ë? ?…ë ¥
            
            start_time = time.time()
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input_ids'],
                output_names=['output'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size', 1: 'sequence'},
                    'output': {0: 'batch_size', 1: 'sequence'}
                }
            )
            conversion_time = time.time() - start_time
            
            # ONNX ëª¨ë¸ ë¶„ì„
            onnx_model = onnx.load(onnx_path)
            onnx_size = os.path.getsize(onnx_path) / 1024 / 1024
            
            # ONNX Runtime ?±ëŠ¥ ?ŒìŠ¤??
            ort_session = InferenceSession(onnx_path)
            onnx_performance = self._test_onnx_performance(ort_session, tokenizer)
            
            # ?•ë¦¬
            del model, tokenizer
            os.remove(onnx_path)
            
            return {
                "conversion_time": conversion_time,
                "onnx_size_mb": onnx_size,
                "onnx_performance": onnx_performance,
                "compression_ratio": onnx_size / self._calculate_model_size(model) if hasattr(self, '_temp_model_size') else 1.0
            }
            
        except Exception as e:
            logger.error(f"ONNX ë³€??ë¶„ì„ ?¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _analyze_pruning(self, model_name: str, model_type: str) -> Dict[str, Any]:
        """?„ë£¨??ë¶„ì„"""
        logger.info(f"{model_name} ?„ë£¨??ë¶„ì„...")
        
        try:
            # ëª¨ë¸ ë¡œë”©
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            if model_type == "seq2seq":
                model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            else:
                model = AutoModelForCausalLM.from_pretrained(model_name)
            
            # êµ¬ì¡°???„ë£¨??(20% ?œê±°)
            pruned_model = self._apply_structural_pruning(model, sparsity=0.2)
            
            # ?±ëŠ¥ ì¸¡ì •
            model_size = self._calculate_model_size(pruned_model)
            memory_usage = self._get_memory_usage()
            inference_performance = self._test_inference_performance(pruned_model, tokenizer, model_type)
            
            # ?•ë¦¬
            del model, pruned_model, tokenizer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            return {
                "model_size_mb": model_size,
                "memory_usage_mb": memory_usage,
                "inference_performance": inference_performance,
                "sparsity": 0.2,
                "compression_ratio": 0.8  # 20% ?•ì¶•
            }
            
        except Exception as e:
            logger.error(f"?„ë£¨??ë¶„ì„ ?¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _apply_structural_pruning(self, model, sparsity: float = 0.2):
        """êµ¬ì¡°???„ë£¨???ìš©"""
        # ê°„ë‹¨??ê°€ì¤‘ì¹˜ ê¸°ë°˜ ?„ë£¨??
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                # ê°€ì¤‘ì¹˜???ˆëŒ“ê°’ì´ ?‘ì? ê²ƒë“¤??0?¼ë¡œ ?¤ì •
                with torch.no_grad():
                    weight = module.weight
                    threshold = torch.quantile(torch.abs(weight), sparsity)
                    mask = torch.abs(weight) > threshold
                    module.weight.data *= mask.float()
        
        return model
    
    def _test_onnx_performance(self, ort_session, tokenizer) -> Dict[str, Any]:
        """ONNX Runtime ?±ëŠ¥ ?ŒìŠ¤??""
        try:
            total_time = 0
            successful_inferences = 0
            
            for text in self.test_data[:3]:  # ì²˜ìŒ 3ê°œë§Œ ?ŒìŠ¤??
                try:
                    # ? í°??
                    inputs = tokenizer(text, return_tensors="np", padding=True, truncation=True)
                    input_ids = inputs["input_ids"].astype(np.int64)
                    
                    # ONNX ì¶”ë¡ 
                    start_time = time.time()
                    outputs = ort_session.run(None, {"input_ids": input_ids})
                    inference_time = time.time() - start_time
                    
                    total_time += inference_time
                    successful_inferences += 1
                    
                except Exception as e:
                    logger.warning(f"ONNX ì¶”ë¡  ?¤íŒ¨: {e}")
                    continue
            
            return {
                "total_time": total_time,
                "average_time": total_time / successful_inferences if successful_inferences > 0 else 0,
                "successful_inferences": successful_inferences
            }
            
        except Exception as e:
            logger.error(f"ONNX ?±ëŠ¥ ?ŒìŠ¤???¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _test_inference_performance(self, model, tokenizer, model_type: str) -> Dict[str, Any]:
        """ì¶”ë¡  ?±ëŠ¥ ?ŒìŠ¤??""
        try:
            total_time = 0
            successful_inferences = 0
            
            for text in self.test_data:
                try:
                    start_time = time.time()
                    
                    if model_type == "seq2seq":
                        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
                        inputs = {k: v.to(self.device) for k, v in inputs.items()}
                        
                        with torch.no_grad():
                            outputs = model.generate(
                                inputs["input_ids"],
                                max_length=100,
                                num_return_sequences=1,
                                temperature=0.7,
                                do_sample=True
                            )
                    else:
                        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
                        inputs = {k: v.to(self.device) for k, v in inputs.items()}
                        
                        with torch.no_grad():
                            outputs = model.generate(
                                inputs["input_ids"],
                                max_length=inputs["input_ids"].shape[1] + 50,
                                num_return_sequences=1,
                                temperature=0.7,
                                do_sample=True
                            )
                    
                    inference_time = time.time() - start_time
                    total_time += inference_time
                    successful_inferences += 1
                    
                except Exception as e:
                    logger.warning(f"ì¶”ë¡  ?¤íŒ¨: {e}")
                    continue
            
            return {
                "total_time": total_time,
                "average_time": total_time / successful_inferences if successful_inferences > 0 else 0,
                "successful_inferences": successful_inferences
            }
            
        except Exception as e:
            logger.error(f"ì¶”ë¡  ?±ëŠ¥ ?ŒìŠ¤???¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _calculate_model_size(self, model) -> float:
        """ëª¨ë¸ ?¬ê¸° ê³„ì‚° (MB)"""
        try:
            param_size = 0
            for param in model.parameters():
                param_size += param.nelement() * param.element_size()
            return param_size / 1024 / 1024
        except:
            return 0
    
    def _get_memory_usage(self) -> float:
        """?„ì¬ ë©”ëª¨ë¦??¬ìš©??ë°˜í™˜ (MB)"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    
    def _generate_optimization_recommendations(self, original, quantization, onnx, pruning) -> List[str]:
        """ìµœì ??ê¶Œì¥?¬í•­ ?ì„±"""
        recommendations = []
        
        try:
            # ë©”ëª¨ë¦??¬ìš©??ê¸°ë°˜ ê¶Œì¥?¬í•­
            if "memory_usage_mb" in original:
                original_memory = original["memory_usage_mb"]
                
                if original_memory > 8000:  # 8GB ?´ìƒ
                    recommendations.append("ë©”ëª¨ë¦??¬ìš©?‰ì´ ?’ìœ¼ë¯€ë¡??‘ì??INT8 ?ëŠ” INT4) ?ìš© ê¶Œì¥")
                
                if "int4" in quantization and "memory_usage_mb" in quantization["int4"]:
                    int4_memory = quantization["int4"]["memory_usage_mb"]
                    if int4_memory < original_memory * 0.5:
                        recommendations.append("INT4 ?‘ì?”ë¡œ ë©”ëª¨ë¦??¬ìš©?‰ì„ 50% ?´ìƒ ?ˆì•½ ê°€??)
            
            # ì¶”ë¡  ?ë„ ê¸°ë°˜ ê¶Œì¥?¬í•­
            if "inference_performance" in original and "average_time" in original["inference_performance"]:
                original_time = original["inference_performance"]["average_time"]
                
                if "onnx" in onnx and "onnx_performance" in onnx and "average_time" in onnx["onnx_performance"]:
                    onnx_time = onnx["onnx_performance"]["average_time"]
                    if onnx_time < original_time * 0.8:
                        recommendations.append("ONNX ë³€?˜ìœ¼ë¡?ì¶”ë¡  ?ë„ 20% ?´ìƒ ?¥ìƒ ê°€??)
            
            # HuggingFace Spaces ?˜ê²½ ê³ ë ¤
            recommendations.append("HuggingFace Spaces ?˜ê²½?ì„œ??INT4 ?‘ì?”ì? ONNX ë³€??ì¡°í•© ê¶Œì¥")
            recommendations.append("ë©”ëª¨ë¦??œí•œ(16GB)??ê³ ë ¤?˜ì—¬ ëª¨ë¸ ?¬ê¸° ìµœì ???„ìˆ˜")
            
        except Exception as e:
            recommendations.append(f"ê¶Œì¥?¬í•­ ?ì„± ì¤??¤ë¥˜: {e}")
        
        return recommendations
    
    def run_analysis(self) -> Dict[str, Any]:
        """?„ì²´ ìµœì ??ë¶„ì„ ?¤í–‰"""
        logger.info("ëª¨ë¸ ìµœì ??ë¶„ì„ ?œì‘...")
        
        # ?œìŠ¤???•ë³´ ?˜ì§‘
        system_info = {
            "device": self.device,
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
            "python_version": sys.version,
            "torch_version": torch.__version__
        }
        
        # ê°?ëª¨ë¸ ìµœì ??ë¶„ì„ ?¤í–‰
        kobart_analysis = self.analyze_kobart_optimization()
        kogpt2_analysis = self.analyze_kogpt2_optimization()
        
        # ê²°ê³¼ ì¢…í•©
        analysis_results = {
            "system_info": system_info,
            "kobart_optimization": kobart_analysis,
            "kogpt2_optimization": kogpt2_analysis,
            "comparison": self._compare_optimizations(kobart_analysis, kogpt2_analysis)
        }
        
        return analysis_results
    
    def _compare_optimizations(self, kobart_analysis, kogpt2_analysis) -> Dict[str, Any]:
        """ìµœì ??ê²°ê³¼ ë¹„êµ"""
        comparison = {
            "memory_optimization": {},
            "speed_optimization": {},
            "size_optimization": {},
            "recommendation": ""
        }
        
        try:
            # ë©”ëª¨ë¦?ìµœì ??ë¹„êµ
            if "optimization_analysis" in kobart_analysis and "original" in kobart_analysis["optimization_analysis"]:
                kobart_original = kobart_analysis["optimization_analysis"]["original"]
                kobart_memory = kobart_original.get("memory_usage_mb", 0)
                
                if "optimization_analysis" in kogpt2_analysis and "original" in kogpt2_analysis["optimization_analysis"]:
                    kogpt2_original = kogpt2_analysis["optimization_analysis"]["original"]
                    kogpt2_memory = kogpt2_original.get("memory_usage_mb", 0)
                    
                    comparison["memory_optimization"] = {
                        "kobart_mb": kobart_memory,
                        "kogpt2_mb": kogpt2_memory,
                        "memory_ratio": kobart_memory / kogpt2_memory if kogpt2_memory > 0 else 0
                    }
            
            # ìµœì ??ê¶Œì¥?¬í•­
            comparison["recommendation"] = "HuggingFace Spaces ?˜ê²½?ì„œ??ë©”ëª¨ë¦??¨ìœ¨?±ì´ ?°ìˆ˜??KoGPT-2 + INT4 ?‘ì??+ ONNX ë³€??ì¡°í•© ê¶Œì¥"
            
        except Exception as e:
            logger.error(f"ìµœì ??ë¹„êµ ì¤??¤ë¥˜: {e}")
            comparison["error"] = str(e)
        
        return comparison
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ë¶„ì„ ê²°ê³¼ ?€??""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"model_optimization_analysis_{timestamp}.json"
        
        filepath = os.path.join("benchmark_results", filename)
        os.makedirs("benchmark_results", exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë¶„ì„ ê²°ê³¼ ?€?? {filepath}")
        return filepath

def main():
    """ë©”ì¸ ?¤í–‰ ?¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ìµœì ??ë¶„ì„")
    parser.add_argument("--device", default="cpu", choices=["cpu", "cuda"], help="?¤í–‰ ?”ë°”?´ìŠ¤")
    parser.add_argument("--output", help="ê²°ê³¼ ?€???Œì¼ëª?)
    
    args = parser.parse_args()
    
    # ë¶„ì„ ?¤í–‰
    analyzer = ModelOptimizationAnalyzer(device=args.device)
    results = analyzer.run_analysis()
    
    # ê²°ê³¼ ?€??
    output_file = analyzer.save_results(results, args.output)
    
    # ê²°ê³¼ ?”ì•½ ì¶œë ¥
    print("\n" + "="*50)
    print("ëª¨ë¸ ìµœì ??ë¶„ì„ ê²°ê³¼ ?”ì•½")
    print("="*50)
    
    if "kobart_optimization" in results and "optimization_analysis" in results["kobart_optimization"]:
        kobart_analysis = results["kobart_optimization"]["optimization_analysis"]
        if "original" in kobart_analysis:
            original = kobart_analysis["original"]
            print(f"KoBART ?ë³¸ - ?¬ê¸°: {original.get('model_size_mb', 0):.1f}MB, "
                  f"ë©”ëª¨ë¦? {original.get('memory_usage_mb', 0):.1f}MB")
    
    if "kogpt2_optimization" in results and "optimization_analysis" in results["kogpt2_optimization"]:
        kogpt2_analysis = results["kogpt2_optimization"]["optimization_analysis"]
        if "original" in kogpt2_analysis:
            original = kogpt2_analysis["original"]
            print(f"KoGPT-2 ?ë³¸ - ?¬ê¸°: {original.get('model_size_mb', 0):.1f}MB, "
                  f"ë©”ëª¨ë¦? {original.get('memory_usage_mb', 0):.1f}MB")
    
    if "comparison" in results and "recommendation" in results["comparison"]:
        print(f"\nê¶Œì¥?¬í•­: {results['comparison']['recommendation']}")
    
    print(f"\n?ì„¸ ê²°ê³¼: {output_file}")

if __name__ == "__main__":
    main()
