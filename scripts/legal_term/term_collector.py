# -*- coding: utf-8 -*-
"""
ë²•ë¥  ìš©ì–´ ìˆ˜ì§‘ê¸° (ë©”ëª¨ë¦¬ ìµœì í™” ë° ì²´í¬í¬ì¸íŠ¸ ì§€ì›)

êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° OpenAPIë¥¼ í™œìš©í•˜ì—¬ ë²•ë¥  ìš©ì–´ë¥¼ ìˆ˜ì§‘í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
- ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œìœ¼ë¡œ ì¤‘ë‹¨ ì‹œ ì¬ê°œ ê°€ëŠ¥
- ì‹¤ì‹œê°„ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì  ë° ìµœì í™”
"""

import os
import sys
import json
import gc
import psutil
import logging
import signal
import atexit
import glob
import time
import random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass, field

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from source.data.legal_term_collection_api import LegalTermCollectionAPI, TermCollectionConfig
from source.data.legal_term_dictionary import LegalTermDictionary

logger = logging.getLogger(__name__)


@dataclass
class CheckpointData:
    """ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° êµ¬ì¡°"""
    session_id: str
    start_time: str
    last_update: str
    total_target: int
    collected_count: int
    processed_categories: List[str] = field(default_factory=list)
    processed_keywords: List[str] = field(default_factory=list)
    current_category: Optional[str] = None
    current_keyword: Optional[str] = None
    memory_usage_mb: float = 0.0
    api_requests_made: int = 0
    errors_count: int = 0
    can_resume: bool = True
    # í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ ì¶”ê°€
    current_page: int = 1
    page_size: int = 100
    consecutive_empty_pages: int = 0
    last_page_terms_count: int = 0


@dataclass
class MemoryConfig:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •"""
    max_memory_mb: int = 2048  # ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
    batch_size: int = 10  # ë°°ì¹˜ í¬ê¸° (10ê°œì”© ì²˜ë¦¬)
    checkpoint_interval: int = 2  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²© (2ê°œ ë°°ì¹˜ë§ˆë‹¤ ì €ì¥ - ë” ìì£¼ ì €ì¥)
    gc_threshold: int = 500  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì„ê³„ê°’
    memory_check_interval: int = 10  # ë©”ëª¨ë¦¬ ì²´í¬ ê°„ê²©
    batch_delay_min: float = 1.0  # ë°°ì¹˜ ê°„ ìµœì†Œ ì§€ì—° ì‹œê°„ (ì´ˆ)
    batch_delay_max: float = 3.0  # ë°°ì¹˜ ê°„ ìµœëŒ€ ì§€ì—° ì‹œê°„ (ì´ˆ)


class LegalTermCollector:
    """ë²•ë¥  ìš©ì–´ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤ (ë©”ëª¨ë¦¬ ìµœì í™” ë° ì²´í¬í¬ì¸íŠ¸ ì§€ì›)"""
    
    def __init__(self, config: TermCollectionConfig = None, memory_config: MemoryConfig = None):
        """ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”"""
        self.config = config or self._create_default_config()
        self.memory_config = memory_config or MemoryConfig()
        self.api_client = LegalTermCollectionAPI(self.config)
        self.dictionary = LegalTermDictionary()
        
        # ì²´í¬í¬ì¸íŠ¸ ê´€ë ¨
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.checkpoint_file = Path("data/raw/legal_terms/checkpoint.json")
        self.checkpoint_data = None
        
        # ë™ì  íŒŒì¼ëª… ìƒì„± (ì„¸ì…˜ë³„ êµ¬ë¶„)
        self.dynamic_file_prefix = None
        self.dictionary_file = None
        self.shutdown_requested = False
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        self.process = psutil.Process()
        self.memory_check_counter = 0
        self.gc_counter = 0
        
        # ê¸°ë³¸ í†µê³„
        self.stats = {
            'total_collected': 0,
            'start_time': None,
            'end_time': None
        }
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
        self._setup_signal_handlers()
        
        # ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‘ì—… ë“±ë¡
        atexit.register(self._cleanup_on_exit)
        
        logger.info("LegalTermCollector initialized with memory optimization")
    
    def _create_default_config(self) -> TermCollectionConfig:
        """ê¸°ë³¸ ì„¤ì • ìƒì„±"""
        config = TermCollectionConfig()
        config.batch_size = 10  # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°
        config.delay_between_requests = 0.05
        config.max_retries = 3
        return config
    
    def _setup_signal_handlers(self):
        """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì • (graceful shutdown ì§€ì›)"""
        def signal_handler(signum, frame):
            signal_name = signal.Signals(signum).name
            logger.info(f"ì‹œê·¸ë„ {signal_name}({signum}) ìˆ˜ì‹  - graceful shutdown ì‹œì‘")
            
            # ì¤‘ë³µ ì¢…ë£Œ ìš”ì²­ ë°©ì§€
            if self.shutdown_requested:
                logger.warning("ì´ë¯¸ ì¢…ë£Œ ìš”ì²­ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                sys.exit(1)
            
            self.shutdown_requested = True
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            if self.checkpoint_data:
                progress = (self.checkpoint_data.collected_count / self.checkpoint_data.total_target * 100) if self.checkpoint_data.total_target > 0 else 0
                logger.info(f"í˜„ì¬ ì§„í–‰ë¥ : {progress:.1f}% ({self.checkpoint_data.collected_count}/{self.checkpoint_data.total_target})")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            try:
                self._save_checkpoint()
                logger.info("ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # ì •ë¦¬ ì‘ì—… ìˆ˜í–‰
            self._perform_cleanup()
            
            logger.info("graceful shutdown ì™„ë£Œ")
            sys.exit(0)
        
        # ë‹¤ì–‘í•œ ì‹œê·¸ë„ì— ëŒ€í•œ í•¸ë“¤ëŸ¬ ë“±ë¡
        signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
        signal.signal(signal.SIGTERM, signal_handler)    # ì¢…ë£Œ ìš”ì²­
        if hasattr(signal, 'SIGHUP'):
            signal.signal(signal.SIGHUP, signal_handler)  # í„°ë¯¸ë„ ì—°ê²° ëŠê¹€
        if hasattr(signal, 'SIGQUIT'):
            signal.signal(signal.SIGQUIT, signal_handler)  # ì¢…ë£Œ + ì½”ì–´ ë¤í”„
    
    def _perform_cleanup(self):
        """ì •ë¦¬ ì‘ì—… ìˆ˜í–‰ (graceful shutdown)"""
        logger.info("ì •ë¦¬ ì‘ì—… ì‹œì‘...")
        
        try:
            # 1. ì§„í–‰ ì¤‘ì¸ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            logger.info("ì§„í–‰ ì¤‘ì¸ ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
            time.sleep(1)  # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì‘ì—…ì´ ì™„ë£Œë  ì‹œê°„ ì œê³µ
            
            # 2. ë©”ëª¨ë¦¬ ì •ë¦¬
            logger.info("ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
            self._force_garbage_collection()
            
            # 3. í†µê³„ ì •ë³´ ë¡œê¹…
            if self.checkpoint_data:
                logger.info("=" * 50)
                logger.info("ìˆ˜ì§‘ ì„¸ì…˜ ìš”ì•½:")
                logger.info(f"  ì„¸ì…˜ ID: {self.checkpoint_data.session_id}")
                logger.info(f"  ìˆ˜ì§‘ëœ ìš©ì–´: {self.checkpoint_data.collected_count}ê°œ")
                logger.info(f"  ëª©í‘œ ìš©ì–´: {self.checkpoint_data.total_target}ê°œ")
                logger.info(f"  ì§„í–‰ë¥ : {self.checkpoint_data.collected_count/self.checkpoint_data.total_target*100:.1f}%")
                logger.info(f"  API ìš”ì²­ ìˆ˜: {self.checkpoint_data.api_requests_made}")
                logger.info(f"  ì—ëŸ¬ ìˆ˜: {self.checkpoint_data.errors_count}")
                logger.info(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {self.checkpoint_data.memory_usage_mb:.1f}MB")
                logger.info("=" * 50)
            
            # 4. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
            if hasattr(self, 'api_client') and hasattr(self.api_client, 'session'):
                self.api_client.session.close()
            
            # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬ (í•„ìš”í•œ ê²½ìš°)
            self._cleanup_temp_files()
            
            logger.info("ì •ë¦¬ ì‘ì—… ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì •ë¦¬ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _cleanup_on_exit(self):
        """ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‘ì—… (atexit í•¸ë“¤ëŸ¬)"""
        try:
            self._perform_cleanup()
        except Exception as e:
            logger.error(f"ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {e}")
    
    def _cleanup_temp_files(self):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        try:
            # ì„ì‹œ íŒŒì¼ì´ ìˆë‹¤ë©´ ì •ë¦¬
            temp_patterns = [
                "data/raw/legal_terms/temp_*.json",
                "data/raw/legal_terms/*.tmp",
                "data/raw/legal_terms/session_*/temp_*.json",
                "data/raw/legal_terms/session_*/*.tmp",
                "logs/temp_*.log"
            ]
            
            for pattern in temp_patterns:
                temp_files = glob.glob(pattern)
                for temp_file in temp_files:
                    try:
                        os.remove(temp_file)
                        logger.debug(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {temp_file}")
                    except OSError:
                        pass  # íŒŒì¼ì´ ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŒ
                        
        except Exception as e:
            logger.debug(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
    
    def _get_memory_usage_mb(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (MB)"""
        try:
            memory_info = self.process.memory_info()
            return memory_info.rss / 1024 / 1024  # MB ë‹¨ìœ„
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _check_memory_limit(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ í™•ì¸"""
        current_memory = self._get_memory_usage_mb()
        
        
        # ë©”ëª¨ë¦¬ ì²´í¬ ì¹´ìš´í„° ì¦ê°€
        self.memory_check_counter += 1
        
        # ë©”ëª¨ë¦¬ ì œí•œ ì´ˆê³¼ ì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        if current_memory > self.memory_config.max_memory_mb:
            logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {current_memory:.1f}MB > {self.memory_config.max_memory_mb}MB")
            self._force_garbage_collection()
            return False
        
        return True
    
    def _force_garbage_collection(self):
        """ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰"""
        gc.collect()
    
    def _save_checkpoint(self) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë¹„ì •ìƒ ì¢…ë£Œ ë°©ì§€)"""
        try:
            if not self.checkpoint_data:
                return False
            
            # í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.checkpoint_data.last_update = datetime.now().isoformat()
            self.checkpoint_data.collected_count = len(self.dictionary.terms)
            self.checkpoint_data.memory_usage_mb = self._get_memory_usage_mb()
            self.checkpoint_data.api_requests_made = self.api_client.request_count
            
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì €ì¥ (ì›ìì  ì“°ê¸°)
            self.checkpoint_file.parent.mkdir(parents=True, exist_ok=True)
            
            # ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì“°ê³  ë‚˜ì¤‘ì— ì´ë¦„ ë³€ê²½ (ì›ìì  ì“°ê¸°)
            temp_file = self.checkpoint_file.with_suffix('.tmp')
            try:
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(self.checkpoint_data.__dict__, f, ensure_ascii=False, indent=2)
                
                # ì„ì‹œ íŒŒì¼ì„ ì‹¤ì œ íŒŒì¼ë¡œ ì´ë¦„ ë³€ê²½ (ì›ìì  ì—°ì‚°)
                temp_file.replace(self.checkpoint_file)
                
                logger.debug(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {self.checkpoint_data.collected_count}ê°œ ìš©ì–´")
                return True
                
            except Exception as e:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if temp_file.exists():
                    temp_file.unlink()
                raise e
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _load_checkpoint(self) -> Optional[CheckpointData]:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ê¸°ì¡´ ì„¸ì…˜ ìš°ì„  ê²€ìƒ‰, ë¹„ì •ìƒ ì¢…ë£Œ ë³µêµ¬ ì§€ì›)"""
        try:
            # 1. í˜„ì¬ ì„¤ì •ëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸
            if self.checkpoint_file.exists():
                checkpoint_data = self._load_checkpoint_file(self.checkpoint_file)
                if checkpoint_data:
                    logger.info(f"í˜„ì¬ ì„¸ì…˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_data.collected_count}ê°œ ìš©ì–´ ìˆ˜ì§‘ë¨")
                    return checkpoint_data
            
            # 2. ëª¨ë“  ì„¸ì…˜ í´ë”ì—ì„œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ìƒ‰ (ë¹„ì •ìƒ ì¢…ë£Œ ë³µêµ¬)
            session_folders = list(Path("data/raw/legal_terms").glob("session_*"))
            if not session_folders:
                logger.info("ê¸°ì¡´ ì„¸ì…˜ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ì •ë ¬
            all_checkpoints = []
            for session_folder in session_folders:
                checkpoint_files = list(session_folder.glob("checkpoint_*.json"))
                for checkpoint_file in checkpoint_files:
                    try:
                        checkpoint_data = self._load_checkpoint_file(checkpoint_file)
                        if checkpoint_data:
                            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì˜ ìˆ˜ì • ì‹œê°„ë„ ê³ ë ¤
                            file_mtime = checkpoint_file.stat().st_mtime
                            all_checkpoints.append((checkpoint_data, checkpoint_file, file_mtime))
                    except Exception as e:
                        logger.debug(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({checkpoint_file}): {e}")
                        continue
            
            if not all_checkpoints:
                logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ì„ íƒ (ì‹œê°„ìˆœ ì •ë ¬)
            all_checkpoints.sort(key=lambda x: x[2], reverse=True)  # íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê¸°ì¤€
            
            # í˜„ì¬ ì—°ë„ì™€ ë§¤ì¹­ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ìš°ì„  ì„ íƒ
            current_year = self._extract_current_year()
            matching_checkpoints = []
            other_checkpoints = []
            
            for checkpoint_data, checkpoint_file, file_mtime in all_checkpoints:
                folder_year = self._extract_folder_year(checkpoint_file.parent.name)
                if current_year and folder_year and current_year == folder_year:
                    matching_checkpoints.append((checkpoint_data, checkpoint_file, file_mtime))
                else:
                    other_checkpoints.append((checkpoint_data, checkpoint_file, file_mtime))
            
            # ìš°ì„ ìˆœìœ„: ê°™ì€ ì—°ë„ > ë‹¤ë¥¸ ì—°ë„ > ê°€ì¥ ìµœê·¼
            selected_checkpoint = None
            if matching_checkpoints:
                selected_checkpoint = matching_checkpoints[0]  # ê°™ì€ ì—°ë„ ì¤‘ ê°€ì¥ ìµœê·¼
                logger.info(f"ê°™ì€ ì—°ë„({current_year}) ì²´í¬í¬ì¸íŠ¸ ë°œê²¬")
            elif other_checkpoints:
                selected_checkpoint = other_checkpoints[0]  # ë‹¤ë¥¸ ì—°ë„ ì¤‘ ê°€ì¥ ìµœê·¼
                logger.info("ë‹¤ë¥¸ ì—°ë„ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬")
            
            if selected_checkpoint:
                checkpoint_data, checkpoint_file, file_mtime = selected_checkpoint
                
                # í˜„ì¬ ì„¸ì…˜ìœ¼ë¡œ ì„¤ì •
                self.checkpoint_file = checkpoint_file
                self.session_id = checkpoint_data.session_id
                
                # ê¸°ì¡´ ì‚¬ì „ íŒŒì¼ë„ ë¡œë“œ
                self._load_existing_dictionary_files(checkpoint_file.parent)
                
                logger.info(f"ğŸ”„ ê¸°ì¡´ ì„¸ì…˜ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_data.collected_count}ê°œ ìš©ì–´ ìˆ˜ì§‘ë¨")
                logger.info(f"ğŸ“ ì‚¬ìš©ëœ ì²´í¬í¬ì¸íŠ¸: {checkpoint_file}")
                logger.info(f"ğŸ†” ì›ë³¸ ì„¸ì…˜ ID: {checkpoint_data.session_id}")
                logger.info(f"ğŸ†” í˜„ì¬ ì„¸ì…˜ ID: {self.session_id}")
                logger.info(f"â° ì²´í¬í¬ì¸íŠ¸ ìˆ˜ì • ì‹œê°„: {datetime.fromtimestamp(file_mtime)}")
                logger.info(f"ğŸ“Š ëª©í‘œ ìš©ì–´ ìˆ˜: {checkpoint_data.total_target}ê°œ")
                logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {checkpoint_data.collected_count/checkpoint_data.total_target*100:.1f}%")
                
                return checkpoint_data
            
            logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_checkpoint_file(self, checkpoint_file: Path) -> Optional[CheckpointData]:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë“œ ë° ê²€ì¦"""
        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_dict = json.load(f)
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ['session_id', 'collected_count', 'total_target']
            for field in required_fields:
                if field not in checkpoint_dict:
                    logger.warning(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì— í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ì—†ìŠµë‹ˆë‹¤: {checkpoint_file}")
                    return None
            
            checkpoint_data = CheckpointData(**checkpoint_dict)
            
            # ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
            if checkpoint_data.collected_count < 0:
                logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë¬´ê²°ì„± ì˜¤ë¥˜ (ìŒìˆ˜ ìˆ˜ì§‘ ìˆ˜): {checkpoint_file}")
                return None
            
            if checkpoint_data.total_target <= 0:
                logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë¬´ê²°ì„± ì˜¤ë¥˜ (ì˜ëª»ëœ ëª©í‘œ ìˆ˜): {checkpoint_file}")
                return None
            
            return checkpoint_data
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({checkpoint_file}): {e}")
            return None
    
    def _extract_current_year(self) -> Optional[str]:
        """í˜„ì¬ ì„¤ì •ì—ì„œ ì—°ë„ ì¶”ì¶œ"""
        try:
            if hasattr(self, 'dynamic_file_prefix') and self.dynamic_file_prefix:
                parts = self.dynamic_file_prefix.split('_')
                if 'year' in parts:
                    year_index = parts.index('year')
                    if year_index + 1 < len(parts):
                        return parts[year_index + 1]
        except Exception:
            pass
        return None
    
    def _extract_folder_year(self, folder_name: str) -> Optional[str]:
        """í´ë”ëª…ì—ì„œ ì—°ë„ ì¶”ì¶œ"""
        try:
            if 'year_' in folder_name:
                year_part = folder_name.split('year_')[1]
                return year_part
        except Exception:
            pass
        return None
    
    def _load_existing_dictionary_files(self, session_folder: Path):
        """ê¸°ì¡´ ì‚¬ì „ íŒŒì¼ë“¤ ë¡œë“œ"""
        try:
            # ë©”ì¸ ì‚¬ì „ íŒŒì¼ ì°¾ê¸°
            dictionary_files = list(session_folder.glob("legal_terms_*.json"))
            if dictionary_files:
                # ê°€ì¥ ìµœê·¼ ì‚¬ì „ íŒŒì¼ ì‚¬ìš©
                latest_dictionary = max(dictionary_files, key=lambda f: f.stat().st_mtime)
                self.dictionary_file = latest_dictionary
                logger.info(f"ê¸°ì¡´ ì‚¬ì „ íŒŒì¼ ë¡œë“œ: {latest_dictionary}")
                
                # ì‚¬ì „ ë°ì´í„° ë¡œë“œ
                if self.dictionary_file.exists():
                    self.dictionary.load_terms_from_file(str(self.dictionary_file))
                    logger.info(f"ê¸°ì¡´ ì‚¬ì „ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.dictionary.terms)}ê°œ ìš©ì–´")
            else:
                logger.info("ê¸°ì¡´ ì‚¬ì „ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ì‚¬ì „ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _merge_existing_data(self, target_year: int):
        """ê¸°ì¡´ ë°ì´í„° ë³‘í•© (ê°™ì€ ì—°ë„ì˜ ë‹¤ë¥¸ ì„¸ì…˜ì—ì„œ)"""
        try:
            # ê°™ì€ ì—°ë„ì˜ ë‹¤ë¥¸ ì„¸ì…˜ í´ë”ë“¤ ì°¾ê¸°
            session_folders = list(Path("data/raw/legal_terms").glob(f"session_*_year_{target_year}"))
            
            merged_count = 0
            for session_folder in session_folders:
                # í˜„ì¬ ì„¸ì…˜ í´ë”ëŠ” ì œì™¸
                if session_folder == self.dictionary_file.parent:
                    continue
                
                # í•´ë‹¹ ì„¸ì…˜ì˜ ì‚¬ì „ íŒŒì¼ë“¤ ë¡œë“œ
                dictionary_files = list(session_folder.glob("legal_terms_*.json"))
                for dict_file in dictionary_files:
                    try:
                        # ì„ì‹œ ì‚¬ì „ ê°ì²´ë¡œ ë¡œë“œ
                        temp_dict = LegalTermDictionary()
                        temp_dict.load_terms_from_file(str(dict_file))
                        
                        # í˜„ì¬ ì‚¬ì „ì— ë³‘í•©
                        for term_id, term_data in temp_dict.terms.items():
                            if term_id not in self.dictionary.terms:
                                self.dictionary.add_term(term_data)
                                merged_count += 1
                        
                        logger.info(f"ì„¸ì…˜ ë³‘í•© ì™„ë£Œ: {dict_file.name} ({len(temp_dict.terms)}ê°œ ìš©ì–´)")
                        
                    except Exception as e:
                        logger.debug(f"ì„¸ì…˜ ë³‘í•© ì‹¤íŒ¨ ({dict_file}): {e}")
                        continue
            
            if merged_count > 0:
                logger.info(f"ê¸°ì¡´ ë°ì´í„° ë³‘í•© ì™„ë£Œ: {merged_count}ê°œ ìš©ì–´ ì¶”ê°€")
                self.checkpoint_data.collected_count = len(self.dictionary.terms)
            else:
                logger.info("ë³‘í•©í•  ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë°ì´í„° ë³‘í•© ì‹¤íŒ¨: {e}")
    
    def _create_checkpoint_data(self, total_target: int) -> CheckpointData:
        """ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ìƒì„±"""
        return CheckpointData(
            session_id=self.session_id,
            start_time=datetime.now().isoformat(),
            last_update=datetime.now().isoformat(),
            total_target=total_target,
            collected_count=0,
            memory_usage_mb=self._get_memory_usage_mb(),
            api_requests_made=0,
            errors_count=0,
            can_resume=True
        )
    
    def collect_all_terms(self, max_terms: int = 5000, use_mock_data: bool = False, resume: bool = True) -> bool:
        """ëª¨ë“  ë²•ë¥  ìš©ì–´ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ ìµœì í™” ë° ì²´í¬í¬ì¸íŠ¸ ì§€ì›)"""
        logger.info(f"ì „ì²´ ë²•ë¥  ìš©ì–´ ìˆ˜ì§‘ ì‹œì‘ - ìµœëŒ€ {max_terms}ê°œ (ë©”ëª¨ë¦¬ ìµœì í™”)")
        
        # ë™ì  íŒŒì¼ëª… ì„¤ì •
        self._setup_dynamic_filenames("all")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë˜ëŠ” ìƒì„±
        if resume:
            self.checkpoint_data = self._load_checkpoint()
        
        if not self.checkpoint_data:
            self.checkpoint_data = self._create_checkpoint_data(max_terms)
            logger.info("ìƒˆë¡œìš´ ìˆ˜ì§‘ ì„¸ì…˜ ì‹œì‘")
        else:
            # ê¸°ì¡´ ì‚¬ì „ ë¡œë“œ í›„ ì‹¤ì œ ìš©ì–´ ìˆ˜ë¡œ ë™ê¸°í™”
            self.load_dictionary()
            actual_count = len(self.dictionary.terms)
            if actual_count != self.checkpoint_data.collected_count:
                logger.info(f"ì²´í¬í¬ì¸íŠ¸ì™€ ì‹¤ì œ ì‚¬ì „ ìš©ì–´ ìˆ˜ ë¶ˆì¼ì¹˜ - ì²´í¬í¬ì¸íŠ¸: {self.checkpoint_data.collected_count}ê°œ, ì‹¤ì œ: {actual_count}ê°œ")
                logger.info(f"ì‹¤ì œ ì‚¬ì „ ìš©ì–´ ìˆ˜ë¡œ ë™ê¸°í™”: {actual_count}ê°œ")
                self.checkpoint_data.collected_count = actual_count
            else:
                logger.info(f"ê¸°ì¡´ ìˆ˜ì§‘ ì„¸ì…˜ ì¬ê°œ: {self.checkpoint_data.collected_count}ê°œ ìš©ì–´ ìˆ˜ì§‘ë¨")
        
        self.stats['start_time'] = datetime.now()
        
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìš©ì–´ ìˆ˜ì§‘
            success = self._collect_terms_in_batches(max_terms, use_mock_data)
            
            self.stats['end_time'] = datetime.now()
            self._log_collection_summary()
            
            return success
            
        except Exception as e:
            logger.error(f"ìš©ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.checkpoint_data.errors_count += 1
            self._save_checkpoint()
            return False
    
    def _collect_terms_in_batches(self, max_terms: int, use_mock_data: bool) -> bool:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìš©ì–´ ìˆ˜ì§‘"""
        batch_size = self.memory_config.batch_size
        collected_count = self.checkpoint_data.collected_count
        remaining_terms = max_terms - collected_count
        
        logger.info(f"ë°°ì¹˜ ìˆ˜ì§‘ ì‹œì‘ - ë°°ì¹˜ í¬ê¸°: {batch_size}, ë‚¨ì€ ìš©ì–´: {remaining_terms}ê°œ")
        
        batch_count = 0
        while collected_count < max_terms and not self.shutdown_requested:
            # ë©”ëª¨ë¦¬ ì²´í¬
            if not self._check_memory_limit():
                logger.warning("ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ ì¸í•œ ì¼ì‹œ ì¤‘ë‹¨")
                break
            
            # í˜„ì¬ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            current_batch_size = min(batch_size, remaining_terms)
            
            try:
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìš©ì–´ ìˆ˜ì§‘
                batch_terms = self.api_client.collect_legal_terms(
                    max_terms=current_batch_size, 
                    use_mock_data=use_mock_data
                )
                
                if not batch_terms:
                    logger.info("ë” ì´ìƒ ìˆ˜ì§‘í•  ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ì‚¬ì „ì— ë°°ì¹˜ ìš©ì–´ ì¶”ê°€
                batch_success = self._add_terms_to_dictionary_batch(batch_terms)
                
                if batch_success:
                    # ì‹¤ì œë¡œ ì‚¬ì „ì— ì €ì¥ëœ ìš©ì–´ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
                    collected_count = len(self.dictionary.terms)
                    self.checkpoint_data.collected_count = collected_count
                    remaining_terms = max_terms - collected_count
                    
                    logger.info(f"ë°°ì¹˜ {batch_count + 1} ì™„ë£Œ - ìˆ˜ì§‘ëœ ìš©ì–´: {collected_count}/{max_terms}ê°œ")
                    
                    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                    if (batch_count + 1) % self.memory_config.checkpoint_interval == 0:
                        self._save_checkpoint()
                
                batch_count += 1
                
                # ë°°ì¹˜ ê°„ ëœë¤ ì§€ì—° (2~5ì´ˆ)
                if not self.shutdown_requested:
                    delay = random.uniform(self.memory_config.batch_delay_min, self.memory_config.batch_delay_max)
                    logger.info(f"ë°°ì¹˜ ê°„ ì§€ì—°: {delay:.1f}ì´ˆ")
                    time.sleep(delay)
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì„ê³„ê°’ ì²´í¬
                if batch_count % self.memory_config.gc_threshold == 0:
                    self._force_garbage_collection()
                
            except Exception as e:
                logger.error(f"ë°°ì¹˜ {batch_count + 1} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                self.checkpoint_data.errors_count += 1
                
                # ì—°ì† ì—ëŸ¬ ì‹œ ì¤‘ë‹¨
                if self.checkpoint_data.errors_count > 10:
                    logger.error("ì—°ì† ì—ëŸ¬ë¡œ ì¸í•œ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
        
        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì‹¤ì œ ì‚¬ì „ ìš©ì–´ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸)
        self.checkpoint_data.collected_count = len(self.dictionary.terms)
        self._save_checkpoint()
        
        logger.info(f"ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ - ì´ {collected_count}ê°œ ìš©ì–´ ìˆ˜ì§‘")
        return collected_count > 0
    
    def collect_terms_by_categories(self, categories: List[str], max_terms_per_category: int = 500, resume: bool = True) -> bool:
        """ì¹´í…Œê³ ë¦¬ë³„ ìš©ì–´ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ ìµœì í™” ë° ì²´í¬í¬ì¸íŠ¸ ì§€ì›)"""
        logger.info(f"ì¹´í…Œê³ ë¦¬ë³„ ìš©ì–´ ìˆ˜ì§‘ ì‹œì‘ - {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ (ë©”ëª¨ë¦¬ ìµœì í™”)")
        
        # ë™ì  íŒŒì¼ëª… ì„¤ì •
        self._setup_dynamic_filenames("categories")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë˜ëŠ” ìƒì„±
        if resume:
            self.checkpoint_data = self._load_checkpoint()
        
        if not self.checkpoint_data:
            total_target = len(categories) * max_terms_per_category
            self.checkpoint_data = self._create_checkpoint_data(total_target)
            logger.info("ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ì„¸ì…˜ ì‹œì‘")
        else:
            logger.info(f"ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ì„¸ì…˜ ì¬ê°œ: {self.checkpoint_data.collected_count}ê°œ ìš©ì–´ ìˆ˜ì§‘ë¨")
        
        self.stats['start_time'] = datetime.now()
        
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ ë°°ì¹˜ ìˆ˜ì§‘
            success = self._collect_categories_in_batches(categories, max_terms_per_category)
            
            self.stats['end_time'] = datetime.now()
            self._log_collection_summary()
            
            return success
            
        except Exception as e:
            logger.error(f"ì¹´í…Œê³ ë¦¬ë³„ ìš©ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.checkpoint_data.errors_count += 1
            self._save_checkpoint()
            return False
    
    def _collect_categories_in_batches(self, categories: List[str], max_terms_per_category: int) -> bool:
        """ì¹´í…Œê³ ë¦¬ë³„ ë°°ì¹˜ ìˆ˜ì§‘"""
        batch_size = self.memory_config.batch_size
        processed_categories = set(self.checkpoint_data.processed_categories)
        
        logger.info(f"ì¹´í…Œê³ ë¦¬ë³„ ë°°ì¹˜ ìˆ˜ì§‘ ì‹œì‘ - ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        for category in categories:
            if self.shutdown_requested:
                logger.info("ì¤‘ë‹¨ ìš”ì²­ìœ¼ë¡œ ì¸í•œ ìˆ˜ì§‘ ì¤‘ë‹¨")
                break
            
            if category in processed_categories:
                logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}' ì´ë¯¸ ì²˜ë¦¬ë¨ - ê±´ë„ˆë›°ê¸°")
                continue
            
            logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}' ìˆ˜ì§‘ ì‹œì‘...")
            
            try:
                # ì¹´í…Œê³ ë¦¬ë³„ ë°°ì¹˜ ìˆ˜ì§‘
                category_success = self._collect_single_category_batches(category, max_terms_per_category, batch_size)
                
                if category_success:
                    self.checkpoint_data.processed_categories.append(category)
                    logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}' ì™„ë£Œ")
                else:
                    logger.warning(f"ì¹´í…Œê³ ë¦¬ '{category}' ìˆ˜ì§‘ ì‹¤íŒ¨")
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                self._save_checkpoint()
                
                # ë©”ëª¨ë¦¬ ì²´í¬ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                if not self._check_memory_limit():
                    logger.warning("ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ ì¸í•œ ì¼ì‹œ ì¤‘ë‹¨")
                    break
                
            except Exception as e:
                logger.error(f"ì¹´í…Œê³ ë¦¬ '{category}' ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                self.checkpoint_data.errors_count += 1
        
        logger.info(f"ì¹´í…Œê³ ë¦¬ë³„ ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ - ì´ {self.checkpoint_data.collected_count}ê°œ ìš©ì–´ ìˆ˜ì§‘")
        return self.checkpoint_data.collected_count > 0
    
    def _collect_single_category_batches(self, category: str, max_terms: int, batch_size: int) -> bool:
        """ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ ë°°ì¹˜ ìˆ˜ì§‘"""
        collected_count = 0
        batch_count = 0
        
        while collected_count < max_terms and not self.shutdown_requested:
            # ë©”ëª¨ë¦¬ ì²´í¬
            if not self._check_memory_limit():
                break
            
            # í˜„ì¬ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            current_batch_size = min(batch_size, max_terms - collected_count)
            
            try:
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìš©ì–´ ìˆ˜ì§‘
                batch_terms = self.api_client.collect_legal_terms(category, current_batch_size)
                
                if not batch_terms:
                    logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}'ì—ì„œ ë” ì´ìƒ ìˆ˜ì§‘í•  ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ì‚¬ì „ì— ë°°ì¹˜ ìš©ì–´ ì¶”ê°€
                batch_success = self._add_terms_to_dictionary_batch(batch_terms)
                
                if batch_success:
                    # ì‹¤ì œë¡œ ì‚¬ì „ì— ì €ì¥ëœ ìš©ì–´ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
                    collected_count = len(self.dictionary.terms)
                    self.checkpoint_data.collected_count = collected_count
                    
                    logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}' ë°°ì¹˜ {batch_count + 1} ì™„ë£Œ - ìˆ˜ì§‘ëœ ìš©ì–´: {collected_count}/{max_terms}ê°œ")
                
                batch_count += 1
                
                # ë°°ì¹˜ ê°„ ëœë¤ ì§€ì—° (2~5ì´ˆ)
                if not self.shutdown_requested:
                    delay = random.uniform(self.memory_config.batch_delay_min, self.memory_config.batch_delay_max)
                    logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}' ë°°ì¹˜ ê°„ ì§€ì—°: {delay:.1f}ì´ˆ")
                    time.sleep(delay)
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì„ê³„ê°’ ì²´í¬
                if batch_count % self.memory_config.gc_threshold == 0:
                    self._force_garbage_collection()
                
            except Exception as e:
                logger.error(f"ì¹´í…Œê³ ë¦¬ '{category}' ë°°ì¹˜ {batch_count + 1} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                self.checkpoint_data.errors_count += 1
                break
        
        return collected_count > 0
    
    def collect_terms_by_keywords(self, keywords: List[str], max_terms_per_keyword: int = 100, resume: bool = True) -> bool:
        """í‚¤ì›Œë“œë³„ ìš©ì–´ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ ìµœì í™” ë° ì²´í¬í¬ì¸íŠ¸ ì§€ì›)"""
        logger.info(f"í‚¤ì›Œë“œë³„ ìš©ì–´ ìˆ˜ì§‘ ì‹œì‘ - {len(keywords)}ê°œ í‚¤ì›Œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)")
        
        # ë™ì  íŒŒì¼ëª… ì„¤ì •
        self._setup_dynamic_filenames("keywords")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë˜ëŠ” ìƒì„±
        if resume:
            self.checkpoint_data = self._load_checkpoint()
        
        if not self.checkpoint_data:
            total_target = len(keywords) * max_terms_per_keyword
            self.checkpoint_data = self._create_checkpoint_data(total_target)
            logger.info("ìƒˆë¡œìš´ í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ ì„¸ì…˜ ì‹œì‘")
        else:
            logger.info(f"ê¸°ì¡´ í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ ì„¸ì…˜ ì¬ê°œ: {self.checkpoint_data.collected_count}ê°œ ìš©ì–´ ìˆ˜ì§‘ë¨")
        
        self.stats['start_time'] = datetime.now()
        
        try:
            # í‚¤ì›Œë“œë³„ ë°°ì¹˜ ìˆ˜ì§‘
            success = self._collect_keywords_in_batches(keywords, max_terms_per_keyword)
            
            self.stats['end_time'] = datetime.now()
            self._log_collection_summary()
            
            return success
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œë³„ ìš©ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.checkpoint_data.errors_count += 1
            self._save_checkpoint()
            return False
    
    def _collect_keywords_in_batches(self, keywords: List[str], max_terms_per_keyword: int) -> bool:
        """í‚¤ì›Œë“œë³„ ë°°ì¹˜ ìˆ˜ì§‘"""
        batch_size = self.memory_config.batch_size
        processed_keywords = set(self.checkpoint_data.processed_keywords)
        
        logger.info(f"í‚¤ì›Œë“œë³„ ë°°ì¹˜ ìˆ˜ì§‘ ì‹œì‘ - ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        for keyword in keywords:
            if self.shutdown_requested:
                logger.info("ì¤‘ë‹¨ ìš”ì²­ìœ¼ë¡œ ì¸í•œ ìˆ˜ì§‘ ì¤‘ë‹¨")
                break
            
            if keyword in processed_keywords:
                logger.info(f"í‚¤ì›Œë“œ '{keyword}' ì´ë¯¸ ì²˜ë¦¬ë¨ - ê±´ë„ˆë›°ê¸°")
                continue
            
            logger.info(f"í‚¤ì›Œë“œ '{keyword}' ìˆ˜ì§‘ ì‹œì‘...")
            
            try:
                # í‚¤ì›Œë“œë³„ ë°°ì¹˜ ìˆ˜ì§‘
                keyword_success = self._collect_single_keyword_batches(keyword, max_terms_per_keyword, batch_size)
                
                if keyword_success:
                    self.checkpoint_data.processed_keywords.append(keyword)
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' ì™„ë£Œ")
                else:
                    logger.warning(f"í‚¤ì›Œë“œ '{keyword}' ìˆ˜ì§‘ ì‹¤íŒ¨")
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                self._save_checkpoint()
                
                # ë©”ëª¨ë¦¬ ì²´í¬ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                if not self._check_memory_limit():
                    logger.warning("ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ ì¸í•œ ì¼ì‹œ ì¤‘ë‹¨")
                    break
                
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ '{keyword}' ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                self.checkpoint_data.errors_count += 1
        
        logger.info(f"í‚¤ì›Œë“œë³„ ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ - ì´ {self.checkpoint_data.collected_count}ê°œ ìš©ì–´ ìˆ˜ì§‘")
        return self.checkpoint_data.collected_count > 0
    
    def _collect_single_keyword_batches(self, keyword: str, max_terms: int, batch_size: int) -> bool:
        """ë‹¨ì¼ í‚¤ì›Œë“œ ë°°ì¹˜ ìˆ˜ì§‘"""
        collected_count = 0
        batch_count = 0
        
        while collected_count < max_terms and not self.shutdown_requested:
            # ë©”ëª¨ë¦¬ ì²´í¬
            if not self._check_memory_limit():
                break
            
            # í˜„ì¬ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            current_batch_size = min(batch_size, max_terms - collected_count)
            
            try:
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìš©ì–´ ìˆ˜ì§‘
                batch_terms = self.api_client.collect_legal_terms(keyword, current_batch_size)
                
                if not batch_terms:
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}'ì—ì„œ ë” ì´ìƒ ìˆ˜ì§‘í•  ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ì‚¬ì „ì— ë°°ì¹˜ ìš©ì–´ ì¶”ê°€
                batch_success = self._add_terms_to_dictionary_batch(batch_terms)
                
                if batch_success:
                    # ì‹¤ì œë¡œ ì‚¬ì „ì— ì €ì¥ëœ ìš©ì–´ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
                    collected_count = len(self.dictionary.terms)
                    self.checkpoint_data.collected_count = collected_count
                    
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' ë°°ì¹˜ {batch_count + 1} ì™„ë£Œ - ìˆ˜ì§‘ëœ ìš©ì–´: {collected_count}/{max_terms}ê°œ")
                
                batch_count += 1
                
                # ë°°ì¹˜ ê°„ ëœë¤ ì§€ì—° (2~5ì´ˆ)
                if not self.shutdown_requested:
                    delay = random.uniform(self.memory_config.batch_delay_min, self.memory_config.batch_delay_max)
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' ë°°ì¹˜ ê°„ ì§€ì—°: {delay:.1f}ì´ˆ")
                    time.sleep(delay)
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì„ê³„ê°’ ì²´í¬
                if batch_count % self.memory_config.gc_threshold == 0:
                    self._force_garbage_collection()
                
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ '{keyword}' ë°°ì¹˜ {batch_count + 1} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                self.checkpoint_data.errors_count += 1
                break
        
        return collected_count > 0
    
    def _setup_dynamic_filenames(self, collection_type: str, target_year: int = None):
        """ë™ì  íŒŒì¼ëª… ì„¤ì • (ì„¸ì…˜ë³„ êµ¬ë¶„)"""
        try:
            # íŒŒì¼ëª… ì ‘ë‘ì‚¬ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if target_year:
                self.dynamic_file_prefix = f"{timestamp}_{collection_type}_{target_year}"
            else:
                self.dynamic_file_prefix = f"{timestamp}_{collection_type}"
            
            # ì„¸ì…˜ë³„ í´ë” ìƒì„±
            session_folder = Path(f"data/raw/legal_terms/session_{self.dynamic_file_prefix}")
            session_folder.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì„¸ì…˜ í´ë” ë‚´ë¶€)
            self.dictionary_file = session_folder / f"legal_terms_{self.dynamic_file_prefix}.json"
            self.checkpoint_file = session_folder / f"checkpoint_{self.dynamic_file_prefix}.json"
            
            logger.info(f"ë™ì  íŒŒì¼ëª… ì„¤ì • ì™„ë£Œ:")
            logger.info(f"  ì„¸ì…˜ í´ë”: {session_folder}")
            logger.info(f"  ì‚¬ì „ íŒŒì¼: {self.dictionary_file}")
            logger.info(f"  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼: {self.checkpoint_file}")
            
        except Exception as e:
            logger.error(f"ë™ì  íŒŒì¼ëª… ì„¤ì • ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ íŒŒì¼ëª…ìœ¼ë¡œ í´ë°±
            self.dictionary_file = Path("data/raw/legal_terms/legal_term_dictionary.json")
            self.checkpoint_file = Path("data/raw/legal_terms/checkpoint.json")
    
    def collect_terms_by_year(self, year: int, max_terms: int = None, resume: bool = True) -> bool:
        """ì§€ì • ì—°ë„ ìš©ì–´ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ ìµœì í™” ë° ì²´í¬í¬ì¸íŠ¸ ì§€ì›)"""
        # ë¬´ì œí•œ ìˆ˜ì§‘ ëª¨ë“œ (max_termsê°€ Noneì´ê±°ë‚˜ ë§¤ìš° í° ê°’ì¸ ê²½ìš°)
        if max_terms is None:
            max_terms = 999999999  # ê±°ì˜ ë¬´ì œí•œ
            logger.info(f"{year}ë…„ ìš©ì–´ ìˆ˜ì§‘ ì‹œì‘ - ë¬´ì œí•œ ëª¨ë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)")
        else:
            logger.info(f"{year}ë…„ ìš©ì–´ ìˆ˜ì§‘ ì‹œì‘ - ìµœëŒ€ {max_terms}ê°œ (ë©”ëª¨ë¦¬ ìµœì í™”)")
        
        # ë™ì  íŒŒì¼ëª… ì„¤ì •
        self._setup_dynamic_filenames("year", year)
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë˜ëŠ” ìƒì„±
        if resume:
            self.checkpoint_data = self._load_checkpoint()
        
        if not self.checkpoint_data:
            self.checkpoint_data = self._create_checkpoint_data(max_terms)
            logger.info(f"ğŸ†• ìƒˆë¡œìš´ {year}ë…„ ìˆ˜ì§‘ ì„¸ì…˜ ì‹œì‘")
            logger.info(f"ğŸ“ ì„¸ì…˜ í´ë”: {self.dictionary_file.parent}")
            logger.info(f"ğŸ“ ì‚¬ì „ íŒŒì¼: {self.dictionary_file}")
            logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼: {self.checkpoint_file}")
            logger.info(f"ğŸ“Š ëª©í‘œ ìš©ì–´ ìˆ˜: {max_terms}ê°œ")
        else:
            # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ì˜ ëª©í‘œ ìš©ì–´ ìˆ˜ ì—…ë°ì´íŠ¸ (ë¬´ì œí•œ ëª¨ë“œì¸ ê²½ìš°)
            if max_terms == 999999999 or self.checkpoint_data.total_target < max_terms:
                logger.info(f"ëª©í‘œ ìš©ì–´ ìˆ˜ ì—…ë°ì´íŠ¸: {self.checkpoint_data.total_target} â†’ {max_terms}")
                self.checkpoint_data.total_target = max_terms
            
            # ê¸°ì¡´ ë°ì´í„° ë³‘í•©
            self._merge_existing_data(year)
            
            # ê¸°ì¡´ ì‚¬ì „ ë¡œë“œ í›„ ì‹¤ì œ ìš©ì–´ ìˆ˜ë¡œ ë™ê¸°í™”
            self.load_dictionary()
            actual_count = len(self.dictionary.terms)
            if actual_count != self.checkpoint_data.collected_count:
                logger.info(f"ì²´í¬í¬ì¸íŠ¸ì™€ ì‹¤ì œ ì‚¬ì „ ìš©ì–´ ìˆ˜ ë¶ˆì¼ì¹˜ - ì²´í¬í¬ì¸íŠ¸: {self.checkpoint_data.collected_count}ê°œ, ì‹¤ì œ: {actual_count}ê°œ")
                logger.info(f"ì‹¤ì œ ì‚¬ì „ ìš©ì–´ ìˆ˜ë¡œ ë™ê¸°í™”: {actual_count}ê°œ")
                self.checkpoint_data.collected_count = actual_count
            else:
                logger.info(f"ê¸°ì¡´ {year}ë…„ ìˆ˜ì§‘ ì„¸ì…˜ ì¬ê°œ: {self.checkpoint_data.collected_count}ê°œ ìš©ì–´ ìˆ˜ì§‘ë¨")
        
        self.stats['start_time'] = datetime.now()
        
        try:
            # ë‹¨ì¼ ì—°ë„ ë°°ì¹˜ ìˆ˜ì§‘
            success = self._collect_single_year_batches(year, max_terms)
            
            self.stats['end_time'] = datetime.now()
            self._log_collection_summary()
            
            return success
            
        except Exception as e:
            logger.error(f"{year}ë…„ ìš©ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.checkpoint_data.errors_count += 1
            self._save_checkpoint()
            return False
    
    
    def _collect_single_year_batches(self, year: int, max_terms: int) -> bool:
        """ë‹¨ì¼ ì—°ë„ ë°°ì¹˜ ìˆ˜ì§‘"""
        batch_size = self.memory_config.batch_size
        collected_count = self.checkpoint_data.collected_count
        
        # ë¬´ì œí•œ ëª¨ë“œì¸ ê²½ìš° ë‚¨ì€ ìš©ì–´ ê³„ì‚° ìƒëµ
        if max_terms == 999999999:
            remaining_terms = "ë¬´ì œí•œ"
            logger.info(f"{year}ë…„ ë°°ì¹˜ ìˆ˜ì§‘ ì‹œì‘ - ë°°ì¹˜ í¬ê¸°: {batch_size}, ëª¨ë“œ: ë¬´ì œí•œ")
        else:
            remaining_terms = max_terms - collected_count
            logger.info(f"{year}ë…„ ë°°ì¹˜ ìˆ˜ì§‘ ì‹œì‘ - ë°°ì¹˜ í¬ê¸°: {batch_size}, ë‚¨ì€ ìš©ì–´: {remaining_terms}ê°œ")
        
        # ì—°ë„ë³„ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        start_date = f"{year}-01-01"
        end_date = f"{year}-12-31"
        
        batch_count = 0
        consecutive_empty_batches = 0  # ì—°ì†ìœ¼ë¡œ ë¹ˆ ë°°ì¹˜ê°€ ë‚˜ì˜¨ íšŸìˆ˜
        max_empty_batches = 5  # ìµœëŒ€ ì—°ì† ë¹ˆ ë°°ì¹˜ í—ˆìš© íšŸìˆ˜
        
        while (max_terms == 999999999 or collected_count < max_terms) and not self.shutdown_requested:
            # shutdown ìš”ì²­ í™•ì¸ (ë” ìì£¼ ì²´í¬)
            if self.shutdown_requested:
                logger.info("ì¢…ë£Œ ìš”ì²­ìœ¼ë¡œ ì¸í•œ ìˆ˜ì§‘ ì¤‘ë‹¨")
                break
            
            # ë©”ëª¨ë¦¬ ì²´í¬
            if not self._check_memory_limit():
                logger.warning("ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ ì¸í•œ ì¼ì‹œ ì¤‘ë‹¨")
                break
            
            # í˜„ì¬ ë°°ì¹˜ í¬ê¸° ê³„ì‚° (ë¬´ì œí•œ ëª¨ë“œì¸ ê²½ìš° ë°°ì¹˜ í¬ê¸° ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            if max_terms == 999999999:
                current_batch_size = batch_size
            else:
                current_batch_size = min(batch_size, remaining_terms)
            
            try:
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìš©ì–´ ìˆ˜ì§‘ (ë‚ ì§œ ë²”ìœ„ í¬í•¨)
                batch_terms = self.api_client.collect_legal_terms(
                    max_terms=current_batch_size,
                    start_date=start_date,
                    end_date=end_date,
                    checkpoint_data=self.checkpoint_data.__dict__ if self.checkpoint_data else None,
                    session_folder=str(self.dictionary_file.parent) if self.dictionary_file else None
                )
                
                # shutdown ìš”ì²­ ì¬í™•ì¸ (API í˜¸ì¶œ í›„)
                if self.shutdown_requested:
                    logger.info("API í˜¸ì¶œ í›„ ì¢…ë£Œ ìš”ì²­ í™•ì¸ - ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
                
                if not batch_terms:
                    consecutive_empty_batches += 1
                    logger.info(f"{year}ë…„ ë°°ì¹˜ {batch_count + 1} - ìˆ˜ì§‘ëœ ìš©ì–´ ì—†ìŒ (ì—°ì† ë¹ˆ ë°°ì¹˜: {consecutive_empty_batches}/{max_empty_batches})")
                    
                    # ì—°ì†ìœ¼ë¡œ ë¹ˆ ë°°ì¹˜ê°€ ë§ì´ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
                    if consecutive_empty_batches >= max_empty_batches:
                        logger.info(f"{year}ë…„ì—ì„œ ì—°ì† {max_empty_batches}íšŒ ë¹ˆ ë°°ì¹˜ë¡œ ì¸í•œ ìˆ˜ì§‘ ì¤‘ë‹¨")
                        break
                    
                    # ë¹ˆ ë°°ì¹˜ í›„ì—ë„ ì§€ì—° ì ìš©
                    if not self.shutdown_requested:
                        delay = random.uniform(self.memory_config.batch_delay_min, self.memory_config.batch_delay_max)
                        logger.info(f"ë¹ˆ ë°°ì¹˜ í›„ ì§€ì—°: {delay:.1f}ì´ˆ")
                        time.sleep(delay)
                    
                    batch_count += 1
                    continue
                else:
                    # ìš©ì–´ê°€ ìˆ˜ì§‘ë˜ë©´ ì—°ì† ë¹ˆ ë°°ì¹˜ ì¹´ìš´í„° ë¦¬ì…‹
                    consecutive_empty_batches = 0
                
                # ì‚¬ì „ì— ë°°ì¹˜ ìš©ì–´ ì¶”ê°€
                batch_success = self._add_terms_to_dictionary_batch(batch_terms)
                
                if batch_success:
                    # ì‹¤ì œë¡œ ì‚¬ì „ì— ì €ì¥ëœ ìš©ì–´ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸ (ê°€ì¥ ì •í™•í•œ ë°©ë²•)
                    collected_count = len(self.dictionary.terms)
                    self.checkpoint_data.collected_count = collected_count
                    
                    # ë¬´ì œí•œ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë‚¨ì€ ìš©ì–´ ê³„ì‚°
                    if max_terms != 999999999:
                        remaining_terms = max_terms - collected_count
                        logger.info(f"âœ… {year}ë…„ ë°°ì¹˜ {batch_count + 1} ì™„ë£Œ - ìˆ˜ì§‘ëœ ìš©ì–´: {collected_count}/{max_terms}ê°œ")
                    else:
                        logger.info(f"âœ… {year}ë…„ ë°°ì¹˜ {batch_count + 1} ì™„ë£Œ - ìˆ˜ì§‘ëœ ìš©ì–´: {collected_count}ê°œ (ë¬´ì œí•œ ëª¨ë“œ)")
                    
                    # íŒŒì¼ ìƒì„± ìƒí™© ë¡œê·¸
                    logger.info(f"ğŸ“ í˜„ì¬ ì‚¬ì „ íŒŒì¼: {self.dictionary_file}")
                    logger.info(f"ğŸ“ í˜„ì¬ ì²´í¬í¬ì¸íŠ¸: {self.checkpoint_file}")
                    logger.info(f"ğŸ“Š ì‚¬ì „ì— ì €ì¥ëœ ìš©ì–´ ìˆ˜: {len(self.dictionary.terms)}ê°œ")
                    
                    
                    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë° í˜ì´ì§€ íŒŒì¼ ì¦‰ì‹œ ì €ì¥
                    if (batch_count + 1) % self.memory_config.checkpoint_interval == 0:
                        logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘... (ë°°ì¹˜ {batch_count + 1}ë§ˆë‹¤ ì €ì¥)")
                        self._save_checkpoint()
                        logger.info(f"ğŸ’¾ í˜ì´ì§€ íŒŒì¼ ì €ì¥ ì¤‘...")
                        self._save_page_files_immediately()
                        logger.info(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ!")
                
                batch_count += 1
                
                # ë°°ì¹˜ ê°„ ëœë¤ ì§€ì—° (2~5ì´ˆ)
                if not self.shutdown_requested:
                    delay = random.uniform(self.memory_config.batch_delay_min, self.memory_config.batch_delay_max)
                    logger.info(f"â³ {year}ë…„ ë°°ì¹˜ ê°„ ì§€ì—°: {delay:.1f}ì´ˆ")
                    time.sleep(delay)
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì„ê³„ê°’ ì²´í¬
                if batch_count % self.memory_config.gc_threshold == 0:
                    self._force_garbage_collection()
                
            except Exception as e:
                logger.error(f"{year}ë…„ ë°°ì¹˜ {batch_count + 1} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                self.checkpoint_data.errors_count += 1
                
                # ì—°ì† ì—ëŸ¬ ì‹œ ì¤‘ë‹¨
                if self.checkpoint_data.errors_count > 10:
                    logger.error("ì—°ì† ì—ëŸ¬ë¡œ ì¸í•œ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
        
        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì‹¤ì œ ì‚¬ì „ ìš©ì–´ ìˆ˜ë¡œ ì—…ë°ì´íŠ¸)
        logger.info(f"ğŸ’¾ ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘...")
        self.checkpoint_data.collected_count = len(self.dictionary.terms)
        self._save_checkpoint()
        logger.info(f"ğŸ’¾ ìµœì¢… í˜ì´ì§€ íŒŒì¼ ì €ì¥ ì¤‘...")
        self._save_page_files_immediately()
        
        logger.info(f"ğŸ‰ {year}ë…„ ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ - ì´ {collected_count}ê°œ ìš©ì–´ ìˆ˜ì§‘")
        logger.info(f"ğŸ“ ìµœì¢… ì‚¬ì „ íŒŒì¼: {self.dictionary_file}")
        logger.info(f"ğŸ“ ìµœì¢… ì²´í¬í¬ì¸íŠ¸: {self.checkpoint_file}")
        logger.info(f"ğŸ“Š ìµœì¢… ì‚¬ì „ ìš©ì–´ ìˆ˜: {len(self.dictionary.terms)}ê°œ")
        return collected_count > 0
    
    def _add_terms_to_dictionary_batch(self, terms: List[Dict[str, Any]]) -> bool:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‚¬ì „ì— ìš©ì–´ ì¶”ê°€ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
        
        success_count = 0
        fail_count = 0
        category_stats = {}
        
        for term_data in terms:
            try:
                if self.dictionary.add_term(term_data):
                    success_count += 1
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ìˆ˜ì§‘
                    category = term_data.get('category', 'ê¸°íƒ€')
                    category_stats[category] = category_stats.get(category, 0) + 1
                else:
                    fail_count += 1
                    
            except Exception as e:
                logger.error(f"ìš©ì–´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
                fail_count += 1
        
        # ê¸°ë³¸ í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['total_collected'] = len(self.dictionary.terms)
        
        return success_count > 0
    
    def _add_terms_to_dictionary(self, terms: List[Dict[str, Any]]) -> bool:
        """ìˆ˜ì§‘ëœ ìš©ì–´ë¥¼ ì‚¬ì „ì— ì¶”ê°€"""
        logger.info(f"ì‚¬ì „ì— {len(terms)}ê°œ ìš©ì–´ ì¶”ê°€ ì¤‘...")
        
        success_count = 0
        fail_count = 0
        category_stats = {}
        
        for i, term_data in enumerate(terms):
            try:
                if self.dictionary.add_term(term_data):
                    success_count += 1
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ìˆ˜ì§‘
                    category = term_data.get('category', 'ê¸°íƒ€')
                    category_stats[category] = category_stats.get(category, 0) + 1
                else:
                    fail_count += 1
                
                # ì§„í–‰ë¥  ë¡œê·¸ (100ê°œë§ˆë‹¤)
                if (i + 1) % 100 == 0:
                    logger.info(f"ìš©ì–´ ì¶”ê°€ ì§„í–‰: {i + 1}/{len(terms)} ({success_count}ê°œ ì„±ê³µ, {fail_count}ê°œ ì‹¤íŒ¨)")
                    
            except Exception as e:
                logger.error(f"ìš©ì–´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
                fail_count += 1
        
        # ê¸°ë³¸ í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['total_collected'] = len(self.dictionary.terms)
        
        logger.info(f"ì‚¬ì „ ì¶”ê°€ ì™„ë£Œ - {success_count}ê°œ ì„±ê³µ, {fail_count}ê°œ ì‹¤íŒ¨")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì¶œë ¥
        logger.info("ì¹´í…Œê³ ë¦¬ë³„ ìš©ì–´ ìˆ˜ì§‘ í†µê³„:")
        for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"  {category}: {count}ê°œ")
        
        return success_count > 0
    
    def _log_collection_summary(self):
        """ìˆ˜ì§‘ ìš”ì•½ ë¡œê·¸ ì¶œë ¥"""
        if self.stats['start_time'] and self.stats['end_time']:
            duration = self.stats['end_time'] - self.stats['start_time']
            logger.info(f"ìˆ˜ì§‘ ì™„ë£Œ - ì´ ìš©ì–´: {len(self.dictionary.terms)}ê°œ, ì†Œìš” ì‹œê°„: {duration.total_seconds():.2f}ì´ˆ")
        else:
            logger.info(f"ìˆ˜ì§‘ ì™„ë£Œ - ì´ ìš©ì–´: {len(self.dictionary.terms)}ê°œ")
    
    
    def get_progress_status(self) -> Dict[str, Any]:
        """ì§„í–‰ ìƒíƒœ ì¡°íšŒ"""
        if not self.checkpoint_data:
            return {'status': 'not_started'}
        
        progress_percent = (self.checkpoint_data.collected_count / self.checkpoint_data.total_target) * 100
        
        return {
            'status': 'in_progress' if not self.shutdown_requested else 'paused',
            'collected_count': len(self.dictionary.terms),
            'total_target': self.checkpoint_data.total_target
        }
    
    def clear_checkpoint(self) -> bool:
        """ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ"""
        try:
            if self.checkpoint_file.exists():
                self.checkpoint_file.unlink()
                logger.info("ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì™„ë£Œ")
                return True
            else:
                logger.info("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return True
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def optimize_memory_settings(self, available_memory_mb: int = None) -> bool:
        """ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™”"""
        try:
            if available_memory_mb is None:
                # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ
                memory_info = psutil.virtual_memory()
                available_memory_mb = memory_info.available / 1024 / 1024
            
            # ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„¤ì • (ì „ì²´ì˜ 70%)
            safe_memory_mb = int(available_memory_mb * 0.7)
            
            # ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
            if safe_memory_mb > 4096:  # 4GB ì´ìƒ
                batch_size = 100
            elif safe_memory_mb > 2048:  # 2GB ì´ìƒ
                batch_size = 50
            else:  # 2GB ë¯¸ë§Œ
                batch_size = 25
            
            # ë©”ëª¨ë¦¬ ì„¤ì • ì—…ë°ì´íŠ¸
            self.memory_config.max_memory_mb = safe_memory_mb
            self.memory_config.batch_size = batch_size
            
            logger.info(f"ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™” ì™„ë£Œ:")
            logger.info(f"  ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {available_memory_mb:.1f}MB")
            logger.info(f"  ì•ˆì „ ë©”ëª¨ë¦¬ í•œê³„: {safe_memory_mb:.1f}MB")
            logger.info(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
            
            return True
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™” ì‹¤íŒ¨: {e}")
            return False
    
    def save_dictionary(self, file_path: str = None) -> bool:
        """ì‚¬ì „ ì €ì¥ (ë°°ì¹˜ë³„ ë¶„ë¦¬ íŒŒì¼ ì§€ì›)"""
        try:
            if file_path is None:
                # ë™ì  íŒŒì¼ëª…ì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ ê¸°ë³¸ íŒŒì¼ëª… ì‚¬ìš©
                if self.dictionary_file:
                    file_path = str(self.dictionary_file)
                else:
                    file_path = "data/raw/legal_terms/legal_term_dictionary.json"
            
            # ë°°ì¹˜ë³„ ë¶„ë¦¬ íŒŒì¼ë¡œ ì €ì¥
            return self._save_dictionary_batch_separated(file_path)
            
        except Exception as e:
            logger.error(f"ì‚¬ì „ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _save_dictionary_batch_separated(self, base_file_path: str) -> bool:
        """ë°°ì¹˜ë³„ë¡œ ë¶„ë¦¬ëœ íŒŒì¼ë¡œ ì‚¬ì „ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)"""
        try:
            base_path = Path(base_file_path)
            session_folder = base_path.parent
            
            # ê¸°ì¡´ ë©”ì¸ ì‚¬ì „ íŒŒì¼ í™•ì¸
            if base_path.exists():
                logger.info(f"ë©”ì¸ ì‚¬ì „ íŒŒì¼ {base_path.name}ì´ ì´ë¯¸ ì¡´ì¬í•¨ - ê±´ë„ˆë›°ê¸°")
            else:
                # 1. ì „ì²´ ì‚¬ì „ íŒŒì¼ ì €ì¥ (ê¸°ì¡´ ë°©ì‹)
                self.dictionary.dictionary_path = base_path
                success = self.dictionary.save_dictionary()
                
                if not success:
                    return False
            
            # 2. í˜ì´ì§€ë³„ë¡œ ë¶„ë¦¬ ì €ì¥ (ê¸°ì¡´ í˜ì´ì§€ íŒŒì¼ë“¤ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€)
            existing_page_files = list(session_folder.glob("legal_terms_page_*.json"))
            if existing_page_files:
                logger.info(f"ê¸°ì¡´ í˜ì´ì§€ íŒŒì¼ {len(existing_page_files)}ê°œ ë°œê²¬ - ì¶”ê°€ ì €ì¥ ìƒëµ")
            else:
                # í˜ì´ì§€ë³„ë¡œ ë¶„ë¦¬ ì €ì¥
                self._save_batch_files(session_folder)
            
            logger.info(f"ë°°ì¹˜ë³„ ë¶„ë¦¬ ì‚¬ì „ ì €ì¥ ì™„ë£Œ: {base_file_path}")
            return True
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ë³„ ë¶„ë¦¬ ì‚¬ì „ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _save_batch_files(self, session_folder: Path):
        """API í˜ì´ì§€ë³„ë¡œ íŒŒì¼ ë¶„ë¦¬ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)"""
        try:
            # API í˜ì´ì§€ í¬ê¸° ì„¤ì • (ê¸°ë³¸ 100ê°œ - API ê¸°ë³¸ê°’)
            page_size = 100
            
            # ê¸°ì¡´ í˜ì´ì§€ íŒŒì¼ë“¤ í™•ì¸
            existing_page_files = list(session_folder.glob("legal_terms_page_*.json"))
            existing_page_numbers = set()
            
            for file_path in existing_page_files:
                try:
                    # íŒŒì¼ëª…ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: legal_terms_page_001.json -> 1)
                    page_num = int(file_path.stem.split('_')[-1])
                    existing_page_numbers.add(page_num)
                except (ValueError, IndexError):
                    continue
            
            if existing_page_numbers:
                logger.info(f"ê¸°ì¡´ í˜ì´ì§€ íŒŒì¼ {len(existing_page_numbers)}ê°œ ë°œê²¬: {sorted(existing_page_numbers)}")
            
            # ìš©ì–´ë“¤ì„ í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”
            terms_list = list(self.dictionary.terms.items())
            total_terms = len(terms_list)
            
            if total_terms == 0:
                logger.info("ì €ì¥í•  ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # í˜ì´ì§€ íŒŒì¼ ì €ì¥
            page_count = 0
            for i in range(0, total_terms, page_size):
                page_terms = dict(terms_list[i:i + page_size])
                page_count += 1
                
                # í˜ì´ì§€ íŒŒì¼ëª… ìƒì„±
                page_file = session_folder / f"legal_terms_page_{page_count:03d}.json"
                
                # ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                if page_file.exists():
                    logger.info(f"í˜ì´ì§€ íŒŒì¼ {page_file.name}ì´ ì´ë¯¸ ì¡´ì¬í•¨ - ê±´ë„ˆë›°ê¸°")
                    continue
                
                # í˜ì´ì§€ ë°ì´í„° êµ¬ì„±
                page_data = {
                    "metadata": {
                        "page_number": page_count,
                        "page_size": len(page_terms),
                        "total_pages": (total_terms + page_size - 1) // page_size,
                        "saved_at": datetime.now().isoformat(),
                        "session_id": self.session_id,
                        "api_page_size": page_size
                    },
                    "terms": page_terms
                }
                
                # í˜ì´ì§€ íŒŒì¼ ì €ì¥
                with open(page_file, 'w', encoding='utf-8') as f:
                    json.dump(page_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ğŸ“„ í˜ì´ì§€ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {page_file.name} ({len(page_terms)}ê°œ ìš©ì–´)")
            
            logger.info(f"ğŸ“Š ì´ {page_count}ê°œ í˜ì´ì§€ íŒŒì¼ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_page_files_immediately(self):
        """ì²´í¬í¬ì¸íŠ¸ë§ˆë‹¤ ì¦‰ì‹œ í˜ì´ì§€ íŒŒì¼ ì €ì¥"""
        try:
            if not self.dictionary_file:
                logger.warning("ì‚¬ì „ íŒŒì¼ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            session_folder = self.dictionary_file.parent
            logger.info(f"ğŸ“ ì„¸ì…˜ í´ë”: {session_folder}")
            self._save_batch_files(session_folder)
            logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ë§ˆë‹¤ í˜ì´ì§€ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì¦‰ì‹œ í˜ì´ì§€ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_dictionary(self, file_path: str = None) -> bool:
        """ì‚¬ì „ ë¡œë“œ (í˜ì´ì§€ íŒŒì¼ ì§€ì›)"""
        try:
            if file_path is None:
                file_path = "data/raw/legal_terms/legal_term_dictionary.json"
            
            # í˜ì´ì§€ íŒŒì¼ë“¤ë„ í•¨ê»˜ ë¡œë“œ
            return self._load_dictionary_with_pages(file_path)
            
        except Exception as e:
            logger.error(f"ì‚¬ì „ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _load_dictionary_with_pages(self, base_file_path: str) -> bool:
        """í˜ì´ì§€ íŒŒì¼ë“¤ì„ í¬í•¨í•˜ì—¬ ì‚¬ì „ ë¡œë“œ"""
        try:
            base_path = Path(base_file_path)
            session_folder = base_path.parent
            
            # 1. ê¸°ë³¸ ì‚¬ì „ íŒŒì¼ ë¡œë“œ
            success = self.dictionary.load_terms_from_file(base_file_path)
            if success:
                logger.info(f"ê¸°ë³¸ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ: {base_file_path}")
            
            # 2. í˜ì´ì§€ íŒŒì¼ë“¤ ë¡œë“œ
            self._load_page_files(session_folder)
            
            return success
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ íŒŒì¼ í¬í•¨ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _load_page_files(self, session_folder: Path):
        """í˜ì´ì§€ íŒŒì¼ë“¤ ë¡œë“œ"""
        try:
            page_files = list(session_folder.glob("legal_terms_page_*.json"))
            if not page_files:
                logger.info("í˜ì´ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # í˜ì´ì§€ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            page_files.sort(key=lambda f: int(f.stem.split('_')[-1]))
            
            loaded_count = 0
            for page_file in page_files:
                try:
                    with open(page_file, 'r', encoding='utf-8') as f:
                        page_data = json.load(f)
                    
                    # í˜ì´ì§€ ë°ì´í„°ì—ì„œ ìš©ì–´ë“¤ ì¶”ì¶œ
                    terms = page_data.get('terms', {})
                    page_number = page_data.get('metadata', {}).get('page_number', 0)
                    
                    for term_id, term_data in terms.items():
                        if self.dictionary.add_term(term_data):
                            loaded_count += 1
                    
                    logger.info(f"í˜ì´ì§€ íŒŒì¼ ë¡œë“œ: {page_file.name} (í˜ì´ì§€ #{page_number}, {len(terms)}ê°œ ìš©ì–´)")
                    
                except Exception as e:
                    logger.debug(f"í˜ì´ì§€ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({page_file}): {e}")
                    continue
            
            if loaded_count > 0:
                logger.info(f"í˜ì´ì§€ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {loaded_count}ê°œ ìš©ì–´ ì¶”ê°€")
                
        except Exception as e:
            logger.error(f"í˜ì´ì§€ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    
    
