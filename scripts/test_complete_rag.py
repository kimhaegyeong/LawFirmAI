#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì™„ì „í•œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import json
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'source'))

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ë¡œê¹… ì„¤ì •
from utils.safe_logging import setup_script_logging
logger = setup_script_logging("test_complete_rag")

def test_complete_rag_system():
    """ì™„ì „í•œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸš€ ì™„ì „í•œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 60)
    
    try:
        # í•„ìš”í•œ íŒ¨í‚¤ì§€ import
        import faiss
        import numpy as np
        from sentence_transformers import SentenceTransformer
        import google.generativeai as genai
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        api_key = os.getenv('GOOGLE_API_KEY')
        if not api_key or api_key in ['your-google-api-key-here', 'test-google-api-key']:
            logger.error("âŒ ì‹¤ì œ Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        logger.info("âœ… Google API í‚¤ í™•ì¸ ì™„ë£Œ")
        
        # Gemini Pro ì„¤ì •
        genai.configure(api_key=api_key)
        model_gemini = genai.GenerativeModel('gemini-pro')
        logger.info("âœ… Gemini Pro ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ
        embeddings_dir = Path("data/embeddings")
        index_path = embeddings_dir / "simple_vector_index"
        metadata_path = embeddings_dir / "simple_vector_metadata.json"
        
        if not index_path.exists() or not metadata_path.exists():
            logger.error("âŒ ë²¡í„° ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # ì¸ë±ìŠ¤ ë¡œë“œ
        index = faiss.read_index(str(index_path))
        model = SentenceTransformer(metadata['model_name'])
        texts = metadata['texts']
        
        logger.info(f"âœ… ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
        
        # RAG í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            "ë¯¼ë²•ì˜ ê¸°ë³¸ ì›ì¹™ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ê³„ì•½ í•´ì„ ì‹œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë¶ˆë²•í–‰ìœ„ë¡œ ì¸í•œ ì†í•´ë°°ìƒì˜ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë²•ë¥  í•´ì„ì˜ ì›ì¹™ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        ]
        
        logger.info("ğŸ¤– RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info("-" * 60)
        
        for i, query in enumerate(test_queries, 1):
            logger.info(f"ğŸ“ í…ŒìŠ¤íŠ¸ {i}: {query}")
            
            try:
                # 1. ë²¡í„° ê²€ìƒ‰
                query_embedding = model.encode([query])
                faiss.normalize_L2(query_embedding)
                scores, indices = index.search(query_embedding.astype('float32'), k=3)
                
                # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
                retrieved_docs = []
                for j, (score, idx) in enumerate(zip(scores[0], indices[0])):
                    retrieved_docs.append({
                        'text': texts[idx],
                        'score': float(score)
                    })
                
                logger.info(f"ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ({len(retrieved_docs)}ê°œ):")
                for j, doc in enumerate(retrieved_docs):
                    logger.info(f"   {j+1}. ì ìˆ˜: {doc['score']:.4f}")
                    logger.info(f"      ë‚´ìš©: {doc['text'][:80]}...")
                
                # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                context = "\n\n".join([doc['text'] for doc in retrieved_docs])
                
                # 3. Gemini Proë¡œ ë‹µë³€ ìƒì„±
                prompt = f"""
ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œë“¤:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
1. ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë²•ë¥ ì  ì •í™•ì„±ì„ ìœ ì§€í•˜ì„¸ìš”
3. ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”
4. ê´€ë ¨ ë²•ì¡°ë¬¸ì´ë‚˜ íŒë¡€ê°€ ìˆë‹¤ë©´ ì–¸ê¸‰í•´ì£¼ì„¸ìš”

ë‹µë³€:
"""
                
                response = model_gemini.generate_content(prompt)
                
                logger.info(f"ğŸ¤– Gemini Pro ë‹µë³€:")
                logger.info(f"   {response.text}")
                
                # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
                response_length = len(response.text)
                logger.info(f"ğŸ“Š ì‘ë‹µ í’ˆì§ˆ: ê¸¸ì´ {response_length}ì")
                
            except Exception as e:
                logger.error(f"âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            logger.info("-" * 60)
        
        logger.info("âœ… RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except ImportError as e:
        logger.error(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {e}")
        return False
    except Exception as e:
        logger.error(f"âŒ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_advanced_rag_features():
    """ê³ ê¸‰ RAG ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ”¬ ê³ ê¸‰ RAG ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)
    
    try:
        import faiss
        import numpy as np
        from sentence_transformers import SentenceTransformer
        import google.generativeai as genai
        
        # ì„¤ì • ë¡œë“œ
        api_key = os.getenv('GOOGLE_API_KEY')
        genai.configure(api_key=api_key)
        model_gemini = genai.GenerativeModel('gemini-pro')
        
        # ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ
        embeddings_dir = Path("data/embeddings")
        index_path = embeddings_dir / "simple_vector_index"
        metadata_path = embeddings_dir / "simple_vector_metadata.json"
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        index = faiss.read_index(str(index_path))
        model = SentenceTransformer(metadata['model_name'])
        texts = metadata['texts']
        
        # 1. ìœ ì‚¬ë„ ì„ê³„ê°’ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ¯ ìœ ì‚¬ë„ ì„ê³„ê°’ í…ŒìŠ¤íŠ¸")
        query = "ë¯¼ë²•ì˜ ê¸°ë³¸ ì›ì¹™"
        query_embedding = model.encode([query])
        faiss.normalize_L2(query_embedding)
        scores, indices = index.search(query_embedding.astype('float32'), k=5)
        
        threshold = 0.3
        relevant_docs = []
        for score, idx in zip(scores[0], indices[0]):
            if score >= threshold:
                relevant_docs.append((texts[idx], score))
        
        logger.info(f"   ì„ê³„ê°’ {threshold} ì´ìƒ ë¬¸ì„œ: {len(relevant_docs)}ê°œ")
        for i, (doc, score) in enumerate(relevant_docs):
            logger.info(f"   {i+1}. ì ìˆ˜: {score:.4f}, ë‚´ìš©: {doc[:50]}...")
        
        # 2. ë‹¤ì¤‘ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ”„ ë‹¤ì¤‘ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸")
        multi_query = ["ë¯¼ë²•", "ê³„ì•½", "ì†í•´ë°°ìƒ"]
        
        all_docs = set()
        for q in multi_query:
            q_embedding = model.encode([q])
            faiss.normalize_L2(q_embedding)
            scores, indices = index.search(q_embedding.astype('float32'), k=2)
            for idx in indices[0]:
                all_docs.add(texts[idx])
        
        logger.info(f"   ë‹¤ì¤‘ ì¿¼ë¦¬ë¡œ ì°¾ì€ ê³ ìœ  ë¬¸ì„œ: {len(all_docs)}ê°œ")
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í…ŒìŠ¤íŠ¸")
        max_context_length = 1000
        context = "\n".join(list(all_docs)[:3])
        
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."
            logger.info(f"   ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ì ìš©: {len(context)}ì")
        
        logger.info("âœ… ê³ ê¸‰ RAG ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ê³ ê¸‰ RAG ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ì™„ì „í•œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 80)
    
    # 1. ê¸°ë³¸ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    rag_success = test_complete_rag_system()
    
    if rag_success:
        logger.info("=" * 80)
        
        # 2. ê³ ê¸‰ RAG ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        advanced_success = test_advanced_rag_features()
        
        logger.info("=" * 80)
        logger.info("ğŸ“Š ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        logger.info(f"   - ê¸°ë³¸ RAG ì‹œìŠ¤í…œ: {'âœ…' if rag_success else 'âŒ'}")
        logger.info(f"   - ê³ ê¸‰ RAG ê¸°ëŠ¥: {'âœ…' if advanced_success else 'âŒ'}")
        
        if rag_success and advanced_success:
            logger.info("ğŸ‰ ì™„ì „í•œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            logger.info("")
            logger.info("ğŸš€ ì‹œìŠ¤í…œì´ í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            logger.info("")
            logger.info("ğŸ”§ ë‹¤ìŒ ë‹¨ê³„:")
            logger.info("   1. ëŒ€ê·œëª¨ ë²•ë¥  ë°ì´í„° ìˆ˜ì§‘")
            logger.info("   2. ê³ ì„±ëŠ¥ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•")
            logger.info("   3. ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ")
            logger.info("   4. HuggingFace Spaces ë°°í¬")
        else:
            logger.info("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        logger.info("âŒ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
