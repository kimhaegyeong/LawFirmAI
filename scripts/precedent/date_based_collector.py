#!/usr/bin/env python3
"""
ë‚ ì§œ ê¸°ë°˜ íŒë¡€ ìˆ˜ì§‘ê¸°

ì´ ëª¨ë“ˆì€ ë‚ ì§œë³„ë¡œ ì²´ê³„ì ì¸ íŒë¡€ ìˆ˜ì§‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- ì—°ë„ë³„, ë¶„ê¸°ë³„, ì›”ë³„, ì£¼ë³„ ìˆ˜ì§‘ ì „ëµ
- ì„ ê³ ì¼ì ë‚´ë¦¼ì°¨ìˆœ ìµœì í™”
- í´ë”ë³„ raw ë°ì´í„° ì €ì¥ êµ¬ì¡°
- ì¤‘ë³µ ë°©ì§€ ë° ì²´í¬í¬ì¸íŠ¸ ì§€ì›
"""

import json
import time
import random
import hashlib
import traceback
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from source.data.law_open_api_client import LawOpenAPIClient, LawOpenAPIConfig
from scripts.precedent.precedent_models import (
    CollectionStats, PrecedentData, CollectionStatus, PrecedentCategory,
    COURT_CODES, CASE_TYPE_CODES
)
import logging

logger = logging.getLogger(__name__)


class DateCollectionStrategy(Enum):
    """ë‚ ì§œ ìˆ˜ì§‘ ì „ëµ ì—´ê±°í˜•"""
    YEARLY = "yearly"
    QUARTERLY = "quarterly"
    MONTHLY = "monthly"
    WEEKLY = "weekly"
    DAILY = "daily"


@dataclass
class DateCollectionConfig:
    """ë‚ ì§œ ìˆ˜ì§‘ ì„¤ì • í´ë˜ìŠ¤"""
    strategy: DateCollectionStrategy
    start_date: str
    end_date: str
    target_count: int
    batch_size: int = 100
    max_retries: int = 3
    retry_delay: int = 5
    api_delay_range: Tuple[float, float] = (1.0, 3.0)
    output_subdir: Optional[str] = None


class DateBasedPrecedentCollector:
    """ë‚ ì§œ ê¸°ë°˜ íŒë¡€ ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self, config: LawOpenAPIConfig, base_output_dir: Optional[Path] = None, 
                 include_details: bool = True):
        """
        ë‚ ì§œ ê¸°ë°˜ íŒë¡€ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        
        Args:
            config: API ì„¤ì • ê°ì²´
            base_output_dir: ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/raw/precedents)
            include_details: íŒë¡€ë³¸ë¬¸ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        self.client = LawOpenAPIClient(config)
        self.base_output_dir = base_output_dir or Path("data/raw/precedents")
        self.base_output_dir.mkdir(parents=True, exist_ok=True)
        self.include_details = include_details  # íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ì—¬ë¶€
        
        # ë°ì´í„° ê´€ë¦¬ (ë©”ëª¨ë¦¬ ìµœì í™”)
        self.collected_precedents: Set[str] = set()
        self.processed_date_ranges: Set[str] = set()
        self.pending_precedents: List[PrecedentData] = []
        self.max_memory_precedents = 50000  # ìµœëŒ€ ë©”ëª¨ë¦¬ ë³´ê´€ ê±´ìˆ˜
        
        # í†µê³„ ë° ìƒíƒœ
        self.stats = CollectionStats()
        self.stats.status = CollectionStatus.PENDING
        
        # ì—ëŸ¬ ì²˜ë¦¬
        self.error_count = 0
        self.max_errors = 50
        
        # ì‹œê°„ ì¸í„°ë²Œ ì„¤ì • (ê¸°ë³¸ê°’)
        self.request_interval_base = 2.0  # ê¸°ë³¸ ê°„ê²©
        self.request_interval_range = 2.0  # ê°„ê²© ë²”ìœ„
        
        # ê¸°ì¡´ ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œ
        self._load_existing_data()
        
        logger.info(f"ë‚ ì§œ ê¸°ë°˜ íŒë¡€ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ (íŒë¡€ë³¸ë¬¸ í¬í•¨: {include_details})")
    
    def set_request_interval(self, base_interval: float, interval_range: float):
        """API ìš”ì²­ ê°„ê²© ì„¤ì •"""
        self.request_interval_base = base_interval
        self.request_interval_range = interval_range
        logger.info(f"â±ï¸ ìš”ì²­ ê°„ê²© ì„¤ì •: {base_interval:.1f} Â± {interval_range:.1f}ì´ˆ")
    
    def _load_existing_data(self, target_year: Optional[int] = None):
        """ê¸°ì¡´ ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œí•˜ì—¬ ì¤‘ë³µ ë°©ì§€ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
        logger.info("ê¸°ì¡´ ìˆ˜ì§‘ëœ ë°ì´í„° í™•ì¸ ì¤‘...")
        
        loaded_count = 0
        error_count = 0
        
        # ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ë°ì´í„° ë¡œë“œ (ìµœì‹  í´ë” ìš°ì„ )
        subdirs = sorted([d for d in self.base_output_dir.iterdir() if d.is_dir()], 
                        key=lambda x: x.name, reverse=True)
        
        for subdir in subdirs:
            if len(self.collected_precedents) >= self.max_memory_precedents:
                logger.info(f"âš ï¸ ë©”ëª¨ë¦¬ í•œê³„ ë„ë‹¬: {len(self.collected_precedents):,}ê±´, ì¶”ê°€ ë¡œë“œ ì¤‘ë‹¨")
                break
                
                for file_path in subdir.glob("*.json"):
                    try:
                        loaded_count += self._load_precedents_from_file(file_path, target_year)
                        
                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                        if len(self.collected_precedents) >= self.max_memory_precedents:
                            logger.info(f"âš ï¸ ë©”ëª¨ë¦¬ í•œê³„ ë„ë‹¬: {len(self.collected_precedents):,}ê±´, ì¶”ê°€ ë¡œë“œ ì¤‘ë‹¨")
                            break
                            
                    except Exception as e:
                        error_count += 1
                        logger.debug(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
            
            if len(self.collected_precedents) >= self.max_memory_precedents:
                break
        
        logger.info(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {loaded_count:,}ê±´, ì˜¤ë¥˜: {error_count:,}ê±´")
        self.stats.collected_count = len(self.collected_precedents)
        logger.info(f"ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ íŒë¡€ ID {len(self.collected_precedents):,}ê°œ ë¡œë“œë¨ (ë©”ëª¨ë¦¬ ìµœì í™”)")
    
    def _load_precedents_from_file(self, file_path: Path, target_year: Optional[int] = None) -> int:
        """íŒŒì¼ì—ì„œ íŒë¡€ ë°ì´í„° ë¡œë“œ (íŠ¹ì • ì—°ë„ í•„í„°ë§)"""
        loaded_count = 0
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì¡° ì²˜ë¦¬
            precedents = []
            if isinstance(data, dict):
                if 'precedents' in data:
                    precedents = data['precedents']
                elif 'basic_info' in data:
                    precedents = [data]
                elif 'by_category' in data:
                    for category_data in data['by_category'].values():
                        precedents.extend(category_data)
            elif isinstance(data, list):
                precedents = data
            
            # íŒë¡€ ID ì¶”ì¶œ (íŠ¹ì • ì—°ë„ í•„í„°ë§)
            for precedent in precedents:
                if isinstance(precedent, dict):
                    # íŠ¹ì • ì—°ë„ê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ì—°ë„ì˜ íŒë¡€ë§Œ ë¡œë“œ
                    if target_year:
                        decision_date = precedent.get('ì„ ê³ ì¼ì', '') or precedent.get('íŒê²°ì¼ì', '')
                        if decision_date:
                            try:
                                # ë‚ ì§œ íŒŒì‹± (YYYY.MM.DD í˜•ì‹)
                                if '.' in decision_date:
                                    date_parts = decision_date.split('.')
                                    if len(date_parts) >= 1:
                                        precedent_year = int(date_parts[0])
                                        if precedent_year != target_year:
                                            continue  # ë‹¤ë¥¸ ì—°ë„ëŠ” ê±´ë„ˆë›°ê¸°
                                else:
                                    continue  # ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš° ê±´ë„ˆë›°ê¸°
                            except (ValueError, IndexError):
                                continue  # ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ ì‹œ ê±´ë„ˆë›°ê¸°
                        else:
                            continue  # ë‚ ì§œê°€ ì—†ëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
                    
                    precedent_id = precedent.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸') or precedent.get('precedent_id')
                    if precedent_id:
                        self.collected_precedents.add(str(precedent_id))
                        loaded_count += 1
        
        except Exception as e:
            logger.debug(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
        
        return loaded_count
    
    def _create_output_subdir(self, strategy: DateCollectionStrategy, date_range: str) -> Path:
        """ì¶œë ¥ í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ì „ëµë³„ ë””ë ‰í† ë¦¬ êµ¬ì¡°
        if strategy == DateCollectionStrategy.YEARLY:
            subdir_name = f"yearly_{date_range}_{timestamp}"
        elif strategy == DateCollectionStrategy.QUARTERLY:
            subdir_name = f"quarterly_{date_range}_{timestamp}"
        elif strategy == DateCollectionStrategy.MONTHLY:
            subdir_name = f"monthly_{date_range}_{timestamp}"
        elif strategy == DateCollectionStrategy.WEEKLY:
            subdir_name = f"weekly_{date_range}_{timestamp}"
        elif strategy == DateCollectionStrategy.DAILY:
            subdir_name = f"daily_{date_range}_{timestamp}"
        else:
            subdir_name = f"date_based_{date_range}_{timestamp}"
        
        output_dir = self.base_output_dir / subdir_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        return output_dir
    
    def _is_duplicate_precedent(self, precedent: Dict[str, Any]) -> bool:
        """íŒë¡€ ì¤‘ë³µ ì—¬ë¶€ í™•ì¸ (ê°œì„ ëœ ë¡œì§)"""
        precedent_id = (
            precedent.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸') or 
            precedent.get('precedent_id') or 
            precedent.get('prec id') or
            precedent.get('id')
        )
        
        # íŒë¡€ì¼ë ¨ë²ˆí˜¸ë¡œ ì¤‘ë³µ í™•ì¸
        if precedent_id and str(precedent_id) in self.collected_precedents:
            return True
        
        # ëŒ€ì²´ ì‹ë³„ìë¡œ í™•ì¸ (ë” ì—„ê²©í•œ ì¡°ê±´)
        case_number = precedent.get('ì‚¬ê±´ë²ˆí˜¸', '')
        case_name = precedent.get('ì‚¬ê±´ëª…', '')
        decision_date = precedent.get('ì„ ê³ ì¼ì', '')
        
        # ì‚¬ê±´ë²ˆí˜¸, ì‚¬ê±´ëª…, ì„ ê³ ì¼ìê°€ ëª¨ë‘ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬
        if case_number and case_name and decision_date:
            alternative_id = f"{case_number}_{case_name}_{decision_date}"
            if alternative_id in self.collected_precedents:
                return True
        
        return False
    
    def _mark_precedent_collected(self, precedent: Dict[str, Any]):
        """íŒë¡€ë¥¼ ìˆ˜ì§‘ë¨ìœ¼ë¡œ í‘œì‹œ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
        precedent_id = (
            precedent.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸') or 
            precedent.get('precedent_id') or 
            precedent.get('prec id') or
            precedent.get('id')
        )
        
        if precedent_id:
            self.collected_precedents.add(str(precedent_id))
        
        # ëŒ€ì²´ ì‹ë³„ìë¡œë„ ì €ì¥
        case_number = precedent.get('ì‚¬ê±´ë²ˆí˜¸', '')
        case_name = precedent.get('ì‚¬ê±´ëª…', '')
        decision_date = precedent.get('ì„ ê³ ì¼ì', '')
        
        if case_number and case_name and decision_date:
            alternative_id = f"{case_number}_{case_name}_{decision_date}"
            self.collected_precedents.add(alternative_id)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ ë° ìµœì í™”
        self._check_memory_usage()
    
    def _check_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ ë° ìµœì í™”"""
        if len(self.collected_precedents) > self.max_memory_precedents:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {len(self.collected_precedents):,}ê±´ > {self.max_memory_precedents:,}ê±´")
            logger.info("ğŸ”„ ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•´ ì¤‘ë³µ ë°ì´í„° ì¼ë¶€ ì •ë¦¬ ì¤‘...")
            
            # ì˜¤ë˜ëœ ë°ì´í„° ì¼ë¶€ ì œê±° (ìµœì‹  ë°ì´í„° ìš°ì„  ë³´ì¡´)
            items_to_remove = len(self.collected_precedents) - self.max_memory_precedents
            items_list = list(self.collected_precedents)
            
            # íŒë¡€ì¼ë ¨ë²ˆí˜¸ëŠ” ë³´ì¡´í•˜ê³  ëŒ€ì²´ ì‹ë³„ìë¶€í„° ì œê±°
            precedent_ids = [item for item in items_list if item.isdigit()]
            alternative_ids = [item for item in items_list if not item.isdigit()]
            
            # ëŒ€ì²´ ì‹ë³„ìë¶€í„° ì œê±°
            removed_count = 0
            for alt_id in alternative_ids[:items_to_remove]:
                self.collected_precedents.discard(alt_id)
                removed_count += 1
            
            # ì—¬ì „íˆ ì´ˆê³¼í•˜ë©´ íŒë¡€ì¼ë ¨ë²ˆí˜¸ë„ ì œê±°
            if len(self.collected_precedents) > self.max_memory_precedents:
                remaining_to_remove = len(self.collected_precedents) - self.max_memory_precedents
                for prec_id in precedent_ids[:remaining_to_remove]:
                    self.collected_precedents.discard(prec_id)
                    removed_count += 1
            
            logger.info(f"âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {removed_count:,}ê±´ ì œê±°, í˜„ì¬ {len(self.collected_precedents):,}ê±´ ë³´ê´€")
    
    def _create_precedent_data(self, raw_data: Dict[str, Any]) -> Optional[PrecedentData]:
        """ì›ì‹œ ë°ì´í„°ì—ì„œ PrecedentData ê°ì²´ ìƒì„±"""
        try:
            # íŒë¡€ ID ì¶”ì¶œ
            precedent_id = (
                raw_data.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸') or 
                raw_data.get('precedent_id') or 
                raw_data.get('prec id') or
                raw_data.get('id')
            )
            
            if not precedent_id:
                case_number = raw_data.get('ì‚¬ê±´ë²ˆí˜¸', '')
                case_name = raw_data.get('ì‚¬ê±´ëª…', '')
                if case_name:
                    precedent_id = f"{case_number}_{case_name}"
                else:
                    precedent_id = f"case_{case_number}"
            
            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            category = self._categorize_precedent(raw_data)
            
            # PrecedentData ê°ì²´ ìƒì„±
            precedent_data = PrecedentData(
                precedent_id=str(precedent_id),
                case_name=raw_data.get('ì‚¬ê±´ëª…', ''),
                case_number=raw_data.get('ì‚¬ê±´ë²ˆí˜¸', ''),
                court=COURT_CODES.get(raw_data.get('ë²•ì›ì½”ë“œ', ''), ''),
                case_type=CASE_TYPE_CODES.get(raw_data.get('ì‚¬ê±´ìœ í˜•ì½”ë“œ', ''), ''),
                decision_date=raw_data.get('íŒê²°ì¼ì', '') or raw_data.get('ì„ ê³ ì¼ì', ''),
                category=category,
                raw_data=raw_data
            )
            
            return precedent_data
            
        except Exception as e:
            logger.error(f"PrecedentData ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _create_precedent_data_with_detail(self, raw_data: Dict[str, Any]) -> Optional[PrecedentData]:
        """ì›ì‹œ ë°ì´í„°ì—ì„œ PrecedentData ê°ì²´ ìƒì„± (íŒë¡€ë³¸ë¬¸ í¬í•¨)"""
        try:
            # ê¸°ë³¸ PrecedentData ìƒì„±
            precedent_data = self._create_precedent_data(raw_data)
            if not precedent_data:
                return None
            
            # íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)
            precedent_id = raw_data.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸')
            if precedent_id:
                logger.info(f"ğŸ” íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {raw_data.get('ì‚¬ê±´ëª…', 'Unknown')} (ID: {precedent_id})")
                detail_info = self._collect_precedent_detail_with_retry(precedent_id)
                precedent_data.detail_info = detail_info
                logger.info(f"âœ… íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {raw_data.get('ì‚¬ê±´ëª…', 'Unknown')} (ID: {precedent_id})")
            else:
                logger.warning(f"âš ï¸ íŒë¡€ì¼ë ¨ë²ˆí˜¸ê°€ ì—†ì–´ íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ë¶ˆê°€: {raw_data.get('ì‚¬ê±´ëª…', 'Unknown')}")
                precedent_data.detail_info = {}
            
            return precedent_data
            
        except Exception as e:
            logger.error(f"PrecedentData ìƒì„± ì‹¤íŒ¨ (íŒë¡€ë³¸ë¬¸ í¬í•¨): {e}")
            return None
    
    def _collect_precedent_detail_with_retry(self, precedent_id: str, max_retries: int = 3) -> Dict[str, Any]:
        """íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)"""
        import time
        
        for attempt in range(max_retries):
            try:
                logger.debug(f"íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œë„ {attempt + 1}/{max_retries}: {precedent_id}")
                
                # API ìš”ì²­ ê°„ ì§€ì—° (API ë¶€í•˜ ë°©ì§€)
                if attempt > 0:
                    delay = min(2 ** attempt, 10)  # ì§€ìˆ˜ ë°±ì˜¤í”„, ìµœëŒ€ 10ì´ˆ
                    logger.info(f"API ì¬ì‹œë„ ì „ {delay}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(delay)
                else:
                    # ì²« ë²ˆì§¸ ìš”ì²­ë„ 0.5ì´ˆ ì§€ì—° (API ë¶€í•˜ ë°©ì§€, ì†ë„ ê°œì„ )
                    time.sleep(0.5)
                
                detail_response = self.client.get_precedent_detail(precedent_id=precedent_id)
                
                if detail_response:
                    # íŒë¡€ë³¸ë¬¸ ì •ë³´ ì¶”ì¶œ
                    detail_info = self._extract_precedent_detail(detail_response)
                    logger.info(f"âœ… íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {precedent_id}")
                    return detail_info
                else:
                    logger.warning(f"íŒë¡€ë³¸ë¬¸ API ì‘ë‹µ ì—†ìŒ (ì‹œë„ {attempt + 1}/{max_retries}): {precedent_id}")
                    
            except Exception as e:
                logger.warning(f"íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {precedent_id} - {e}")
                
                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ê³„ì†
                if attempt < max_retries - 1:
                    continue
                else:
                    logger.error(f"íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ìµœì¢… ì‹¤íŒ¨: {precedent_id} - {e}")
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        logger.error(f"íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ì „ ì‹¤íŒ¨: {precedent_id} (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼)")
        return {'ì˜¤ë¥˜': f'íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (ì¬ì‹œë„ {max_retries}íšŒ ì´ˆê³¼)', 'íŒë¡€ì¼ë ¨ë²ˆí˜¸': precedent_id}
    
    def _extract_precedent_detail(self, detail_response: Dict[str, Any]) -> Dict[str, Any]:
        """íŒë¡€ ìƒì„¸ ì‘ë‹µì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            extracted_info = {}
            
            # ë‹¤ì–‘í•œ API ì‘ë‹µ êµ¬ì¡° ì²˜ë¦¬
            if 'PrecService' in detail_response:
                prec_service = detail_response['PrecService']
                logger.debug(f"PrecService êµ¬ì¡° ë°œê²¬: {type(prec_service)}")
                
                # ì‹¤ì œ API ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                # PrecServiceì— ì§ì ‘ íŒë¡€ ì •ë³´ê°€ ìˆìŒ (ë°°ì—´ì´ ì•„ë‹˜)
                if isinstance(prec_service, dict):
                    # ì£¼ìš” ì •ë³´ ì¶”ì¶œ
                    extracted_info = {
                        'íŒë¡€ì¼ë ¨ë²ˆí˜¸': prec_service.get('íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸', ''),
                        'ì‚¬ê±´ëª…': prec_service.get('ì‚¬ê±´ëª…', ''),
                        'ì‚¬ê±´ë²ˆí˜¸': prec_service.get('ì‚¬ê±´ë²ˆí˜¸', ''),
                        'ë²•ì›ëª…': prec_service.get('ë²•ì›ëª…', ''),
                        'ì„ ê³ ì¼ì': prec_service.get('ì„ ê³ ì¼ì', ''),
                        'íŒê²°ìœ í˜•': prec_service.get('íŒê²°ìœ í˜•', ''),
                        'ì‚¬ê±´ìœ í˜•': prec_service.get('ì‚¬ê±´ì¢…ë¥˜ëª…', ''),
                        'íŒê²°ìš”ì§€': prec_service.get('íŒê²°ìš”ì§€', ''),
                        'íŒì‹œì‚¬í•­': prec_service.get('íŒì‹œì‚¬í•­', ''),
                        'ì°¸ì¡°ì¡°ë¬¸': prec_service.get('ì°¸ì¡°ì¡°ë¬¸', ''),
                        'ì°¸ì¡°íŒë¡€': prec_service.get('ì°¸ì¡°íŒë¡€', ''),
                        'íŒë¡€ë‚´ìš©': prec_service.get('íŒë¡€ë‚´ìš©', ''),
                        'ì‚¬ê±´ì¢…ë¥˜ì½”ë“œ': prec_service.get('ì‚¬ê±´ì¢…ë¥˜ì½”ë“œ', ''),
                        'ë²•ì›ì¢…ë¥˜ì½”ë“œ': prec_service.get('ë²•ì›ì¢…ë¥˜ì½”ë“œ', ''),
                        'ì„ ê³ ': prec_service.get('ì„ ê³ ', '')
                    }
                    
                    # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì¤‘ë³µ ì œê±°)
                    extracted_info['ìˆ˜ì§‘ì¼ì‹œ'] = datetime.now().isoformat()
                    # API_ì‘ë‹µ_ì›ë³¸ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ (ì¤‘ë³µ ë°ì´í„° ë°©ì§€)
                    
                    logger.info(f"íŒë¡€ë³¸ë¬¸ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {extracted_info.get('ì‚¬ê±´ëª…', 'Unknown')}")
                else:
                    logger.warning(f"PrecServiceê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(prec_service)}")
                    
            elif 'Law' in detail_response:
                # Law êµ¬ì¡° ì²˜ë¦¬ (êµ­ì„¸ì²­ íŒë¡€ ë“±)
                law_data = detail_response['Law']
                logger.debug(f"Law êµ¬ì¡° ë°œê²¬ (êµ­ì„¸ì²­ íŒë¡€): {type(law_data)}")
                
                if isinstance(law_data, dict):
                    # Law êµ¬ì¡°ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„
                    extracted_info = {
                        'íŒë¡€ì¼ë ¨ë²ˆí˜¸': law_data.get('íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸', ''),
                        'ì‚¬ê±´ëª…': law_data.get('ì‚¬ê±´ëª…', ''),
                        'ì‚¬ê±´ë²ˆí˜¸': law_data.get('ì‚¬ê±´ë²ˆí˜¸', ''),
                        'ë²•ì›ëª…': law_data.get('ë²•ì›ëª…', ''),
                        'ì„ ê³ ì¼ì': law_data.get('ì„ ê³ ì¼ì', ''),
                        'íŒê²°ìœ í˜•': law_data.get('íŒê²°ìœ í˜•', ''),
                        'ì‚¬ê±´ìœ í˜•': law_data.get('ì‚¬ê±´ì¢…ë¥˜ëª…', ''),
                        'íŒê²°ìš”ì§€': law_data.get('íŒê²°ìš”ì§€', ''),
                        'íŒì‹œì‚¬í•­': law_data.get('íŒì‹œì‚¬í•­', ''),
                        'ì°¸ì¡°ì¡°ë¬¸': law_data.get('ì°¸ì¡°ì¡°ë¬¸', ''),
                        'ì°¸ì¡°íŒë¡€': law_data.get('ì°¸ì¡°íŒë¡€', ''),
                        'íŒë¡€ë‚´ìš©': law_data.get('íŒë¡€ë‚´ìš©', ''),
                        'ì‚¬ê±´ì¢…ë¥˜ì½”ë“œ': law_data.get('ì‚¬ê±´ì¢…ë¥˜ì½”ë“œ', ''),
                        'ë²•ì›ì¢…ë¥˜ì½”ë“œ': law_data.get('ë²•ì›ì¢…ë¥˜ì½”ë“œ', ''),
                        'ì„ ê³ ': law_data.get('ì„ ê³ ', ''),
                        'ë°ì´í„°ì¶œì²˜': 'êµ­ì„¸ì²­'
                    }
                    
                    # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì¤‘ë³µ ì œê±°)
                    extracted_info['ìˆ˜ì§‘ì¼ì‹œ'] = datetime.now().isoformat()
                    # API_ì‘ë‹µ_ì›ë³¸ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ (ì¤‘ë³µ ë°ì´í„° ë°©ì§€)
                    
                    logger.info(f"íŒë¡€ë³¸ë¬¸ ì •ë³´ ì¶”ì¶œ ì„±ê³µ (êµ­ì„¸ì²­ íŒë¡€): {extracted_info.get('ì‚¬ê±´ëª…', 'Unknown')}")
                    
                elif isinstance(law_data, str):
                    extracted_info = {
                        'íŒë¡€ì¼ë ¨ë²ˆí˜¸': '',
                        'ì‚¬ê±´ëª…': '',
                        'ì‚¬ê±´ë²ˆí˜¸': '',
                        'ë²•ì›ëª…': '',
                        'ì„ ê³ ì¼ì': '',
                        'íŒê²°ìœ í˜•': '',
                        'ì‚¬ê±´ìœ í˜•': '',
                        'íŒê²°ìš”ì§€': '',
                        'íŒì‹œì‚¬í•­': '',
                        'ì°¸ì¡°ì¡°ë¬¸': '',
                        'ì°¸ì¡°íŒë¡€': '',
                        'íŒë¡€ë‚´ìš©': law_data[:1000] + '...' if len(law_data) > 1000 else law_data,  # HTML ë‚´ìš© ì¼ë¶€ ì €ì¥
                        'ì‚¬ê±´ì¢…ë¥˜ì½”ë“œ': '',
                        'ë²•ì›ì¢…ë¥˜ì½”ë“œ': '',
                        'ì„ ê³ ': '',
                        'ë°ì´í„°ì¶œì²˜': 'êµ­ì„¸ì²­ (HTML í˜•íƒœ)',
                        'ìˆ˜ì§‘ì¼ì‹œ': datetime.now().isoformat(),
                        'ì˜¤ë¥˜': 'HTML í˜•íƒœì˜ ì‘ë‹µìœ¼ë¡œ JSON íŒŒì‹± ë¶ˆê°€'
                    }
                    
                    logger.info(f"íŒë¡€ë³¸ë¬¸ ì •ë³´ ì¶”ì¶œ ì„±ê³µ (êµ­ì„¸ì²­ HTML): ê¸¸ì´ {len(law_data)}")
                    
                else:
                    logger.warning(f"Lawê°€ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…: {type(law_data)}")
                    extracted_info = {
                        'ìˆ˜ì§‘ì¼ì‹œ': datetime.now().isoformat(),
                        'ì˜¤ë¥˜': f"Law íƒ€ì… ì˜¤ë¥˜: {type(law_data)}"
                    }
                    
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ êµ¬ì¡°: {list(detail_response.keys())}")
                # ì „ì²´ ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ì €ì¥ (ì¤‘ë³µ ì œê±°)
                extracted_info = {
                    'ìˆ˜ì§‘ì¼ì‹œ': datetime.now().isoformat(),
                    'ì˜¤ë¥˜': f"ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ êµ¬ì¡°: {list(detail_response.keys())}"
                }
                        
            return extracted_info
            
        except Exception as e:
            logger.error(f"íŒë¡€ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {'ì˜¤ë¥˜': str(e)}
    
    def _categorize_precedent(self, precedent: Dict[str, Any]) -> PrecedentCategory:
        """íŒë¡€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì‚¬ê±´ìœ í˜•ì½”ë“œ ê¸°ë°˜)"""
        case_type_code = precedent.get('ì‚¬ê±´ìœ í˜•ì½”ë“œ', '')
        
        # ì‚¬ê±´ìœ í˜•ì½”ë“œ ê¸°ë°˜ ë¶„ë¥˜
        case_type_mapping = {
            '01': PrecedentCategory.CIVIL_CONTRACT,
            '02': PrecedentCategory.CRIMINAL,
            '03': PrecedentCategory.ADMINISTRATIVE,
            '04': PrecedentCategory.CIVIL_FAMILY,
            '05': PrecedentCategory.OTHER
        }
        
        return case_type_mapping.get(case_type_code, PrecedentCategory.OTHER)
    
    def _random_delay(self, min_seconds: float = None, max_seconds: float = None):
        """API ìš”ì²­ ê°„ ëœë¤ ì§€ì—° - ì‚¬ìš©ì ì„¤ì • ê°„ê²© ì‚¬ìš©"""
        if min_seconds is None:
            min_seconds = max(0.1, self.request_interval_base - self.request_interval_range)
        if max_seconds is None:
            max_seconds = self.request_interval_base + self.request_interval_range
        
        delay = random.uniform(min_seconds, max_seconds)
        logger.debug(f"API ìš”ì²­ ê°„ {delay:.2f}ì´ˆ ëŒ€ê¸°...")
        time.sleep(delay)
    
    def _collect_by_date_range(self, start_date: str, end_date: str, max_count: int, 
                              output_dir: Path, base_params: Dict[str, Any]) -> List[PrecedentData]:
        """ë‚ ì§œ ë²”ìœ„ë¡œ íŒë¡€ ìˆ˜ì§‘"""
        precedents = []
        page = 1
        consecutive_empty_pages = 0
        max_empty_pages = 3
        total_collected = 0
        total_duplicates = 0
        total_errors = 0
        
        # ë¬´ì œí•œ ëª¨ë“œ í™•ì¸
        unlimited_mode = max_count >= 999999999
        
        if unlimited_mode:
            logger.info(f"ğŸ“… ë‚ ì§œ ë²”ìœ„ {start_date} ~ {end_date} ìˆ˜ì§‘ ì‹œì‘ (ë¬´ì œí•œ ëª¨ë“œ)")
        else:
            logger.info(f"ğŸ“… ë‚ ì§œ ë²”ìœ„ {start_date} ~ {end_date} ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {max_count:,}ê±´)")
        
        logger.info("=" * 80)
        
        while (unlimited_mode or len(precedents) < max_count) and consecutive_empty_pages < max_empty_pages:
            try:
                # API íŒŒë¼ë¯¸í„° êµ¬ì„± (ì˜¬ë°”ë¥¸ prncYd íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                params = base_params.copy()
                params.update({
                    "from_date": start_date,
                    "to_date": end_date,
                    "display": 20,  # ë°°ì¹˜ í¬ê¸°ë¥¼ 100ì—ì„œ 20ìœ¼ë¡œ ì¤„ì„ (íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œ ì†ë„ ê°œì„ )
                    "page": page
                })
                
                # API ìš”ì²­ ê°„ ì§€ì—°
                if page > 1:
                    self._random_delay()
                
                # API í˜¸ì¶œ
                logger.info(f"ğŸ” í˜ì´ì§€ {page} ìš”ì²­ ì¤‘... (ë‚ ì§œ í•„í„°ë§: {start_date}~{end_date})")
                results = self.client.get_precedent_list(**params)
                
                if not results:
                    consecutive_empty_pages += 1
                    logger.warning(f"âš ï¸  í˜ì´ì§€ {page}: ê²°ê³¼ ì—†ìŒ (ì—°ì† ë¹ˆ í˜ì´ì§€: {consecutive_empty_pages}/{max_empty_pages})")
                    page += 1
                    continue
                else:
                    consecutive_empty_pages = 0
                
                # ê²°ê³¼ ì²˜ë¦¬
                new_count = 0
                duplicate_count = 0
                page_precedents = []
                
                for result in results:
                    # ë¬´ì œí•œ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê±´ìˆ˜ ì œí•œ í™•ì¸
                    if not unlimited_mode and len(precedents) >= max_count:
                        break
                    
                    # ì¤‘ë³µ í™•ì¸ (ê°œì„ ëœ ë¡œì§)
                    if self._is_duplicate_precedent(result):
                        duplicate_count += 1
                        self.stats.duplicate_count += 1
                        continue
                    
                    # PrecedentData ê°ì²´ ìƒì„± (íŒë¡€ë³¸ë¬¸ í¬í•¨ ì—¬ë¶€ì— ë”°ë¼)
                    if self.include_details:
                        logger.info(f"ğŸ“„ íŒë¡€ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘: {result.get('ì‚¬ê±´ëª…', 'Unknown')} (ID: {result.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸', 'N/A')})")
                        precedent_data = self._create_precedent_data_with_detail(result)
                    else:
                        precedent_data = self._create_precedent_data(result)
                    
                    if not precedent_data:
                        self.stats.failed_count += 1
                        total_errors += 1
                        continue
                    
                    # ì‹ ê·œ íŒë¡€ ì¶”ê°€
                    precedents.append(precedent_data)
                    page_precedents.append(precedent_data)
                    self._mark_precedent_collected(result)
                    new_count += 1
                
                # í˜ì´ì§€ë³„ ì¦‰ì‹œ ì €ì¥
                if page_precedents:
                    self._save_page_precedents(page_precedents, output_dir, page, start_date, end_date)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                total_collected += new_count
                total_duplicates += duplicate_count
                
                # ì§„í–‰ìƒí™© ë¡œê·¸
                progress_percent = (len(precedents) / max_count * 100) if not unlimited_mode else 0
                logger.info(f"âœ… í˜ì´ì§€ {page} ì™„ë£Œ: ì‹ ê·œ {new_count:,}ê±´, ì¤‘ë³µ {duplicate_count:,}ê±´")
                logger.info(f"ğŸ“Š ëˆ„ì  í˜„í™©: ì´ {len(precedents):,}ê±´ ìˆ˜ì§‘ (ì‹ ê·œ: {total_collected:,}, ì¤‘ë³µ: {total_duplicates:,}, ì˜¤ë¥˜: {total_errors:,})")
                if not unlimited_mode:
                    logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {progress_percent:.1f}% ({len(precedents):,}/{max_count:,}ê±´)")
                logger.info("-" * 60)
                
                page += 1
                
                # API ìš”ì²­ ì œí•œ í™•ì¸
                if self._check_api_limits():
                    break
                
            except Exception as e:
                total_errors += 1
                self.error_count += 1
                logger.error(f"âŒ í˜ì´ì§€ {page} ì˜¤ë¥˜ ë°œìƒ: {e}")
                logger.error(f"ğŸ”„ ì˜¤ë¥˜ ì¹´ìš´íŠ¸: {self.error_count}/{self.max_errors}")
                
                if self.error_count >= self.max_errors:
                    logger.error("ğŸ›‘ ìµœëŒ€ ì˜¤ë¥˜ ìˆ˜ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
                
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥
                if precedents:
                    logger.info(f"ğŸ’¾ ì˜¤ë¥˜ ë°œìƒ ì „ê¹Œì§€ ìˆ˜ì§‘ëœ {len(precedents):,}ê±´ ì €ì¥ ì¤‘...")
                    self._save_page_precedents(precedents, output_dir, page, start_date, end_date)
                
                continue
        
        # ìµœì¢… í†µê³„ ë¡œê·¸
        logger.info("=" * 80)
        logger.info(f"ğŸ‰ ë‚ ì§œ ë²”ìœ„ {start_date} ~ {end_date} ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ìµœì¢… í†µê³„: ì´ {len(precedents):,}ê±´ ìˆ˜ì§‘ (ì‹ ê·œ: {total_collected:,}, ì¤‘ë³µ: {total_duplicates:,}, ì˜¤ë¥˜: {total_errors:,})")
        logger.info("=" * 80)
        
        return precedents
    
    def _save_page_precedents(self, precedents: List[PrecedentData], output_dir: Path, 
                             page: int, start_date: str, end_date: str):
        """í˜ì´ì§€ë³„ íŒë¡€ ì¦‰ì‹œ ì €ì¥ (íŒë¡€ì¼ë ¨ë²ˆí˜¸ ê¸°ì¤€)"""
        if not precedents:
            return
        
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”
            by_category = {}
            for precedent in precedents:
                category = precedent.category.value
                if category not in by_category:
                    by_category[category] = []
                by_category[category].append(precedent)
            
            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            saved_files = []
            
            for category, category_precedents in by_category.items():
                # íŒë¡€ì¼ë ¨ë²ˆí˜¸ ë²”ìœ„ë¡œ íŒŒì¼ëª… ìƒì„±
                serial_numbers = [p.raw_data.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸', '') for p in category_precedents]
                serial_numbers = [s for s in serial_numbers if s]  # ë¹ˆ ê°’ ì œê±°
                
                if serial_numbers:
                    # íŒë¡€ì¼ë ¨ë²ˆí˜¸ ì •ë ¬
                    serial_numbers.sort()
                    start_serial = serial_numbers[0]
                    end_serial = serial_numbers[-1]
                    
                    # íŒŒì¼ëª… ìƒì„± (ì¹´í…Œê³ ë¦¬ ì œê±°)
                    filename = f"page_{page:03d}_{start_serial}-{end_serial}_{len(category_precedents)}ê±´_{timestamp}.json"
                else:
                    # íŒë¡€ì¼ë ¨ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì²´ íŒŒì¼ëª…
                    filename = f"page_{page:03d}_{len(category_precedents)}ê±´_{timestamp}.json"
                
                filepath = output_dir / filename
                
                # íŒë¡€ ë°ì´í„° êµ¬ì„± (ê¸°ë³¸ ì •ë³´ + ìƒì„¸ ì •ë³´)
                precedents_data = []
                for p in category_precedents:
                    precedent_data = p.raw_data.copy()
                    if p.detail_info:
                        precedent_data['detail_info'] = p.detail_info
                    precedents_data.append(precedent_data)
                
                batch_data = {
                    'metadata': {
                        'page': page,
                        'category': category,
                        'count': len(category_precedents),
                        'date_range': f"{start_date}~{end_date}",
                        'saved_at': datetime.now().isoformat(),
                        'batch_id': f"page_{page}_{timestamp}",
                        'serial_number_range': f"{start_serial}-{end_serial}" if serial_numbers else None,
                        'include_details': self.include_details
                    },
                    'precedents': precedents_data
                }
                
                with open(filepath, 'w', encoding='utf-8', newline='\n') as f:
                    json.dump(batch_data, f, ensure_ascii=False, indent=2)
                
                saved_files.append(filepath)
                logger.info(f"ğŸ’¾ í˜ì´ì§€ {page} ì €ì¥ ì™„ë£Œ: {category} ì¹´í…Œê³ ë¦¬ {len(category_precedents):,}ê±´ -> {filename}")
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats.saved_count += len(precedents)
            
            logger.info(f"ğŸ“ í˜ì´ì§€ {page} ì €ì¥ ì™„ë£Œ: ì´ {len(saved_files):,}ê°œ íŒŒì¼")
            
        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ {page} ì €ì¥ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
    
    def _save_batch_precedents(self, output_dir: Path):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ íŒë¡€ ì €ì¥ (íŒë¡€ì¼ë ¨ë²ˆí˜¸ ê¸°ì¤€)"""
        if not self.pending_precedents:
            return
        
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”
            by_category = {}
            for precedent in self.pending_precedents:
                category = precedent.category.value
                if category not in by_category:
                    by_category[category] = []
                by_category[category].append(precedent)
            
            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            saved_files = []
            
            for category, precedents in by_category.items():
                # íŒë¡€ì¼ë ¨ë²ˆí˜¸ ë²”ìœ„ë¡œ íŒŒì¼ëª… ìƒì„±
                serial_numbers = [p.raw_data.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸', '') for p in precedents]
                serial_numbers = [s for s in serial_numbers if s]  # ë¹ˆ ê°’ ì œê±°
                
                if serial_numbers:
                    # íŒë¡€ì¼ë ¨ë²ˆí˜¸ ì •ë ¬
                    serial_numbers.sort()
                    start_serial = serial_numbers[0]
                    end_serial = serial_numbers[-1]
                    
                    # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
                    safe_category = category.replace('_', '-')
                    filename = f"batch_{safe_category}_{start_serial}-{end_serial}_{len(precedents)}ê±´_{timestamp}.json"
                else:
                    # íŒë¡€ì¼ë ¨ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì²´ íŒŒì¼ëª…
                    safe_category = category.replace('_', '-')
                    filename = f"batch_{safe_category}_{len(precedents)}ê±´_{timestamp}.json"
                
                filepath = output_dir / filename
                
                batch_data = {
                    'metadata': {
                        'category': category,
                        'count': len(precedents),
                        'saved_at': datetime.now().isoformat(),
                        'batch_id': timestamp,
                        'serial_number_range': f"{start_serial}-{end_serial}" if serial_numbers else None
                    },
                    'precedents': [p.raw_data for p in precedents]
                }
                
                with open(filepath, 'w', encoding='utf-8', newline='\n') as f:
                    json.dump(batch_data, f, ensure_ascii=False, indent=2)
                
                saved_files.append(filepath)
                logger.info(f"ğŸ’¾ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {category} ì¹´í…Œê³ ë¦¬ {len(precedents):,}ê±´ -> {filename}")
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats.saved_count += len(self.pending_precedents)
            
            # ì„ì‹œ ì €ì¥ì†Œ ì´ˆê¸°í™”
            self.pending_precedents = []
            
            logger.info(f"ğŸ“ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: ì´ {len(saved_files):,}ê°œ íŒŒì¼")
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
    
    def _check_api_limits(self) -> bool:
        """API ìš”ì²­ ì œí•œ í™•ì¸"""
        try:
            stats = self.client.get_request_stats()
            remaining = stats.get('remaining_requests', 0)
            if remaining < 100:
                logger.warning(f"API ìš”ì²­ í•œë„ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‚¨ì€ ìš”ì²­: {remaining}íšŒ")
                return True
        except Exception as e:
            logger.warning(f"API ìš”ì²­ ì œí•œ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False
    
    def collect_by_yearly_strategy(self, years: List[int], target_per_year: int) -> Dict[str, Any]:
        """ì—°ë„ë³„ ìˆ˜ì§‘ ì „ëµ"""
        logger.info(f"ì—°ë„ë³„ ìˆ˜ì§‘ ì‹œì‘: {years}ë…„, ì—°ê°„ ëª©í‘œ {target_per_year}ê±´")
        
        all_precedents = []
        collection_summary = {
            'strategy': 'yearly',
            'years': years,
            'target_per_year': target_per_year,
            'collected_by_year': {},
            'total_collected': 0,
            'start_time': datetime.now().isoformat()
        }
        
        for year in years:
            if self.stats.collected_count >= self.stats.target_count:
                break
            
            year_str = str(year)
            start_date = f"{year_str}0101"
            end_date = f"{year_str}1231"
            
            # í•´ë‹¹ ì—°ë„ì˜ ê¸°ì¡´ ë°ì´í„°ë§Œ ì¤‘ë³µ í™•ì¸í•˜ë„ë¡ ìˆ˜ì§‘ê¸° ì¬ì´ˆê¸°í™”
            logger.info(f"ğŸ”„ {year}ë…„ ì¤‘ë³µ í™•ì¸ì„ ìœ„í•œ ë°ì´í„° ë¡œë“œ ì¤‘...")
            self.collected_precedents.clear()  # ê¸°ì¡´ ì¤‘ë³µ ë°ì´í„° ì´ˆê¸°í™”
            self._load_existing_data(target_year=year)  # í•´ë‹¹ ì—°ë„ë§Œ ë¡œë“œ
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = self._create_output_subdir(DateCollectionStrategy.YEARLY, year_str)
            
            logger.info(f"ğŸ“Š {year}ë…„ íŒë¡€ ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ: {target_per_year}ê±´)")
            
            year_precedents = self._collect_by_date_range(
                start_date, end_date, target_per_year, output_dir,
                {"search": 1, "sort": "ddes"}
            )
            
            all_precedents.extend(year_precedents)
            collection_summary['collected_by_year'][year_str] = len(year_precedents)
            
            logger.info(f"âœ… {year}ë…„ ì™„ë£Œ: {len(year_precedents)}ê±´ ìˆ˜ì§‘ (ëˆ„ì : {len(all_precedents)}ê±´)")
        
        # ìµœì¢… ë°°ì¹˜ ì €ì¥
        if self.pending_precedents:
            final_output_dir = self._create_output_subdir(DateCollectionStrategy.YEARLY, "final")
            self._save_batch_precedents(final_output_dir)
        
        collection_summary['total_collected'] = len(all_precedents)
        collection_summary['end_time'] = datetime.now().isoformat()
        
        # ìˆ˜ì§‘ ìš”ì•½ ì €ì¥
        summary_file = self.base_output_dir / f"yearly_collection_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(collection_summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ì—°ë„ë³„ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_precedents)}ê±´")
        return collection_summary
    
    def collect_by_quarterly_strategy(self, quarters: List[Tuple[str, str, str]], target_per_quarter: int) -> Dict[str, Any]:
        """ë¶„ê¸°ë³„ ìˆ˜ì§‘ ì „ëµ"""
        logger.info(f"ë¶„ê¸°ë³„ ìˆ˜ì§‘ ì‹œì‘: {len(quarters)}ê°œ ë¶„ê¸°, ë¶„ê¸°ë‹¹ ëª©í‘œ {target_per_quarter}ê±´")
        
        all_precedents = []
        collection_summary = {
            'strategy': 'quarterly',
            'quarters': [q[0] for q in quarters],
            'target_per_quarter': target_per_quarter,
            'collected_by_quarter': {},
            'total_collected': 0,
            'start_time': datetime.now().isoformat()
        }
        
        for quarter_name, start_date, end_date in quarters:
            if self.stats.collected_count >= self.stats.target_count:
                break
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = self._create_output_subdir(DateCollectionStrategy.QUARTERLY, quarter_name)
            
            logger.info(f"ğŸ“Š {quarter_name} íŒë¡€ ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ: {target_per_quarter}ê±´)")
            
            quarter_precedents = self._collect_by_date_range(
                start_date, end_date, target_per_quarter, output_dir,
                {"search": 1, "sort": "ddes"}
            )
            
            all_precedents.extend(quarter_precedents)
            collection_summary['collected_by_quarter'][quarter_name] = len(quarter_precedents)
            
            logger.info(f"âœ… {quarter_name} ì™„ë£Œ: {len(quarter_precedents)}ê±´ ìˆ˜ì§‘ (ëˆ„ì : {len(all_precedents)}ê±´)")
        
        # ìµœì¢… ë°°ì¹˜ ì €ì¥
        if self.pending_precedents:
            final_output_dir = self._create_output_subdir(DateCollectionStrategy.QUARTERLY, "final")
            self._save_batch_precedents(final_output_dir)
        
        collection_summary['total_collected'] = len(all_precedents)
        collection_summary['end_time'] = datetime.now().isoformat()
        
        # ìˆ˜ì§‘ ìš”ì•½ ì €ì¥
        summary_file = self.base_output_dir / f"quarterly_collection_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(collection_summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë¶„ê¸°ë³„ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_precedents)}ê±´")
        return collection_summary
    
    def collect_by_monthly_strategy(self, months: List[Tuple[str, str, str]], target_per_month: int) -> Dict[str, Any]:
        """ì›”ë³„ ìˆ˜ì§‘ ì „ëµ"""
        logger.info(f"ì›”ë³„ ìˆ˜ì§‘ ì‹œì‘: {len(months)}ê°œ ì›”, ì›”ê°„ ëª©í‘œ {target_per_month}ê±´")
        
        all_precedents = []
        collection_summary = {
            'strategy': 'monthly',
            'months': [m[0] for m in months],
            'target_per_month': target_per_month,
            'collected_by_month': {},
            'total_collected': 0,
            'start_time': datetime.now().isoformat()
        }
        
        for month_name, start_date, end_date in months:
            if self.stats.collected_count >= self.stats.target_count:
                break
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = self._create_output_subdir(DateCollectionStrategy.MONTHLY, month_name)
            
            logger.info(f"ğŸ“Š {month_name} íŒë¡€ ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ: {target_per_month}ê±´)")
            
            month_precedents = self._collect_by_date_range(
                start_date, end_date, target_per_month, output_dir,
                {"search": 1, "sort": "ddes"}
            )
            
            all_precedents.extend(month_precedents)
            collection_summary['collected_by_month'][month_name] = len(month_precedents)
            
            logger.info(f"âœ… {month_name} ì™„ë£Œ: {len(month_precedents)}ê±´ ìˆ˜ì§‘ (ëˆ„ì : {len(all_precedents)}ê±´)")
        
        # ìµœì¢… ë°°ì¹˜ ì €ì¥
        if self.pending_precedents:
            final_output_dir = self._create_output_subdir(DateCollectionStrategy.MONTHLY, "final")
            self._save_batch_precedents(final_output_dir)
        
        collection_summary['total_collected'] = len(all_precedents)
        collection_summary['end_time'] = datetime.now().isoformat()
        
        # ìˆ˜ì§‘ ìš”ì•½ ì €ì¥
        summary_file = self.base_output_dir / f"monthly_collection_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(collection_summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ì›”ë³„ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_precedents)}ê±´")
        return collection_summary
    
    def collect_by_weekly_strategy(self, weeks: List[Tuple[str, str, str]], target_per_week: int) -> Dict[str, Any]:
        """ì£¼ë³„ ìˆ˜ì§‘ ì „ëµ"""
        logger.info(f"ì£¼ë³„ ìˆ˜ì§‘ ì‹œì‘: {len(weeks)}ê°œ ì£¼, ì£¼ê°„ ëª©í‘œ {target_per_week}ê±´")
        
        all_precedents = []
        collection_summary = {
            'strategy': 'weekly',
            'weeks': [w[0] for w in weeks],
            'target_per_week': target_per_week,
            'collected_by_week': {},
            'total_collected': 0,
            'start_time': datetime.now().isoformat()
        }
        
        for week_name, start_date, end_date in weeks:
            if self.stats.collected_count >= self.stats.target_count:
                break
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = self._create_output_subdir(DateCollectionStrategy.WEEKLY, week_name)
            
            logger.info(f"ğŸ“Š {week_name} íŒë¡€ ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ: {target_per_week}ê±´)")
            
            week_precedents = self._collect_by_date_range(
                start_date, end_date, target_per_week, output_dir,
                {"search": 1, "sort": "ddes"}
            )
            
            all_precedents.extend(week_precedents)
            collection_summary['collected_by_week'][week_name] = len(week_precedents)
            
            logger.info(f"âœ… {week_name} ì™„ë£Œ: {len(week_precedents)}ê±´ ìˆ˜ì§‘ (ëˆ„ì : {len(all_precedents)}ê±´)")
        
        # ìµœì¢… ë°°ì¹˜ ì €ì¥
        if self.pending_precedents:
            final_output_dir = self._create_output_subdir(DateCollectionStrategy.WEEKLY, "final")
            self._save_batch_precedents(final_output_dir)
        
        collection_summary['total_collected'] = len(all_precedents)
        collection_summary['end_time'] = datetime.now().isoformat()
        
        # ìˆ˜ì§‘ ìš”ì•½ ì €ì¥
        summary_file = self.base_output_dir / f"weekly_collection_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(collection_summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ì£¼ë³„ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_precedents)}ê±´")
        return collection_summary
    
    def generate_date_ranges(self, strategy: DateCollectionStrategy, count: int) -> List[Tuple[str, str, str]]:
        """ë‚ ì§œ ë²”ìœ„ ìƒì„±"""
        ranges = []
        current = datetime.now()
        
        if strategy == DateCollectionStrategy.YEARLY:
            for i in range(count):
                year = current.year - i
                ranges.append((f"{year}ë…„", f"{year}0101", f"{year}1231"))
        
        elif strategy == DateCollectionStrategy.QUARTERLY:
            for i in range(count):
                target_date = current - timedelta(days=90*i)
                year = target_date.year
                quarter = (target_date.month - 1) // 3 + 1
                
                if quarter == 1:
                    start_date = f"{year}0101"
                    end_date = f"{year}0331"
                elif quarter == 2:
                    start_date = f"{year}0401"
                    end_date = f"{year}0630"
                elif quarter == 3:
                    start_date = f"{year}0701"
                    end_date = f"{year}0930"
                else:
                    start_date = f"{year}1001"
                    end_date = f"{year}1231"
                
                ranges.append((f"{year}Q{quarter}", start_date, end_date))
        
        elif strategy == DateCollectionStrategy.MONTHLY:
            for i in range(count):
                target_date = current - timedelta(days=30*i)
                year = target_date.year
                month = target_date.month
                
                start_date = datetime(year, month, 1)
                if month == 12:
                    end_date = datetime(year+1, 1, 1) - timedelta(days=1)
                else:
                    end_date = datetime(year, month+1, 1) - timedelta(days=1)
                
                ranges.append((
                    f"{year}ë…„{month:02d}ì›”",
                    start_date.strftime('%Y%m%d'),
                    end_date.strftime('%Y%m%d')
                ))
        
        elif strategy == DateCollectionStrategy.WEEKLY:
            for i in range(count):
                target_date = current - timedelta(weeks=i)
                start_of_week = target_date - timedelta(days=target_date.weekday())
                end_of_week = start_of_week + timedelta(days=6)
                
                ranges.append((
                    f"{start_of_week.strftime('%Y%m%d')}ì£¼",
                    start_of_week.strftime('%Y%m%d'),
                    end_of_week.strftime('%Y%m%d')
                ))
        
        return ranges
