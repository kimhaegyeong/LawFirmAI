#!/usr/bin/env python3
"""
ë¨¸ì‹ ?¬ë‹ ê¸°ë°˜ ì¡°ë¬¸ ë¶„ë¥˜ê¸?
?¤ì œ ë²•ë¥  ë¬¸ì„œ ?¨í„´???™ìŠµ?˜ì—¬ ì¡°ë¬¸ ì°¸ì¡°?€ ?¤ì œ ì¡°ë¬¸??êµ¬ë¶„
"""

import re
import json
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import logging

# ë¡œê¹… ?¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ArticleMLClassifier:
    """ë¨¸ì‹ ?¬ë‹ ê¸°ë°˜ ì¡°ë¬¸ ë¶„ë¥˜ê¸?""
    
    def __init__(self, model_type: str = "random_forest"):
        """
        ì´ˆê¸°??
        
        Args:
            model_type: ?¬ìš©??ëª¨ë¸ ?€??("random_forest", "gradient_boosting")
        """
        self.model_type = model_type
        self.model = None
        self.vectorizer = None
        self.label_encoder = None
        self.feature_names = []
        
        # ëª¨ë¸ ? íƒ
        if model_type == "random_forest":
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                class_weight='balanced'
            )
        elif model_type == "gradient_boosting":
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            )
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
    
    def extract_features(self, content: str, position: int, article_number: str) -> Dict[str, Any]:
        """
        ì¡°ë¬¸???¹ì„±??ì¶”ì¶œ
        
        Args:
            content: ?„ì²´ ë¬¸ì„œ ?´ìš©
            position: ì¡°ë¬¸ ?„ì¹˜
            article_number: ì¡°ë¬¸ ë²ˆí˜¸
            
        Returns:
            ì¶”ì¶œ???¹ì„± ?•ì…”?ˆë¦¬
        """
        features = {}
        
        # 1. ?„ì¹˜ ê¸°ë°˜ ?¹ì„±
        features['position_ratio'] = position / len(content) if len(content) > 0 else 0
        features['is_at_start'] = 1 if position < 200 else 0
        features['is_at_end'] = 1 if position > len(content) * 0.8 else 0
        
        # 2. ë¬¸ë§¥ ê¸°ë°˜ ?¹ì„±
        context_before = content[max(0, position - 200):position]
        context_after = content[position:min(len(content), position + 200)]
        
        # ë¬¸ì¥ ???¨í„´
        features['has_sentence_end'] = 1 if re.search(r'[.!?]\s*$', context_before) else 0
        
        # ì¡°ë¬¸ ì°¸ì¡° ?¨í„´
        reference_patterns = [
            r'??d+ì¡°ì—\s*?°ë¼',
            r'??d+ì¡°ì œ\d+??,
            r'??d+ì¡°ì˜\d+',
            r'??d+ì¡?*???s*?˜í•˜??,
            r'??d+ì¡?*???s*?°ë¼',
        ]
        
        features['has_reference_pattern'] = 0
        for pattern in reference_patterns:
            if re.search(pattern, context_before):
                features['has_reference_pattern'] = 1
                break
        
        # 3. ì¡°ë¬¸ ë²ˆí˜¸ ?¹ì„±
        article_num = int(re.search(r'\d+', article_number).group()) if re.search(r'\d+', article_number) else 0
        features['article_number'] = article_num
        features['is_supplementary'] = 1 if 'ë¶€ì¹? in article_number else 0
        
        # 4. ?ìŠ¤??ê¸¸ì´ ?¹ì„±
        features['context_before_length'] = len(context_before)
        features['context_after_length'] = len(context_after)
        
        # 5. ì¡°ë¬¸ ?œëª© ? ë¬´
        title_match = re.search(r'??d+ì¡?s*\(([^)]+)\)', context_after)
        features['has_title'] = 1 if title_match else 0
        
        # 6. ?¹ìˆ˜ ë¬¸ì ?¨í„´
        features['has_parentheses'] = 1 if '(' in context_after[:50] else 0
        features['has_quotes'] = 1 if '"' in context_after[:50] or "'" in context_after[:50] else 0
        
        # 7. ë²•ë¥  ?©ì–´ ?¨í„´
        legal_terms = [
            'ë²•ë¥ ', 'ë²•ë ¹', 'ê·œì •', 'ì¡°í•­', '??, '??, 'ëª?,
            '?œí–‰', 'ê³µí¬', 'ê°œì •', '?ì?', '?œì •'
        ]
        
        features['legal_term_count'] = sum(1 for term in legal_terms if term in context_after[:100])
        
        # 8. ?«ì ?¨í„´
        features['number_count'] = len(re.findall(r'\d+', context_after[:100]))
        
        # 9. ì¡°ë¬¸ ?´ìš© ê¸¸ì´ (?¤ìŒ ì¡°ë¬¸ê¹Œì???ê±°ë¦¬)
        next_article_match = re.search(r'??d+ì¡?, content[position + 1:])
        if next_article_match:
            features['article_length'] = next_article_match.start()
        else:
            features['article_length'] = len(content) - position
        
        # 10. ë¬¸ë§¥ ë°€??(ì¡°ë¬¸ ì°¸ì¡° ë¹ˆë„)
        article_refs_in_context = len(re.findall(r'??d+ì¡?, context_before))
        features['reference_density'] = article_refs_in_context / max(len(context_before), 1) * 1000
        
        return features
    
    def prepare_training_data(self, data_dir: str) -> Tuple[List[Dict], List[str]]:
        """
        ?ˆë ¨ ?°ì´??ì¤€ë¹?
        
        Args:
            data_dir: ?°ì´???”ë ‰? ë¦¬ ê²½ë¡œ
            
        Returns:
            ?¹ì„± ë¦¬ìŠ¤?¸ì? ?ˆì´ë¸?ë¦¬ìŠ¤??
        """
        features_list = []
        labels = []
        
        data_path = Path(data_dir)
        json_files = list(data_path.glob("**/*.json"))
        
        logger.info(f"Found {len(json_files)} JSON files for training")
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if 'articles' not in data:
                    continue
                
                # ?ë³¸ ë²•ë¥  ?´ìš© ê°€?¸ì˜¤ê¸?(raw ?°ì´?°ì—??
                law_content = self._get_raw_law_content(data.get('law_id', ''))
                if not law_content:
                    continue
                
                for article in data['articles']:
                    article_number = article.get('article_number', '')
                    article_title = article.get('article_title', '')
                    
                    # ì¡°ë¬¸ ?„ì¹˜ ì°¾ê¸°
                    position = self._find_article_position(law_content, article_number)
                    if position == -1:
                        continue
                    
                    # ?¹ì„± ì¶”ì¶œ
                    features = self.extract_features(law_content, position, article_number)
                    
                    # ?ˆì´ë¸?ê²°ì • (?œëª©???ˆìœ¼ë©??¤ì œ ì¡°ë¬¸, ?†ìœ¼ë©?ì°¸ì¡°)
                    label = 'real_article' if article_title else 'reference'
                    
                    features_list.append(features)
                    labels.append(label)
                    
            except Exception as e:
                logger.warning(f"Error processing {json_file}: {e}")
                continue
        
        logger.info(f"Prepared {len(features_list)} training samples")
        return features_list, labels
    
    def _get_raw_law_content(self, law_id: str) -> str:
        """?ë³¸ ë²•ë¥  ?´ìš© ê°€?¸ì˜¤ê¸?""
        # ?¤ì œ êµ¬í˜„?ì„œ??raw ?°ì´?°ì—???´ë‹¹ ë²•ë¥  ?´ìš©??ì°¾ì•„????
        # ?¬ê¸°?œëŠ” ê°„ë‹¨???ˆì‹œë¡?êµ¬í˜„
        return ""
    
    def _find_article_position(self, content: str, article_number: str) -> int:
        """ì¡°ë¬¸ ?„ì¹˜ ì°¾ê¸°"""
        pattern = re.escape(article_number)
        match = re.search(pattern, content)
        return match.start() if match else -1
    
    def train(self, features_list: List[Dict], labels: List[str]) -> Dict[str, Any]:
        """
        ëª¨ë¸ ?ˆë ¨
        
        Args:
            features_list: ?¹ì„± ë¦¬ìŠ¤??
            labels: ?ˆì´ë¸?ë¦¬ìŠ¤??
            
        Returns:
            ?ˆë ¨ ê²°ê³¼ ?•ì…”?ˆë¦¬
        """
        # ?¹ì„±??DataFrame?¼ë¡œ ë³€??
        df = pd.DataFrame(features_list)
        
        # ?ìŠ¤???¹ì„± ì¶”ì¶œ
        text_features = []
        for features in features_list:
            # ë¬¸ë§¥ ?ìŠ¤???¹ì„± (ê°„ë‹¨???ˆì‹œ)
            text_features.append(f"article_{features.get('article_number', 0)}")
        
        # TF-IDF ë²¡í„°??
        self.vectorizer = TfidfVectorizer(max_features=1000)
        text_matrix = self.vectorizer.fit_transform(text_features)
        
        # ?˜ì¹˜ ?¹ì„±ê³??ìŠ¤???¹ì„± ê²°í•©
        numeric_features = df.drop(['article_number'], axis=1, errors='ignore')
        combined_features = np.hstack([numeric_features.values, text_matrix.toarray()])
        
        # ?ˆì´ë¸??¸ì½”??
        self.label_encoder = LabelEncoder()
        encoded_labels = self.label_encoder.fit_transform(labels)
        
        # ?ˆë ¨/?ŒìŠ¤??ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            combined_features, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels
        )
        
        # ëª¨ë¸ ?ˆë ¨
        self.model.fit(X_train, y_train)
        
        # ?ˆì¸¡ ë°??‰ê?
        y_pred = self.model.predict(X_test)
        
        # êµì°¨ ê²€ì¦?
        cv_scores = cross_val_score(self.model, combined_features, encoded_labels, cv=5)
        
        # ?¹ì„± ì¤‘ìš”??
        feature_importance = None
        if hasattr(self.model, 'feature_importances_'):
            feature_importance = self.model.feature_importances_
        
        # ê²°ê³¼ ë°˜í™˜
        results = {
            'accuracy': self.model.score(X_test, y_test),
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'classification_report': classification_report(y_test, y_pred, target_names=self.label_encoder.classes_),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
            'feature_importance': feature_importance.tolist() if feature_importance is not None else None
        }
        
        logger.info(f"Model training completed. Accuracy: {results['accuracy']:.3f}")
        logger.info(f"Cross-validation score: {results['cv_mean']:.3f} (+/- {results['cv_std']:.3f})")
        
        return results
    
    def predict(self, content: str, position: int, article_number: str) -> Tuple[str, float]:
        """
        ì¡°ë¬¸ ë¶„ë¥˜ ?ˆì¸¡
        
        Args:
            content: ?„ì²´ ë¬¸ì„œ ?´ìš©
            position: ì¡°ë¬¸ ?„ì¹˜
            article_number: ì¡°ë¬¸ ë²ˆí˜¸
            
        Returns:
            ?ˆì¸¡???´ë˜?¤ì? ? ë¢°??
        """
        if self.model is None:
            raise ValueError("Model not trained yet")
        
        # ?¹ì„± ì¶”ì¶œ
        features = self.extract_features(content, position, article_number)
        
        # DataFrame?¼ë¡œ ë³€??
        df = pd.DataFrame([features])
        
        # ?ìŠ¤???¹ì„±
        text_feature = f"article_{features.get('article_number', 0)}"
        
        # TF-IDF ë³€??
        text_matrix = self.vectorizer.transform([text_feature])
        
        # ?˜ì¹˜ ?¹ì„±ê³??ìŠ¤???¹ì„± ê²°í•©
        numeric_features = df.drop(['article_number'], axis=1, errors='ignore')
        combined_features = np.hstack([numeric_features.values, text_matrix.toarray()])
        
        # ?ˆì¸¡
        prediction = self.model.predict(combined_features)[0]
        confidence = self.model.predict_proba(combined_features)[0].max()
        
        # ?ˆì´ë¸??”ì½”??
        predicted_class = self.label_encoder.inverse_transform([prediction])[0]
        
        return predicted_class, confidence
    
    def save_model(self, filepath: str):
        """ëª¨ë¸ ?€??""
        model_data = {
            'model': self.model,
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder,
            'model_type': self.model_type
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"Model saved to {filepath}")
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model = model_data['model']
        self.vectorizer = model_data['vectorizer']
        self.label_encoder = model_data['label_encoder']
        self.model_type = model_data['model_type']
        
        logger.info(f"Model loaded from {filepath}")


def main():
    """ë©”ì¸ ?¨ìˆ˜"""
    # ë¶„ë¥˜ê¸??ì„±
    classifier = ArticleMLClassifier(model_type="random_forest")
    
    # ?ˆë ¨ ?°ì´??ì¤€ë¹?
    data_dir = "data/processed/assembly/law"
    features_list, labels = classifier.prepare_training_data(data_dir)
    
    if len(features_list) == 0:
        logger.error("No training data found")
        return
    
    # ëª¨ë¸ ?ˆë ¨
    results = classifier.train(features_list, labels)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== Training Results ===")
    print(f"Accuracy: {results['accuracy']:.3f}")
    print(f"Cross-validation: {results['cv_mean']:.3f} (+/- {results['cv_std']:.3f})")
    print("\nClassification Report:")
    print(results['classification_report'])
    
    # ëª¨ë¸ ?€??
    model_path = "models/article_classifier.pkl"
    Path("models").mkdir(exist_ok=True)
    classifier.save_model(model_path)
    
    print(f"\nModel saved to {model_path}")


if __name__ == "__main__":
    main()

