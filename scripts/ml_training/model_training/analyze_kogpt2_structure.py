#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KoGPT-2 ëª¨ë¸ êµ¬ì¡° ë¶„ì„ ë°??¬ë°”ë¥?LoRA target modules ì°¾ê¸°
LawFirmAI ?„ë¡œ?íŠ¸ - TASK 3.1 ?ˆë ¨ ?˜ê²½ êµ¬ì„±
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging

def setup_logging():
    """ë¡œê¹… ?¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def analyze_model_structure(model_name: str = "skt/kogpt2-base-v2"):
    """ëª¨ë¸ êµ¬ì¡° ë¶„ì„"""
    logger = setup_logging()
    
    try:
        # ëª¨ë¸ ë¡œë”©
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        
        logger.info(f"Analyzing model structure: {model_name}")
        logger.info(f"Model type: {type(model).__name__}")
        
        # ëª¨ë¸??ëª¨ë“  ?ˆì´???´ë¦„ ì¶œë ¥
        logger.info("\n=== Model Layer Names ===")
        for name, module in model.named_modules():
            if len(name) > 0:  # ë£¨íŠ¸ ëª¨ë“ˆ ?œì™¸
                logger.info(f"{name}: {type(module).__name__}")
        
        # Transformer ë¸”ë¡??attention ?ˆì´??ì°¾ê¸°
        logger.info("\n=== Looking for Attention Layers ===")
        attention_layers = []
        
        for name, module in model.named_modules():
            if 'attn' in name.lower() or 'attention' in name.lower():
                logger.info(f"Found attention layer: {name} - {type(module).__name__}")
                attention_layers.append(name)
        
        # Linear ?ˆì´??ì°¾ê¸° (LoRA ?ìš© ?€??
        logger.info("\n=== Looking for Linear Layers ===")
        linear_layers = []
        
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                logger.info(f"Found Linear layer: {name} - {module}")
                linear_layers.append(name)
        
        # GPT-2 ?¤í??¼ì˜ attention ?ˆì´??ì°¾ê¸°
        logger.info("\n=== Looking for GPT-2 Style Attention Layers ===")
        gpt2_attention_layers = []
        
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear) and any(x in name for x in ['c_attn', 'c_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj']):
                logger.info(f"Found GPT-2 style layer: {name} - {module}")
                gpt2_attention_layers.append(name)
        
        # ëª¨ë¸??ì²?ë²ˆì§¸ transformer ë¸”ë¡ ë¶„ì„
        logger.info("\n=== First Transformer Block Analysis ===")
        if hasattr(model, 'transformer'):
            transformer = model.transformer
            logger.info(f"Transformer type: {type(transformer).__name__}")
            
            if hasattr(transformer, 'h'):
                first_block = transformer.h[0]
                logger.info(f"First block type: {type(first_block).__name__}")
                
                for name, module in first_block.named_modules():
                    if len(name) > 0:
                        logger.info(f"Block layer: {name} - {type(module).__name__}")
        
        return {
            "model_name": model_name,
            "attention_layers": attention_layers,
            "linear_layers": linear_layers,
            "gpt2_style_layers": gpt2_attention_layers
        }
        
    except Exception as e:
        logger.error(f"Model analysis failed: {e}")
        return {"error": str(e)}

def find_optimal_lora_targets(model_name: str = "skt/kogpt2-base-v2"):
    """ìµœì ??LoRA target modules ì°¾ê¸°"""
    logger = setup_logging()
    
    try:
        # ëª¨ë¸ ë¡œë”©
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        
        logger.info(f"Finding optimal LoRA targets for: {model_name}")
        
        # ?¼ë°˜?ì¸ GPT-2 ?¤í???target modules ?œë„
        common_targets = [
            ["c_attn", "c_proj"],  # GPT-2 ?¤í???
            ["q_proj", "k_proj", "v_proj", "o_proj"],  # LLaMA ?¤í???
            ["query", "key", "value", "dense"],  # BERT ?¤í???
            ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "self_attn.o_proj"],  # ?„ì²´ ê²½ë¡œ
        ]
        
        for targets in common_targets:
            logger.info(f"\nTrying targets: {targets}")
            
            # ê°?target??ëª¨ë¸??ì¡´ì¬?˜ëŠ”ì§€ ?•ì¸
            found_targets = []
            for target in targets:
                for name, module in model.named_modules():
                    if target in name and isinstance(module, torch.nn.Linear):
                        found_targets.append(name)
                        logger.info(f"Found: {name}")
                        break
            
            if found_targets:
                logger.info(f"??Found {len(found_targets)} matching targets: {found_targets}")
                return found_targets
            else:
                logger.info(f"??No matching targets found for: {targets}")
        
        # ëª¨ë“  Linear ?ˆì´??ì¤‘ì—??attention ê´€??ì°¾ê¸°
        logger.info("\n=== Searching for Attention-related Linear Layers ===")
        attention_linear_layers = []
        
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                # attention ê´€???¤ì›Œ?œê? ?¬í•¨???ˆì´??ì°¾ê¸°
                if any(keyword in name.lower() for keyword in ['attn', 'attention', 'query', 'key', 'value', 'proj']):
                    attention_linear_layers.append(name)
                    logger.info(f"Attention-related Linear layer: {name}")
        
        if attention_linear_layers:
            logger.info(f"??Found {len(attention_linear_layers)} attention-related Linear layers")
            return attention_linear_layers[:4]  # ìµœë? 4ê°?ë°˜í™˜
        
        # ëª¨ë“  Linear ?ˆì´??ë°˜í™˜ (ìµœí›„???˜ë‹¨)
        logger.info("\n=== Fallback: All Linear Layers ===")
        all_linear_layers = []
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                all_linear_layers.append(name)
        
        logger.info(f"Found {len(all_linear_layers)} total Linear layers")
        return all_linear_layers[:8]  # ìµœë? 8ê°?ë°˜í™˜
        
    except Exception as e:
        logger.error(f"Target finding failed: {e}")
        return []

def test_lora_with_correct_targets(model_name: str = "skt/kogpt2-base-v2"):
    """?¬ë°”ë¥?target modulesë¡?LoRA ?ŒìŠ¤??""
    logger = setup_logging()
    
    try:
        from peft import LoraConfig, get_peft_model, TaskType
        
        # ìµœì ??target modules ì°¾ê¸°
        target_modules = find_optimal_lora_targets(model_name)
        
        if not target_modules:
            logger.error("No suitable target modules found")
            return {"status": "error", "error": "No target modules found"}
        
        logger.info(f"Using target modules: {target_modules}")
        
        # ëª¨ë¸ ë¡œë”©
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        
        # LoRA ?¤ì •
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=target_modules
        )
        
        # LoRA ëª¨ë¸ ?ì„±
        peft_model = get_peft_model(model, lora_config)
        
        logger.info("??LoRA model created successfully!")
        
        # ?Œë¼ë¯¸í„° ???•ì¸
        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in peft_model.parameters())
        
        logger.info(f"Trainable parameters: {trainable_params:,}")
        logger.info(f"Total parameters: {total_params:,}")
        logger.info(f"Trainable ratio: {trainable_params/total_params:.2%}")
        
        return {
            "status": "success",
            "target_modules": target_modules,
            "trainable_params": trainable_params,
            "total_params": total_params,
            "trainable_ratio": trainable_params/total_params
        }
        
    except Exception as e:
        logger.error(f"LoRA test failed: {e}")
        return {"status": "error", "error": str(e)}

def main():
    """ë©”ì¸ ?¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="KoGPT-2 Model Structure Analysis")
    parser.add_argument("--model", default="skt/kogpt2-base-v2", help="Model name to analyze")
    parser.add_argument("--test-lora", action="store_true", help="Test LoRA with correct targets")
    
    args = parser.parse_args()
    
    logger = setup_logging()
    
    if args.test_lora:
        # LoRA ?ŒìŠ¤???¤í–‰
        logger.info("Testing LoRA with correct target modules...")
        result = test_lora_with_correct_targets(args.model)
        
        if result["status"] == "success":
            print(f"\n??LoRA configuration successful!")
            print(f"Target modules: {result['target_modules']}")
            print(f"Trainable parameters: {result['trainable_params']:,}")
            print(f"Trainable ratio: {result['trainable_ratio']:.2%}")
        else:
            print(f"\n??LoRA configuration failed: {result['error']}")
    else:
        # ëª¨ë¸ êµ¬ì¡° ë¶„ì„
        logger.info("Analyzing model structure...")
        result = analyze_model_structure(args.model)
        
        if "error" not in result:
            print(f"\n=== Analysis Results ===")
            print(f"Attention layers found: {len(result['attention_layers'])}")
            print(f"Linear layers found: {len(result['linear_layers'])}")
            print(f"GPT-2 style layers found: {len(result['gpt2_style_layers'])}")
            
            if result['gpt2_style_layers']:
                print(f"\nRecommended target modules: {result['gpt2_style_layers']}")
        else:
            print(f"\n??Analysis failed: {result['error']}")

if __name__ == "__main__":
    main()
