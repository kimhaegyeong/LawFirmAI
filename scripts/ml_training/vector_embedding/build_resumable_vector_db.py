#!/usr/bin/env python3
"""
ì¤‘ë‹¨??ë³µêµ¬ ê¸°ëŠ¥???ˆëŠ” ML ê°•í™” ë²¡í„° ?„ë² ???ì„±ê¸?
"""

import gc
import json
import logging
import os
import pickle
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from tqdm import tqdm

# ?„ë¡œ?íŠ¸ ë£¨íŠ¸ë¥?Python ê²½ë¡œ??ì¶”ê?
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from source.data.vector_store import LegalVectorStore

# Windows ì½˜ì†”?ì„œ UTF-8 ?¸ì½”???¤ì •
if os.name == 'nt':  # Windows
    try:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())
    except AttributeError:
        # ?´ë? UTF-8ë¡??¤ì •??ê²½ìš° ë¬´ì‹œ
        pass

logger = logging.getLogger(__name__)


class ResumableVectorBuilder:
    """ì¤‘ë‹¨??ë³µêµ¬ ê¸°ëŠ¥???ˆëŠ” ë²¡í„° ë¹Œë”"""

    def __init__(self, model_name: str = "jhgan/ko-sroberta-multitask",
                 batch_size: int = 20, chunk_size: int = 200,
                 checkpoint_interval: int = 100):
        """
        ì¤‘ë‹¨??ë³µêµ¬ ê°€?¥í•œ ë²¡í„° ë¹Œë” ì´ˆê¸°??

        Args:
            model_name: ?¬ìš©???„ë² ??ëª¨ë¸ëª?
            batch_size: ?Œì¼ ë°°ì¹˜ ?¬ê¸°
            chunk_size: ë¬¸ì„œ ì²?¬ ?¬ê¸°
            checkpoint_interval: ì²´í¬?¬ì¸???€??ê°„ê²© (ë¬¸ì„œ ??
        """
        self.model_name = model_name
        self.batch_size = batch_size
        self.chunk_size = chunk_size
        self.checkpoint_interval = checkpoint_interval

        # Sentence-BERT ëª¨ë¸???„ë² ??ì°¨ì› (768)
        embedding_dimension = 768

        # ë²¡í„° ?¤í† ??ì´ˆê¸°??
        self.vector_store = LegalVectorStore(
            model_name=model_name,
            dimension=embedding_dimension,
            index_type="flat"
        )

        # ?µê³„ ì´ˆê¸°??
        self.stats = {
            'total_files_processed': 0,
            'total_laws_processed': 0,
            'total_articles_processed': 0,
            'main_articles_processed': 0,
            'supplementary_articles_processed': 0,
            'total_chunks_created': 0,
            'total_documents_created': 0,
            'errors': [],
            'start_time': datetime.now().isoformat(),
            'last_checkpoint': None,
            'processed_files': set(),
            'processed_documents': 0
        }

        logger.info(f"ResumableVectorBuilder initialized with model: {model_name}")
        logger.info(f"Batch size: {batch_size}, Chunk size: {chunk_size}")
        logger.info(f"Checkpoint interval: {checkpoint_interval}")

    def build_embeddings(self, input_dir: str, output_dir: str, resume: bool = True) -> Dict[str, Any]:
        """
        ì¤‘ë‹¨??ë³µêµ¬ ê°€?¥í•œ ë²¡í„° ?„ë² ???ì„±

        Args:
            input_dir: ?…ë ¥ ?”ë ‰? ë¦¬
            output_dir: ì¶œë ¥ ?”ë ‰? ë¦¬
            resume: ?´ì „ ?‘ì—… ?´ì–´??ì§„í–‰? ì? ?¬ë?

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ?µê³„
        """
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # ì²´í¬?¬ì¸???Œì¼ ê²½ë¡œ
        checkpoint_file = output_path / "checkpoint.json"
        progress_file = output_path / "progress.pkl"

        # ?´ì „ ?‘ì—… ë³µêµ¬
        if resume and checkpoint_file.exists():
            self._load_checkpoint(checkpoint_file, progress_file)
            logger.info(f"Resumed from checkpoint: {self.stats['last_checkpoint']}")
            logger.info(f"Already processed: {self.stats['total_documents_created']} documents")

        logger.info(f"Starting resumable vector embedding generation from: {input_path}")

        # JSON ?Œì¼ ì°¾ê¸°
        json_files = list(input_path.rglob("ml_enhanced_*.json"))
        logger.info(f"Found {len(json_files)} ML-enhanced files to process")

        if not json_files:
            logger.warning("No ML-enhanced files found!")
            return self.stats

        # ?´ë? ì²˜ë¦¬???Œì¼???œì™¸
        remaining_files = [f for f in json_files if str(f) not in self.stats['processed_files']]
        logger.info(f"Remaining files to process: {len(remaining_files)}")

        if not remaining_files:
            logger.info("All files already processed!")
            return self.stats

        # ?Œì¼?¤ì„ ?‘ì? ë°°ì¹˜ë¡??˜ëˆ„???œì°¨ ì²˜ë¦¬
        batches = [remaining_files[i:i + self.batch_size] for i in range(0, len(remaining_files), self.batch_size)]
        logger.info(f"Processing {len(batches)} batches with {self.batch_size} files each")

        try:
            for batch_idx, batch_files in enumerate(tqdm(batches, desc="Processing batches", unit="batch")):
                try:
                    batch_documents = self._process_batch_sequential(batch_files, batch_idx)

                    # ë¬¸ì„œ?¤ì„ ?‘ì? ì²?¬ë¡??˜ëˆ„??ì²˜ë¦¬
                    for i in range(0, len(batch_documents), self.chunk_size):
                        chunk = batch_documents[i:i + self.chunk_size]
                        texts = [doc['text'] for doc in chunk]
                        metadatas = [doc['metadata'] for doc in chunk]

                        try:
                            self.vector_store.add_documents(texts, metadatas)
                            self.stats['total_documents_created'] += len(chunk)

                            # ë©”ëª¨ë¦??•ë¦¬
                            del texts, metadatas
                            gc.collect()

                            # ì²´í¬?¬ì¸???€??
                            if self.stats['total_documents_created'] % self.checkpoint_interval == 0:
                                self._save_checkpoint(checkpoint_file, progress_file)
                                logger.info(f"Checkpoint saved: {self.stats['total_documents_created']} documents processed")

                        except Exception as e:
                            logger.error(f"Error creating embeddings for chunk {i//self.chunk_size}: {e}")
                            self.stats['errors'].append(f"Embedding chunk error: {e}")

                    # ë°°ì¹˜ ?„ë£Œ ??ì²´í¬?¬ì¸???€??
                    self._save_checkpoint(checkpoint_file, progress_file)

                    # ë©”ëª¨ë¦??•ë¦¬
                    gc.collect()

                    # ì§„í–‰ ?í™© ë¡œê¹…
                    if (batch_idx + 1) % 5 == 0:
                        logger.info(f"Processed {batch_idx + 1}/{len(batches)} batches")

                except Exception as e:
                    logger.error(f"Error processing batch {batch_idx}: {e}")
                    self.stats['errors'].append(f"Batch {batch_idx} error: {e}")
                    # ?ëŸ¬ê°€ ë°œìƒ?´ë„ ?¤ìŒ ë°°ì¹˜ ê³„ì† ì²˜ë¦¬
                    continue

        except KeyboardInterrupt:
            logger.info("Process interrupted by user. Saving checkpoint...")
            self._save_checkpoint(checkpoint_file, progress_file)
            logger.info("Checkpoint saved. You can resume later with --resume flag.")
            return self.stats

        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            self._save_checkpoint(checkpoint_file, progress_file)
            raise

        # ìµœì¢… ?¸ë±???€??
        index_path = output_path / "ml_enhanced_faiss_index"
        self.vector_store.save_index(str(index_path))

        # ìµœì¢… ?µê³„ ?€??
        self.stats['end_time'] = datetime.now().isoformat()
        stats_path = output_path / "ml_enhanced_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)

        # ì²´í¬?¬ì¸???Œì¼ ?•ë¦¬
        if checkpoint_file.exists():
            checkpoint_file.unlink()
        if progress_file.exists():
            progress_file.unlink()

        logger.info("Resumable vector embedding generation completed!")
        logger.info(f"Total documents processed: {self.stats['total_documents_created']}")
        logger.info(f"Total articles processed: {self.stats['total_articles_processed']}")
        logger.info(f"Errors: {len(self.stats['errors'])}")

        return self.stats

    def _process_batch_sequential(self, batch_files: List[Path], batch_idx: int) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ?Œì¼?¤ì„ ?œì°¨ ì²˜ë¦¬"""
        batch_documents = []

        for file_path in batch_files:
            try:
                # ?Œì¼???´ë? ì²˜ë¦¬?˜ì—ˆ?”ì? ?•ì¸
                if str(file_path) in self.stats['processed_files']:
                    continue

                documents = self._process_single_file(file_path)
                batch_documents.extend(documents)
                self.stats['total_files_processed'] += 1
                self.stats['processed_files'].add(str(file_path))

            except Exception as e:
                error_msg = f"Error processing file {file_path}: {e}"
                logger.error(error_msg)
                self.stats['errors'].append(error_msg)

        return batch_documents

    def _process_single_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """?¨ì¼ ?Œì¼ ì²˜ë¦¬"""
        documents = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                file_data = json.load(f)

            # ?Œì¼ êµ¬ì¡° ?•ì¸
            if isinstance(file_data, dict) and 'laws' in file_data:
                laws = file_data['laws']
            elif isinstance(file_data, list):
                laws = file_data
            else:
                laws = [file_data]

            for law_data in laws:
                try:
                    # ë²•ë¥  ë©”í??°ì´??ì¶”ì¶œ
                    law_metadata = self._extract_law_metadata(law_data)

                    # ë³¸ì¹™ ì¡°ë¬¸ ì²˜ë¦¬
                    articles = law_data.get('articles', [])
                    if not isinstance(articles, list):
                        articles = []

                    # ë¶€ì¹?ì¡°ë¬¸ ì²˜ë¦¬
                    supplementary_articles = law_data.get('supplementary_articles', [])
                    if not isinstance(supplementary_articles, list):
                        supplementary_articles = []

                    # ëª¨ë“  ì¡°ë¬¸???˜ë‚˜??ë¦¬ìŠ¤?¸ë¡œ ?©ì¹˜ê¸?
                    all_articles = articles + supplementary_articles

                    # ë¬¸ì„œ ?ì„±
                    article_documents = self._create_article_documents_batch(
                        all_articles, law_metadata
                    )
                    documents.extend(article_documents)

                    # ?µê³„ ?…ë°?´íŠ¸
                    self.stats['total_laws_processed'] += 1
                    self.stats['total_articles_processed'] += len(all_articles)

                    main_articles = [a for a in all_articles if not a.get('is_supplementary', False)]
                    supp_articles = [a for a in all_articles if a.get('is_supplementary', False)]

                    self.stats['main_articles_processed'] += len(main_articles)
                    self.stats['supplementary_articles_processed'] += len(supp_articles)

                except Exception as e:
                    error_msg = f"Error processing law {law_data.get('law_name', 'Unknown')}: {e}"
                    logger.error(error_msg)
                    continue

        except Exception as e:
            logger.error(f"Error reading file {file_path}: {e}")
            raise

        return documents

    def _create_article_documents_batch(self, articles: List[Dict[str, Any]],
                                      law_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì¡°ë¬¸?¤ì„ ë°°ì¹˜ë¡?ë¬¸ì„œ ë³€??""
        documents = []

        for article in articles:
            try:
                # ì¡°ë¬¸ ë©”í??°ì´???ì„±
                article_metadata = {
                    **law_metadata,
                    'article_number': article.get('article_number', ''),
                    'article_title': article.get('article_title', ''),
                    'article_type': 'main' if not article.get('is_supplementary', False) else 'supplementary',
                    'is_supplementary': article.get('is_supplementary', False),
                    'ml_confidence_score': article.get('ml_confidence_score'),
                    'parsing_method': article.get('parsing_method', 'ml_enhanced'),
                    'word_count': article.get('word_count', 0),
                    'char_count': article.get('char_count', 0),
                    'sub_articles_count': len(article.get('sub_articles', [])) if isinstance(article.get('sub_articles'), list) else 0,
                    'references_count': len(article.get('references', [])) if isinstance(article.get('references'), list) else 0
                }

                # ë¬¸ì„œ ID ?ì„±
                document_id = f"{law_metadata['law_id']}_article_{article_metadata['article_number']}"
                article_metadata['document_id'] = document_id

                # ?ìŠ¤??êµ¬ì„±
                text_parts = []

                # ì¡°ë¬¸ ë²ˆí˜¸?€ ?œëª©
                if article_metadata['article_number']:
                    if article_metadata['article_title']:
                        text_parts.append(f"{article_metadata['article_number']}({article_metadata['article_title']})")
                    else:
                        text_parts.append(article_metadata['article_number'])

                # ì¡°ë¬¸ ?´ìš©
                article_content = article.get('article_content', '')
                if article_content:
                    text_parts.append(article_content)

                # ?˜ìœ„ ì¡°ë¬¸??
                sub_articles = article.get('sub_articles', [])
                if isinstance(sub_articles, list):
                    for sub_article in sub_articles:
                        if isinstance(sub_article, dict):
                            sub_content = sub_article.get('content', '')
                            if sub_content:
                                text_parts.append(sub_content)

                # ìµœì¢… ?ìŠ¤??
                full_text = ' '.join(text_parts)

                if full_text.strip():
                    document = {
                        'id': document_id,
                        'text': full_text,
                        'metadata': article_metadata,
                        'chunks': [{
                            'id': f"{document_id}_chunk_0",
                            'text': full_text,
                            'start_pos': 0,
                            'end_pos': len(full_text),
                            'entities': article.get('references', []) if isinstance(article.get('references'), list) else []
                        }]
                    }
                    documents.append(document)

            except Exception as e:
                logger.error(f"Error processing article {article.get('article_number', 'Unknown')}: {e}")
                continue

        return documents

    def _extract_law_metadata(self, law_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë²•ë¥  ë©”í??°ì´??ì¶”ì¶œ"""
        return {
            'law_id': law_data.get('law_id') or f"ml_enhanced_{law_data.get('law_name', 'unknown').replace(' ', '_')}",
            'law_name': law_data.get('law_name', ''),
            'law_type': law_data.get('law_type', ''),
            'category': law_data.get('category', ''),
            'promulgation_number': law_data.get('promulgation_number', ''),
            'promulgation_date': law_data.get('promulgation_date', ''),
            'enforcement_date': law_data.get('enforcement_date', ''),
            'amendment_type': law_data.get('amendment_type', ''),
            'ministry': law_data.get('ministry', ''),
            'ml_enhanced': law_data.get('ml_enhanced', True),
            'parsing_quality_score': law_data.get('data_quality', {}).get('parsing_quality_score', 0.0) if isinstance(law_data.get('data_quality'), dict) else 0.0,
            'processing_version': law_data.get('processing_version', 'ml_enhanced_v1.0'),
            'control_characters_removed': True
        }

    def _save_checkpoint(self, checkpoint_file: Path, progress_file: Path):
        """ì²´í¬?¬ì¸???€??""
        try:
            self.stats['last_checkpoint'] = datetime.now().isoformat()

            # set??listë¡?ë³€?˜í•˜??JSON ì§ë ¬??ê°€?¥í•˜ê²?ë§Œë“¤ê¸?
            checkpoint_stats = self.stats.copy()
            checkpoint_stats['processed_files'] = list(checkpoint_stats['processed_files'])

            # ?µê³„ ?•ë³´ ?€??
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_stats, f, ensure_ascii=False, indent=2)

            # ë²¡í„° ?¤í† ???íƒœ ?€??
            vector_state = {
                'document_count': len(self.vector_store.document_metadata),
                'index_trained': self.vector_store.index.is_trained if self.vector_store.index else False
            }

            with open(progress_file, 'wb') as f:
                pickle.dump(vector_state, f)

        except Exception as e:
            logger.error(f"Failed to save checkpoint: {e}")

    def _load_checkpoint(self, checkpoint_file: Path, progress_file: Path):
        """ì²´í¬?¬ì¸??ë¡œë“œ"""
        try:
            # ?µê³„ ?•ë³´ ë¡œë“œ
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                saved_stats = json.load(f)

            # ?µê³„ ?•ë³´ ë³µì›
            self.stats.update(saved_stats)

            # processed_filesë¥?set?¼ë¡œ ë³€??
            if isinstance(self.stats['processed_files'], list):
                self.stats['processed_files'] = set(self.stats['processed_files'])

            # ë²¡í„° ?¤í† ???íƒœ ?•ì¸
            if progress_file.exists():
                with open(progress_file, 'rb') as f:
                    vector_state = pickle.load(f)
                logger.info(f"Vector store state: {vector_state['document_count']} documents, trained: {vector_state['index_trained']}")

        except Exception as e:
            logger.error(f"Failed to load checkpoint: {e}")
            # ì²´í¬?¬ì¸??ë¡œë“œ ?¤íŒ¨ ??ì²˜ìŒë¶€???œì‘
            self.stats['processed_files'] = set()


def main():
    """ë©”ì¸ ?¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="ì¤‘ë‹¨??ë³µêµ¬ ê¸°ëŠ¥???ˆëŠ” ML ê°•í™” ë²¡í„° ?„ë² ???ì„±ê¸?)
    parser.add_argument("--input", required=True, help="?…ë ¥ ?”ë ‰? ë¦¬")
    parser.add_argument("--output", required=True, help="ì¶œë ¥ ?”ë ‰? ë¦¬")
    parser.add_argument("--batch-size", type=int, default=20, help="ë°°ì¹˜ ?¬ê¸° (ê¸°ë³¸ê°? 20)")
    parser.add_argument("--chunk-size", type=int, default=200, help="ì²?¬ ?¬ê¸° (ê¸°ë³¸ê°? 200)")
    parser.add_argument("--checkpoint-interval", type=int, default=100, help="ì²´í¬?¬ì¸???€??ê°„ê²© (ê¸°ë³¸ê°? 100)")
    parser.add_argument("--resume", action="store_true", help="?´ì „ ?‘ì—… ?´ì–´??ì§„í–‰")
    parser.add_argument("--log-level", default="INFO", help="ë¡œê·¸ ?ˆë²¨")

    args = parser.parse_args()

    # ë¡œê¹… ?¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # ë²¡í„° ë¹Œë” ì´ˆê¸°??ë°??¤í–‰
    builder = ResumableVectorBuilder(
        batch_size=args.batch_size,
        chunk_size=args.chunk_size,
        checkpoint_interval=args.checkpoint_interval
    )

    stats = builder.build_embeddings(args.input, args.output, resume=args.resume)

    print(f"\n=== ì²˜ë¦¬ ?„ë£Œ ===")
    print(f"ì´??Œì¼ ?? {stats['total_files_processed']}")
    print(f"ì´?ë²•ë¥  ?? {stats['total_laws_processed']}")
    print(f"ì´?ì¡°ë¬¸ ?? {stats['total_articles_processed']}")
    print(f"ì´?ë¬¸ì„œ ?? {stats['total_documents_created']}")
    print(f"?ëŸ¬ ?? {len(stats['errors'])}")

    if stats.get('start_time') and stats.get('end_time'):
        start_time = datetime.fromisoformat(stats['start_time'])
        end_time = datetime.fromisoformat(stats['end_time'])
        duration = end_time - start_time
        print(f"ì´??Œìš” ?œê°„: {duration}")


if __name__ == "__main__":
    main()
