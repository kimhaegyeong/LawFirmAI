#!/usr/bin/env python3
"""
ML ê°•í™” ë²¡í„° ?„ë² ???ì„±ê¸?

ML ê°•í™” ?Œì‹±??ë²•ë¥  ?°ì´?°ë¡œë¶€??ë²¡í„° ?„ë² ?©ì„ ?ì„±?˜ê³  FAISS ?¸ë±?¤ë? êµ¬ì¶•?©ë‹ˆ??
ë³¸ì¹™ê³?ë¶€ì¹™ì„ êµ¬ë¶„?˜ì—¬ ?„ë² ?©í•˜ê³? ML ? ë¢°?„ì? ?ˆì§ˆ ?ìˆ˜ë¥?ë©”í??°ì´?°ì— ?¬í•¨?©ë‹ˆ??
"""

import logging
import json
import sys
import os
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import argparse
from tqdm import tqdm

# ?„ë¡œ?íŠ¸ ë£¨íŠ¸ë¥?Python ê²½ë¡œ??ì¶”ê?
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from source.data.vector_store import LegalVectorStore

# Windows ì½˜ì†”?ì„œ UTF-8 ?¸ì½”???¤ì •
if os.name == 'nt':  # Windows
    try:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())
    except AttributeError:
        # ?´ë? UTF-8ë¡??¤ì •??ê²½ìš° ë¬´ì‹œ
        pass

logger = logging.getLogger(__name__)


class MLEnhancedVectorBuilder:
    """ML ê°•í™” ë²¡í„° ?„ë² ???ì„±ê¸?""
    
    def __init__(self, model_name: str = "jhgan/ko-sroberta-multitask", 
                 dimension: int = 768, index_type: str = "flat"):
        """
        ML ê°•í™” ë²¡í„° ë¹Œë” ì´ˆê¸°??
        
        Args:
            model_name: ?¬ìš©??Sentence-BERT ëª¨ë¸ëª?
            dimension: ë²¡í„° ì°¨ì›
            index_type: FAISS ?¸ë±???€??
        """
        self.model_name = model_name
        self.dimension = dimension
        self.index_type = index_type
        
        # ë²¡í„° ?¤í† ??ì´ˆê¸°??
        self.vector_store = LegalVectorStore(
            model_name=model_name,
            dimension=dimension,
            index_type=index_type
        )
        
        # ?µê³„ ?•ë³´
        self.stats = {
            'total_laws_processed': 0,
            'total_articles_processed': 0,
            'main_articles_processed': 0,
            'supplementary_articles_processed': 0,
            'total_chunks_created': 0,
            'processing_time': 0,
            'errors': []
        }
        
        logger.info(f"MLEnhancedVectorBuilder initialized with model: {model_name}")
    
    def build_embeddings(self, processed_dir: Path, batch_size: int = 100) -> bool:
        """
        ML ê°•í™” ë²•ë¥  ?°ì´?°ë¡œë¶€??ë²¡í„° ?„ë² ???ì„±
        
        Args:
            processed_dir: ML ê°•í™” ì²˜ë¦¬???°ì´???”ë ‰? ë¦¬
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ ?¬ê¸°
            
        Returns:
            bool: ?±ê³µ ?¬ë?
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"Starting ML-enhanced vector embedding generation from: {processed_dir}")
            
            # JSON ?Œì¼ ëª©ë¡ ?˜ì§‘
            json_files = list(processed_dir.rglob("ml_enhanced_*.json"))
            logger.info(f"Found {len(json_files)} ML-enhanced files to process")
            
            if not json_files:
                logger.error("No ML-enhanced files found")
                return False
            
            # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
            all_documents = []
            batch_count = 0
            
            for i in range(0, len(json_files), batch_size):
                batch_files = json_files[i:i + batch_size]
                batch_count += 1
                
                logger.info(f"Processing batch {batch_count}: {len(batch_files)} files")
                
                batch_documents = self._process_batch(batch_files)
                all_documents.extend(batch_documents)
                
                # ì¤‘ê°„ ?€??(ë©”ëª¨ë¦?ê´€ë¦?
                if len(all_documents) >= batch_size * 2:
                    self._add_documents_to_index(all_documents)
                    all_documents = []
            
            # ?¨ì? ë¬¸ì„œ??ì²˜ë¦¬
            if all_documents:
                self._add_documents_to_index(all_documents)
            
            # ì²˜ë¦¬ ?œê°„ ê¸°ë¡
            self.stats['processing_time'] = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"Vector embedding generation completed in {self.stats['processing_time']:.2f} seconds")
            logger.info(f"Statistics: {self.stats}")
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to build embeddings: {e}")
            self.stats['errors'].append(str(e))
            return False
    
    def _process_batch(self, batch_files: List[Path]) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ?Œì¼?¤ì„ ì²˜ë¦¬?˜ì—¬ ë¬¸ì„œ ë¦¬ìŠ¤???ì„±"""
        batch_documents = []
        
        for file_path in tqdm(batch_files, desc="Processing files"):
            try:
                documents = self._process_single_file(file_path)
                batch_documents.extend(documents)
                self.stats['total_laws_processed'] += 1
                
            except Exception as e:
                error_msg = f"Error processing {file_path}: {e}"
                logger.error(error_msg)
                self.stats['errors'].append(error_msg)
        
        return batch_documents
    
    def _process_single_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """?¨ì¼ ?Œì¼ ì²˜ë¦¬"""
        with open(file_path, 'r', encoding='utf-8') as f:
            file_data = json.load(f)
        
        documents = []
        
        # ?Œì¼ êµ¬ì¡° ?•ì¸
        if isinstance(file_data, dict) and 'laws' in file_data:
            laws = file_data['laws']
        elif isinstance(file_data, list):
            laws = file_data
        else:
            laws = [file_data]
        
        for law_data in laws:
            try:
                # ë²•ë¥  ë©”í??°ì´??ì¶”ì¶œ
                law_metadata = self._extract_law_metadata(law_data)
                
                # ë³¸ì¹™ ì¡°ë¬¸ ì²˜ë¦¬
                articles = law_data.get('articles', [])
                if not isinstance(articles, list):
                    articles = []
                main_documents = self._create_article_documents(
                    articles,
                    law_metadata,
                    article_type='main'
                )
                documents.extend(main_documents)
                
                # ë¶€ì¹?ì¡°ë¬¸ ì²˜ë¦¬ (ë³„ë„ ?„ë“œê°€ ?ˆëŠ” ê²½ìš°)
                # ë¶€ì¹?ì¡°ë¬¸ ì²˜ë¦¬
                supplementary_articles = law_data.get('supplementary_articles', [])
                if not isinstance(supplementary_articles, list):
                    supplementary_articles = []
                supp_documents = self._create_article_documents(
                    supplementary_articles,
                    law_metadata,
                    article_type='supplementary'
                )
                documents.extend(supp_documents)
                
                # ?µê³„ ?…ë°?´íŠ¸
                all_articles = law_data.get('articles', [])
                if not isinstance(all_articles, list):
                    all_articles = []
                main_articles = [a for a in all_articles if not a.get('is_supplementary', False)]
                supp_articles = [a for a in all_articles if a.get('is_supplementary', False)]
                
                self.stats['total_articles_processed'] += len(all_articles)
                self.stats['main_articles_processed'] += len(main_articles)
                self.stats['supplementary_articles_processed'] += len(supp_articles)
                
            except Exception as e:
                error_msg = f"Error processing law {law_data.get('law_name', 'Unknown')}: {e}"
                logger.error(error_msg)
                logger.error(f"Law data keys: {list(law_data.keys())}")
                if law_data.get('articles'):
                    logger.error(f"First article keys: {list(law_data['articles'][0].keys())}")
                    logger.error(f"First article sub_articles type: {type(law_data['articles'][0].get('sub_articles'))}")
                    logger.error(f"First article references type: {type(law_data['articles'][0].get('references'))}")
                    # ì²?ë²ˆì§¸ ì¡°ë¬¸??sub_articles ?´ìš© ?•ì¸
                    first_article = law_data['articles'][0]
                    sub_articles = first_article.get('sub_articles', [])
                    if sub_articles:
                        logger.error(f"First sub_article type: {type(sub_articles[0])}")
                        logger.error(f"First sub_article value: {sub_articles[0]}")
                        if isinstance(sub_articles[0], dict):
                            logger.error(f"First sub_article keys: {list(sub_articles[0].keys())}")
                import traceback
                logger.error(f"Full traceback: {traceback.format_exc()}")
                self.stats['errors'].append(error_msg)
        
        return documents
    
    def _extract_law_metadata(self, law_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë²•ë¥  ë©”í??°ì´??ì¶”ì¶œ"""
        return {
            'law_id': law_data.get('law_id') or f"ml_enhanced_{law_data.get('law_name', 'unknown').replace(' ', '_')}",
            'law_name': law_data.get('law_name', ''),
            'law_type': law_data.get('law_type', ''),
            'category': law_data.get('category', ''),
            'promulgation_number': law_data.get('promulgation_number', ''),
            'promulgation_date': law_data.get('promulgation_date', ''),
            'enforcement_date': law_data.get('enforcement_date', ''),
            'amendment_type': law_data.get('amendment_type', ''),
            'ministry': law_data.get('ministry', ''),
            'ml_enhanced': law_data.get('ml_enhanced', True),
            'parsing_quality_score': law_data.get('data_quality', {}).get('parsing_quality_score', 0.0) if isinstance(law_data.get('data_quality'), dict) else 0.0,
            'processing_version': law_data.get('processing_version', 'ml_enhanced_v1.0'),
            'control_characters_removed': True
        }
    
    def _create_article_documents(self, articles: List[Dict[str, Any]], 
                                law_metadata: Dict[str, Any], 
                                article_type: str) -> List[Dict[str, Any]]:
        """ì¡°ë¬¸?¤ì„ ë¬¸ì„œë¡?ë³€??""
        documents = []
        
        for article in articles:
            try:
                # ì¡°ë¬¸ ë©”í??°ì´???ì„±
                article_metadata = {
                    **law_metadata,
                    'article_number': article.get('article_number', ''),
                    'article_title': article.get('article_title', ''),
                    'article_type': article_type,
                    'is_supplementary': article.get('is_supplementary', article_type == 'supplementary'),
                    'ml_confidence_score': article.get('ml_confidence_score'),
                    'parsing_method': article.get('parsing_method', 'ml_enhanced'),
                    'word_count': article.get('word_count', 0),
                    'char_count': article.get('char_count', 0),
                    'sub_articles_count': len(article.get('sub_articles', [])) if isinstance(article.get('sub_articles'), list) else 0,
                    'references_count': len(article.get('references', [])) if isinstance(article.get('references'), list) else 0
                }
                
                # ë¬¸ì„œ ID ?ì„±
                document_id = f"{law_metadata['law_id']}_article_{article_metadata['article_number']}"
                article_metadata['document_id'] = document_id
                
                # ?ìŠ¤??êµ¬ì„±
                text_parts = []
                
                # ì¡°ë¬¸ ë²ˆí˜¸?€ ?œëª©
                if article_metadata['article_number']:
                    if article_metadata['article_title']:
                        text_parts.append(f"{article_metadata['article_number']}({article_metadata['article_title']})")
                    else:
                        text_parts.append(article_metadata['article_number'])
                
                # ì¡°ë¬¸ ?´ìš©
                article_content = article.get('article_content', '')
                if article_content:
                    text_parts.append(article_content)
                
                # ?˜ìœ„ ì¡°ë¬¸??
                sub_articles = article.get('sub_articles', [])
                if not isinstance(sub_articles, list):
                    sub_articles = []
                for sub_idx, sub_article in enumerate(sub_articles):
                    try:
                        if not isinstance(sub_article, dict):
                            logger.error(f"Sub-article {sub_idx} is not a dict: {type(sub_article)} = {sub_article}")
                            continue
                        sub_content = sub_article.get('content', '')
                        if sub_content:
                            text_parts.append(sub_content)
                    except Exception as e:
                        logger.error(f"Error processing sub-article {sub_idx}: {e}")
                        logger.error(f"Sub-article type: {type(sub_article)}")
                        logger.error(f"Sub-article value: {sub_article}")
                        continue
                
                # ìµœì¢… ?ìŠ¤??
                full_text = ' '.join(text_parts)
                
                if full_text.strip():
                    document = {
                        'id': document_id,
                        'text': full_text,
                        'metadata': article_metadata,
                        'chunks': [{
                            'id': f"{document_id}_chunk_0",
                            'text': full_text,
                            'start_pos': 0,
                            'end_pos': len(full_text),
                            'entities': article.get('references', [])
                        }]
                    }
                    documents.append(document)
                    self.stats['total_chunks_created'] += 1
                
            except Exception as e:
                error_msg = f"Error processing article {article.get('article_number', 'Unknown')}: {e}"
                logger.error(error_msg)
                self.stats['errors'].append(error_msg)
        
        return documents
    
    def _add_documents_to_index(self, documents: List[Dict[str, Any]]) -> bool:
        """ë¬¸ì„œ?¤ì„ ë²¡í„° ?¸ë±?¤ì— ì¶”ê?"""
        try:
            if not documents:
                return True
            
            # ?ìŠ¤?¸ì? ë©”í??°ì´??ì¶”ì¶œ
            texts = []
            metadatas = []
            
            for doc in documents:
                chunks = doc.get('chunks', [])
                for chunk in chunks:
                    texts.append(chunk.get('text', ''))
                    metadatas.append({
                        'document_id': doc.get('id', ''),
                        'document_type': 'law_article',
                        'chunk_id': chunk.get('id', ''),
                        'chunk_start': chunk.get('start_pos', 0),
                        'chunk_end': chunk.get('end_pos', 0),
                        'law_name': doc.get('metadata', {}).get('law_name', ''),
                        'category': doc.get('metadata', {}).get('category', ''),
                        'entities': chunk.get('entities', []) if isinstance(chunk.get('entities'), list) else [],
                        # ML ê°•í™” ë©”í??°ì´??ì¶”ê?
                        'article_number': doc.get('metadata', {}).get('article_number', ''),
                        'article_title': doc.get('metadata', {}).get('article_title', ''),
                        'article_type': doc.get('metadata', {}).get('article_type', ''),
                        'is_supplementary': doc.get('metadata', {}).get('is_supplementary', False),
                        'ml_confidence_score': doc.get('metadata', {}).get('ml_confidence_score'),
                        'parsing_method': doc.get('metadata', {}).get('parsing_method', 'ml_enhanced'),
                        'parsing_quality_score': doc.get('metadata', {}).get('parsing_quality_score', 0.0),
                        'ml_enhanced': doc.get('metadata', {}).get('ml_enhanced', True),
                        'word_count': doc.get('metadata', {}).get('word_count', 0),
                        'char_count': doc.get('metadata', {}).get('char_count', 0)
                    })
            
            # ë²¡í„° ?¤í† ?´ì— ì¶”ê?
            success = self.vector_store.add_documents(texts, metadatas)
            
            if success:
                logger.info(f"Added {len(texts)} document chunks to vector index")
            else:
                logger.error("Failed to add documents to vector index")
            
            return success
            
        except Exception as e:
            logger.error(f"Failed to add documents to index: {e}")
            return False
    
    def save_index(self, output_dir: Path) -> bool:
        """?ì„±???¸ë±???€??""
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ?¸ë±???Œì¼ëª?
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            index_filename = f"ml_enhanced_faiss_index_{timestamp}"
            index_path = output_dir / index_filename
            
            # ?¸ë±???€??
            success = self.vector_store.save_index(str(index_path))
            
            if success:
                # ?µê³„ ?•ë³´ ?€??
                stats_path = output_dir / f"ml_enhanced_vector_stats_{timestamp}.json"
                with open(stats_path, 'w', encoding='utf-8') as f:
                    json.dump(self.stats, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ML-enhanced vector index saved to: {index_path}")
                logger.info(f"Statistics saved to: {stats_path}")
                
                return True
            else:
                logger.error("Failed to save vector index")
                return False
                
        except Exception as e:
            logger.error(f"Failed to save index: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """?µê³„ ?•ë³´ ë°˜í™˜"""
        vector_stats = self.vector_store.get_stats()
        return {
            **self.stats,
            'vector_store_stats': vector_stats
        }


def main():
    """ë©”ì¸ ?¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ML ê°•í™” ë²¡í„° ?„ë² ???ì„±ê¸?)
    parser.add_argument("--input", type=str, required=True,
                       help="ML ê°•í™” ì²˜ë¦¬???°ì´???”ë ‰? ë¦¬ ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="data/embeddings/ml_enhanced",
                       help="ì¶œë ¥ ?”ë ‰? ë¦¬ ê²½ë¡œ")
    parser.add_argument("--model", type=str, default="jhgan/ko-sroberta-multitask",
                       help="?¬ìš©??Sentence-BERT ëª¨ë¸ëª?)
    parser.add_argument("--dimension", type=int, default=768,
                       help="ë²¡í„° ì°¨ì›")
    parser.add_argument("--index-type", type=str, default="flat",
                       choices=["flat", "ivf", "hnsw"],
                       help="FAISS ?¸ë±???€??)
    parser.add_argument("--batch-size", type=int, default=100,
                       help="ë°°ì¹˜ ì²˜ë¦¬ ?¬ê¸°")
    parser.add_argument("--log-level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="ë¡œê·¸ ?ˆë²¨")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ?¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(f'ml_enhanced_vector_builder_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        ]
    )
    
    # ?…ë ¥ ?”ë ‰? ë¦¬ ?•ì¸
    input_dir = Path(args.input)
    if not input_dir.exists():
        logger.error(f"Input directory does not exist: {input_dir}")
        return 1
    
    # ì¶œë ¥ ?”ë ‰? ë¦¬ ?ì„±
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # ML ê°•í™” ë²¡í„° ë¹Œë” ì´ˆê¸°??
        builder = MLEnhancedVectorBuilder(
            model_name=args.model,
            dimension=args.dimension,
            index_type=args.index_type
        )
        
        # ë²¡í„° ?„ë² ???ì„±
        logger.info("Starting ML-enhanced vector embedding generation...")
        success = builder.build_embeddings(input_dir, batch_size=args.batch_size)
        
        if not success:
            logger.error("Vector embedding generation failed")
            return 1
        
        # ?¸ë±???€??
        logger.info("Saving vector index...")
        save_success = builder.save_index(output_dir)
        
        if not save_success:
            logger.error("Failed to save vector index")
            return 1
        
        # ìµœì¢… ?µê³„ ì¶œë ¥
        stats = builder.get_stats()
        logger.info("=== ML Enhanced Vector Building Completed ===")
        logger.info(f"Total laws processed: {stats['total_laws_processed']}")
        logger.info(f"Total articles processed: {stats['total_articles_processed']}")
        logger.info(f"Main articles: {stats['main_articles_processed']}")
        logger.info(f"Supplementary articles: {stats['supplementary_articles_processed']}")
        logger.info(f"Total chunks created: {stats['total_chunks_created']}")
        logger.info(f"Processing time: {stats['processing_time']:.2f} seconds")
        logger.info(f"Vector store documents: {stats['vector_store_stats']['documents_count']}")
        
        if stats['errors']:
            logger.warning(f"Errors encountered: {len(stats['errors'])}")
            for error in stats['errors'][:5]:  # ì²˜ìŒ 5ê°??¤ë¥˜ë§?ì¶œë ¥
                logger.warning(f"  - {error}")
        
        logger.info("ML-enhanced vector embedding generation completed successfully!")
        return 0
        
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
