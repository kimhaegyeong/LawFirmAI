#!/usr/bin/env python3
"""
ML ê°•í™” ë²¡í„° ?„ë² ???ì„±ê¸?(CPU ìµœì ??ë²„ì „)
"""

import json
import logging
import sys
import os
import signal
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import gc

# ?„ë¡œ?íŠ¸ ë£¨íŠ¸ë¥?Python ê²½ë¡œ??ì¶”ê?
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from source.data.vector_store import LegalVectorStore

# Windows ì½˜ì†”?ì„œ UTF-8 ?¸ì½”???¤ì • (ê°œì„ ??ë²„ì „)
if os.name == 'nt':  # Windows
    try:
        # ?˜ê²½ë³€???¤ì •
        os.environ['PYTHONIOENCODING'] = 'utf-8'
        
        # ì½˜ì†” ?¸ì½”???¤ì • (?ˆì „??ë°©ë²•)
        if hasattr(sys.stdout, 'buffer'):
            import io
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
        else:
            # ?´ë? ?¤ì •??ê²½ìš° ë¬´ì‹œ
            pass
    except Exception as e:
        # ?¸ì½”???¤ì • ?¤íŒ¨ ??ê¸°ë³¸ ?¤ì • ? ì?
        print(f"Warning: Could not set UTF-8 encoding: {e}")

# ë¡œê¹… ?¤ì • (?ˆì „??ë°©ë²•)
def setup_safe_logging():
    """?ˆì „??ë¡œê¹… ?¤ì •"""
    try:
        # ê¸°ì¡´ ?¸ë“¤???œê±°
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
        
        # ?ˆë¡œ???¸ë“¤???¤ì •
        handler = logging.StreamHandler(sys.stdout)
        handler.setLevel(logging.INFO)
        
        # ?¬ë§·???¤ì •
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        
        # ë£¨íŠ¸ ë¡œê±°???¸ë“¤??ì¶”ê?
        logging.root.addHandler(handler)
        logging.root.setLevel(logging.INFO)
        
    except Exception as e:
        print(f"Warning: Could not setup logging: {e}")

# ?ˆì „??ë¡œê¹… ?¤ì • ?ìš©
setup_safe_logging()

logger = logging.getLogger(__name__)


class CheckpointManager:
    """ì²´í¬?¬ì¸??ê´€ë¦??´ë˜??""
    
    def __init__(self, checkpoint_file: str):
        self.checkpoint_file = Path(checkpoint_file)
        self.checkpoint_data = self._load_checkpoint()
    
    def _load_checkpoint(self) -> Dict[str, Any]:
        """ì²´í¬?¬ì¸??ë¡œë“œ"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load checkpoint: {e}")
        return {
            'completed_chunks': [],
            'total_chunks': 0,
            'start_time': None,
            'last_update': None
        }
    
    def save_checkpoint(self, completed_chunks: List[int], total_chunks: int):
        """ì²´í¬?¬ì¸???€??""
        try:
            checkpoint_data = {
                'completed_chunks': completed_chunks,
                'total_chunks': total_chunks,
                'start_time': self.checkpoint_data.get('start_time', time.time()),
                'last_update': time.time()
            }
            
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            
            self.checkpoint_data = checkpoint_data
            logger.info(f"Checkpoint saved: {len(completed_chunks)}/{total_chunks} chunks completed")
            
        except Exception as e:
            logger.error(f"Failed to save checkpoint: {e}")
    
    def get_remaining_chunks(self, total_chunks: int) -> List[int]:
        """?¨ì? ì²?¬ ëª©ë¡ ë°˜í™˜"""
        completed = set(self.checkpoint_data.get('completed_chunks', []))
        return [i for i in range(total_chunks) if i not in completed]
    
    def is_resume_needed(self) -> bool:
        """?¬ì‹œ?‘ì´ ?„ìš”?œì? ?•ì¸"""
        return len(self.checkpoint_data.get('completed_chunks', [])) > 0
    
    def get_progress_info(self) -> Dict[str, Any]:
        """ì§„í–‰ ?í™© ?•ë³´ ë°˜í™˜"""
        completed = len(self.checkpoint_data.get('completed_chunks', []))
        total = self.checkpoint_data.get('total_chunks', 0)
        start_time = self.checkpoint_data.get('start_time')
        
        progress_info = {
            'completed_chunks': completed,
            'total_chunks': total,
            'progress_percentage': (completed / max(total, 1)) * 100 if total > 0 else 0
        }
        
        if start_time:
            elapsed_time = time.time() - start_time
            progress_info['elapsed_time'] = elapsed_time
            if completed > 0:
                avg_time_per_chunk = elapsed_time / completed
                remaining_chunks = total - completed
                progress_info['estimated_remaining_time'] = avg_time_per_chunk * remaining_chunks
        
        return progress_info


class CPUOptimizedVectorBuilder:
    """CPU ?¬ìš©??ìµœì ?”ëœ ë²¡í„° ë¹Œë” (ko-sroberta-multitask ì§€??"""
    
    def __init__(self, model_name: str = "jhgan/ko-sroberta-multitask", 
                 batch_size: int = 20, chunk_size: int = 200):
        """
        CPU ìµœì ?”ëœ ë²¡í„° ë¹Œë” ì´ˆê¸°??
        
        Args:
            model_name: ?¬ìš©???„ë² ??ëª¨ë¸ëª?(ko-sroberta-multitask ì§€??
            batch_size: ?Œì¼ ë°°ì¹˜ ?¬ê¸° (?‘ê²Œ ?¤ì •)
            chunk_size: ë¬¸ì„œ ì²?¬ ?¬ê¸° (?‘ê²Œ ?¤ì •)
        """
        self.model_name = model_name
        self.batch_size = batch_size
        self.chunk_size = chunk_size
        
        # Graceful shutdown ?¤ì •
        self.shutdown_requested = False
        self._setup_signal_handlers()
        
        # ko-sroberta-multitask ëª¨ë¸???„ë² ??ì°¨ì› (768)
        embedding_dimension = 768 if "ko-sroberta-multitask" in model_name.lower() else 1024
        
        # ë²¡í„° ?¤í† ??ì´ˆê¸°??
        self.vector_store = LegalVectorStore(
            model_name=model_name,
            dimension=embedding_dimension,
            index_type="flat"
        )
        
        # ?µê³„ ì´ˆê¸°??
        self.stats = {
            'total_files_processed': 0,
            'total_laws_processed': 0,
            'total_articles_processed': 0,
            'main_articles_processed': 0,
            'supplementary_articles_processed': 0,
            'total_chunks_created': 0,
            'total_documents_created': 0,
            'errors': []
        }
        
        logger.info(f"CPUOptimizedVectorBuilder initialized with model: {model_name}")
        logger.info(f"Batch size: {batch_size}, Chunk size: {chunk_size}")
    
    def _setup_signal_handlers(self):
        """?œê·¸???¸ë“¤???¤ì • (Graceful shutdown)"""
        def signal_handler(signum, frame):
            logger.info(f"Received signal {signum}. Initiating graceful shutdown...")
            self.shutdown_requested = True
        
        # Windows?€ Unix ëª¨ë‘ ì§€??
        if hasattr(signal, 'SIGTERM'):
            signal.signal(signal.SIGTERM, signal_handler)
        if hasattr(signal, 'SIGINT'):
            signal.signal(signal.SIGINT, signal_handler)
        if hasattr(signal, 'SIGBREAK'):  # Windows
            signal.signal(signal.SIGBREAK, signal_handler)
    
    def build_embeddings(self, input_dir: str, output_dir: str, resume: bool = True) -> Dict[str, Any]:
        """
        CPU ìµœì ?”ëœ ë²¡í„° ?„ë² ???ì„± (ì²´í¬?¬ì¸??ì§€??
        
        Args:
            input_dir: ?…ë ¥ ?”ë ‰? ë¦¬
            output_dir: ì¶œë ¥ ?”ë ‰? ë¦¬
            resume: ì¤‘ë‹¨???‘ì—… ?¬ì‹œ???¬ë?
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ?µê³„
        """
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ì²´í¬?¬ì¸??ê´€ë¦¬ì ì´ˆê¸°??
        checkpoint_file = output_path / "embedding_checkpoint.json"
        checkpoint_manager = CheckpointManager(str(checkpoint_file))
        
        logger.info(f"Starting CPU-optimized vector embedding generation from: {input_path}")
        
        # ?¬ì‹œ???•ì¸
        if resume and checkpoint_manager.is_resume_needed():
            progress_info = checkpoint_manager.get_progress_info()
            logger.info(f"Resuming from checkpoint: {progress_info['completed_chunks']}/{progress_info['total_chunks']} chunks completed")
            logger.info(f"Progress: {progress_info['progress_percentage']:.1f}%")
            if 'estimated_remaining_time' in progress_info:
                remaining_hours = progress_info['estimated_remaining_time'] / 3600
                logger.info(f"Estimated remaining time: {remaining_hours:.1f} hours")
        
        # JSON ?Œì¼ ì°¾ê¸°
        json_files = list(input_path.rglob("ml_enhanced_*.json"))
        logger.info(f"Found {len(json_files)} ML-enhanced files to process")
        
        if not json_files:
            logger.warning("No ML-enhanced files found!")
            return self.stats
        
        # ?œì°¨ ì²˜ë¦¬ë¡?CPU ?¬ìš©??ìµœì†Œ??
        all_documents = []
        
        # ?Œì¼?¤ì„ ?‘ì? ë°°ì¹˜ë¡??˜ëˆ„???œì°¨ ì²˜ë¦¬
        batches = [json_files[i:i + self.batch_size] for i in range(0, len(json_files), self.batch_size)]
        logger.info(f"Processing {len(batches)} batches with {self.batch_size} files each")
        
        for batch_idx, batch_files in enumerate(tqdm(batches, desc="Processing batches", unit="batch")):
            try:
                batch_documents = self._process_batch_sequential(batch_files, batch_idx)
                all_documents.extend(batch_documents)
                
                # ë©”ëª¨ë¦??•ë¦¬
                gc.collect()
                
                # ì§„í–‰ ?í™© ë¡œê¹… (?ˆì „??ë°©ë²•)
                if (batch_idx + 1) % 5 == 0:
                    try:
                        logger.info(f"Processed {batch_idx + 1}/{len(batches)} batches")
                    except Exception:
                        # ë¡œê¹… ?¤íŒ¨ ??print ?¬ìš©
                        print(f"Processed {batch_idx + 1}/{len(batches)} batches")
                
            except Exception as e:
                error_msg = f"Error processing batch {batch_idx}: {e}"
                try:
                    logger.error(error_msg)
                except Exception:
                    print(f"ERROR: {error_msg}")
                self.stats['errors'].append(f"Batch {batch_idx} error: {e}")
        
        # ë²¡í„° ?„ë² ???ì„± ë°??€??(ì²´í¬?¬ì¸??ì§€??
        logger.info(f"Creating embeddings for {len(all_documents)} documents...")
        
        total_chunks = (len(all_documents) + self.chunk_size - 1) // self.chunk_size
        
        # ?¬ì‹œ?????¨ì? ì²?¬ë§?ì²˜ë¦¬
        if resume and checkpoint_manager.is_resume_needed():
            remaining_chunks = checkpoint_manager.get_remaining_chunks(total_chunks)
            logger.info(f"Processing {len(remaining_chunks)} remaining chunks out of {total_chunks}")
            chunk_indices = remaining_chunks
        else:
            chunk_indices = list(range(total_chunks))
        
        completed_chunks = checkpoint_manager.checkpoint_data.get('completed_chunks', [])
        
        for chunk_idx in tqdm(chunk_indices, desc="Creating embeddings"):
            # Graceful shutdown ?•ì¸
            if self.shutdown_requested:
                logger.info("Graceful shutdown requested. Saving checkpoint and exiting...")
                checkpoint_manager.save_checkpoint(completed_chunks, total_chunks)
                logger.info("Checkpoint saved. You can resume later with --resume flag.")
                return self.stats
            
            start_idx = chunk_idx * self.chunk_size
            end_idx = min(start_idx + self.chunk_size, len(all_documents))
            chunk = all_documents[start_idx:end_idx]
            
            texts = [doc['text'] for doc in chunk]
            metadatas = [doc['metadata'] for doc in chunk]
            
            try:
                self.vector_store.add_documents(texts, metadatas)
                completed_chunks.append(chunk_idx)
                
                # ì²´í¬?¬ì¸???€??(ë§?10ê°?ì²?¬ë§ˆë‹¤)
                if len(completed_chunks) % 10 == 0:
                    checkpoint_manager.save_checkpoint(completed_chunks, total_chunks)
                
                # ë©”ëª¨ë¦??•ë¦¬
                del texts, metadatas
                gc.collect()
                
            except Exception as e:
                error_msg = f"Error creating embeddings for chunk {chunk_idx}: {e}"
                try:
                    logger.error(error_msg)
                except Exception:
                    print(f"ERROR: {error_msg}")
                self.stats['errors'].append(f"Embedding chunk error: {e}")
        
        # ?¸ë±???€??
        index_path = output_path / "ml_enhanced_faiss_index"
        self.vector_store.save_index(str(index_path))
        
        # ?µê³„ ?€??
        stats_path = output_path / "ml_enhanced_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        # ?„ë£Œ ??ì²´í¬?¬ì¸???•ë¦¬
        if len(completed_chunks) == total_chunks:
            logger.info("All chunks completed. Cleaning up checkpoint file...")
            if checkpoint_file.exists():
                checkpoint_file.unlink()
                logger.info("Checkpoint file removed.")
        
        logger.info("CPU-optimized vector embedding generation completed!")
        logger.info(f"Total documents processed: {self.stats['total_documents_created']}")
        logger.info(f"Total articles processed: {self.stats['total_articles_processed']}")
        logger.info(f"Errors: {len(self.stats['errors'])}")
        
        return self.stats
    
    def _process_batch_sequential(self, batch_files: List[Path], batch_idx: int) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ?Œì¼?¤ì„ ?œì°¨ ì²˜ë¦¬"""
        batch_documents = []
        
        for file_path in batch_files:
            try:
                documents = self._process_single_file(file_path)
                batch_documents.extend(documents)
                self.stats['total_files_processed'] += 1
                self.stats['total_documents_created'] += len(documents)
                
            except Exception as e:
                error_msg = f"Error processing file {file_path}: {e}"
                try:
                    logger.error(error_msg)
                except Exception:
                    print(f"ERROR: {error_msg}")
                self.stats['errors'].append(error_msg)
        
        return batch_documents
    
    def _process_single_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """?¨ì¼ ?Œì¼ ì²˜ë¦¬ (ìµœì ??ë²„ì „)"""
        documents = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                file_data = json.load(f)
            
            # ?Œì¼ êµ¬ì¡° ?•ì¸
            if isinstance(file_data, dict) and 'laws' in file_data:
                laws = file_data['laws']
            elif isinstance(file_data, list):
                laws = file_data
            else:
                laws = [file_data]
            
            for law_data in laws:
                try:
                    # ë²•ë¥  ë©”í??°ì´??ì¶”ì¶œ
                    law_metadata = self._extract_law_metadata(law_data)
                    
                    # ë³¸ì¹™ ì¡°ë¬¸ ì²˜ë¦¬
                    articles = law_data.get('articles', [])
                    if not isinstance(articles, list):
                        articles = []
                    
                    # ë¶€ì¹?ì¡°ë¬¸ ì²˜ë¦¬
                    supplementary_articles = law_data.get('supplementary_articles', [])
                    if not isinstance(supplementary_articles, list):
                        supplementary_articles = []
                    
                    # ëª¨ë“  ì¡°ë¬¸???˜ë‚˜??ë¦¬ìŠ¤?¸ë¡œ ?©ì¹˜ê¸?
                    all_articles = articles + supplementary_articles
                    
                    # ë¬¸ì„œ ?ì„±
                    article_documents = self._create_article_documents_batch(
                        all_articles, law_metadata
                    )
                    documents.extend(article_documents)
                    
                    # ?µê³„ ?…ë°?´íŠ¸
                    self.stats['total_laws_processed'] += 1
                    self.stats['total_articles_processed'] += len(all_articles)
                    
                    main_articles = [a for a in all_articles if not a.get('is_supplementary', False)]
                    supp_articles = [a for a in all_articles if a.get('is_supplementary', False)]
                    
                    self.stats['main_articles_processed'] += len(main_articles)
                    self.stats['supplementary_articles_processed'] += len(supp_articles)
                    
                except Exception as e:
                    error_msg = f"Error processing law {law_data.get('law_name', 'Unknown')}: {e}"
                    try:
                        logger.error(error_msg)
                        logger.error(f"Law data keys: {list(law_data.keys())}")
                        if 'articles' in law_data and law_data['articles']:
                            first_article = law_data['articles'][0]
                            logger.error(f"First article keys: {list(first_article.keys())}")
                            logger.error(f"First article sub_articles type: {type(first_article.get('sub_articles'))}")
                            logger.error(f"First article references type: {type(first_article.get('references'))}")
                    except Exception:
                        print(f"ERROR: {error_msg}")
                    continue
        
        except Exception as e:
            error_msg = f"Error reading file {file_path}: {e}"
            try:
                logger.error(error_msg)
            except Exception:
                print(f"ERROR: {error_msg}")
            raise
        
        return documents
    
    def _create_article_documents_batch(self, articles: List[Dict[str, Any]], 
                                      law_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì¡°ë¬¸?¤ì„ ë°°ì¹˜ë¡?ë¬¸ì„œ ë³€??(ìµœì ??ë²„ì „)"""
        documents = []
        
        for article in articles:
            try:
                # ì¡°ë¬¸ ë©”í??°ì´???ì„±
                article_metadata = {
                    **law_metadata,
                    'article_number': article.get('article_number', ''),
                    'article_title': article.get('article_title', ''),
                    'article_type': 'main' if not article.get('is_supplementary', False) else 'supplementary',
                    'is_supplementary': article.get('is_supplementary', False),
                    'ml_confidence_score': article.get('ml_confidence_score'),
                    'parsing_method': article.get('parsing_method', 'ml_enhanced'),
                    'word_count': article.get('word_count', 0),
                    'char_count': article.get('char_count', 0),
                    'sub_articles_count': len(article.get('sub_articles', [])) if isinstance(article.get('sub_articles'), list) else 0,
                    'references_count': len(article.get('references', [])) if isinstance(article.get('references'), list) else 0
                }
                
                # ë¬¸ì„œ ID ?ì„±
                document_id = f"{law_metadata['law_id']}_article_{article_metadata['article_number']}"
                article_metadata['document_id'] = document_id
                
                # ?ìŠ¤??êµ¬ì„± (ìµœì ??
                text_parts = []
                
                # ì¡°ë¬¸ ë²ˆí˜¸?€ ?œëª©
                if article_metadata['article_number']:
                    if article_metadata['article_title']:
                        text_parts.append(f"{article_metadata['article_number']}({article_metadata['article_title']})")
                    else:
                        text_parts.append(article_metadata['article_number'])
                
                # ì¡°ë¬¸ ?´ìš©
                article_content = article.get('article_content', '')
                if article_content:
                    text_parts.append(article_content)
                
                # ?˜ìœ„ ì¡°ë¬¸??(?ˆì „??ì²˜ë¦¬)
                sub_articles = article.get('sub_articles', [])
                if isinstance(sub_articles, list):
                    for sub_article in sub_articles:
                        if isinstance(sub_article, dict):
                            sub_content = sub_article.get('content', '')
                            if sub_content:
                                text_parts.append(sub_content)
                elif isinstance(sub_articles, (int, float)):
                    # sub_articlesê°€ ?«ì??ê²½ìš° ë¬´ì‹œ
                    pass
                
                # ìµœì¢… ?ìŠ¤??
                full_text = ' '.join(text_parts)
                
                if full_text.strip():
                    document = {
                        'id': document_id,
                        'text': full_text,
                        'metadata': article_metadata,
                        'chunks': [{
                            'id': f"{document_id}_chunk_0",
                            'text': full_text,
                            'start_pos': 0,
                            'end_pos': len(full_text),
                            'entities': article.get('references', []) if isinstance(article.get('references'), list) else []
                        }]
                    }
                    documents.append(document)
                
            except Exception as e:
                error_msg = f"Error processing article {article.get('article_number', 'Unknown')}: {e}"
                try:
                    logger.error(error_msg)
                    logger.error(f"Article keys: {list(article.keys())}")
                    logger.error(f"Article sub_articles: {article.get('sub_articles')}")
                    logger.error(f"Article references: {article.get('references')}")
                except Exception:
                    print(f"ERROR: {error_msg}")
                continue
        
        return documents
    
    def _extract_law_metadata(self, law_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë²•ë¥  ë©”í??°ì´??ì¶”ì¶œ"""
        return {
            'law_id': law_data.get('law_id') or f"ml_enhanced_{law_data.get('law_name', 'unknown').replace(' ', '_')}",
            'law_name': law_data.get('law_name', ''),
            'law_type': law_data.get('law_type', ''),
            'category': law_data.get('category', ''),
            'promulgation_number': law_data.get('promulgation_number', ''),
            'promulgation_date': law_data.get('promulgation_date', ''),
            'enforcement_date': law_data.get('enforcement_date', ''),
            'amendment_type': law_data.get('amendment_type', ''),
            'ministry': law_data.get('ministry', ''),
            'ml_enhanced': law_data.get('ml_enhanced', True),
            'parsing_quality_score': law_data.get('data_quality', {}).get('parsing_quality_score', 0.0) if isinstance(law_data.get('data_quality'), dict) else 0.0,
            'processing_version': law_data.get('processing_version', 'ml_enhanced_v1.0'),
            'control_characters_removed': True
        }


def main():
    """ë©”ì¸ ?¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ML ê°•í™” ë²¡í„° ?„ë² ???ì„±ê¸?(CPU ìµœì ??ë²„ì „)")
    parser.add_argument("--input", required=True, help="?…ë ¥ ?”ë ‰? ë¦¬")
    parser.add_argument("--output", required=True, help="ì¶œë ¥ ?”ë ‰? ë¦¬")
    parser.add_argument("--batch-size", type=int, default=20, help="ë°°ì¹˜ ?¬ê¸° (ê¸°ë³¸ê°? 20)")
    parser.add_argument("--chunk-size", type=int, default=200, help="ì²?¬ ?¬ê¸° (ê¸°ë³¸ê°? 200)")
    parser.add_argument("--log-level", default="INFO", help="ë¡œê·¸ ?ˆë²¨")
    parser.add_argument("--resume", action="store_true", help="ì¤‘ë‹¨???‘ì—… ?¬ì‹œ??)
    parser.add_argument("--no-resume", action="store_true", help="ì²´í¬?¬ì¸??ë¬´ì‹œ?˜ê³  ì²˜ìŒë¶€???œì‘")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ?¤ì • (?ˆì „??ë°©ë²•)
    try:
        logging.basicConfig(
            level=getattr(logging, args.log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            force=True  # ê¸°ì¡´ ?¤ì • ê°•ì œ ?¬ì„¤??
        )
    except Exception as e:
        print(f"Warning: Could not setup logging: {e}")
        # ê¸°ë³¸ print ?¬ìš©
        print("Using basic print for output")
    
    # ë²¡í„° ë¹Œë” ì´ˆê¸°??ë°??¤í–‰ (ko-sroberta-multitask ?¬ìš©)
    builder = CPUOptimizedVectorBuilder(
        model_name="jhgan/ko-sroberta-multitask",
        batch_size=args.batch_size,
        chunk_size=args.chunk_size
    )
    
    stats = builder.build_embeddings(args.input, args.output, resume=not args.no_resume)
    
    print(f"\n=== ì²˜ë¦¬ ?„ë£Œ ===")
    print(f"ì´??Œì¼ ?? {stats['total_files_processed']}")
    print(f"ì´?ë²•ë¥  ?? {stats['total_laws_processed']}")
    print(f"ì´?ì¡°ë¬¸ ?? {stats['total_articles_processed']}")
    print(f"ì´?ë¬¸ì„œ ?? {stats['total_documents_created']}")
    print(f"?ëŸ¬ ?? {len(stats['errors'])}")


if __name__ == "__main__":
    main()
