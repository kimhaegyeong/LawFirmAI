#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë²•ë¥  ìˆ˜ì§‘ ë¡œê·¸ ë¶„ì„ ë„êµ¬

ìˆ˜ì§‘ëœ ë¡œê·¸ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì„±ëŠ¥ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import argparse
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import seaborn as sns

class LogAnalyzer:
    """ë¡œê·¸ ë¶„ì„ê¸°"""
    
    def __init__(self, log_dir: str = "data/raw/assembly"):
        self.log_dir = Path(log_dir)
        self.analysis_results = {}
    
    def analyze_collection_logs(self, date: str = None) -> Dict[str, Any]:
        """ìˆ˜ì§‘ ë¡œê·¸ ë¶„ì„"""
        if date is None:
            date = datetime.now().strftime("%Y%m%d")
        
        law_dir = self.log_dir / "law" / date
        
        if not law_dir.exists():
            print(f"No data found for date: {date}")
            return {}
        
        # í˜ì´ì§€ íŒŒì¼ë“¤ ë¶„ì„
        page_files = list(law_dir.glob("law_page_*.json"))
        
        if not page_files:
            print(f"No page files found in {law_dir}")
            return {}
        
        print(f"Analyzing {len(page_files)} page files...")
        
        # ë°ì´í„° ìˆ˜ì§‘
        pages_data = []
        laws_data = []
        
        for page_file in page_files:
            try:
                with open(page_file, 'r', encoding='utf-8') as f:
                    page_data = json.load(f)
                
                page_info = page_data.get('page_info', {})
                laws = page_data.get('laws', [])
                
                # í˜ì´ì§€ ì •ë³´ ìˆ˜ì§‘
                pages_data.append({
                    'page_number': page_info.get('page_number', 'unknown'),
                    'laws_count': page_info.get('laws_count', 0),
                    'saved_at': page_info.get('saved_at', ''),
                    'processing_time': page_info.get('processing_time', 0),
                    'file_name': page_file.name
                })
                
                # ë²•ë¥  ì •ë³´ ìˆ˜ì§‘
                for law in laws:
                    laws_data.append({
                        'cont_id': law.get('cont_id', ''),
                        'law_name': law.get('law_name', ''),
                        'law_type': law.get('law_type', ''),
                        'category': law.get('category', ''),
                        'page_number': page_info.get('page_number', 'unknown'),
                        'collected_at': law.get('collected_at', '')
                    })
                
            except Exception as e:
                print(f"Error reading {page_file}: {e}")
                continue
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        pages_df = pd.DataFrame(pages_data)
        laws_df = pd.DataFrame(laws_data)
        
        # ë¶„ì„ ìˆ˜í–‰
        analysis = self._perform_analysis(pages_df, laws_df)
        
        self.analysis_results[date] = analysis
        return analysis
    
    def _perform_analysis(self, pages_df: pd.DataFrame, laws_df: pd.DataFrame) -> Dict[str, Any]:
        """ë¶„ì„ ìˆ˜í–‰"""
        analysis = {
            'summary': {},
            'performance': {},
            'content': {},
            'timeline': {}
        }
        
        # ê¸°ë³¸ í†µê³„
        analysis['summary'] = {
            'total_pages': len(pages_df),
            'total_laws': len(laws_df),
            'avg_laws_per_page': laws_df.groupby('page_number').size().mean(),
            'total_processing_time': pages_df['processing_time'].sum(),
            'avg_processing_time': pages_df['processing_time'].mean(),
            'min_processing_time': pages_df['processing_time'].min(),
            'max_processing_time': pages_df['processing_time'].max()
        }
        
        # ì„±ëŠ¥ ë¶„ì„
        analysis['performance'] = {
            'throughput_laws_per_minute': len(laws_df) / (pages_df['processing_time'].sum() / 60) if pages_df['processing_time'].sum() > 0 else 0,
            'processing_time_trend': pages_df['processing_time'].tolist(),
            'laws_per_page_trend': pages_df['laws_count'].tolist(),
            'efficiency_score': len(laws_df) / pages_df['processing_time'].sum() if pages_df['processing_time'].sum() > 0 else 0
        }
        
        # ì½˜í…ì¸  ë¶„ì„
        analysis['content'] = {
            'law_types': laws_df['law_type'].value_counts().to_dict(),
            'categories': laws_df['category'].value_counts().to_dict(),
            'unique_laws': laws_df['law_name'].nunique(),
            'duplicate_laws': len(laws_df) - laws_df['law_name'].nunique()
        }
        
        # íƒ€ì„ë¼ì¸ ë¶„ì„
        if not pages_df.empty:
            pages_df['saved_at'] = pd.to_datetime(pages_df['saved_at'])
            analysis['timeline'] = {
                'start_time': pages_df['saved_at'].min().isoformat(),
                'end_time': pages_df['saved_at'].max().isoformat(),
                'duration_minutes': (pages_df['saved_at'].max() - pages_df['saved_at'].min()).total_seconds() / 60,
                'pages_per_minute': len(pages_df) / ((pages_df['saved_at'].max() - pages_df['saved_at'].min()).total_seconds() / 60) if pages_df['saved_at'].max() != pages_df['saved_at'].min() else 0
            }
        
        return analysis
    
    def generate_report(self, analysis: Dict[str, Any], output_file: str = None) -> str:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not analysis:
            return "No analysis data available"
        
        report = []
        report.append("# ë²•ë¥  ìˆ˜ì§‘ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ìš”ì•½ ì •ë³´
        summary = analysis.get('summary', {})
        report.append("## ğŸ“Š ìš”ì•½ ì •ë³´")
        report.append(f"- ì´ í˜ì´ì§€ ìˆ˜: {summary.get('total_pages', 0)}")
        report.append(f"- ì´ ë²•ë¥  ìˆ˜: {summary.get('total_laws', 0)}")
        report.append(f"- í˜ì´ì§€ë‹¹ í‰ê·  ë²•ë¥  ìˆ˜: {summary.get('avg_laws_per_page', 0):.2f}")
        report.append(f"- ì´ ì²˜ë¦¬ ì‹œê°„: {summary.get('total_processing_time', 0):.2f}ì´ˆ")
        report.append(f"- í‰ê·  ì²˜ë¦¬ ì‹œê°„: {summary.get('avg_processing_time', 0):.2f}ì´ˆ")
        report.append("")
        
        # ì„±ëŠ¥ ì •ë³´
        performance = analysis.get('performance', {})
        report.append("## âš¡ ì„±ëŠ¥ ë¶„ì„")
        report.append(f"- ì²˜ë¦¬ëŸ‰: {performance.get('throughput_laws_per_minute', 0):.2f} ë²•ë¥ /ë¶„")
        report.append(f"- íš¨ìœ¨ì„± ì ìˆ˜: {performance.get('efficiency_score', 0):.2f}")
        report.append("")
        
        # ì½˜í…ì¸  ì •ë³´
        content = analysis.get('content', {})
        report.append("## ğŸ“š ì½˜í…ì¸  ë¶„ì„")
        report.append(f"- ê³ ìœ  ë²•ë¥  ìˆ˜: {content.get('unique_laws', 0)}")
        report.append(f"- ì¤‘ë³µ ë²•ë¥  ìˆ˜: {content.get('duplicate_laws', 0)}")
        report.append("")
        
        # ë²•ë¥  ìœ í˜•ë³„ ë¶„í¬
        if content.get('law_types'):
            report.append("### ë²•ë¥  ìœ í˜•ë³„ ë¶„í¬")
            for law_type, count in content['law_types'].items():
                report.append(f"- {law_type}: {count}ê°œ")
            report.append("")
        
        # íƒ€ì„ë¼ì¸ ì •ë³´
        timeline = analysis.get('timeline', {})
        if timeline:
            report.append("## â° íƒ€ì„ë¼ì¸ ë¶„ì„")
            report.append(f"- ì‹œì‘ ì‹œê°„: {timeline.get('start_time', 'N/A')}")
            report.append(f"- ì¢…ë£Œ ì‹œê°„: {timeline.get('end_time', 'N/A')}")
            report.append(f"- ì´ ì†Œìš” ì‹œê°„: {timeline.get('duration_minutes', 0):.2f}ë¶„")
            report.append(f"- ë¶„ë‹¹ í˜ì´ì§€ ìˆ˜: {timeline.get('pages_per_minute', 0):.2f}")
            report.append("")
        
        report_text = "\n".join(report)
        
        # íŒŒì¼ ì €ì¥
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"Report saved to: {output_file}")
        
        return report_text
    
    def compare_runs(self, date1: str, date2: str) -> str:
        """ë‘ ì‹¤í–‰ ê²°ê³¼ ë¹„êµ"""
        analysis1 = self.analyze_collection_logs(date1)
        analysis2 = self.analyze_collection_logs(date2)
        
        if not analysis1 or not analysis2:
            return "Cannot compare - insufficient data"
        
        comparison = []
        comparison.append("# ë²•ë¥  ìˆ˜ì§‘ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸")
        comparison.append(f"ë¹„êµ ëŒ€ìƒ: {date1} vs {date2}")
        comparison.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        comparison.append("")
        
        # ì„±ëŠ¥ ë¹„êµ
        perf1 = analysis1.get('performance', {})
        perf2 = analysis2.get('performance', {})
        
        comparison.append("## âš¡ ì„±ëŠ¥ ë¹„êµ")
        throughput1 = perf1.get('throughput_laws_per_minute', 0)
        throughput2 = perf2.get('throughput_laws_per_minute', 0)
        throughput_change = ((throughput2 - throughput1) / throughput1 * 100) if throughput1 > 0 else 0
        
        comparison.append(f"- ì²˜ë¦¬ëŸ‰ ë³€í™”: {throughput1:.2f} â†’ {throughput2:.2f} ë²•ë¥ /ë¶„ ({throughput_change:+.1f}%)")
        
        # ìš”ì•½ ë¹„êµ
        summary1 = analysis1.get('summary', {})
        summary2 = analysis2.get('summary', {})
        
        comparison.append("## ğŸ“Š ìˆ˜ì§‘ëŸ‰ ë¹„êµ")
        comparison.append(f"- ì´ ë²•ë¥  ìˆ˜: {summary1.get('total_laws', 0)} â†’ {summary2.get('total_laws', 0)}")
        comparison.append(f"- í‰ê·  ì²˜ë¦¬ ì‹œê°„: {summary1.get('avg_processing_time', 0):.2f} â†’ {summary2.get('avg_processing_time', 0):.2f}ì´ˆ")
        
        return "\n".join(comparison)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë²•ë¥  ìˆ˜ì§‘ ë¡œê·¸ ë¶„ì„ ë„êµ¬')
    parser.add_argument('--date', type=str, help='ë¶„ì„í•  ë‚ ì§œ (YYYYMMDD í˜•ì‹)')
    parser.add_argument('--output', type=str, help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--compare', nargs=2, metavar=('DATE1', 'DATE2'), 
                       help='ë‘ ë‚ ì§œì˜ ì‹¤í–‰ ê²°ê³¼ ë¹„êµ')
    parser.add_argument('--log-dir', type=str, default='data/raw/assembly',
                       help='ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    analyzer = LogAnalyzer(args.log_dir)
    
    if args.compare:
        # ë¹„êµ ë¶„ì„
        result = analyzer.compare_runs(args.compare[0], args.compare[1])
        print(result)
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(result)
            print(f"Comparison report saved to: {args.output}")
    
    else:
        # ë‹¨ì¼ ë¶„ì„
        analysis = analyzer.analyze_collection_logs(args.date)
        
        if analysis:
            report = analyzer.generate_report(analysis, args.output)
            print(report)
        else:
            print("No analysis data available")


if __name__ == "__main__":
    main()
