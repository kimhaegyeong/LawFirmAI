#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë²•ë¥  ?˜ì§‘ ë¡œê·¸ ë¶„ì„ ?„êµ¬

?˜ì§‘??ë¡œê·¸ ?Œì¼??ë¶„ì„?˜ì—¬ ?±ëŠ¥ ë¦¬í¬?¸ë? ?ì„±?©ë‹ˆ??
"""

import argparse
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import seaborn as sns

class LogAnalyzer:
    """ë¡œê·¸ ë¶„ì„ê¸?""
    
    def __init__(self, log_dir: str = "data/raw/assembly"):
        self.log_dir = Path(log_dir)
        self.analysis_results = {}
    
    def analyze_collection_logs(self, date: str = None) -> Dict[str, Any]:
        """?˜ì§‘ ë¡œê·¸ ë¶„ì„"""
        if date is None:
            date = datetime.now().strftime("%Y%m%d")
        
        law_dir = self.log_dir / "law" / date
        
        if not law_dir.exists():
            print(f"No data found for date: {date}")
            return {}
        
        # ?˜ì´ì§€ ?Œì¼??ë¶„ì„
        page_files = list(law_dir.glob("law_page_*.json"))
        
        if not page_files:
            print(f"No page files found in {law_dir}")
            return {}
        
        print(f"Analyzing {len(page_files)} page files...")
        
        # ?°ì´???˜ì§‘
        pages_data = []
        laws_data = []
        
        for page_file in page_files:
            try:
                with open(page_file, 'r', encoding='utf-8') as f:
                    page_data = json.load(f)
                
                page_info = page_data.get('page_info', {})
                laws = page_data.get('laws', [])
                
                # ?˜ì´ì§€ ?•ë³´ ?˜ì§‘
                pages_data.append({
                    'page_number': page_info.get('page_number', 'unknown'),
                    'laws_count': page_info.get('laws_count', 0),
                    'saved_at': page_info.get('saved_at', ''),
                    'processing_time': page_info.get('processing_time', 0),
                    'file_name': page_file.name
                })
                
                # ë²•ë¥  ?•ë³´ ?˜ì§‘
                for law in laws:
                    laws_data.append({
                        'cont_id': law.get('cont_id', ''),
                        'law_name': law.get('law_name', ''),
                        'law_type': law.get('law_type', ''),
                        'category': law.get('category', ''),
                        'page_number': page_info.get('page_number', 'unknown'),
                        'collected_at': law.get('collected_at', '')
                    })
                
            except Exception as e:
                print(f"Error reading {page_file}: {e}")
                continue
        
        # ?°ì´?°í”„?ˆì„ ?ì„±
        pages_df = pd.DataFrame(pages_data)
        laws_df = pd.DataFrame(laws_data)
        
        # ë¶„ì„ ?˜í–‰
        analysis = self._perform_analysis(pages_df, laws_df)
        
        self.analysis_results[date] = analysis
        return analysis
    
    def _perform_analysis(self, pages_df: pd.DataFrame, laws_df: pd.DataFrame) -> Dict[str, Any]:
        """ë¶„ì„ ?˜í–‰"""
        analysis = {
            'summary': {},
            'performance': {},
            'content': {},
            'timeline': {}
        }
        
        # ê¸°ë³¸ ?µê³„
        analysis['summary'] = {
            'total_pages': len(pages_df),
            'total_laws': len(laws_df),
            'avg_laws_per_page': laws_df.groupby('page_number').size().mean(),
            'total_processing_time': pages_df['processing_time'].sum(),
            'avg_processing_time': pages_df['processing_time'].mean(),
            'min_processing_time': pages_df['processing_time'].min(),
            'max_processing_time': pages_df['processing_time'].max()
        }
        
        # ?±ëŠ¥ ë¶„ì„
        analysis['performance'] = {
            'throughput_laws_per_minute': len(laws_df) / (pages_df['processing_time'].sum() / 60) if pages_df['processing_time'].sum() > 0 else 0,
            'processing_time_trend': pages_df['processing_time'].tolist(),
            'laws_per_page_trend': pages_df['laws_count'].tolist(),
            'efficiency_score': len(laws_df) / pages_df['processing_time'].sum() if pages_df['processing_time'].sum() > 0 else 0
        }
        
        # ì½˜í…ì¸?ë¶„ì„
        analysis['content'] = {
            'law_types': laws_df['law_type'].value_counts().to_dict(),
            'categories': laws_df['category'].value_counts().to_dict(),
            'unique_laws': laws_df['law_name'].nunique(),
            'duplicate_laws': len(laws_df) - laws_df['law_name'].nunique()
        }
        
        # ?€?„ë¼??ë¶„ì„
        if not pages_df.empty:
            pages_df['saved_at'] = pd.to_datetime(pages_df['saved_at'])
            analysis['timeline'] = {
                'start_time': pages_df['saved_at'].min().isoformat(),
                'end_time': pages_df['saved_at'].max().isoformat(),
                'duration_minutes': (pages_df['saved_at'].max() - pages_df['saved_at'].min()).total_seconds() / 60,
                'pages_per_minute': len(pages_df) / ((pages_df['saved_at'].max() - pages_df['saved_at'].min()).total_seconds() / 60) if pages_df['saved_at'].max() != pages_df['saved_at'].min() else 0
            }
        
        return analysis
    
    def generate_report(self, analysis: Dict[str, Any], output_file: str = None) -> str:
        """ë¶„ì„ ë¦¬í¬???ì„±"""
        if not analysis:
            return "No analysis data available"
        
        report = []
        report.append("# ë²•ë¥  ?˜ì§‘ ?±ëŠ¥ ë¶„ì„ ë¦¬í¬??)
        report.append(f"?ì„± ?œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ?”ì•½ ?•ë³´
        summary = analysis.get('summary', {})
        report.append("## ?“Š ?”ì•½ ?•ë³´")
        report.append(f"- ì´??˜ì´ì§€ ?? {summary.get('total_pages', 0)}")
        report.append(f"- ì´?ë²•ë¥  ?? {summary.get('total_laws', 0)}")
        report.append(f"- ?˜ì´ì§€???‰ê·  ë²•ë¥  ?? {summary.get('avg_laws_per_page', 0):.2f}")
        report.append(f"- ì´?ì²˜ë¦¬ ?œê°„: {summary.get('total_processing_time', 0):.2f}ì´?)
        report.append(f"- ?‰ê·  ì²˜ë¦¬ ?œê°„: {summary.get('avg_processing_time', 0):.2f}ì´?)
        report.append("")
        
        # ?±ëŠ¥ ?•ë³´
        performance = analysis.get('performance', {})
        report.append("## ???±ëŠ¥ ë¶„ì„")
        report.append(f"- ì²˜ë¦¬?? {performance.get('throughput_laws_per_minute', 0):.2f} ë²•ë¥ /ë¶?)
        report.append(f"- ?¨ìœ¨???ìˆ˜: {performance.get('efficiency_score', 0):.2f}")
        report.append("")
        
        # ì½˜í…ì¸??•ë³´
        content = analysis.get('content', {})
        report.append("## ?“š ì½˜í…ì¸?ë¶„ì„")
        report.append(f"- ê³ ìœ  ë²•ë¥  ?? {content.get('unique_laws', 0)}")
        report.append(f"- ì¤‘ë³µ ë²•ë¥  ?? {content.get('duplicate_laws', 0)}")
        report.append("")
        
        # ë²•ë¥  ? í˜•ë³?ë¶„í¬
        if content.get('law_types'):
            report.append("### ë²•ë¥  ? í˜•ë³?ë¶„í¬")
            for law_type, count in content['law_types'].items():
                report.append(f"- {law_type}: {count}ê°?)
            report.append("")
        
        # ?€?„ë¼???•ë³´
        timeline = analysis.get('timeline', {})
        if timeline:
            report.append("## ???€?„ë¼??ë¶„ì„")
            report.append(f"- ?œì‘ ?œê°„: {timeline.get('start_time', 'N/A')}")
            report.append(f"- ì¢…ë£Œ ?œê°„: {timeline.get('end_time', 'N/A')}")
            report.append(f"- ì´??Œìš” ?œê°„: {timeline.get('duration_minutes', 0):.2f}ë¶?)
            report.append(f"- ë¶„ë‹¹ ?˜ì´ì§€ ?? {timeline.get('pages_per_minute', 0):.2f}")
            report.append("")
        
        report_text = "\n".join(report)
        
        # ?Œì¼ ?€??
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"Report saved to: {output_file}")
        
        return report_text
    
    def compare_runs(self, date1: str, date2: str) -> str:
        """???¤í–‰ ê²°ê³¼ ë¹„êµ"""
        analysis1 = self.analyze_collection_logs(date1)
        analysis2 = self.analyze_collection_logs(date2)
        
        if not analysis1 or not analysis2:
            return "Cannot compare - insufficient data"
        
        comparison = []
        comparison.append("# ë²•ë¥  ?˜ì§‘ ?±ëŠ¥ ë¹„êµ ë¦¬í¬??)
        comparison.append(f"ë¹„êµ ?€?? {date1} vs {date2}")
        comparison.append(f"?ì„± ?œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        comparison.append("")
        
        # ?±ëŠ¥ ë¹„êµ
        perf1 = analysis1.get('performance', {})
        perf2 = analysis2.get('performance', {})
        
        comparison.append("## ???±ëŠ¥ ë¹„êµ")
        throughput1 = perf1.get('throughput_laws_per_minute', 0)
        throughput2 = perf2.get('throughput_laws_per_minute', 0)
        throughput_change = ((throughput2 - throughput1) / throughput1 * 100) if throughput1 > 0 else 0
        
        comparison.append(f"- ì²˜ë¦¬??ë³€?? {throughput1:.2f} ??{throughput2:.2f} ë²•ë¥ /ë¶?({throughput_change:+.1f}%)")
        
        # ?”ì•½ ë¹„êµ
        summary1 = analysis1.get('summary', {})
        summary2 = analysis2.get('summary', {})
        
        comparison.append("## ?“Š ?˜ì§‘??ë¹„êµ")
        comparison.append(f"- ì´?ë²•ë¥  ?? {summary1.get('total_laws', 0)} ??{summary2.get('total_laws', 0)}")
        comparison.append(f"- ?‰ê·  ì²˜ë¦¬ ?œê°„: {summary1.get('avg_processing_time', 0):.2f} ??{summary2.get('avg_processing_time', 0):.2f}ì´?)
        
        return "\n".join(comparison)


def main():
    """ë©”ì¸ ?¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë²•ë¥  ?˜ì§‘ ë¡œê·¸ ë¶„ì„ ?„êµ¬')
    parser.add_argument('--date', type=str, help='ë¶„ì„??? ì§œ (YYYYMMDD ?•ì‹)')
    parser.add_argument('--output', type=str, help='ì¶œë ¥ ?Œì¼ ê²½ë¡œ')
    parser.add_argument('--compare', nargs=2, metavar=('DATE1', 'DATE2'), 
                       help='??? ì§œ???¤í–‰ ê²°ê³¼ ë¹„êµ')
    parser.add_argument('--log-dir', type=str, default='data/raw/assembly',
                       help='ë¡œê·¸ ?”ë ‰? ë¦¬ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    analyzer = LogAnalyzer(args.log_dir)
    
    if args.compare:
        # ë¹„êµ ë¶„ì„
        result = analyzer.compare_runs(args.compare[0], args.compare[1])
        print(result)
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(result)
            print(f"Comparison report saved to: {args.output}")
    
    else:
        # ?¨ì¼ ë¶„ì„
        analysis = analyzer.analyze_collection_logs(args.date)
        
        if analysis:
            report = analyzer.generate_report(analysis, args.output)
            print(report)
        else:
            print("No analysis data available")


if __name__ == "__main__":
    main()
