"""
Day 4 ëª¨ë¸ í‰ê°€ ë° ìµœì í™” í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ê³ ë„í™”ëœ í‰ê°€, ìµœì í™”, A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•© ì‹¤í–‰
"""

import argparse
import json
import logging
import sys
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import torch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from source.models.legal_finetuner import LegalModelFineTuner, LegalModelEvaluator
from source.models.advanced_evaluator import AdvancedLegalEvaluator
from source.models.model_optimizer import LegalModelOptimizer
from source.models.ab_test_framework import ABTestFramework, ModelVariant

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/day4_evaluation_optimization.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class Day4EvaluationOptimizationPipeline:
    """Day 4 í‰ê°€ ë° ìµœì í™” íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.models = {}
        self.evaluation_results = {}
        self.optimization_results = {}
        self.ab_test_results = {}
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        Path("logs").mkdir(exist_ok=True)
        
        logger.info("Day4EvaluationOptimizationPipeline initialized")
    
    def load_test_data(self) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        logger.info("Loading test data...")
        
        try:
            test_path = Path(self.config["data"]["test_path"])
            with open(test_path, "r", encoding="utf-8") as f:
                test_data = json.load(f)
            
            logger.info(f"Test data loaded successfully: {len(test_data)} samples")
            return test_data
            
        except Exception as e:
            logger.error(f"Failed to load test data: {e}")
            raise
    
    def load_models(self):
        """ëª¨ë¸ë“¤ ë¡œë“œ"""
        logger.info("Loading models...")
        
        try:
            # ì›ë³¸ ëª¨ë¸ ë¡œë“œ
            original_model_path = self.config["models"]["original_path"]
            if Path(original_model_path).exists():
                self.models["original"] = LegalModelFineTuner(device="cpu")
                self.models["original"].load_model(original_model_path)
                logger.info("Original model loaded successfully")
            
            # LoRA íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ
            lora_model_path = self.config["models"]["lora_path"]
            if Path(lora_model_path).exists():
                self.models["lora"] = LegalModelFineTuner(device="cpu")
                self.models["lora"].load_model(lora_model_path)
                logger.info("LoRA model loaded successfully")
            
            # ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
            optimized_model_path = self.config["models"].get("optimized_path")
            if optimized_model_path and Path(optimized_model_path).exists():
                self.models["optimized"] = LegalModelFineTuner(device="cpu")
                self.models["optimized"].load_model(optimized_model_path)
                logger.info("Optimized model loaded successfully")
            
            logger.info(f"Total models loaded: {len(self.models)}")
            
        except Exception as e:
            logger.error(f"Failed to load models: {e}")
            raise
    
    def run_comprehensive_evaluation(self, test_data: List[Dict[str, Any]]):
        """ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        logger.info("Starting comprehensive evaluation...")
        
        try:
            for model_name, model in self.models.items():
                logger.info(f"Evaluating model: {model_name}")
                
                # ê³ ë„í™”ëœ í‰ê°€ ì‹¤í–‰
                evaluator = AdvancedLegalEvaluator(model.model, model.tokenizer)
                evaluation_results = evaluator.comprehensive_evaluation(test_data)
                
                self.evaluation_results[model_name] = evaluation_results
                
                # ê²°ê³¼ ì €ì¥
                self._save_evaluation_results(model_name, evaluation_results)
            
            logger.info("Comprehensive evaluation completed")
            
        except Exception as e:
            logger.error(f"Comprehensive evaluation failed: {e}")
            raise
    
    def run_model_optimization(self):
        """ëª¨ë¸ ìµœì í™” ì‹¤í–‰"""
        logger.info("Starting model optimization...")
        
        try:
            for model_name, model in self.models.items():
                logger.info(f"Optimizing model: {model_name}")
                
                # ëª¨ë¸ ìµœì í™” ì‹¤í–‰
                optimizer = LegalModelOptimizer(model.model, model.tokenizer, device="cpu")
                optimization_results = optimizer.comprehensive_optimization(
                    f"models/optimized/{model_name}"
                )
                
                self.optimization_results[model_name] = optimization_results
                
                # ê²°ê³¼ ì €ì¥
                self._save_optimization_results(model_name, optimization_results)
            
            logger.info("Model optimization completed")
            
        except Exception as e:
            logger.error(f"Model optimization failed: {e}")
            raise
    
    def run_ab_testing(self, test_data: List[Dict[str, Any]]):
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("Starting A/B testing...")
        
        try:
            # A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™”
            ab_test = ABTestFramework("legal_model_comparison")
            
            # ëª¨ë¸ ë³€í˜• ì¶”ê°€
            for model_name, model in self.models.items():
                variant = ModelVariant(
                    name=model_name,
                    model_path=f"models/{model_name}",
                    description=f"{model_name} ëª¨ë¸",
                    config={"model_type": model_name},
                    weight=1.0
                )
                ab_test.add_variant(variant)
            
            # í…ŒìŠ¤íŠ¸ êµ¬ì„±
            ab_test.configure_test(
                test_duration_days=self.config["ab_test"]["duration_days"],
                min_sample_size=self.config["ab_test"]["min_sample_size"],
                confidence_level=self.config["ab_test"]["confidence_level"],
                primary_metric=self.config["ab_test"]["primary_metric"]
            )
            
            # A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            ab_test_results = ab_test.run_ab_test(test_data)
            self.ab_test_results = ab_test_results
            
            logger.info("A/B testing completed")
            
        except Exception as e:
            logger.error(f"A/B testing failed: {e}")
            raise
    
    def generate_comprehensive_report(self):
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        logger.info("Generating comprehensive report...")
        
        try:
            report = {
                "report_info": {
                    "generation_time": datetime.now().isoformat(),
                    "total_models": len(self.models),
                    "evaluation_completed": len(self.evaluation_results),
                    "optimization_completed": len(self.optimization_results),
                    "ab_test_completed": bool(self.ab_test_results)
                },
                "evaluation_summary": self._summarize_evaluations(),
                "optimization_summary": self._summarize_optimizations(),
                "ab_test_summary": self._summarize_ab_test(),
                "recommendations": self._generate_final_recommendations()
            }
            
            # ë³´ê³ ì„œ ì €ì¥
            self._save_comprehensive_report(report)
            
            logger.info("Comprehensive report generated")
            return report
            
        except Exception as e:
            logger.error(f"Failed to generate comprehensive report: {e}")
            raise
    
    def _summarize_evaluations(self) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ ìš”ì•½"""
        summary = {
            "model_rankings": {},
            "best_performing_model": None,
            "performance_comparison": {}
        }
        
        if not self.evaluation_results:
            return summary
        
        # ê° ëª¨ë¸ì˜ ì¢…í•© ì ìˆ˜ ìˆ˜ì§‘
        comprehensive_scores = {}
        for model_name, results in self.evaluation_results.items():
            comprehensive_score = results["summary"]["comprehensive_score"]
            comprehensive_scores[model_name] = comprehensive_score
        
        # ëª¨ë¸ ìˆœìœ„ ê²°ì •
        sorted_models = sorted(comprehensive_scores.items(), key=lambda x: x[1], reverse=True)
        summary["model_rankings"] = {model: rank + 1 for rank, (model, _) in enumerate(sorted_models)}
        summary["best_performing_model"] = sorted_models[0][0] if sorted_models else None
        
        # ì„±ëŠ¥ ë¹„êµ
        summary["performance_comparison"] = comprehensive_scores
        
        return summary
    
    def _summarize_optimizations(self) -> Dict[str, Any]:
        """ìµœì í™” ê²°ê³¼ ìš”ì•½"""
        summary = {
            "optimization_methods": set(),
            "performance_improvements": {},
            "deployment_readiness": {}
        }
        
        if not self.optimization_results:
            return summary
        
        for model_name, results in self.optimization_results.items():
            optimization_summary = results["optimization_summary"]
            
            # ìµœì í™” ë°©ë²• ìˆ˜ì§‘
            summary["optimization_methods"].update(optimization_summary["optimization_methods"])
            
            # ì„±ëŠ¥ ê°œì„  ì‚¬í•­
            summary["performance_improvements"][model_name] = optimization_summary["performance_improvements"]
            
            # ë°°í¬ ì¤€ë¹„ë„
            summary["deployment_readiness"][model_name] = optimization_summary["deployment_readiness"]
        
        summary["optimization_methods"] = list(summary["optimization_methods"])
        
        return summary
    
    def _summarize_ab_test(self) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½"""
        if not self.ab_test_results:
            return {"status": "not_completed"}
        
        return {
            "winner": self.ab_test_results["winner"],
            "statistical_analysis": self.ab_test_results["statistical_analysis"],
            "recommendations": self.ab_test_results["recommendations"]
        }
    
    def _generate_final_recommendations(self) -> List[str]:
        """ìµœì¢… ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # í‰ê°€ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if self.evaluation_results:
            best_model = self._summarize_evaluations()["best_performing_model"]
            if best_model:
                recommendations.append(f"{best_model} ëª¨ë¸ì´ ìµœê³  ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. í”„ë¡œë•ì…˜ ë°°í¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        # ìµœì í™” ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if self.optimization_results:
            recommendations.append("ëª¨ë¸ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ONNX ë³€í™˜ê³¼ ì–‘ìí™”ë¥¼ í†µí•´ ë°°í¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if self.ab_test_results:
            winner = self.ab_test_results["winner"]
            if winner["statistically_significant"]:
                recommendations.append(f"A/B í…ŒìŠ¤íŠ¸ì—ì„œ {winner['variant_name']}ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
            else:
                recommendations.append("A/B í…ŒìŠ¤íŠ¸ì—ì„œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤. ë” í° ìƒ˜í”Œ í¬ê¸°ë¡œ ì¬í…ŒìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        recommendations.extend([
            "ì •ê¸°ì ì¸ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì„ ì„¤ì •í•˜ì„¸ìš”.",
            "ìƒˆë¡œìš´ ë²•ë¥  ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”.",
            "ì‚¬ìš©ì í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í•˜ì„¸ìš”."
        ])
        
        return recommendations
    
    def _save_evaluation_results(self, model_name: str, results: Dict[str, Any]):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        output_dir = Path("results") / "evaluations" / model_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        results_path = output_dir / "evaluation_results.json"
        with open(results_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
    
    def _save_optimization_results(self, model_name: str, results: Dict[str, Any]):
        """ìµœì í™” ê²°ê³¼ ì €ì¥"""
        output_dir = Path("results") / "optimizations" / model_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        results_path = output_dir / "optimization_results.json"
        with open(results_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
    
    def _save_comprehensive_report(self, report: Dict[str, Any]):
        """ì¢…í•© ë³´ê³ ì„œ ì €ì¥"""
        output_dir = Path("results") / "day4_comprehensive"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # JSON ë³´ê³ ì„œ ì €ì¥
        report_path = output_dir / "comprehensive_report.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=4)
        
        # í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        summary_report = self._generate_text_summary_report(report)
        summary_path = output_dir / "summary_report.txt"
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write(summary_report)
        
        logger.info(f"Comprehensive report saved to {output_dir}")
    
    def _generate_text_summary_report(self, report: Dict[str, Any]) -> str:
        """í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        report_info = report["report_info"]
        eval_summary = report["evaluation_summary"]
        opt_summary = report["optimization_summary"]
        ab_summary = report["ab_test_summary"]
        
        summary = f"""
Day 4 ëª¨ë¸ í‰ê°€ ë° ìµœì í™” ì¢…í•© ë³´ê³ ì„œ
=====================================

ìƒì„± ì‹œê°„: {report_info['generation_time']}
ì´ ëª¨ë¸ ìˆ˜: {report_info['total_models']}
í‰ê°€ ì™„ë£Œ: {report_info['evaluation_completed']}
ìµœì í™” ì™„ë£Œ: {report_info['optimization_completed']}
A/B í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {'Yes' if report_info['ab_test_completed'] else 'No'}

í‰ê°€ ê²°ê³¼ ìš”ì•½:
"""
        
        if eval_summary["best_performing_model"]:
            summary += f"- ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {eval_summary['best_performing_model']}\n"
            summary += "- ëª¨ë¸ ìˆœìœ„:\n"
            for model, rank in eval_summary["model_rankings"].items():
                score = eval_summary["performance_comparison"].get(model, 0)
                summary += f"  {rank}. {model}: {score:.3f}\n"
        
        summary += f"""
ìµœì í™” ê²°ê³¼ ìš”ì•½:
- ì ìš©ëœ ìµœì í™” ë°©ë²•: {', '.join(opt_summary.get('optimization_methods', []))}
"""
        
        if ab_summary.get("status") != "not_completed":
            winner = ab_summary["winner"]
            summary += f"""
A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼:
- ìŠ¹ì: {winner['variant_name']}
- ì£¼ìš” ë©”íŠ¸ë¦­ ì ìˆ˜: {winner['primary_metric_score']:.3f}
- í†µê³„ì  ìœ ì˜ì„±: {'Yes' if winner['statistically_significant'] else 'No'}
- ì‹ ë¢°ë„ ì ìˆ˜: {winner['confidence_score']:.3f}
"""
        
        summary += """
ìµœì¢… ê¶Œì¥ì‚¬í•­:
"""
        for i, recommendation in enumerate(report["recommendations"], 1):
            summary += f"{i}. {recommendation}\n"
        
        summary += f"""
ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {datetime.now().isoformat()}
"""
        
        return summary
    
    def run_complete_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("Starting Day 4 complete pipeline...")
        
        start_time = datetime.now()
        
        try:
            # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
            test_data = self.load_test_data()
            
            # 2. ëª¨ë¸ë“¤ ë¡œë“œ
            self.load_models()
            
            # 3. ì¢…í•© í‰ê°€ ì‹¤í–‰
            self.run_comprehensive_evaluation(test_data)
            
            # 4. ëª¨ë¸ ìµœì í™” ì‹¤í–‰
            self.run_model_optimization()
            
            # 5. A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            self.run_ab_testing(test_data)
            
            # 6. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
            report = self.generate_comprehensive_report()
            
            # ì™„ë£Œ ë³´ê³ 
            end_time = datetime.now()
            duration = end_time - start_time
            
            print("\n" + "="*60)
            print("ğŸ‰ Day 4 ëª¨ë¸ í‰ê°€ ë° ìµœì í™” ì™„ë£Œ!")
            print("="*60)
            print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {duration.total_seconds()/60:.1f}ë¶„")
            print(f"ğŸ“Š í‰ê°€ëœ ëª¨ë¸: {len(self.models)}ê°œ")
            print(f"ğŸ”§ ìµœì í™” ì™„ë£Œ: {len(self.optimization_results)}ê°œ")
            print(f"ğŸ§ª A/B í…ŒìŠ¤íŠ¸: {'ì™„ë£Œ' if self.ab_test_results else 'ë¯¸ì™„ë£Œ'}")
            
            if report["evaluation_summary"]["best_performing_model"]:
                best_model = report["evaluation_summary"]["best_performing_model"]
                print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model}")
            
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: results/day4_comprehensive/")
            print("="*60)
            
            logger.info("Day 4 complete pipeline finished successfully!")
            
        except Exception as e:
            logger.error(f"Day 4 pipeline failed: {e}")
            raise


def create_default_config() -> Dict[str, Any]:
    """ê¸°ë³¸ ì„¤ì • ìƒì„±"""
    return {
        "data": {
            "test_path": "data/training/test_split.json"
        },
        "models": {
            "original_path": "models/original",
            "lora_path": "models/test/kogpt2-legal-lora-test",
            "optimized_path": "models/optimized"
        },
        "ab_test": {
            "duration_days": 7,
            "min_sample_size": 50,
            "confidence_level": 0.95,
            "primary_metric": "comprehensive_score"
        }
    }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Day 4 ëª¨ë¸ í‰ê°€ ë° ìµœì í™”")
    parser.add_argument("--config", type=str, help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--test-data", type=str, default="data/training/test_split.json", help="í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("--models", type=str, nargs="+", help="í‰ê°€í•  ëª¨ë¸ ê²½ë¡œë“¤")
    parser.add_argument("--skip-optimization", action="store_true", help="ìµœì í™” ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--skip-ab-test", action="store_true", help="A/B í…ŒìŠ¤íŠ¸ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ ë˜ëŠ” ìƒì„±
    if args.config and Path(args.config).exists():
        with open(args.config, "r", encoding="utf-8") as f:
            config = json.load(f)
    else:
        config = create_default_config()
        
        # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸
        config["data"]["test_path"] = args.test_data
        if args.models:
            config["models"]["lora_path"] = args.models[0] if len(args.models) > 0 else config["models"]["lora_path"]
    
    logger.info(f"Starting Day 4 evaluation and optimization with config: {config}")
    
    try:
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = Day4EvaluationOptimizationPipeline(config)
        pipeline.run_complete_pipeline()
        
        logger.info("Day 4 evaluation and optimization completed successfully!")
        
    except Exception as e:
        logger.error(f"Day 4 evaluation and optimization failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
