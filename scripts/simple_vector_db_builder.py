#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°„ë‹¨í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import json
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'source'))

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ë¡œê¹… ì„¤ì • (ê°„ë‹¨í•˜ê²Œ)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

def create_sample_data():
    """ìƒ˜í”Œ ë²•ë¥  ë°ì´í„° ìƒì„±"""
    logger.info("ğŸ“ ìƒ˜í”Œ ë²•ë¥  ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ìƒ˜í”Œ ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    sample_dir = Path("data/raw/sample")
    sample_dir.mkdir(parents=True, exist_ok=True)
    
    # ìƒ˜í”Œ ë²•ë¥  ë°ì´í„°
    sample_laws = [
        {
            "title": "ë¯¼ë²• ì œ1ì¡° (ë²•ì›)",
            "content": "ë¯¼ì‚¬ì— ê´€í•˜ì—¬ ë²•ë¥ ì— íŠ¹ë³„í•œ ê·œì •ì´ ì—†ìœ¼ë©´ ê´€ìŠµë²•ì— ì˜í•˜ê³ , ê´€ìŠµë²•ì´ ì—†ìœ¼ë©´ ì¡°ë¦¬ì— ì˜í•œë‹¤.",
            "category": "civil_law",
            "source": "ë¯¼ë²•",
            "article": "ì œ1ì¡°"
        },
        {
            "title": "ë¯¼ë²• ì œ2ì¡° (ì‹ ì˜ì„±ì‹¤ì˜ ì›ì¹™)",
            "content": "ê¶Œë¦¬ì˜ í–‰ì‚¬ì™€ ì˜ë¬´ì˜ ì´í–‰ì€ ì‹ ì˜ì— ì¢‡ì•„ ì„±ì‹¤íˆ í•˜ì—¬ì•¼ í•œë‹¤.",
            "category": "civil_law", 
            "source": "ë¯¼ë²•",
            "article": "ì œ2ì¡°"
        },
        {
            "title": "í˜•ë²• ì œ1ì¡° (ë²”ì£„ì˜ ì„±ë¦½ê³¼ ì²˜ë²Œ)",
            "content": "ë²”ì£„ì˜ ì„±ë¦½ê³¼ ì²˜ë²Œì€ í–‰ìœ„ì‹œì˜ ë²•ë¥ ì— ì˜í•œë‹¤.",
            "category": "criminal_law",
            "source": "í˜•ë²•", 
            "article": "ì œ1ì¡°"
        }
    ]
    
    # ìƒ˜í”Œ íŒë¡€ ë°ì´í„°
    sample_precedents = [
        {
            "title": "ëŒ€ë²•ì› 2023ë‹¤12345 íŒê²°",
            "content": "ê³„ì•½ì˜ í•´ì„ì€ ë‹¹ì‚¬ìê°€ ê³„ì•½ì„ ì²´ê²°í•  ë•Œì˜ ì§„ì •í•œ ì˜ì‚¬ë¥¼ íƒêµ¬í•˜ì—¬ì•¼ í•˜ë©°, ê³„ì•½ì˜ ë¬¸ì–¸ì— ì˜í•˜ì—¬ ë‹¹ì‚¬ìì˜ ì˜ì‚¬ê°€ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” ê³„ì•½ì˜ ë‚´ìš©ê³¼ ëª©ì , ê³„ì•½ ì²´ê²°ì˜ ê²½ìœ„ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ì•¼ í•œë‹¤.",
            "category": "precedent",
            "source": "ëŒ€ë²•ì›",
            "case_number": "2023ë‹¤12345"
        },
        {
            "title": "ëŒ€ë²•ì› 2023ë‹¤67890 íŒê²°", 
            "content": "ë¶ˆë²•í–‰ìœ„ë¡œ ì¸í•œ ì†í•´ë°°ìƒì±…ì„ì€ ê³ ì˜ ë˜ëŠ” ê³¼ì‹¤ë¡œ ì¸í•œ ìœ„ë²•í•œ í–‰ìœ„ë¡œ íƒ€ì¸ì—ê²Œ ì†í•´ë¥¼ ê°€í•œ ìê°€ ê·¸ ì†í•´ë¥¼ ë°°ìƒí•  ì±…ì„ì„ ì§„ë‹¤.",
            "category": "precedent",
            "source": "ëŒ€ë²•ì›",
            "case_number": "2023ë‹¤67890"
        }
    ]
    
    # ë²•ë¥  ë°ì´í„° ì €ì¥
    laws_file = sample_dir / "laws_sample.json"
    with open(laws_file, 'w', encoding='utf-8') as f:
        json.dump(sample_laws, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… ë²•ë¥  ë°ì´í„° ì €ì¥: {laws_file}")
    
    # íŒë¡€ ë°ì´í„° ì €ì¥
    precedents_file = sample_dir / "precedents_sample.json"
    with open(precedents_file, 'w', encoding='utf-8') as f:
        json.dump(sample_precedents, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… íŒë¡€ ë°ì´í„° ì €ì¥: {precedents_file}")
    
    return sample_dir

def build_simple_vector_db():
    """ê°„ë‹¨í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
    logger.info("ğŸš€ ê°„ë‹¨í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘")
    
    try:
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        sample_dir = create_sample_data()
        
        # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        logger.info("ğŸ“¦ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
        
        # ê°„ë‹¨í•œ FAISS ì¸ë±ìŠ¤ ìƒì„±
        try:
            import faiss
            import numpy as np
            from sentence_transformers import SentenceTransformer
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            model_name = os.getenv('EMBEDDING_MODEL', 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            logger.info(f"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {model_name}")
            model = SentenceTransformer(model_name)
            
            # ìƒ˜í”Œ í…ìŠ¤íŠ¸ë“¤
            texts = [
                "ë¯¼ì‚¬ì— ê´€í•˜ì—¬ ë²•ë¥ ì— íŠ¹ë³„í•œ ê·œì •ì´ ì—†ìœ¼ë©´ ê´€ìŠµë²•ì— ì˜í•˜ê³ , ê´€ìŠµë²•ì´ ì—†ìœ¼ë©´ ì¡°ë¦¬ì— ì˜í•œë‹¤.",
                "ê¶Œë¦¬ì˜ í–‰ì‚¬ì™€ ì˜ë¬´ì˜ ì´í–‰ì€ ì‹ ì˜ì— ì¢‡ì•„ ì„±ì‹¤íˆ í•˜ì—¬ì•¼ í•œë‹¤.",
                "ë²”ì£„ì˜ ì„±ë¦½ê³¼ ì²˜ë²Œì€ í–‰ìœ„ì‹œì˜ ë²•ë¥ ì— ì˜í•œë‹¤.",
                "ê³„ì•½ì˜ í•´ì„ì€ ë‹¹ì‚¬ìê°€ ê³„ì•½ì„ ì²´ê²°í•  ë•Œì˜ ì§„ì •í•œ ì˜ì‚¬ë¥¼ íƒêµ¬í•˜ì—¬ì•¼ í•œë‹¤.",
                "ë¶ˆë²•í–‰ìœ„ë¡œ ì¸í•œ ì†í•´ë°°ìƒì±…ì„ì€ ê³ ì˜ ë˜ëŠ” ê³¼ì‹¤ë¡œ ì¸í•œ ìœ„ë²•í•œ í–‰ìœ„ë¡œ íƒ€ì¸ì—ê²Œ ì†í•´ë¥¼ ê°€í•œ ìê°€ ê·¸ ì†í•´ë¥¼ ë°°ìƒí•  ì±…ì„ì„ ì§„ë‹¤."
            ]
            
            # ì„ë² ë”© ìƒì„±
            logger.info("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = model.encode(texts)
            
            # FAISS ì¸ë±ìŠ¤ ìƒì„±
            dimension = embeddings.shape[1]
            index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)
            
            # ì •ê·œí™” (cosine similarityë¥¼ ìœ„í•´)
            faiss.normalize_L2(embeddings)
            index.add(embeddings.astype('float32'))
            
            # ì¸ë±ìŠ¤ ì €ì¥
            embeddings_dir = Path("data/embeddings")
            embeddings_dir.mkdir(parents=True, exist_ok=True)
            
            index_path = embeddings_dir / "simple_vector_index"
            faiss.write_index(index, str(index_path))
            logger.info(f"âœ… ë²¡í„° ì¸ë±ìŠ¤ ì €ì¥: {index_path}")
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "model_name": model_name,
                "dimension": dimension,
                "num_vectors": len(texts),
                "texts": texts,
                "index_type": "flat"
            }
            
            metadata_path = embeddings_dir / "simple_vector_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
            
            # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_query = "ê³„ì•½ í•´ì„ì— ëŒ€í•œ ì›ì¹™"
            query_embedding = model.encode([test_query])
            faiss.normalize_L2(query_embedding)
            
            scores, indices = index.search(query_embedding.astype('float32'), k=3)
            
            logger.info(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")
            logger.info("ğŸ” ê²€ìƒ‰ ê²°ê³¼:")
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                logger.info(f"   {i+1}. ì ìˆ˜: {score:.4f}, í…ìŠ¤íŠ¸: {texts[idx][:50]}...")
            
            return True
            
        except ImportError as e:
            logger.error(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {e}")
            logger.info("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
            logger.info("pip install faiss-cpu sentence-transformers")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB êµ¬ì¶• ì‹¤íŒ¨: {e}")
        return False

def test_gemini_with_vector_db():
    """ë²¡í„° DBì™€ í•¨ê»˜ Gemini Pro í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ¤– Gemini Pro + ë²¡í„° DB í…ŒìŠ¤íŠ¸")
    
    try:
        # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        import faiss
        import numpy as np
        from sentence_transformers import SentenceTransformer
        
        # ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ
        embeddings_dir = Path("data/embeddings")
        index_path = embeddings_dir / "simple_vector_index"
        metadata_path = embeddings_dir / "simple_vector_metadata.json"
        
        if not index_path.exists() or not metadata_path.exists():
            logger.error("âŒ ë²¡í„° ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„° DBë¥¼ êµ¬ì¶•í•˜ì„¸ìš”.")
            return False
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # ì¸ë±ìŠ¤ ë¡œë“œ
        index = faiss.read_index(str(index_path))
        model = SentenceTransformer(metadata['model_name'])
        texts = metadata['texts']
        
        logger.info(f"âœ… ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
        
        # Gemini Proì™€ ì—°ë™ í…ŒìŠ¤íŠ¸
        api_key = os.getenv('GOOGLE_API_KEY')
        if api_key and api_key not in ['your-google-api-key-here', 'test-google-api-key']:
            try:
                import google.generativeai as genai
                
                genai.configure(api_key=api_key)
                model_gemini = genai.GenerativeModel('gemini-pro')
                
                # RAG í…ŒìŠ¤íŠ¸
                query = "ë¯¼ë²•ì˜ ê¸°ë³¸ ì›ì¹™ì€ ë¬´ì—‡ì¸ê°€ìš”?"
                logger.info(f"ğŸ“ RAG í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {query}")
                
                # ë²¡í„° ê²€ìƒ‰
                query_embedding = model.encode([query])
                faiss.normalize_L2(query_embedding)
                scores, indices = index.search(query_embedding.astype('float32'), k=2)
                
                # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
                retrieved_docs = [texts[idx] for idx in indices[0]]
                context = "\n".join(retrieved_docs)
                
                logger.info("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ:")
                for i, doc in enumerate(retrieved_docs):
                    logger.info(f"   {i+1}. {doc[:100]}...")
                
                # Gemini Proë¡œ ë‹µë³€ ìƒì„±
                prompt = f"""
ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ë¬¸ì„œë“¤:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:
"""
                
                response = model_gemini.generate_content(prompt)
                logger.info(f"ğŸ¤– Gemini Pro ë‹µë³€: {response.text}")
                
                return True
                
            except Exception as e:
                logger.error(f"âŒ Gemini Pro í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                return False
        else:
            logger.info("â„¹ï¸ ì‹¤ì œ API í‚¤ê°€ ì—†ì–´ Gemini Pro í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return True
            
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ê°„ë‹¨í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)
    
    # 1. ë²¡í„° DB êµ¬ì¶•
    success = build_simple_vector_db()
    if not success:
        logger.error("âŒ ë²¡í„° DB êµ¬ì¶• ì‹¤íŒ¨")
        return
    
    logger.info("=" * 60)
    
    # 2. Gemini Pro + ë²¡í„° DB í…ŒìŠ¤íŠ¸
    test_success = test_gemini_with_vector_db()
    
    logger.info("=" * 60)
    logger.info("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    if success and test_success:
        logger.info("ğŸ‰ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° Gemini Pro ì—°ë™ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        logger.info()
        logger.info("ğŸ”§ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("   1. ì‹¤ì œ ë²•ë¥  ë°ì´í„° ìˆ˜ì§‘")
        logger.info("   2. ëŒ€ê·œëª¨ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•")
        logger.info("   3. í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œ êµ¬ì¶•")
    else:
        logger.info("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
