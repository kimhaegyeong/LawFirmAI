"""
MLflow í†µí•© ê´€ë¦¬ ëª¨ë“ˆ

FAISS ì¸ë±ìŠ¤ ë²„ì „ ê´€ë¦¬ë¥¼ MLflowë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import logging
import json
import pickle
import shutil
import tempfile
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import os
import sys

project_root = Path(__file__).parent.parent.parent

logger = logging.getLogger(__name__)

try:
    import mlflow
    import mlflow.tracking
    from mlflow.tracking import MlflowClient
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    logger.warning("MLflow not available. Install with: pip install mlflow")


class MLflowFAISSManager:
    """MLflowë¥¼ ì‚¬ìš©í•œ FAISS ì¸ë±ìŠ¤ ë²„ì „ ê´€ë¦¬"""
    
    def __init__(
        self,
        experiment_name: str = "faiss_index_versions",
        tracking_uri: Optional[str] = None
    ):
        """
        MLflow FAISS ê´€ë¦¬ì ì´ˆê¸°í™”
        
        Args:
            experiment_name: MLflow ì‹¤í—˜ ì´ë¦„
            tracking_uri: MLflow tracking URI (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’)
        """
        if not MLFLOW_AVAILABLE:
            raise ImportError("MLflow is not available. Install with: pip install mlflow")
        
        self.experiment_name = experiment_name
        
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)
        else:
            tracking_uri = os.getenv("MLFLOW_TRACKING_URI")
            if not tracking_uri:
                default_path = project_root / "mlflow" / "mlruns"
                default_path.mkdir(parents=True, exist_ok=True)
                tracking_uri = f"file:///{str(default_path).replace(os.sep, '/')}"
            mlflow.set_tracking_uri(tracking_uri)
        
        self.tracking_uri = mlflow.get_tracking_uri()
        
        self.is_local_filesystem = self._is_local_filesystem()
        if self.is_local_filesystem:
            self.local_base_path = self._get_local_base_path()
            logger.info(f"âœ… ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ëª¨ë“œ: {self.local_base_path}")
        else:
            logger.info(f"ğŸŒ ì›ê²© ì„œë²„ ëª¨ë“œ: {self.tracking_uri}")
        
        try:
            self.client = MlflowClient(tracking_uri=self.tracking_uri)
        except Exception as e:
            if "is not a valid remote uri" in str(e) or "ì§€ì •ëœ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in str(e):
                tracking_uri_path = self.tracking_uri.replace("file://", "").replace("file:///", "")
                if not os.path.isabs(tracking_uri_path):
                    tracking_uri_path = os.path.abspath(tracking_uri_path)
                else:
                    tracking_uri_path = os.path.normpath(tracking_uri_path)
                
                os.makedirs(tracking_uri_path, exist_ok=True)
                
                if os.name == 'nt':
                    self.tracking_uri = f"file:///{tracking_uri_path.replace(os.sep, '/')}"
                else:
                    self.tracking_uri = f"file://{tracking_uri_path}"
                
                mlflow.set_tracking_uri(self.tracking_uri)
                self.client = MlflowClient(tracking_uri=self.tracking_uri)
            else:
                raise
        
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if experiment is None:
            self.experiment_id = mlflow.create_experiment(experiment_name)
            logger.info(f"Created MLflow experiment: {experiment_name} (id: {self.experiment_id})")
        else:
            self.experiment_id = experiment.experiment_id
            logger.info(f"Using existing MLflow experiment: {experiment_name} (id: {self.experiment_id})")
    
    def _is_local_filesystem(self) -> bool:
        """ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì¸ì§€ í™•ì¸"""
        return self.tracking_uri.startswith("file://")
    
    def _get_local_base_path(self) -> Path:
        """ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì˜ ê¸°ë³¸ ê²½ë¡œ ë°˜í™˜"""
        uri_path = self.tracking_uri.replace("file://", "").replace("file:///", "")
        
        if os.name == 'nt':
            if uri_path.startswith('/') and len(uri_path) > 1 and uri_path[1].isalpha() and uri_path[2:4] == ':/':
                uri_path = uri_path[1:]
        
        if not os.path.isabs(uri_path):
            uri_path = os.path.abspath(uri_path)
        else:
            uri_path = os.path.normpath(uri_path)
        
        return Path(uri_path)
    
    def _get_local_artifact_path(self, run_id: str, artifact_path: str = "faiss_index") -> Path:
        """ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì•„í‹°íŒ©íŠ¸ ê²½ë¡œ ê³„ì‚°"""
        return self.local_base_path / str(self.experiment_id) / run_id / "artifacts" / artifact_path
    
    def load_version_info_from_local(self, run_id: str) -> Optional[Dict[str, Any]]:
        """ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ version_info.json ì§ì ‘ ë¡œë“œ"""
        if not self.is_local_filesystem:
            logger.debug("ì›ê²© ì„œë²„ ëª¨ë“œ: ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ì§ì ‘ ì ‘ê·¼ ë¶ˆê°€")
            return None
        
        try:
            version_info_path = self.local_base_path / str(self.experiment_id) / run_id / "artifacts" / "version_info.json"
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì‚¬ì „ í™•ì¸ ë° ê²½ë¡œ ê²€ì¦
            if not version_info_path.parent.exists():
                logger.debug(f"âš ï¸  ì•„í‹°íŒ©íŠ¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {version_info_path.parent}")
                return None
            
            if not version_info_path.exists():
                logger.debug(f"âš ï¸  ë¡œì»¬ ê²½ë¡œì— version_info.json ì—†ìŒ: {version_info_path}")
                return None
            
            # íŒŒì¼ í¬ê¸° í™•ì¸ (ë¹ˆ íŒŒì¼ ë°©ì§€)
            file_size = version_info_path.stat().st_size
            if file_size == 0:
                logger.warning(f"âš ï¸  version_info.jsonì´ ë¹„ì–´ìˆìŒ: {version_info_path}")
                return None
            
            # íŒŒì¼ ì½ê¸° ì‹œë„
            logger.info(f"âœ… ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ version_info.json ì§ì ‘ ë¡œë“œ: {version_info_path} (í¬ê¸°: {file_size} bytes)")
            with open(version_info_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if not isinstance(data, dict):
                    logger.warning(f"âš ï¸  version_info.jsonì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(data)}")
                    return None
                return data
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸  ë¡œì»¬ version_info.json JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸  ë¡œì»¬ version_info.json ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def create_run(
        self,
        version_name: str,
        embedding_version_id: int,
        chunking_strategy: str,
        chunking_config: Dict[str, Any],
        embedding_config: Dict[str, Any],
        document_count: int = 0,
        total_chunks: int = 0,
        status: str = "active"
    ) -> str:
        """
        MLflow run ìƒì„±
        
        Args:
            version_name: ë²„ì „ ì´ë¦„
            embedding_version_id: ì„ë² ë”© ë²„ì „ ID
            chunking_strategy: ì²­í‚¹ ì „ëµ
            chunking_config: ì²­í‚¹ ì„¤ì •
            embedding_config: ì„ë² ë”© ì„¤ì •
            document_count: ë¬¸ì„œ ìˆ˜
            total_chunks: ì´ ì²­í¬ ìˆ˜
            status: ë²„ì „ ìƒíƒœ
        
        Returns:
            str: run_id
        """
        mlflow.set_experiment(self.experiment_name)
        
        with mlflow.start_run(run_name=version_name) as run:
            run_id = run.info.run_id
            
            mlflow.set_tags({
                "version": version_name,
                "status": status,
                "embedding_version_id": str(embedding_version_id),
                "chunking_strategy": chunking_strategy
            })
            
            mlflow.log_params({
                "embedding_version_id": embedding_version_id,
                "chunking_strategy": chunking_strategy,
                "document_count": document_count,
                "total_chunks": total_chunks,
                "status": status
            })
            
            mlflow.log_metrics({
                "document_count": document_count,
                "total_chunks": total_chunks
            })
            
            version_info = {
                "version": version_name,
                "embedding_version_id": embedding_version_id,
                "chunking_strategy": chunking_strategy,
                "created_at": datetime.now().isoformat(),
                "chunking_config": chunking_config,
                "embedding_config": embedding_config,
                "document_count": document_count,
                "total_chunks": total_chunks,
                "status": status,
                "mlflow_run_id": run_id,
                "mlflow_tracking_uri": self.tracking_uri
            }
            
            mlflow.log_dict(version_info, "version_info.json")
            
            logger.info(f"Created MLflow run: {version_name} (run_id: {run_id})")
            return run_id
    
    def save_index(
        self,
        run_id: str,
        index: Any,
        id_mapping: Dict[int, int],
        metadata: List[Dict[str, Any]],
        index_path: Optional[Path] = None
    ) -> bool:
        """
        FAISS ì¸ë±ìŠ¤ë¥¼ MLflow ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥
        
        Args:
            run_id: MLflow run ID
            index: FAISS ì¸ë±ìŠ¤ ê°ì²´
            id_mapping: ID ë§¤í•‘
            metadata: ë©”íƒ€ë°ì´í„°
            index_path: ë¡œì»¬ ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì„ì‹œ íŒŒì¼ ì‚¬ìš©)
        
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            import faiss
            
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir) / "faiss_index"
                temp_path.mkdir()
                
                if index_path and index_path.exists():
                    shutil.copy(index_path, temp_path / "index.faiss")
                else:
                    faiss.write_index(index, str(temp_path / "index.faiss"))
                
                with open(temp_path / "id_mapping.json", 'w', encoding='utf-8') as f:
                    json.dump(id_mapping, f, indent=2)
                
                with open(temp_path / "metadata.pkl", 'wb') as f:
                    pickle.dump(metadata, f)
                
                index_stats = {
                    "num_vectors": index.ntotal,
                    "dimension": index.d,
                    "index_type": type(index).__name__,
                    "id_mapping_size": len(id_mapping),
                    "metadata_size": len(metadata)
                }
                
                with open(temp_path / "index_stats.json", 'w', encoding='utf-8') as f:
                    json.dump(index_stats, f, indent=2)
                
                # í™œì„± run ì»¨í…ìŠ¤íŠ¸ ì„¤ì • í›„ ì•„í‹°íŒ©íŠ¸ ì €ì¥
                with mlflow.start_run(run_id=run_id):
                    mlflow.log_artifacts(str(temp_path), "faiss_index")
                    mlflow.log_metrics({
                        "num_vectors": index.ntotal,
                        "dimension": index.d,
                        "id_mapping_size": len(id_mapping),
                        "metadata_size": len(metadata)
                    })
                
                logger.info(f"Saved FAISS index to MLflow run: {run_id}")
                return True
                
        except Exception as e:
            logger.error(f"Failed to save index to MLflow: {e}")
            return False
    
    def load_index(
        self,
        run_id: str,
        output_dir: Optional[Path] = None
    ) -> Optional[Dict[str, Any]]:
        """
        MLflowì—ì„œ FAISS ì¸ë±ìŠ¤ ë¡œë“œ (ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ìµœì í™”)
        
        Args:
            run_id: MLflow run ID
            output_dir: ë‹¤ìš´ë¡œë“œí•  ë””ë ‰í† ë¦¬ (Noneì´ë©´ ë¡œì»¬ ì§ì ‘ ì ‘ê·¼ ë˜ëŠ” ì„ì‹œ ë””ë ‰í† ë¦¬)
        
        Returns:
            Optional[Dict]: ì¸ë±ìŠ¤, id_mapping, metadataë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        """
        try:
            import faiss
            
            if self.is_local_filesystem and not output_dir:
                return self._load_index_from_local_path(run_id)
            
            return self._load_index_from_download(run_id, output_dir)
            
        except Exception as e:
            logger.error(f"Failed to load index from MLflow run {run_id}: {e}", exc_info=True)
            return None
    
    def _load_index_from_local_path(self, run_id: str) -> Optional[Dict[str, Any]]:
        """ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì§ì ‘ ì¸ë±ìŠ¤ ë¡œë“œ (ë³µì‚¬ ì—†ìŒ)"""
        try:
            import faiss
            
            run_info = self.client.get_run(run_id)
            run_tags = run_info.data.tags if hasattr(run_info.data, 'tags') else {}
            version_name = run_tags.get('version', None)
            
            vector_store_path = project_root / "data" / "vector_store"
            
            if version_name and vector_store_path.exists():
                version_path = vector_store_path / version_name
                index_path = version_path / "index.faiss"
                
                if index_path.exists():
                    logger.info(f"âœ… data/vector_storeì—ì„œ ì§ì ‘ ë¡œë“œ: {index_path}")
                    logger.info(f"   ğŸ“ ë¡œì»¬ ì¸ë±ìŠ¤ ê²½ë¡œ: {version_path}")
                    
                    index = faiss.read_index(str(index_path))
                    
                    id_mapping_path = version_path / "id_mapping.json"
                    id_mapping = {}
                    if id_mapping_path.exists():
                        with open(id_mapping_path, 'r', encoding='utf-8') as f:
                            id_mapping = json.load(f)
                    
                    metadata_path = version_path / "metadata.pkl"
                    metadata = []
                    if metadata_path.exists():
                        with open(metadata_path, 'rb') as f:
                            metadata = pickle.load(f)
                    
                    version_info_path = version_path / "version_info.json"
                    stats = {}
                    if version_info_path.exists():
                        with open(version_info_path, 'r', encoding='utf-8') as f:
                            stats = json.load(f)
                    
                    return {
                        'index': index,
                        'id_mapping': id_mapping,
                        'metadata': metadata,
                        'stats': stats,
                        'run_info': run_info.to_dictionary(),
                        'local_path': str(version_path)
                    }
            
            artifacts_path = self._get_local_artifact_path(run_id, "faiss_index")
            index_path = artifacts_path / "index.faiss"
            
            if not index_path.exists():
                logger.warning(f"Index file not found at local path: {index_path}")
                logger.info("Falling back to download method...")
                return self._load_index_from_download(run_id, None)
            
            logger.info(f"âœ… MLflow ë¡œì»¬ ê²½ë¡œì—ì„œ ì§ì ‘ ë¡œë“œ: {index_path}")
            logger.info(f"   ğŸ“ MLflow ì•„í‹°íŒ©íŠ¸ ê²½ë¡œ: {artifacts_path}")
            
            index = faiss.read_index(str(index_path))
            
            id_mapping_path = artifacts_path / "id_mapping.json"
            id_mapping = {}
            if id_mapping_path.exists():
                with open(id_mapping_path, 'r', encoding='utf-8') as f:
                    id_mapping = json.load(f)
            
            metadata_path = artifacts_path / "metadata.pkl"
            metadata = []
            if metadata_path.exists():
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
            
            stats_path = artifacts_path / "index_stats.json"
            stats = {}
            if stats_path.exists():
                with open(stats_path, 'r', encoding='utf-8') as f:
                    stats = json.load(f)
            
            return {
                'index': index,
                'id_mapping': id_mapping,
                'metadata': metadata,
                'stats': stats,
                'run_info': run_info.to_dictionary(),
                'local_path': str(artifacts_path)
            }
            
        except Exception as e:
            logger.error(f"Failed to load index from local path: {e}", exc_info=True)
            logger.info("Falling back to download method...")
            return self._load_index_from_download(run_id, None)
    
    def _load_index_from_download(
        self,
        run_id: str,
        output_dir: Optional[Path] = None
    ) -> Optional[Dict[str, Any]]:
        """ê¸°ì¡´ ë‹¤ìš´ë¡œë“œ ë°©ì‹ìœ¼ë¡œ ì¸ë±ìŠ¤ ë¡œë“œ (ì›ê²© ì„œë²„ìš©)"""
        try:
            import faiss
            
            artifacts = self.client.list_artifacts(run_id, "faiss_index")
            if not artifacts:
                logger.warning(f"No FAISS artifacts found for run {run_id}")
                return None
            
            if output_dir:
                download_path = output_dir
                download_path.mkdir(parents=True, exist_ok=True)
                use_temp = False
            else:
                temp_dir = tempfile.mkdtemp()
                download_path = Path(temp_dir)
                use_temp = True
            
            try:
                logger.info(f"ğŸ“¥ MLflowì—ì„œ ì¸ë±ìŠ¤ ë‹¤ìš´ë¡œë“œ: run_id={run_id}")
                
                for artifact in artifacts:
                    artifact_dest = download_path / artifact.path
                    artifact_dest.parent.mkdir(parents=True, exist_ok=True)
                    self.client.download_artifacts(run_id, artifact.path, str(artifact_dest.parent))
                
                index_path = download_path / "faiss_index" / "index.faiss"
                
                if not index_path.exists():
                    possible_paths = [
                        download_path / "index.faiss",
                        download_path / "faiss_index" / "index.faiss",
                    ]
                    
                    for candidate_path in possible_paths:
                        if candidate_path.exists():
                            index_path = candidate_path
                            logger.info(f"Found index file at: {index_path}")
                            break
                    
                    if not index_path.exists():
                        faiss_files = list(download_path.rglob("*.faiss"))
                        if faiss_files:
                            index_path = faiss_files[0]
                            logger.info(f"Found index file (auto-detected): {index_path}")
                        else:
                            logger.error(f"Index file not found. Searched in: {download_path}")
                            logger.error(f"Download path structure: {list(download_path.rglob('*')) if download_path.exists() else 'Directory does not exist'}")
                            return None
                
                index = faiss.read_index(str(index_path))
                
                id_mapping_path = download_path / "faiss_index" / "id_mapping.json"
                id_mapping = {}
                if id_mapping_path.exists():
                    with open(id_mapping_path, 'r', encoding='utf-8') as f:
                        id_mapping = json.load(f)
                
                metadata_path = download_path / "faiss_index" / "metadata.pkl"
                metadata = []
                if metadata_path.exists():
                    with open(metadata_path, 'rb') as f:
                        metadata = pickle.load(f)
                
                stats_path = download_path / "faiss_index" / "index_stats.json"
                stats = {}
                if stats_path.exists():
                    with open(stats_path, 'r', encoding='utf-8') as f:
                        stats = json.load(f)
                
                run_info = self.client.get_run(run_id)
                
                return {
                    'index': index,
                    'id_mapping': id_mapping,
                    'metadata': metadata,
                    'stats': stats,
                    'run_info': run_info.to_dictionary(),
                    'download_path': str(download_path) if not use_temp else None
                }
                
            finally:
                if use_temp:
                    shutil.rmtree(download_path, ignore_errors=True)
                    
        except Exception as e:
            logger.error(f"Failed to load index from download: {e}", exc_info=True)
            return None
    
    def list_runs(
        self,
        filter_string: Optional[str] = None,
        max_results: int = 100
    ) -> List[Dict[str, Any]]:
        """
        MLflow runs ëª©ë¡ ì¡°íšŒ
        
        Args:
            filter_string: MLflow filter string
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            List[Dict]: run ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            runs = mlflow.search_runs(
                experiment_ids=[self.experiment_id],
                filter_string=filter_string,
                max_results=max_results,
                order_by=["start_time DESC"]
            )
            
            results = []
            for _, run in runs.iterrows():
                results.append({
                    "run_id": run["run_id"],
                    "version": run.get("tags.version", ""),
                    "status": run.get("tags.status", ""),
                    "start_time": run["start_time"],
                    "metrics": run.filter(regex="^metrics\.").to_dict(),
                    "params": run.filter(regex="^params\.").to_dict()
                })
            
            return results
            
        except Exception as e:
            logger.error(f"Failed to list runs: {e}")
            return []
    
    def get_production_run(self) -> Optional[str]:
        """
        í”„ë¡œë•ì…˜ íƒœê·¸ê°€ ìˆëŠ” run ID ì¡°íšŒ
        
        Returns:
            Optional[str]: í”„ë¡œë•ì…˜ run ID, ì—†ìœ¼ë©´ None
        """
        try:
            runs = mlflow.search_runs(
                experiment_ids=[self.experiment_id],
                filter_string="tags.status='production_ready'",
                max_results=1,
                order_by=["start_time DESC"]
            )
            
            if not runs.empty:
                return runs.iloc[0]["run_id"]
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to get production run: {e}")
            return None
    
    def get_run_by_version(self, version_name: str) -> Optional[str]:
        """
        ë²„ì „ ì´ë¦„ìœ¼ë¡œ run ID ì¡°íšŒ
        
        Args:
            version_name: ë²„ì „ ì´ë¦„
        
        Returns:
            Optional[str]: run ID, ì—†ìœ¼ë©´ None
        """
        try:
            runs = mlflow.search_runs(
                experiment_ids=[self.experiment_id],
                filter_string=f"tags.version='{version_name}'",
                max_results=1,
                order_by=["start_time DESC"]
            )
            
            if not runs.empty:
                return runs.iloc[0]["run_id"]
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to get run by version: {e}")
            return None

