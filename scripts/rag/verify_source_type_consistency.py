#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FAISS ì¸ë±ìŠ¤ì™€ DBì˜ source_type ì¼ì¹˜ì„± ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

Usage:
    python scripts/rag/verify_source_type_consistency.py
    python scripts/rag/verify_source_type_consistency.py --sample-size 1000
    python scripts/rag/verify_source_type_consistency.py --mlflow-run-id <run_id>
"""

import sys
import os
import sqlite3
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import argparse
from collections import defaultdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "lawfirm_langgraph"))

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("âš ï¸  FAISS not available. Install with: pip install faiss-cpu")

try:
    import mlflow
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    print("âš ï¸  MLflow not available. Install with: pip install mlflow")


def get_db_connection(db_path: str) -> sqlite3.Connection:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn


def load_mlflow_index_via_engine(run_id: Optional[str] = None) -> Tuple[Optional[faiss.Index], Optional[List[int]], Optional[Dict]]:
    """SemanticSearchEngineV2ë¥¼ í†µí•´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ"""
    try:
        from lawfirm_langgraph.core.search.engines.semantic_search_engine_v2 import SemanticSearchEngineV2
        from lawfirm_langgraph.core.utils.config import Config
        
        # Config ì„¤ì •
        config = Config()
        if run_id:
            os.environ['MLFLOW_RUN_ID'] = run_id
        
        # SemanticSearchEngineV2 ì´ˆê¸°í™” (ì¸ë±ìŠ¤ ë¡œë“œ)
        print("ğŸ”„ Initializing SemanticSearchEngineV2 to load index...")
        engine = SemanticSearchEngineV2(
            db_path=config.database_path,
            use_mlflow_index=True
        )
        
        if engine.index is None:
            print("âŒ Failed to load index from SemanticSearchEngineV2")
            return None, None, None
        
        index = engine.index
        chunk_ids = engine._chunk_ids if hasattr(engine, '_chunk_ids') and engine._chunk_ids else None
        
        print(f"âœ… Loaded FAISS index via SemanticSearchEngineV2: {index.ntotal} vectors")
        if chunk_ids:
            print(f"âœ… Loaded chunk_ids: {len(chunk_ids)} chunks")
        else:
            print("âš ï¸  chunk_ids not available, will use sequential IDs")
            chunk_ids = list(range(index.ntotal))
        
        # version_infoëŠ” engineì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ None
        version_info = None
        
        return index, chunk_ids, version_info
        
    except Exception as e:
        print(f"âŒ Failed to load index via SemanticSearchEngineV2: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


def load_mlflow_index(run_id: Optional[str] = None) -> Tuple[Optional[faiss.Index], Optional[List[int]], Optional[Dict]]:
    """MLflowì—ì„œ FAISS ì¸ë±ìŠ¤ ë¡œë“œ (fallback: SemanticSearchEngineV2 ì‚¬ìš©)"""
    if not MLFLOW_AVAILABLE:
        print("âš ï¸  MLflow not available, using SemanticSearchEngineV2")
        return load_mlflow_index_via_engine(run_id)
    
    try:
        # MLflow tracking URI ì„¤ì •
        mlflow_uri = str(project_root / "mlflow" / "mlruns")
        os.environ['MLFLOW_TRACKING_URI'] = f"file:///{mlflow_uri.replace(chr(92), '/')}"
        mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
        
        # run_idê°€ ì—†ìœ¼ë©´ í”„ë¡œë•ì…˜ run ì°¾ê¸°, ì—†ìœ¼ë©´ ìµœê·¼ run ì‚¬ìš©
        if not run_id:
            try:
                client = mlflow.tracking.MlflowClient()
                # ë¨¼ì € í”„ë¡œë•ì…˜ run ì°¾ê¸°
                runs = client.search_runs(
                    experiment_ids=["0"],
                    filter_string="tags.status='production_ready'",
                    max_results=1,
                    order_by=["start_time DESC"]
                )
                if runs:
                    run_id = runs[0].info.run_id
                    print(f"âœ… Found production run: {run_id}")
                else:
                    # í”„ë¡œë•ì…˜ runì´ ì—†ìœ¼ë©´ ìµœê·¼ run ì‚¬ìš©
                    runs = client.search_runs(
                        experiment_ids=["0"],
                        max_results=1,
                        order_by=["start_time DESC"]
                    )
                    if runs:
                        run_id = runs[0].info.run_id
                        print(f"âš ï¸  No production run found. Using most recent run: {run_id}")
            except Exception as e:
                print(f"âš ï¸  Failed to search MLflow runs: {e}")
                print("   Using SemanticSearchEngineV2 instead...")
                return load_mlflow_index_via_engine(None)
        
        if not run_id:
            print("âš ï¸  No run_id specified, using SemanticSearchEngineV2...")
            return load_mlflow_index_via_engine(None)
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        try:
            index_path = mlflow.artifacts.download_artifacts(
                run_id=run_id,
                artifact_path="faiss_index"
            )
        except Exception as e:
            print(f"âš ï¸  Failed to download artifacts from MLflow: {e}")
            print("   Using SemanticSearchEngineV2 instead...")
            return load_mlflow_index_via_engine(run_id)
        
        if not os.path.exists(index_path):
            print(f"âš ï¸  FAISS index not found at: {index_path}")
            print("   Using SemanticSearchEngineV2 instead...")
            return load_mlflow_index_via_engine(run_id)
        
        index = faiss.read_index(index_path)
        print(f"âœ… Loaded FAISS index: {index.ntotal} vectors")
        
        # chunk_ids ë¡œë“œ
        chunk_ids_path = os.path.join(os.path.dirname(index_path), "chunk_ids.npy")
        if os.path.exists(chunk_ids_path):
            import numpy as np
            chunk_ids = np.load(chunk_ids_path).tolist()
            print(f"âœ… Loaded chunk_ids: {len(chunk_ids)} chunks")
        else:
            print("âš ï¸  chunk_ids.npy not found. Using sequential IDs")
            chunk_ids = list(range(index.ntotal))
        
        # version_info ë¡œë“œ
        version_info = None
        try:
            version_info = mlflow.artifacts.load_dict(f"runs:/{run_id}/version_info.json")
            print(f"âœ… Loaded version_info.json")
        except Exception as e:
            print(f"âš ï¸  Failed to load version_info.json: {e}")
        
        return index, chunk_ids, version_info
        
    except Exception as e:
        print(f"âš ï¸  Failed to load MLflow index: {e}")
        print("   Using SemanticSearchEngineV2 instead...")
        return load_mlflow_index_via_engine(run_id)


def verify_source_type_consistency(
    db_path: str,
    chunk_ids: List[int],
    sample_size: int = 1000
) -> Dict[str, any]:
    """source_type ì¼ì¹˜ì„± ê²€ì¦"""
    conn = get_db_connection(db_path)
    
    # ìƒ˜í”Œë§
    if len(chunk_ids) > sample_size:
        import random
        sampled_chunk_ids = random.sample(chunk_ids, sample_size)
        print(f"ğŸ“Š Sampling {sample_size} chunks from {len(chunk_ids)} total chunks")
    else:
        sampled_chunk_ids = chunk_ids
        print(f"ğŸ“Š Verifying all {len(chunk_ids)} chunks")
    
    # DBì—ì„œ source_type ì¡°íšŒ
    results = {
        'total_checked': len(sampled_chunk_ids),
        'found_in_db': 0,
        'not_found_in_db': 0,
        'source_type_distribution': defaultdict(int),
        'missing_chunks': [],
        'type_mismatches': []
    }
    
    # ë°°ì¹˜ë¡œ ì¡°íšŒ (ì„±ëŠ¥ ìµœì í™”)
    batch_size = 100
    for i in range(0, len(sampled_chunk_ids), batch_size):
        batch_ids = sampled_chunk_ids[i:i+batch_size]
        placeholders = ','.join(['?'] * len(batch_ids))
        
        cursor = conn.execute(
            f"SELECT id, source_type FROM text_chunks WHERE id IN ({placeholders})",
            batch_ids
        )
        rows = cursor.fetchall()
        
        found_ids = {row['id'] for row in rows}
        results['found_in_db'] += len(found_ids)
        results['not_found_in_db'] += len(batch_ids) - len(found_ids)
        
        for row in rows:
            chunk_id = row['id']
            source_type = row['source_type']
            results['source_type_distribution'][source_type] += 1
        
        # DBì— ì—†ëŠ” chunk_id ê¸°ë¡
        for chunk_id in batch_ids:
            if chunk_id not in found_ids:
                results['missing_chunks'].append(chunk_id)
    
    conn.close()
    
    return results


def analyze_type_distribution(results: Dict[str, any]) -> None:
    """íƒ€ì… ë¶„í¬ ë¶„ì„ ë° ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š source_type ë¶„í¬ ë¶„ì„")
    print("="*80)
    
    print(f"\nâœ… DBì—ì„œ ì°¾ì€ chunk: {results['found_in_db']}/{results['total_checked']}")
    print(f"âŒ DBì—ì„œ ì°¾ì§€ ëª»í•œ chunk: {results['not_found_in_db']}/{results['total_checked']}")
    
    if results['not_found_in_db'] > 0:
        print(f"\nâš ï¸  DBì— ì—†ëŠ” chunk_id ìƒ˜í”Œ (ìµœëŒ€ 10ê°œ):")
        for chunk_id in results['missing_chunks'][:10]:
            print(f"   - chunk_id: {chunk_id}")
        if len(results['missing_chunks']) > 10:
            print(f"   ... (ì´ {len(results['missing_chunks'])}ê°œ)")
    
    print(f"\nğŸ“ˆ source_type ë¶„í¬:")
    type_dist = results['source_type_distribution']
    total = sum(type_dist.values())
    
    for source_type, count in sorted(type_dist.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total * 100) if total > 0 else 0
        print(f"   - {source_type}: {count}ê°œ ({percentage:.1f}%)")
    
    # íƒ€ì…ë³„ ê²€ìƒ‰ ê°€ëŠ¥ì„± ë¶„ì„
    print(f"\nğŸ” íƒ€ì…ë³„ ê²€ìƒ‰ ê°€ëŠ¥ì„± ë¶„ì„:")
    type_mapping = {
        'statute_article': 'ë²•ë ¹ ì¡°ë¬¸',
        'case_paragraph': 'íŒë¡€',
        'decision_paragraph': 'ê²°ì •ë¡€',
        'interpretation_paragraph': 'í•´ì„ë¡€'
    }
    
    for source_type, korean_name in type_mapping.items():
        count = type_dist.get(source_type, 0)
        if count == 0:
            print(f"   âš ï¸  {korean_name} ({source_type}): 0ê°œ - ì´ íƒ€ì…ìœ¼ë¡œ ê²€ìƒ‰í•˜ë©´ ê²°ê³¼ê°€ ì—†ì„ ìˆ˜ ìˆìŒ")
        elif count < 10:
            print(f"   âš ï¸  {korean_name} ({source_type}): {count}ê°œ - ë§¤ìš° ì ìŒ, ê²€ìƒ‰ ê²°ê³¼ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŒ")
        else:
            print(f"   âœ… {korean_name} ({source_type}): {count}ê°œ")


def check_type_search_feasibility(results: Dict[str, any]) -> None:
    """íƒ€ì…ë³„ ê²€ìƒ‰ ê°€ëŠ¥ì„± í™•ì¸"""
    print("\n" + "="*80)
    print("ğŸ” íƒ€ì…ë³„ ê²€ìƒ‰ ê°€ëŠ¥ì„± í™•ì¸")
    print("="*80)
    
    type_dist = results['source_type_distribution']
    required_types = ['statute_article', 'case_paragraph', 'decision_paragraph', 'interpretation_paragraph']
    
    print(f"\níƒ€ì…ë³„ ê²€ìƒ‰ ìš”ì²­ ì‹œ ì˜ˆìƒ ê²°ê³¼:")
    for req_type in required_types:
        count = type_dist.get(req_type, 0)
        total = results['found_in_db']
        percentage = (count / total * 100) if total > 0 else 0
        
        if count == 0:
            print(f"   âŒ {req_type}: 0ê°œ - ê²€ìƒ‰ ì‹œ ëª¨ë“  ê²°ê³¼ê°€ í•„í„°ë§ë¨")
        elif count < total * 0.01:  # 1% ë¯¸ë§Œ
            print(f"   âš ï¸  {req_type}: {count}ê°œ ({percentage:.2f}%) - ë§¤ìš° ì ìŒ, ëŒ€ë¶€ë¶„ í•„í„°ë§ë  ê°€ëŠ¥ì„±")
        elif count < total * 0.05:  # 5% ë¯¸ë§Œ
            print(f"   âš ï¸  {req_type}: {count}ê°œ ({percentage:.2f}%) - ì ìŒ, ì¼ë¶€ í•„í„°ë§ë  ê°€ëŠ¥ì„±")
        else:
            print(f"   âœ… {req_type}: {count}ê°œ ({percentage:.2f}%) - ì¶©ë¶„í•¨")


def main():
    parser = argparse.ArgumentParser(description='FAISS ì¸ë±ìŠ¤ì™€ DBì˜ source_type ì¼ì¹˜ì„± ê²€ì¦')
    parser.add_argument('--sample-size', type=int, default=1000, help='ê²€ì¦í•  chunk ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 1000)')
    parser.add_argument('--mlflow-run-id', type=str, default=None, help='MLflow run ID (ì—†ìœ¼ë©´ í”„ë¡œë•ì…˜ run ì‚¬ìš©)')
    parser.add_argument('--db-path', type=str, default=None, help='ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ (ê¸°ë³¸ê°’: .envì—ì„œ ì½ìŒ)')
    
    args = parser.parse_args()
    
    print("="*80)
    print("FAISS ì¸ë±ìŠ¤ì™€ DBì˜ source_type ì¼ì¹˜ì„± ê²€ì¦")
    print("="*80)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ í™•ì¸
    if args.db_path:
        db_path = args.db_path
    else:
        # .envì—ì„œ DATABASE_PATH ì½ê¸°
        try:
            from dotenv import load_dotenv
            load_dotenv()
            db_path = os.getenv("DATABASE_PATH")
            if not db_path:
                # ê¸°ë³¸ ê²½ë¡œ
                db_path = str(project_root / "data" / "lawfirm_v2.db")
        except Exception:
            db_path = str(project_root / "data" / "lawfirm_v2.db")
    
    if not os.path.exists(db_path):
        print(f"âŒ Database not found: {db_path}")
        return 1
    
    print(f"\nğŸ“ Database: {db_path}")
    
    # MLflow ì¸ë±ìŠ¤ ë¡œë“œ
    if not FAISS_AVAILABLE:
        print("âŒ FAISS not available")
        return 1
    
    index, chunk_ids, version_info = load_mlflow_index(args.mlflow_run_id)
    if index is None or chunk_ids is None:
        print("âŒ Failed to load FAISS index")
        return 1
    
    print(f"\nğŸ“Š FAISS Index Info:")
    print(f"   - Total vectors: {index.ntotal}")
    print(f"   - Chunk IDs: {len(chunk_ids)}")
    print(f"   - Dimension: {index.d}")
    
    if version_info:
        embedding_config = version_info.get('embedding_config', {})
        if embedding_config:
            print(f"   - Embedding model: {embedding_config.get('model', 'N/A')}")
            print(f"   - Dimension: {embedding_config.get('dimension', 'N/A')}")
    
    # source_type ì¼ì¹˜ì„± ê²€ì¦
    print(f"\nğŸ” Verifying source_type consistency...")
    results = verify_source_type_consistency(db_path, chunk_ids, args.sample_size)
    
    # ê²°ê³¼ ë¶„ì„
    analyze_type_distribution(results)
    check_type_search_feasibility(results)
    
    # ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“‹ ê²€ì¦ ìš”ì•½")
    print("="*80)
    
    if results['not_found_in_db'] > 0:
        missing_ratio = results['not_found_in_db'] / results['total_checked']
        if missing_ratio > 0.1:  # 10% ì´ìƒ
            print(f"âŒ CRITICAL: {missing_ratio:.1%}ì˜ chunkê°€ DBì— ì—†ìŠµë‹ˆë‹¤!")
            print(f"   â†’ FAISS ì¸ë±ìŠ¤ì™€ DBê°€ ë™ê¸°í™”ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print(f"   â†’ ì¸ë±ìŠ¤ë¥¼ ì¬ë¹Œë“œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        else:
            print(f"âš ï¸  {missing_ratio:.1%}ì˜ chunkê°€ DBì— ì—†ìŠµë‹ˆë‹¤.")
            print(f"   â†’ ì¼ë¶€ chunkê°€ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì¸ë±ìŠ¤ì™€ DBê°€ ë¶ˆì¼ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ… ëª¨ë“  chunkê°€ DBì— ì¡´ì¬í•©ë‹ˆë‹¤.")
    
    # íƒ€ì…ë³„ ê²€ìƒ‰ ê°€ëŠ¥ì„± ìš”ì•½
    type_dist = results['source_type_distribution']
    required_types = ['statute_article', 'case_paragraph', 'decision_paragraph', 'interpretation_paragraph']
    missing_types = [t for t in required_types if type_dist.get(t, 0) == 0]
    
    if missing_types:
        print(f"\nâŒ CRITICAL: ë‹¤ìŒ íƒ€ì…ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤:")
        for t in missing_types:
            print(f"   - {t}")
        print(f"   â†’ ì´ íƒ€ì…ìœ¼ë¡œ ê²€ìƒ‰í•˜ë©´ ëª¨ë“  ê²°ê³¼ê°€ í•„í„°ë§ë©ë‹ˆë‹¤.")
        print(f"   â†’ source_type í•„í„°ë¥¼ ì™„í™”í•˜ê±°ë‚˜ ë°ì´í„°ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.")
    else:
        low_count_types = [(t, type_dist.get(t, 0)) for t in required_types if type_dist.get(t, 0) < 10]
        if low_count_types:
            print(f"\nâš ï¸  ë‹¤ìŒ íƒ€ì…ì˜ ë°ì´í„°ê°€ ë§¤ìš° ì ìŠµë‹ˆë‹¤:")
            for t, count in low_count_types:
                print(f"   - {t}: {count}ê°œ")
            print(f"   â†’ ì´ íƒ€ì…ìœ¼ë¡œ ê²€ìƒ‰í•˜ë©´ ëŒ€ë¶€ë¶„ í•„í„°ë§ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"\nâœ… ëª¨ë“  í•„ìˆ˜ íƒ€ì…ì˜ ë°ì´í„°ê°€ ì¶©ë¶„í•©ë‹ˆë‹¤.")
    
    print("\n" + "="*80)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

