#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Assembly Collection Performance Test
êµ­íšŒ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ë“¤ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import time
import psutil
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
import sys
sys.path.append(str(project_root))

from scripts.assembly.common_utils import (
    MemoryManager, CollectionConfig, CollectionLogger,
    get_system_memory_info, check_system_requirements
)


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™”"""
        self.start_time = None
        self.end_time = None
        self.memory_samples = []
        self.cpu_samples = []
        self.logger = CollectionLogger.setup_logging("performance_test")
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.start_time = time.time()
        self.logger.info("Performance monitoring started")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.end_time = time.time()
        self.logger.info("Performance monitoring stopped")
    
    def sample_system_metrics(self):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìƒ˜í”Œë§"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.memory_samples.append(memory_mb)
            
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent()
            self.cpu_samples.append(cpu_percent)
            
        except Exception as e:
            self.logger.error(f"Failed to sample metrics: {e}")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.start_time or not self.end_time:
            return {"error": "Monitoring not completed"}
        
        duration = self.end_time - self.start_time
        
        # ë©”ëª¨ë¦¬ í†µê³„
        memory_stats = {}
        if self.memory_samples:
            memory_stats = {
                "min_mb": min(self.memory_samples),
                "max_mb": max(self.memory_samples),
                "avg_mb": sum(self.memory_samples) / len(self.memory_samples),
                "samples": len(self.memory_samples)
            }
        
        # CPU í†µê³„
        cpu_stats = {}
        if self.cpu_samples:
            cpu_stats = {
                "min_percent": min(self.cpu_samples),
                "max_percent": max(self.cpu_samples),
                "avg_percent": sum(self.cpu_samples) / len(self.cpu_samples),
                "samples": len(self.cpu_samples)
            }
        
        return {
            "duration_seconds": duration,
            "memory_stats": memory_stats,
            "cpu_stats": cpu_stats,
            "system_info": get_system_memory_info(),
            "timestamp": datetime.now().isoformat()
        }


class CollectionBenchmark:
    """ìˆ˜ì§‘ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”"""
        self.logger = CollectionLogger.setup_logging("collection_benchmark")
        self.results = []
    
    def benchmark_memory_manager(self, iterations: int = 100) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info(f"Benchmarking MemoryManager with {iterations} iterations")
        
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸
        memory_manager = MemoryManager(memory_limit_mb=600)
        
        for i in range(iterations):
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            memory_manager.get_memory_usage()
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ìƒ˜í”Œë§
            if i % 10 == 0:
                monitor.sample_system_metrics()
        
        monitor.stop_monitoring()
        
        result = {
            "test_name": "MemoryManager",
            "iterations": iterations,
            "performance": monitor.get_performance_report()
        }
        
        self.results.append(result)
        return result
    
    def benchmark_data_optimizer(self, test_data_size: int = 1000) -> Dict[str, Any]:
        """ë°ì´í„° ìµœì í™” ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info(f"Benchmarking DataOptimizer with {test_data_size} items")
        
        from scripts.assembly.common_utils import DataOptimizer
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_items = []
        for i in range(test_data_size):
            test_items.append({
                'content_html': 'x' * 2000000,  # 2MB HTML
                'precedent_content': 'y' * 1500000,  # 1.5MB content
                'structured_content': {
                    'full_text': 'z' * 1000000,  # 1MB text
                    'case_info': 'a' * 100000
                }
            })
        
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # ë°ì´í„° ìµœì í™” í…ŒìŠ¤íŠ¸
        optimized_items = []
        for i, item in enumerate(test_items):
            optimized_item = DataOptimizer.optimize_item(item)
            optimized_items.append(optimized_item)
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ìƒ˜í”Œë§
            if i % 100 == 0:
                monitor.sample_system_metrics()
        
        monitor.stop_monitoring()
        
        # í¬ê¸° ë¹„êµ
        original_size = sum(len(str(item)) for item in test_items)
        optimized_size = sum(len(str(item)) for item in optimized_items)
        compression_ratio = optimized_size / original_size if original_size > 0 else 0
        
        result = {
            "test_name": "DataOptimizer",
            "items_processed": test_data_size,
            "original_size_bytes": original_size,
            "optimized_size_bytes": optimized_size,
            "compression_ratio": compression_ratio,
            "performance": monitor.get_performance_report()
        }
        
        self.results.append(result)
        return result
    
    def benchmark_collection_config(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ ì„¤ì • ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info("Benchmarking CollectionConfig")
        
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # ì„¤ì • ìƒì„± ë° ì¡°íšŒ í…ŒìŠ¤íŠ¸
        configs = []
        for i in range(1000):
            config = CollectionConfig(
                memory_limit_mb=600 + i,
                batch_size=20 + (i % 10),
                max_retries=3 + (i % 3)
            )
            configs.append(config)
        
        # ì„¤ì • ì¡°íšŒ í…ŒìŠ¤íŠ¸
        for config in configs:
            _ = config.get('memory_limit_mb')
            _ = config.get('batch_size')
            _ = config.get('max_retries')
        
        monitor.stop_monitoring()
        
        result = {
            "test_name": "CollectionConfig",
            "configs_created": len(configs),
            "performance": monitor.get_performance_report()
        }
        
        self.results.append(result)
        return result
    
    def run_all_benchmarks(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        self.logger.info("Starting comprehensive benchmark suite")
        
        # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
        if not check_system_requirements(min_memory_gb=2.0):
            self.logger.warning("System requirements not met, proceeding with caution")
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        self.benchmark_memory_manager(iterations=100)
        self.benchmark_data_optimizer(test_data_size=500)
        self.benchmark_collection_config()
        
        self.logger.info(f"Completed {len(self.results)} benchmarks")
        return self.results
    
    def save_results(self, output_file: Path):
        """ê²°ê³¼ ì €ì¥"""
        results_data = {
            "benchmark_info": {
                "timestamp": datetime.now().isoformat(),
                "total_tests": len(self.results),
                "system_info": get_system_memory_info()
            },
            "results": self.results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"Benchmark results saved to {output_file}")
    
    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š BENCHMARK RESULTS SUMMARY")
        print("="*60)
        
        for result in self.results:
            test_name = result["test_name"]
            performance = result.get("performance", {})
            duration = performance.get("duration_seconds", 0)
            
            print(f"\nğŸ”¬ {test_name}:")
            print(f"   Duration: {duration:.3f} seconds")
            
            if "memory_stats" in performance:
                mem_stats = performance["memory_stats"]
                print(f"   Memory: {mem_stats.get('avg_mb', 0):.1f}MB avg "
                      f"({mem_stats.get('min_mb', 0):.1f}-{mem_stats.get('max_mb', 0):.1f}MB)")
            
            if "compression_ratio" in result:
                ratio = result["compression_ratio"]
                print(f"   Compression: {ratio:.2%} of original size")
        
        print("\n" + "="*60)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Assembly Collection Performance Test',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python performance_test.py --all                    # ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
  python performance_test.py --memory-manager         # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ë§Œ í…ŒìŠ¤íŠ¸
  python performance_test.py --data-optimizer         # ë°ì´í„° ìµœì í™”ë§Œ í…ŒìŠ¤íŠ¸
  python performance_test.py --config                # ì„¤ì • ê´€ë¦¬ë§Œ í…ŒìŠ¤íŠ¸
        """
    )
    
    parser.add_argument('--all', action='store_true',
                        help='ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰')
    parser.add_argument('--memory-manager', action='store_true',
                        help='ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ë²¤ì¹˜ë§ˆí¬')
    parser.add_argument('--data-optimizer', action='store_true',
                        help='ë°ì´í„° ìµœì í™” ë²¤ì¹˜ë§ˆí¬')
    parser.add_argument('--config', action='store_true',
                        help='ì„¤ì • ê´€ë¦¬ ë²¤ì¹˜ë§ˆí¬')
    parser.add_argument('--iterations', type=int, default=100,
                        help='ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 100)')
    parser.add_argument('--data-size', type=int, default=500,
                        help='í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸° (ê¸°ë³¸: 500)')
    parser.add_argument('--output', type=str, default='benchmark_results.json',
                        help='ê²°ê³¼ ì €ì¥ íŒŒì¼ (ê¸°ë³¸: benchmark_results.json)')
    
    args = parser.parse_args()
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = CollectionBenchmark()
    
    if args.all or not any([args.memory_manager, args.data_optimizer, args.config]):
        # ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        benchmark.run_all_benchmarks()
    else:
        # ì„ íƒì  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        if args.memory_manager:
            benchmark.benchmark_memory_manager(args.iterations)
        if args.data_optimizer:
            benchmark.benchmark_data_optimizer(args.data_size)
        if args.config:
            benchmark.benchmark_collection_config()
    
    # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
    output_file = Path(args.output)
    benchmark.save_results(output_file)
    benchmark.print_summary()


if __name__ == "__main__":
    main()
