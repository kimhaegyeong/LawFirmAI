# -*- coding: utf-8 -*-
"""
Assembly Collector
ë©”ëª¨ë¦¬ ì•ˆì „ ìˆ˜ì§‘ê¸° (ë‚ ì§œ/ì¹´í…Œê³ ë¦¬ë³„ ì €ìž¥)

ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì €ìž¥í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.
- ë°°ì¹˜ ë‹¨ìœ„ ì €ìž¥ (50ê°œì”©)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- ë‚ ì§œ/ì¹´í…Œê³ ë¦¬ë³„ ë””ë ‰í† ë¦¬ êµ¬ì¡°
- ì‹¤íŒ¨ í•­ëª© ì¶”ì 
"""

from pathlib import Path
from datetime import datetime
import json
import logging
import psutil
import gc
from typing import Dict, Any, List, Optional

# ê³µí†µ ìœ í‹¸ë¦¬í‹° import
try:
    from scripts.assembly.common_utils import DataOptimizer, CollectionLogger
    COMMON_UTILS_AVAILABLE = True
except ImportError:
    COMMON_UTILS_AVAILABLE = False

# ì••ì¶• ëª¨ë“ˆ import
try:
    from scripts.assembly.law_data_compressor import compress_law_data
    COMPRESSION_AVAILABLE = True
except ImportError:
    COMPRESSION_AVAILABLE = False

logger = logging.getLogger(__name__)


class AssemblyCollector:
    """ë©”ëª¨ë¦¬ ì•ˆì „ ìˆ˜ì§‘ê¸° (ë‚ ì§œ/ì¹´í…Œê³ ë¦¬ë³„ ì €ìž¥)"""
    
    def __init__(self, 
                 base_dir: str,
                 data_type: str,  # 'law' or 'precedent'
                 category: Optional[str] = None,  # 'ë¯¼ì‚¬', 'í˜•ì‚¬' etc
                 batch_size: int = 20,  # ê¸°ë³¸ê°’ ê°ì†Œ (50 â†’ 20)
                 memory_limit_mb: int = 600):  # ê¸°ë³¸ê°’ ê°ì†Œ (800 â†’ 600)
        """
        ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        
        Args:
            base_dir: ê¸°ë³¸ ì €ìž¥ ë””ë ‰í† ë¦¬
            data_type: ë°ì´í„° íƒ€ìž… ('law' ë˜ëŠ” 'precedent')
            category: ì¹´í…Œê³ ë¦¬ (íŒë¡€ì˜ ê²½ìš° 'ë¯¼ì‚¬', 'í˜•ì‚¬' ë“±)
            batch_size: ë°°ì¹˜ í¬ê¸°
            memory_limit_mb: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ (MB)
        """
        self.data_type = data_type
        self.category = category
        self.batch_size = batch_size
        self.memory_limit_mb = memory_limit_mb
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡°: base_dir/data_type/YYYYMMDD/category/
        self.base_dir = Path(base_dir)
        today = datetime.now().strftime("%Y%m%d")
        
        # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬
        self.date_dir = self.base_dir / data_type / today
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë””ë ‰í† ë¦¬ (íŒë¡€ë§Œ)
        if category:
            self.output_dir = self.date_dir / category
        else:
            self.output_dir = self.date_dir
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ìˆ˜ì§‘ ìƒíƒœ
        self.batch = []
        self.collected_count = 0
        self.failed_items = []
        self.batch_count = 0
        
        # íŽ˜ì´ì§€ ì •ë³´ ì¶”ì 
        self.current_page = None
        self.page_start_item = 0
        
        self.logger = logging.getLogger(__name__)
        
        print(f"ðŸ“ Collector initialized:")
        print(f"   Data type: {data_type}")
        print(f"   Category: {category or 'None'}")
        print(f"   Output dir: {self.output_dir}")
        print(f"   Batch size: {batch_size}")
        print(f"   Memory limit: {memory_limit_mb}MB")
    
    def set_page_info(self, page_number: int):
        """íŽ˜ì´ì§€ ì •ë³´ ì„¤ì •"""
        self.current_page = page_number
        self.page_start_item = self.collected_count
        self.logger.info(f"ðŸ“„ Page {page_number} started, item count: {self.page_start_item}")
    
    def save_item(self, item: Dict[str, Any]):
        """
        í•­ëª© ì €ìž¥ (ê°œì„ ëœ ë²„ì „)
        
        Args:
            item: ì €ìž¥í•  ë°ì´í„° í•­ëª©
        """
        # ë°ì´í„° íƒ€ìž…ì— ë”°ë¥¸ ìµœì í™”
        if COMMON_UTILS_AVAILABLE:
            optimized_item = DataOptimizer.optimize_item(item, self.data_type)
        else:
            optimized_item = self._optimize_item_memory(item)
        
        # ì••ì¶•ì€ íŒë¡€ ë°ì´í„°ì—ëŠ” ì ìš©í•˜ì§€ ì•ŠìŒ (ë²•ë¥  ë°ì´í„°ë§Œ)
        if COMPRESSION_AVAILABLE and self.data_type == 'law':
            compressed_item = compress_law_data(optimized_item)
            self.batch.append(compressed_item)
        else:
            self.batch.append(optimized_item)
        
        self.collected_count += 1
        
        if len(self.batch) >= self.batch_size:
            self._save_batch()
            self._check_memory()
    
    def _optimize_item_memory(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì•„ì´í…œ ë©”ëª¨ë¦¬ ìµœì í™”
        
        Args:
            item: ìµœì í™”í•  ì•„ì´í…œ
            
        Returns:
            Dict: ìµœì í™”ëœ ì•„ì´í…œ
        """
        optimized = item.copy()
        
        # ëŒ€ìš©ëŸ‰ í•„ë“œ í¬ê¸° ì œí•œ
        large_fields = ['content_html', 'precedent_content', 'law_content', 'full_text']
        
        for field in large_fields:
            if field in optimized and isinstance(optimized[field], str):
                if len(optimized[field]) > 500000:  # 500KB ì œí•œ
                    optimized[field] = optimized[field][:500000] + "... [TRUNCATED]"
                    self.logger.info(f"âš ï¸ {field} truncated to 500KB")
        
        # structured_content ë‚´ë¶€ í•„ë“œë„ ìµœì í™”
        if 'structured_content' in optimized and isinstance(optimized['structured_content'], dict):
            structured = optimized['structured_content']
            for key, value in structured.items():
                if isinstance(value, str) and len(value) > 200000:  # 200KB ì œí•œ
                    structured[key] = value[:200000] + "... [TRUNCATED]"
                    self.logger.info(f"âš ï¸ structured_content.{key} truncated to 200KB")
        
        return optimized
    
    def _save_batch(self):
        """ë°°ì¹˜ íŒŒì¼ ì €ìž¥ (íŽ˜ì´ì§€ ì •ë³´ í¬í•¨)"""
        if not self.batch:
            return
        
        self.batch_count += 1
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # íŒŒì¼ëª… êµ¬ì„± (íŽ˜ì´ì§€ ì •ë³´ í¬í•¨)
        if self.category and self.current_page is not None:
            filename = f"{self.data_type}_{self.category}_page_{self.current_page:03d}_{timestamp}_{len(self.batch)}.json"
        elif self.category:
            filename = f"{self.data_type}_{self.category}_{timestamp}_{len(self.batch)}.json"
        elif self.current_page is not None:
            filename = f"{self.data_type}_page_{self.current_page:03d}_{timestamp}_{len(self.batch)}.json"
        else:
            filename = f"{self.data_type}_{timestamp}_{len(self.batch)}.json"
        
        filepath = self.output_dir / filename
        
        try:
            batch_data = {
                'metadata': {
                    'data_type': self.data_type,
                    'category': self.category,
                    'page_number': self.current_page,
                    'page_start_item': self.page_start_item,
                    'batch_number': self.batch_count,
                    'count': len(self.batch),
                    'collected_at': datetime.now().isoformat(),
                    'file_version': '1.0',
                    'total_collected': self.collected_count
                },
                'items': self.batch
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                # ì••ì¶•ëœ JSON í˜•ì‹ìœ¼ë¡œ ì €ìž¥ (ê³µë°± ì œê±°)
                json.dump(batch_data, f, ensure_ascii=False, separators=(',', ':'))
            
            print(f"âœ… Batch saved: {filename} ({len(self.batch)} items)")
            self.batch = []
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to save batch: {e}")
            raise
    
    def _check_memory(self):
        """ë©”ëª¨ë¦¬ ì²´í¬ ë° ìžë™ ì •ë¦¬ (ìµœì í™”ëœ ë²„ì „)"""
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            
            self.logger.info(f"ðŸ“Š Memory: {memory_mb:.1f}MB / {self.memory_limit_mb}MB ({memory_mb/self.memory_limit_mb*100:.1f}%)")
            
            # ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
            new_batch_size = self._calculate_adaptive_batch_size(memory_mb)
            if new_batch_size != self.batch_size:
                self.logger.info(f"ðŸ”„ Batch size adjusted: {self.batch_size} â†’ {new_batch_size}")
                self.batch_size = new_batch_size
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìœ¼ë©´ ê°•ì œ ì •ë¦¬
            if memory_mb > self.memory_limit_mb * 0.6:  # 60%ì—ì„œ ì •ë¦¬ ì‹œìž‘
                self.logger.warning(f"âš ï¸ High memory ({memory_mb:.1f}MB), forcing cleanup")
                
                # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
                
                memory_after = process.memory_info().rss / 1024 / 1024
                self.logger.info(f"âœ… After GC: {memory_after:.1f}MB")
                
                if memory_after > self.memory_limit_mb:
                    raise MemoryError(f"Memory limit exceeded: {memory_after:.1f}MB")
                    
        except Exception as e:
            self.logger.error(f"âŒ Memory check failed: {e}")
            raise
    
    def _calculate_adaptive_batch_size(self, current_memory_mb: float) -> int:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ë™ì  ê³„ì‚°"""
        memory_ratio = current_memory_mb / self.memory_limit_mb
        
        if memory_ratio > 0.7:  # 70% ì´ˆê³¼ì‹œ
            return max(5, self.batch_size - 3)
        elif memory_ratio > 0.5:  # 50% ì´ˆê³¼ì‹œ
            return max(8, self.batch_size - 2)
        elif memory_ratio < 0.3:  # 30% ë¯¸ë§Œì‹œ
            return min(20, self.batch_size + 2)
        else:
            return self.batch_size
    
    def add_failed_item(self, item_data: Dict[str, Any], error: str):
        """
        ì‹¤íŒ¨ í•­ëª© ì¶”ê°€
        
        Args:
            item_data: ì‹¤íŒ¨í•œ í•­ëª© ë°ì´í„°
            error: ì˜¤ë¥˜ ë©”ì‹œì§€
        """
        failed_item = {
            'item_data': item_data,
            'error': error,
            'timestamp': datetime.now().isoformat()
        }
        
        self.failed_items.append(failed_item)
        self.logger.warning(f"âš ï¸ Failed item added: {error}")
    
    def finalize(self):
        """ìˆ˜ì§‘ ì¢…ë£Œ ì²˜ë¦¬"""
        try:
            # ë‚¨ì€ ë°°ì¹˜ ì €ìž¥
            if self.batch:
                self._save_batch()
            
            # ì‹¤íŒ¨ í•­ëª© ì €ìž¥
            if self.failed_items:
                fail_file = self.output_dir / f"failed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                
                fail_data = {
                    'metadata': {
                        'data_type': self.data_type,
                        'category': self.category,
                        'failed_count': len(self.failed_items),
                        'created_at': datetime.now().isoformat()
                    },
                    'failed_items': self.failed_items
                }
                
                with open(fail_file, 'w', encoding='utf-8') as f:
                    json.dump(fail_data, f, indent=2, ensure_ascii=False)
                
                self.logger.info(f"ðŸ“ Failed items saved: {fail_file}")
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            # ìˆ˜ì§‘ ìš”ì•½ ìƒì„±
            summary = self._create_summary()
            summary_file = self.output_dir / f"collection_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… Collection finalized:")
            print(f"   Total collected: {self.collected_count} items")
            print(f"   Failed: {len(self.failed_items)} items")
            print(f"   Batches: {self.batch_count}")
            print(f"   Summary: {summary_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ Finalization failed: {e}")
            raise
    
    def _create_summary(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ ìš”ì•½ ìƒì„±"""
        return {
            'collection_info': {
                'data_type': self.data_type,
                'category': self.category,
                'start_time': getattr(self, 'start_time', None),
                'end_time': datetime.now().isoformat(),
                'total_collected': self.collected_count,
                'total_failed': len(self.failed_items),
                'batch_count': self.batch_count,
                'batch_size': self.batch_size,
                'memory_limit_mb': self.memory_limit_mb
            },
            'output_directory': str(self.output_dir),
            'files_created': {
                'batch_files': self.batch_count,
                'failed_file': 1 if self.failed_items else 0,
                'summary_file': 1
            },
            'statistics': {
                'success_rate': self.collected_count / (self.collected_count + len(self.failed_items)) if (self.collected_count + len(self.failed_items)) > 0 else 0,
                'average_batch_size': self.collected_count / self.batch_count if self.batch_count > 0 else 0
            }
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """í˜„ìž¬ ìˆ˜ì§‘ í†µê³„ ë°˜í™˜"""
        return {
            'collected_count': self.collected_count,
            'failed_count': len(self.failed_items),
            'batch_count': self.batch_count,
            'current_batch_size': len(self.batch),
            'data_type': self.data_type,
            'category': self.category,
            'output_dir': str(self.output_dir)
        }
    
    def set_start_time(self, start_time: str):
        """ìˆ˜ì§‘ ì‹œìž‘ ì‹œê°„ ì„¤ì •"""
        self.start_time = start_time
