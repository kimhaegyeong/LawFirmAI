#!/usr/bin/env python3
"""
í—Œì¬ê²°ì •ë¡€ ë‚ ì§œ ê¸°ë°˜ ìˆ˜ì§‘ê¸°

ì´ ëª¨ë“ˆì€ ë‚ ì§œë³„ë¡œ ì²´ê³„ì ì¸ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- ì—°ë„ë³„, ë¶„ê¸°ë³„, ì›”ë³„, ì£¼ë³„ ìˆ˜ì§‘ ì „ëµ
- ê²°ì •ì¼ì ë‚´ë¦¼ì°¨ìˆœ ìµœì í™”
- í´ë”ë³„ raw ë°ì´í„° ì €ì¥ êµ¬ì¡°
- ì¤‘ë³µ ë°©ì§€ ë° ì²´í¬í¬ì¸íŠ¸ ì§€ì›
"""

import json
import time
import random
import hashlib
import traceback
import gc
import psutil
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from source.data.law_open_api_client import LawOpenAPIClient, LawOpenAPIConfig
import logging

logger = logging.getLogger(__name__)


class DateCollectionStrategy(Enum):
    """ë‚ ì§œ ìˆ˜ì§‘ ì „ëµ ì—´ê±°í˜•"""
    YEARLY = "yearly"
    QUARTERLY = "quarterly"
    MONTHLY = "monthly"
    WEEKLY = "weekly"
    DAILY = "daily"


@dataclass
class DateCollectionConfig:
    """ë‚ ì§œ ìˆ˜ì§‘ ì„¤ì • í´ë˜ìŠ¤"""
    strategy: DateCollectionStrategy
    start_date: str
    end_date: str
    target_count: int
    batch_size: int = 100
    max_retries: int = 3
    retry_delay: int = 5
    api_delay_range: Tuple[float, float] = (1.0, 3.0)
    output_subdir: Optional[str] = None


@dataclass
class ConstitutionalDecisionData:
    """í—Œì¬ê²°ì •ë¡€ ë°ì´í„° í´ë˜ìŠ¤ - ëª©ë¡ ë°ì´í„° ë‚´ë¶€ì— ë³¸ë¬¸ ë°ì´í„° í¬í•¨"""
    # ëª©ë¡ ì¡°íšŒ API ì‘ë‹µ (ê¸°ë³¸ ì •ë³´)
    id: str  # ê²€ìƒ‰ê²°ê³¼ë²ˆí˜¸
    ì‚¬ê±´ë²ˆí˜¸: str
    ì¢…êµ­ì¼ì: str
    í—Œì¬ê²°ì •ë¡€ì¼ë ¨ë²ˆí˜¸: str
    ì‚¬ê±´ëª…: str
    í—Œì¬ê²°ì •ë¡€ìƒì„¸ë§í¬: str
    
    # ìƒì„¸ ì¡°íšŒ API ì‘ë‹µ (ë³¸ë¬¸ ë°ì´í„°) - ëª©ë¡ ë°ì´í„° ë‚´ë¶€ì— í¬í•¨
    ì‚¬ê±´ì¢…ë¥˜ëª…: Optional[str] = None
    íŒì‹œì‚¬í•­: Optional[str] = None
    ê²°ì •ìš”ì§€: Optional[str] = None
    ì „ë¬¸: Optional[str] = None
    ì°¸ì¡°ì¡°ë¬¸: Optional[str] = None
    ì°¸ì¡°íŒë¡€: Optional[str] = None
    ì‹¬íŒëŒ€ìƒì¡°ë¬¸: Optional[str] = None
    
    # ë©”íƒ€ë°ì´í„°
    document_type: str = "constitutional_decision"
    collected_at: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class CollectionStats:
    """ìˆ˜ì§‘ í†µê³„ í´ë˜ìŠ¤"""
    total_collected: int = 0
    total_duplicates: int = 0
    total_errors: int = 0
    api_requests_made: int = 0
    api_errors: int = 0
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    status: str = "PENDING"
    target_count: int = 0
    retry_delay: int = 5
    collected_decisions: Set[str] = field(default_factory=set)


class DateBasedConstitutionalCollector:
    """ë‚ ì§œ ê¸°ë°˜ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self, config: LawOpenAPIConfig, base_output_dir: Optional[Path] = None):
        """
        ë‚ ì§œ ê¸°ë°˜ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        
        Args:
            config: API ì„¤ì • ê°ì²´
            base_output_dir: ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/raw/constitutional_decisions)
        """
        self.client = LawOpenAPIClient(config)
        self.base_output_dir = base_output_dir or Path("data/raw/constitutional_decisions")
        self.base_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„° ê´€ë¦¬ (ë©”ëª¨ë¦¬ ìµœì í™”)
        self.collected_decisions: Set[str] = set()
        self.processed_date_ranges: Set[str] = set()
        self.pending_decisions: List[ConstitutionalDecisionData] = []
        self.max_memory_decisions = 10000  # ìµœëŒ€ ë©”ëª¨ë¦¬ ë³´ê´€ ê±´ìˆ˜
        
        # í†µê³„ ë° ìƒíƒœ
        self.stats = CollectionStats()
        
        # ì—ëŸ¬ ì²˜ë¦¬
        self.error_count = 0
        self.max_errors = 50
        
        # ê¸°ì¡´ ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œ
        self._load_existing_data()
        
        # ì‹œê°„ ì¸í„°ë²Œ ì„¤ì • (ê¸°ë³¸ê°’)
        self.request_interval_base = 2.0  # ê¸°ë³¸ ê°„ê²©
        self.request_interval_range = 2.0  # ê°„ê²© ë²”ìœ„
        
        # ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ ëª¨ë“œ (ê¸°ë³¸ê°’: False)
        self.resume_mode = False
        
        logger.info("ë‚ ì§œ ê¸°ë°˜ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _monitor_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        try:
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
            
            if memory_mb > 1000:  # 1GB ì´ìƒ ì‚¬ìš© ì‹œ ê²½ê³ 
                logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {memory_mb:.1f}MB")
                self._cleanup_memory()
                
            return memory_mb
        except Exception as e:
            logger.debug(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
            return 0
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            collected = gc.collect()
            logger.debug(f"ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì™„ë£Œ: {collected}ê°œ ê°ì²´ ì •ë¦¬")
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° êµ¬ì¡° ì •ë¦¬
            if len(self.collected_decisions) > self.max_memory_decisions:
                # ì˜¤ë˜ëœ ë°ì´í„° ì¼ë¶€ ì œê±° (ìµœê·¼ 50%ë§Œ ìœ ì§€)
                sorted_decisions = sorted(self.collected_decisions)
                keep_count = len(sorted_decisions) // 2
                self.collected_decisions = set(sorted_decisions[-keep_count:])
                logger.info(f"ë©”ëª¨ë¦¬ ì •ë¦¬: ìˆ˜ì§‘ëœ ê²°ì •ë¡€ {len(sorted_decisions) - keep_count}ê°œ ì œê±°")
            
            # ëŒ€ê¸° ì¤‘ì¸ ë°ì´í„° ì •ë¦¬
            if len(self.pending_decisions) > 1000:
                self.pending_decisions = self.pending_decisions[-500:]  # ìµœê·¼ 500ê°œë§Œ ìœ ì§€
                logger.info("ë©”ëª¨ë¦¬ ì •ë¦¬: ëŒ€ê¸° ì¤‘ì¸ ê²°ì •ë¡€ ë°ì´í„° ì •ë¦¬")
                
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _check_memory_and_cleanup(self):
        """ë©”ëª¨ë¦¬ ì²´í¬ ë° ì •ë¦¬"""
        memory_mb = self._monitor_memory_usage()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìœ¼ë©´ ì •ë¦¬
        if memory_mb > 800:  # 800MB ì´ìƒ
            self._cleanup_memory()
            
        return memory_mb
    
    def set_request_interval(self, base_interval: float, interval_range: float):
        """API ìš”ì²­ ê°„ê²© ì„¤ì •"""
        self.request_interval_base = base_interval
        self.request_interval_range = interval_range
        logger.info(f"â±ï¸ ìš”ì²­ ê°„ê²© ì„¤ì •: {base_interval:.1f} Â± {interval_range:.1f}ì´ˆ")
    
    def enable_resume_mode(self):
        """ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ ëª¨ë“œ í™œì„±í™”"""
        self.resume_mode = True
        logger.info("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def _load_existing_data(self, target_year: Optional[int] = None):
        """ê¸°ì¡´ ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œ"""
        try:
            if target_year:
                # íŠ¹ì • ì—°ë„ ë°ì´í„°ë§Œ ë¡œë“œ
                pattern = f"yearly_{target_year}_*"
                existing_dirs = list(self.base_output_dir.glob(pattern))
            else:
                # ëª¨ë“  ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
                existing_dirs = list(self.base_output_dir.glob("*"))
            
            for dir_path in existing_dirs:
                if dir_path.is_dir():
                    json_files = list(dir_path.glob("page_*.json"))
                    for json_file in json_files:
                        try:
                            with open(json_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                decisions = data.get('decisions', [])
                                for decision in decisions:
                                    decision_id = decision.get('í—Œì¬ê²°ì •ë¡€ì¼ë ¨ë²ˆí˜¸')
                                    if decision_id:
                                        self.collected_decisions.add(decision_id)
                        except Exception as e:
                            logger.warning(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {json_file} - {e}")
            
            logger.info(f"ê¸°ì¡´ ìˆ˜ì§‘ëœ í—Œì¬ê²°ì •ë¡€: {len(self.collected_decisions)}ê±´")
            
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _create_output_directory(self, strategy: DateCollectionStrategy, 
                               year: Optional[int] = None, 
                               quarter: Optional[int] = None,
                               month: Optional[int] = None,
                               week_start: Optional[str] = None) -> Path:
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚¬ìš©)"""
        
        # ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        if self.resume_mode:
            existing_dir = self._find_existing_directory(strategy, year, quarter, month, week_start)
            if existing_dir:
                logger.info(f"ğŸ”„ ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {existing_dir}")
                return existing_dir
        
        # ìƒˆë¡œìš´ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if strategy == DateCollectionStrategy.YEARLY:
            if year:
                dir_name = f"yearly_{year}_{timestamp}"
            else:
                dir_name = f"yearly_collection_{timestamp}"
        elif strategy == DateCollectionStrategy.QUARTERLY:
            if year and quarter:
                dir_name = f"quarterly_{year}Q{quarter}_{timestamp}"
            else:
                dir_name = f"quarterly_collection_{timestamp}"
        elif strategy == DateCollectionStrategy.MONTHLY:
            if year and month:
                dir_name = f"monthly_{year}ë…„{month}ì›”_{timestamp}"
            else:
                dir_name = f"monthly_collection_{timestamp}"
        elif strategy == DateCollectionStrategy.WEEKLY:
            if week_start:
                dir_name = f"weekly_{week_start}ì£¼_{timestamp}"
            else:
                dir_name = f"weekly_collection_{timestamp}"
        else:
            dir_name = f"daily_collection_{timestamp}"
        
        output_dir = self.base_output_dir / dir_name
        
        # ë””ë ‰í† ë¦¬ ìƒì„± ê°•í™”
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            logger.debug(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}")
        except Exception as e:
            logger.error(f"ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {output_dir} - {e}")
            raise
        
        return output_dir
    
    def _find_existing_directory(self, strategy: DateCollectionStrategy, 
                               year: Optional[int] = None, 
                               quarter: Optional[int] = None,
                               month: Optional[int] = None,
                               week_start: Optional[str] = None) -> Optional[Path]:
        """ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
        try:
            # íŒ¨í„´ ìƒì„±
            if strategy == DateCollectionStrategy.YEARLY and year:
                pattern = f"yearly_{year}_*"
            elif strategy == DateCollectionStrategy.QUARTERLY and year and quarter:
                pattern = f"quarterly_{year}Q{quarter}_*"
            elif strategy == DateCollectionStrategy.MONTHLY and year and month:
                pattern = f"monthly_{year}ë…„{month}ì›”_*"
            elif strategy == DateCollectionStrategy.WEEKLY and week_start:
                pattern = f"weekly_{week_start}ì£¼_*"
            else:
                return None
            
            # í•´ë‹¹ íŒ¨í„´ì˜ ë””ë ‰í† ë¦¬ë“¤ ì°¾ê¸°
            matching_dirs = list(self.base_output_dir.glob(pattern))
            
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ì°¾ê¸°
            for dir_path in sorted(matching_dirs, reverse=True):  # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
                checkpoint_file = dir_path / "checkpoint.json"
                if checkpoint_file.exists():
                    logger.info(f"ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_file}")
                    return dir_path
            
            logger.info(f"ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒ¨í„´: {pattern}")
            return None
            
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def _save_batch(self, decisions: List[ConstitutionalDecisionData], 
                   output_dir: Path, page_num: int, 
                   category: str = "constitutional") -> bool:
        """ë°°ì¹˜ ë°ì´í„° ì €ì¥"""
        try:
            if not decisions:
                return True
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
            if not output_dir.exists():
                logger.warning(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {output_dir}")
                output_dir.mkdir(parents=True, exist_ok=True)
                logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¬ìƒì„±: {output_dir}")
            
            # íŒŒì¼ëª… ìƒì„±
            start_id = decisions[0].í—Œì¬ê²°ì •ë¡€ì¼ë ¨ë²ˆí˜¸
            end_id = decisions[-1].í—Œì¬ê²°ì •ë¡€ì¼ë ¨ë²ˆí˜¸
            count = len(decisions)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            filename = f"page_{page_num:03d}_{category}_{start_id}-{end_id}_{count}ê±´_{timestamp}.json"
            file_path = output_dir / filename
            
            # ë°ì´í„° êµ¬ì¡°í™”
            batch_data = {
                "metadata": {
                    "category": category,
                    "page": page_num,
                    "count": count,
                    "start_id": start_id,
                    "end_id": end_id,
                    "collected_at": timestamp,
                    "strategy": "date_based"
                },
                "decisions": [decision.__dict__ for decision in decisions]
            }
            
            # JSON íŒŒì¼ ì €ì¥
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(batch_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {filename} ({count}ê±´)")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì§„í–‰ ìƒí™© ê¸°ë¡)
            self._save_checkpoint(output_dir, page_num, collected_count=len(decisions))
            
            return True
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _save_checkpoint(self, output_dir: Path, page_num: int, collected_count: int):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì§„í–‰ ìƒí™© ê¸°ë¡)"""
        try:
            checkpoint_data = {
                "checkpoint_info": {
                    "last_page": page_num,
                    "collected_count": collected_count,
                    "timestamp": datetime.now().isoformat(),
                    "status": "in_progress"
                }
            }
            
            checkpoint_file = output_dir / "checkpoint.json"
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _load_checkpoint(self, output_dir: Path) -> dict:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ)"""
        try:
            checkpoint_file = output_dir / "checkpoint.json"
            if checkpoint_file.exists():
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                logger.info(f"ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: í˜ì´ì§€ {checkpoint_data['checkpoint_info']['last_page']}, ìˆ˜ì§‘ëœ ê±´ìˆ˜ {checkpoint_data['checkpoint_info']['collected_count']}")
                return checkpoint_data
        except Exception as e:
            logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def _save_summary(self, output_dir: Path, strategy: DateCollectionStrategy, 
                     total_collected: int, total_duplicates: int, 
                     total_errors: int, duration: timedelta):
        """ìˆ˜ì§‘ ìš”ì•½ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_data = {
                "collection_info": {
                    "strategy": strategy.value,
                    "start_time": self.stats.start_time,
                    "end_time": self.stats.end_time,
                    "duration_seconds": duration.total_seconds(),
                    "duration_str": str(duration)
                },
                "statistics": {
                    "total_collected": total_collected,
                    "total_duplicates": total_duplicates,
                    "total_errors": total_errors,
                    "api_requests_made": self.stats.api_requests_made,
                    "api_errors": self.stats.api_errors,
                    "success_rate": (total_collected / (total_collected + total_errors)) * 100 if (total_collected + total_errors) > 0 else 0
                },
                "collected_decisions": list(self.collected_decisions),
                "metadata": {
                    "collected_at": timestamp,
                    "output_directory": str(output_dir),
                    "total_files": len(list(output_dir.glob("page_*.json")))
                }
            }
            
            summary_file = output_dir / f"{strategy.value}_collection_summary_{timestamp}.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ìš”ì•½ ì €ì¥: {summary_file.name}")
            
        except Exception as e:
            logger.error(f"ìˆ˜ì§‘ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def collect_by_year(self, year: int, target_count: Optional[int] = None, 
                       unlimited: bool = False, use_final_date: bool = False) -> bool:
        """íŠ¹ì • ì—°ë„ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘"""
        try:
            date_type = "ì¢…êµ­ì¼ì" if use_final_date else "ì„ ê³ ì¼ì"
            logger.info(f"ğŸ—“ï¸ {year}ë…„ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ ì‹œì‘ ({date_type} ê¸°ì¤€)")
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = self._create_output_directory(DateCollectionStrategy.YEARLY, year=year)
            
            # ì²´í¬í¬ì¸íŠ¸ í™•ì¸ (ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ)
            checkpoint = None
            start_page = 1
            if self.resume_mode:
                checkpoint = self._load_checkpoint(output_dir)
                if checkpoint:
                    start_page = checkpoint['checkpoint_info']['last_page'] + 1
                    logger.info(f"ğŸ”„ ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ: í˜ì´ì§€ {start_page}ë¶€í„° ì‹œì‘")
                else:
                    logger.info("ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            else:
                logger.info("ğŸ†• ìƒˆë¡œìš´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (íŠ¹ì • ì—°ë„ë§Œ)
            self._load_existing_data(target_year=year)
            
            # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
            start_date = f"{year}0101"
            end_date = f"{year}1231"
            
            # ëª©í‘œ ê±´ìˆ˜ ì„¤ì •
            if unlimited:
                target_count = 999999  # ë¬´ì œí•œ
            elif target_count is None:
                target_count = 2000  # ê¸°ë³¸ê°’
            
            self.stats.target_count = target_count
            self.stats.start_time = datetime.now().isoformat()
            
            logger.info(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date} ({date_type} ê¸°ì¤€)")
            logger.info(f"ğŸ¯ ëª©í‘œ ê±´ìˆ˜: {target_count:,}ê±´")
            logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
            
            # ìˆ˜ì§‘ ì‹¤í–‰
            success = self._collect_decisions_by_date_range(
                start_date=start_date,
                end_date=end_date,
                target_count=target_count,
                output_dir=output_dir,
                category=f"{year}ë…„",
                use_final_date=use_final_date,
                start_page=start_page
            )
            
            # ìˆ˜ì§‘ ì™„ë£Œ ì²˜ë¦¬
            self.stats.end_time = datetime.now().isoformat()
            duration = datetime.fromisoformat(self.stats.end_time) - datetime.fromisoformat(self.stats.start_time)
            
            # ìš”ì•½ ì €ì¥
            self._save_summary(
                output_dir=output_dir,
                strategy=DateCollectionStrategy.YEARLY,
                total_collected=self.stats.total_collected,
                total_duplicates=self.stats.total_duplicates,
                total_errors=self.stats.total_errors,
                duration=duration
            )
            
            logger.info(f"âœ… {year}ë…„ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {self.stats.total_collected:,}ê±´ ìˆ˜ì§‘, {self.stats.total_duplicates:,}ê±´ ì¤‘ë³µ, {self.stats.total_errors:,}ê±´ ì˜¤ë¥˜")
            logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
            
            return success
            
        except Exception as e:
            logger.error(f"{year}ë…„ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def collect_by_quarter(self, year: int, quarter: int, target_count: int = 500) -> bool:
        """íŠ¹ì • ë¶„ê¸° í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘"""
        try:
            logger.info(f"ğŸ—“ï¸ {year}ë…„ {quarter}ë¶„ê¸° í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ ì‹œì‘")
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = self._create_output_directory(DateCollectionStrategy.QUARTERLY, year=year, quarter=quarter)
            
            # ì²´í¬í¬ì¸íŠ¸ í™•ì¸ (ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ)
            checkpoint = None
            start_page = 1
            if self.resume_mode:
                checkpoint = self._load_checkpoint(output_dir)
                if checkpoint:
                    start_page = checkpoint['checkpoint_info']['last_page'] + 1
                    logger.info(f"ğŸ”„ ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ: í˜ì´ì§€ {start_page}ë¶€í„° ì‹œì‘")
                else:
                    logger.info("ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            else:
                logger.info("ğŸ†• ìƒˆë¡œìš´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # ë¶„ê¸°ë³„ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
            quarter_months = {
                1: (1, 3),    # 1ë¶„ê¸°: 1-3ì›”
                2: (4, 6),    # 2ë¶„ê¸°: 4-6ì›”
                3: (7, 9),    # 3ë¶„ê¸°: 7-9ì›”
                4: (10, 12)   # 4ë¶„ê¸°: 10-12ì›”
            }
            
            start_month, end_month = quarter_months[quarter]
            start_date = f"{year}{start_month:02d}01"
            end_date = f"{year}{end_month:02d}31"
            
            self.stats.target_count = target_count
            self.stats.start_time = datetime.now().isoformat()
            
            logger.info(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
            logger.info(f"ğŸ¯ ëª©í‘œ ê±´ìˆ˜: {target_count:,}ê±´")
            logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
            
            # ìˆ˜ì§‘ ì‹¤í–‰
            success = self._collect_decisions_by_date_range(
                start_date=start_date,
                end_date=end_date,
                target_count=target_count,
                output_dir=output_dir,
                category=f"{year}ë…„{quarter}ë¶„ê¸°",
                start_page=start_page
            )
            
            # ìˆ˜ì§‘ ì™„ë£Œ ì²˜ë¦¬
            self.stats.end_time = datetime.now().isoformat()
            duration = datetime.fromisoformat(self.stats.end_time) - datetime.fromisoformat(self.stats.start_time)
            
            # ìš”ì•½ ì €ì¥
            self._save_summary(
                output_dir=output_dir,
                strategy=DateCollectionStrategy.QUARTERLY,
                total_collected=self.stats.total_collected,
                total_duplicates=self.stats.total_duplicates,
                total_errors=self.stats.total_errors,
                duration=duration
            )
            
            logger.info(f"âœ… {year}ë…„ {quarter}ë¶„ê¸° í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {self.stats.total_collected:,}ê±´ ìˆ˜ì§‘, {self.stats.total_duplicates:,}ê±´ ì¤‘ë³µ, {self.stats.total_errors:,}ê±´ ì˜¤ë¥˜")
            logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
            
            return success
            
        except Exception as e:
            logger.error(f"{year}ë…„ {quarter}ë¶„ê¸° í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def collect_by_month(self, year: int, month: int, target_count: int = 200) -> bool:
        """íŠ¹ì • ì›” í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘"""
        try:
            logger.info(f"ğŸ—“ï¸ {year}ë…„ {month}ì›” í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ ì‹œì‘")
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = self._create_output_directory(DateCollectionStrategy.MONTHLY, year=year, month=month)
            
            # ì²´í¬í¬ì¸íŠ¸ í™•ì¸ (ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ)
            checkpoint = None
            start_page = 1
            if self.resume_mode:
                checkpoint = self._load_checkpoint(output_dir)
                if checkpoint:
                    start_page = checkpoint['checkpoint_info']['last_page'] + 1
                    logger.info(f"ğŸ”„ ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ: í˜ì´ì§€ {start_page}ë¶€í„° ì‹œì‘")
                else:
                    logger.info("ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            else:
                logger.info("ğŸ†• ìƒˆë¡œìš´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            # ì›”ë³„ ë‚ ì§œ ë²”ìœ„ ì„¤ì •
            start_date = f"{year}{month:02d}01"
            # ì›”ë§ ë‚ ì§œ ê³„ì‚°
            if month == 12:
                end_date = f"{year}1231"
            else:
                next_month = month + 1
                next_year = year if next_month <= 12 else year + 1
                if next_month > 12:
                    next_month = 1
                end_date = f"{next_year}{next_month:02d}01"
                # í•˜ë£¨ ì „ìœ¼ë¡œ ì„¤ì •
                end_date_obj = datetime.strptime(end_date, "%Y%m%d") - timedelta(days=1)
                end_date = end_date_obj.strftime("%Y%m%d")
            
            self.stats.target_count = target_count
            self.stats.start_time = datetime.now().isoformat()
            
            logger.info(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
            logger.info(f"ğŸ¯ ëª©í‘œ ê±´ìˆ˜: {target_count:,}ê±´")
            logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
            
            # ìˆ˜ì§‘ ì‹¤í–‰
            success = self._collect_decisions_by_date_range(
                start_date=start_date,
                end_date=end_date,
                target_count=target_count,
                output_dir=output_dir,
                category=f"{year}ë…„{month}ì›”",
                start_page=start_page
            )
            
            # ìˆ˜ì§‘ ì™„ë£Œ ì²˜ë¦¬
            self.stats.end_time = datetime.now().isoformat()
            duration = datetime.fromisoformat(self.stats.end_time) - datetime.fromisoformat(self.stats.start_time)
            
            # ìš”ì•½ ì €ì¥
            self._save_summary(
                output_dir=output_dir,
                strategy=DateCollectionStrategy.MONTHLY,
                total_collected=self.stats.total_collected,
                total_duplicates=self.stats.total_duplicates,
                total_errors=self.stats.total_errors,
                duration=duration
            )
            
            logger.info(f"âœ… {year}ë…„ {month}ì›” í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {self.stats.total_collected:,}ê±´ ìˆ˜ì§‘, {self.stats.total_duplicates:,}ê±´ ì¤‘ë³µ, {self.stats.total_errors:,}ê±´ ì˜¤ë¥˜")
            logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
            
            return success
            
        except Exception as e:
            logger.error(f"{year}ë…„ {month}ì›” í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def _collect_decisions_by_date_range(self, start_date: str, end_date: str, 
                                       target_count: int, output_dir: Path, 
                                       category: str, use_final_date: bool = False, 
                                       start_page: int = 1) -> bool:
        """ë‚ ì§œ ë²”ìœ„ë³„ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ (ì„ ê³ ì¼ì ë‚´ë¦¼ì°¨ìˆœ)"""
        try:
            page = start_page
            collected_count = 0
            batch_decisions = []
            date_type = "ì¢…êµ­ì¼ì" if use_final_date else "ì„ ê³ ì¼ì"
            
            while collected_count < target_count:
                try:
                    # ë©”ëª¨ë¦¬ ì²´í¬ ë° ì •ë¦¬ (ë§¤ 10í˜ì´ì§€ë§ˆë‹¤)
                    if page % 10 == 0:
                        memory_mb = self._check_memory_and_cleanup()
                        logger.info(f"ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f}MB")
                    
                    # API ìš”ì²­ ì§€ì—° - ì‚¬ìš©ì ì„¤ì • ê°„ê²© ì‚¬ìš©
                    min_interval = max(0.1, self.request_interval_base - self.request_interval_range)
                    max_interval = self.request_interval_base + self.request_interval_range
                    delay = random.uniform(min_interval, max_interval)
                    time.sleep(delay)
                    
                    # í—Œì¬ê²°ì •ë¡€ ëª©ë¡ ì¡°íšŒ (ì„ ê³ ì¼ì ë‚´ë¦¼ì°¨ìˆœ)
                    logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ì¡°íšŒ ì¤‘... (ìˆ˜ì§‘ëœ ê±´ìˆ˜: {collected_count:,}/{target_count:,}) [{date_type} ê¸°ì¤€]")
                    
                    # ì¢…êµ­ì¼ì ê¸°ì¤€ ìˆ˜ì§‘ì¸ ê²½ìš° edYd íŒŒë¼ë¯¸í„° ì‚¬ìš©
                    if use_final_date:
                        # ì¢…êµ­ì¼ì ê¸°ê°„ ê²€ìƒ‰ (edYd: YYYYMMDD-YYYYMMDD í˜•ì‹)
                        edYd_range = f"{start_date}-{end_date}"
                        results = self.client.get_constitutional_list(
                            query=None,  # í‚¤ì›Œë“œ ì—†ì´ ë‚ ì§œ ë²”ìœ„ë¡œë§Œ ê²€ìƒ‰
                            display=100,  # í˜ì´ì§€ë‹¹ ìµœëŒ€ ê±´ìˆ˜
                            page=page,
                            search=1,  # ê²€ìƒ‰ ë²”ìœ„
                            sort="efdes",  # ì¢…êµ­ì¼ì ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                            edYd=edYd_range  # ì¢…êµ­ì¼ì ê¸°ê°„ ê²€ìƒ‰
                        )
                    else:
                        # ì„ ê³ ì¼ì ê¸°ì¤€ ìˆ˜ì§‘ (ê¸°ì¡´ ë°©ì‹)
                        results = self.client.get_constitutional_list(
                            query=None,  # í‚¤ì›Œë“œ ì—†ì´ ë‚ ì§œ ë²”ìœ„ë¡œë§Œ ê²€ìƒ‰
                            display=100,  # í˜ì´ì§€ë‹¹ ìµœëŒ€ ê±´ìˆ˜
                            page=page,
                            from_date=start_date,
                            to_date=end_date,
                            search=1,  # ê²€ìƒ‰ ë²”ìœ„
                            sort="ddes"  # ì„ ê³ ì¼ì ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                        )
                    
                    if not results:
                        logger.info(f"ğŸ“„ í˜ì´ì§€ {page}: ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    new_decisions = 0
                    for result in results:
                        if collected_count >= target_count:
                            break
                        
                        # í—Œì¬ê²°ì •ë¡€ ID í™•ì¸
                        decision_id = result.get('í—Œì¬ê²°ì •ë¡€ì¼ë ¨ë²ˆí˜¸')
                        if not decision_id:
                            continue
                        
                        # ì¤‘ë³µ í™•ì¸
                        if decision_id in self.collected_decisions:
                            self.stats.total_duplicates += 1
                            continue
                        
                        # ìƒì„¸ ì •ë³´ ì¡°íšŒ
                        try:
                            detail = self.client.get_constitutional_detail(decision_id)
                            
                            # ConstitutionalDecisionData ê°ì²´ ìƒì„± (ëª©ë¡ ë°ì´í„° ë‚´ë¶€ì— ë³¸ë¬¸ ë°ì´í„° í¬í•¨)
                            decision_data = ConstitutionalDecisionData(
                                # ëª©ë¡ ì¡°íšŒ API ì‘ë‹µ (ê¸°ë³¸ ì •ë³´)
                                id=result.get('id', ''),
                                ì‚¬ê±´ë²ˆí˜¸=result.get('ì‚¬ê±´ë²ˆí˜¸', ''),
                                ì¢…êµ­ì¼ì=result.get('ì¢…êµ­ì¼ì', ''),
                                í—Œì¬ê²°ì •ë¡€ì¼ë ¨ë²ˆí˜¸=decision_id,
                                ì‚¬ê±´ëª…=result.get('ì‚¬ê±´ëª…', ''),
                                í—Œì¬ê²°ì •ë¡€ìƒì„¸ë§í¬=result.get('í—Œì¬ê²°ì •ë¡€ìƒì„¸ë§í¬', ''),
                                
                                # ìƒì„¸ ì¡°íšŒ API ì‘ë‹µ (ë³¸ë¬¸ ë°ì´í„°)
                                ì‚¬ê±´ì¢…ë¥˜ëª…=detail.get('ì‚¬ê±´ì¢…ë¥˜ëª…') if detail else None,
                                íŒì‹œì‚¬í•­=detail.get('íŒì‹œì‚¬í•­') if detail else None,
                                ê²°ì •ìš”ì§€=detail.get('ê²°ì •ìš”ì§€') if detail else None,
                                ì „ë¬¸=detail.get('ì „ë¬¸') if detail else None,
                                ì°¸ì¡°ì¡°ë¬¸=detail.get('ì°¸ì¡°ì¡°ë¬¸') if detail else None,
                                ì°¸ì¡°íŒë¡€=detail.get('ì°¸ì¡°íŒë¡€') if detail else None,
                                ì‹¬íŒëŒ€ìƒì¡°ë¬¸=detail.get('ì‹¬íŒëŒ€ìƒì¡°ë¬¸') if detail else None,
                                
                                # ë©”íƒ€ë°ì´í„°
                                document_type="constitutional_decision",
                                collected_at=datetime.now().isoformat()
                            )
                            
                            batch_decisions.append(decision_data)
                            self.collected_decisions.add(decision_id)
                            collected_count += 1
                            new_decisions += 1
                            
                            logger.info(f"âœ… ìƒˆë¡œìš´ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘: {decision_data.ì‚¬ê±´ëª…} (ID: {decision_id})")
                            
                            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¤‘ê°„ ì €ì¥ (10ê±´ë§ˆë‹¤)
                            if len(batch_decisions) >= 10:
                                self._save_batch(batch_decisions, output_dir, page, category)
                                batch_decisions = []  # ë°°ì¹˜ ì´ˆê¸°í™”
                            
                        except Exception as e:
                            logger.error(f"ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {decision_id} - {e}")
                            # ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì •ë³´ëŠ” ì €ì¥
                            decision_data = ConstitutionalDecisionData(
                                # ëª©ë¡ ì¡°íšŒ API ì‘ë‹µ (ê¸°ë³¸ ì •ë³´)
                                id=result.get('id', ''),
                                ì‚¬ê±´ë²ˆí˜¸=result.get('ì‚¬ê±´ë²ˆí˜¸', ''),
                                ì¢…êµ­ì¼ì=result.get('ì¢…êµ­ì¼ì', ''),
                                í—Œì¬ê²°ì •ë¡€ì¼ë ¨ë²ˆí˜¸=decision_id,
                                ì‚¬ê±´ëª…=result.get('ì‚¬ê±´ëª…', ''),
                                í—Œì¬ê²°ì •ë¡€ìƒì„¸ë§í¬=result.get('í—Œì¬ê²°ì •ë¡€ìƒì„¸ë§í¬', ''),
                                
                                # ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨ë¡œ ë³¸ë¬¸ ë°ì´í„°ëŠ” None
                                ì‚¬ê±´ì¢…ë¥˜ëª…=None,
                                íŒì‹œì‚¬í•­=None,
                                ê²°ì •ìš”ì§€=None,
                                ì „ë¬¸=None,
                                ì°¸ì¡°ì¡°ë¬¸=None,
                                ì°¸ì¡°íŒë¡€=None,
                                ì‹¬íŒëŒ€ìƒì¡°ë¬¸=None,
                                
                                # ë©”íƒ€ë°ì´í„°
                                document_type="constitutional_decision",
                                collected_at=datetime.now().isoformat()
                            )
                            
                            batch_decisions.append(decision_data)
                            self.collected_decisions.add(decision_id)
                            collected_count += 1
                            new_decisions += 1
                            
                            logger.warning(f"âš ï¸ ê¸°ë³¸ ì •ë³´ë§Œ ìˆ˜ì§‘: {decision_data.ì‚¬ê±´ëª…} (ID: {decision_id})")
                            
                            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¤‘ê°„ ì €ì¥ (10ê±´ë§ˆë‹¤)
                            if len(batch_decisions) >= 10:
                                self._save_batch(batch_decisions, output_dir, page, category)
                                batch_decisions = []  # ë°°ì¹˜ ì´ˆê¸°í™”
                            self.stats.total_errors += 1
                    
                    # ë°°ì¹˜ ì €ì¥ (100ê±´ë§ˆë‹¤) - ì¶”ê°€ ì•ˆì „ì¥ì¹˜
                    if len(batch_decisions) >= 100:
                        self._save_batch(batch_decisions, output_dir, page, category)
                        batch_decisions = []
                    
                    logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ì™„ë£Œ: {new_decisions}ê±´ì˜ ìƒˆë¡œìš´ ê²°ì •ë¡€ ìˆ˜ì§‘")
                    logger.info(f"   ğŸ“Š ëˆ„ì  ìˆ˜ì§‘: {collected_count:,}/{target_count:,}ê±´ ({collected_count/target_count*100:.1f}%)")
                    
                    page += 1
                    self.stats.api_requests_made += 1
                    
                    # API ìš”ì²­ ì œí•œ í™•ì¸
                    stats = self.client.get_request_stats()
                    if stats['remaining_requests'] < 10:
                        logger.warning("API ìš”ì²­ í•œë„ê°€ ê±°ì˜ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        break
                    
                except Exception as e:
                    logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    self.stats.api_errors += 1
                    self.error_count += 1
                    
                    if self.error_count >= self.max_errors:
                        logger.error(f"ìµœëŒ€ ì˜¤ë¥˜ íšŸìˆ˜({self.max_errors})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                        break
                    
                    # ì¬ì‹œë„ ì§€ì—°
                    time.sleep(self.stats.retry_delay)
                    continue
            
            # ë‚¨ì€ ë°°ì¹˜ ì €ì¥
            if batch_decisions:
                self._save_batch(batch_decisions, output_dir, page, category)
            
            self.stats.total_collected = collected_count
            logger.info(f"ğŸ¯ ìˆ˜ì§‘ ì™„ë£Œ: {collected_count:,}ê±´ ìˆ˜ì§‘")
            
            return True
            
        except Exception as e:
            logger.error(f"ë‚ ì§œ ë²”ìœ„ë³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def collect_multiple_years(self, start_year: int, end_year: int, 
                              target_per_year: int = 2000) -> bool:
        """ì—¬ëŸ¬ ì—°ë„ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘"""
        try:
            logger.info(f"ğŸ—“ï¸ {start_year}ë…„ ~ {end_year}ë…„ í—Œì¬ê²°ì •ë¡€ ìˆ˜ì§‘ ì‹œì‘")
            
            total_success = True
            for year in range(start_year, end_year + 1):
                logger.info(f"ğŸ“… {year}ë…„ ìˆ˜ì§‘ ì‹œì‘...")
                success = self.collect_by_year(year, target_per_year)
                if not success:
                    logger.warning(f"âš ï¸ {year}ë…„ ìˆ˜ì§‘ ì‹¤íŒ¨")
                    total_success = False
                else:
                    logger.info(f"âœ… {year}ë…„ ìˆ˜ì§‘ ì™„ë£Œ")
                
                # ì—°ë„ ê°„ ì§€ì—°
                time.sleep(5)
            
            logger.info(f"ğŸ¯ ë‹¤ì¤‘ ì—°ë„ ìˆ˜ì§‘ ì™„ë£Œ: {start_year}ë…„ ~ {end_year}ë…„")
            return total_success
            
        except Exception as e:
            logger.error(f"ë‹¤ì¤‘ ì—°ë„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return False
