#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œìŠ¤í…œ
3000ê°œì˜ í…ŒìŠ¤íŠ¸ ì§ˆì˜ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import sys
import os
import json
import time
import asyncio
import multiprocessing
from typing import Dict, List, Any, Optional
from datetime import datetime
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import traceback

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from source.services.multi_stage_validation_system import MultiStageValidationSystem
from source.services.chat_service import ChatService
from source.utils.config import Config

# --------------------
# ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì›Œì»¤ ì „ì—­ ë° í•¨ìˆ˜ (Windows í˜¸í™˜)
# --------------------
mp_multi_stage_system = None
mp_chat_service = None

def mp_init_worker(enable_chat: bool = False):
    """í”„ë¡œì„¸ìŠ¤ ì›Œì»¤ ì´ˆê¸°í™” (ëª¨ë“ˆ ì „ì—­)"""
    global mp_multi_stage_system, mp_chat_service
    try:
        # í™˜ê²½ ë³€ìˆ˜ë¡œ ê°œì„ ëœ ê²€ì¦ ì‹œìŠ¤í…œ ì‚¬ìš© ì—¬ë¶€ ì„ íƒ
        use_improved = os.getenv("USE_IMPROVED_VALIDATION", "0") == "1"
        if use_improved:
            try:
                from source.services.improved_multi_stage_validation_system import ImprovedMultiStageValidationSystem as _VSys
                mp_multi_stage_system = _VSys()
            except Exception as _e:
                print(f"ê°œì„ ëœ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´: {_e}")
                mp_multi_stage_system = MultiStageValidationSystem()
        else:
            mp_multi_stage_system = MultiStageValidationSystem()
        if enable_chat:
            config = Config()
            mp_chat_service = ChatService(config)
        else:
            mp_chat_service = None
    except Exception as e:
        print(f"ì›Œì»¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        mp_multi_stage_system = None
        mp_chat_service = None

def mp_process_query_worker(query_data: Dict[str, Any]) -> Dict[str, Any]:
    """ë‹¨ì¼ ì§ˆì˜ ì²˜ë¦¬(í”„ë¡œì„¸ìŠ¤ ì›Œì»¤ìš©) - ì§ë ¬í™” ê°€ëŠ¥í•œ dict ë°˜í™˜"""
    from dataclasses import asdict as _asdict
    import time as _time
    global mp_multi_stage_system
    if mp_multi_stage_system is None:
        return {
            "query": query_data.get("query", ""),
            "category": query_data.get("category", ""),
            "subcategory": query_data.get("subcategory", ""),
            "expected_restricted": query_data.get("expected_restricted", False),
            "actual_restricted": False,
            "is_correct": False,
            "confidence": 0.0,
            "total_score": 0.0,
            "processing_time": 0.0,
            "error_message": "ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨"
        }
    start_time = _time.time()
    try:
        validation_result = mp_multi_stage_system.validate(query_data["query"], category=query_data.get("category"))
        # ê°œì„ ëœ/ê¸°ì¡´ ê²€ì¦ê¸° ëª¨ë‘ í˜¸í™˜ ì²˜ë¦¬
        if isinstance(validation_result, dict):
            final_decision = validation_result.get("final_decision", "restricted")
            confidence = float(validation_result.get("confidence", 0.0))
            total_score = float(validation_result.get("total_score", 0.0))
        else:
            final_decision = getattr(getattr(validation_result, "final_decision", None), "value", "restricted")
            confidence = float(getattr(validation_result, "confidence", 0.0))
            total_score = float(getattr(validation_result, "total_score", 0.0))
        actual_restricted = (final_decision == "restricted")
        is_correct = query_data["expected_restricted"] == actual_restricted
        processing_time = _time.time() - start_time
        return {
            "query": query_data["query"],
            "category": query_data["category"],
            "subcategory": query_data["subcategory"],
            "expected_restricted": query_data["expected_restricted"],
            "actual_restricted": actual_restricted,
            "is_correct": is_correct,
            "confidence": confidence,
            "total_score": total_score,
            "processing_time": processing_time,
            "error_message": None
        }
    except Exception as e:
        processing_time = _time.time() - start_time
        return {
            "query": query_data.get("query", ""),
            "category": query_data.get("category", ""),
            "subcategory": query_data.get("subcategory", ""),
            "expected_restricted": query_data.get("expected_restricted", False),
            "actual_restricted": False,
            "is_correct": False,
            "confidence": 0.0,
            "total_score": 0.0,
            "processing_time": processing_time,
            "error_message": str(e)
        }

@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    query: str
    category: str
    subcategory: str
    expected_restricted: bool
    actual_restricted: bool
    is_correct: bool
    confidence: float
    total_score: float
    processing_time: float
    error_message: Optional[str] = None
    stage_results: Optional[List[Dict]] = None
    chat_service_result: Optional[Dict] = None

@dataclass
class TestSummary:
    """í…ŒìŠ¤íŠ¸ ìš”ì•½ ë°ì´í„° í´ë˜ìŠ¤"""
    total_tests: int
    correct_predictions: int
    incorrect_predictions: int
    overall_accuracy: float
    category_accuracies: Dict[str, float]
    processing_time: float
    error_count: int
    average_confidence: float
    average_score: float

class MassiveTestRunner:
    """ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self, max_workers: int = None, enable_chat: bool = False, store_details: bool = False):
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)
        self.enable_chat = enable_chat
        self.store_details = store_details
        self.multi_stage_system = None
        self.chat_service = None
        self.results = []
        self.start_time = None
        self.end_time = None
        
    def initialize_services(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        print("ğŸ”§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            # ë‹¤ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ML í†µí•© / ê°œì„ ëœ / ê¸°ë³¸ ìˆœì„œ)
            use_ml_integrated = os.getenv("USE_ML_INTEGRATED_VALIDATION", "0") == "1"
            use_improved = os.getenv("USE_IMPROVED_VALIDATION", "0") == "1"
            if use_ml_integrated:
                try:
                    from source.services.ml_integrated_validation_system import MLIntegratedValidationSystem as _VSys
                    self.multi_stage_system = _VSys()
                    print("  âœ… ML í†µí•© ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as _e:
                    print(f"  âš ï¸ ML í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê°œì„ ëœ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´: {_e}")
                    use_improved = True
            if not use_ml_integrated and use_improved:
                try:
                    from source.services.improved_multi_stage_validation_system import ImprovedMultiStageValidationSystem as _VSys
                    self.multi_stage_system = _VSys()
                    print("  âœ… ê°œì„ ëœ ë‹¤ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as _e:
                    print(f"  âš ï¸ ê°œì„ ëœ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´: {_e}")
                    self.multi_stage_system = MultiStageValidationSystem()
            if not use_ml_integrated and not use_improved:
                self.multi_stage_system = MultiStageValidationSystem()
            print("  âœ… ë‹¤ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ChatService ì´ˆê¸°í™” (ì˜µì…˜)
            if self.enable_chat:
                config = Config()
                self.chat_service = ChatService(config)
                print("  âœ… ChatService ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"  âŒ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def process_single_query(self, query_data: Dict[str, Any]) -> TestResult:
        """ë‹¨ì¼ ì§ˆì˜ ì²˜ë¦¬"""
        try:
            start_time = time.time()
            
            # ë‹¤ë‹¨ê³„ ê²€ì¦ ìˆ˜í–‰
            validation_result = self.multi_stage_system.validate(query_data["query"], category=query_data.get("category"))
        
        # ê°œì„ ëœ/ê¸°ì¡´ ê²€ì¦ê¸° ëª¨ë‘ í˜¸í™˜ ì²˜ë¦¬
            if isinstance(validation_result, dict):
                final_decision = validation_result.get("final_decision", "restricted")
                confidence = float(validation_result.get("confidence", 0.0))
                total_score = float(validation_result.get("total_score", 0.0))
            else:
                # ê¸°ì¡´ ê°ì²´í˜• ê²°ê³¼
                final_decision = getattr(getattr(validation_result, "final_decision", None), "value", "restricted")
                confidence = float(getattr(validation_result, "confidence", 0.0))
                total_score = float(getattr(validation_result, "total_score", 0.0))
        
        # ì‹¤ì œ ê²°ê³¼
            actual_restricted = (final_decision == "restricted")
        
        # ì •í™•ë„ ê³„ì‚°
            is_correct = query_data["expected_restricted"] == actual_restricted
            
            processing_time = time.time() - start_time
            
            # ChatService í†µí•© í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
            chat_service_result = None
            if self.enable_chat and self.chat_service is not None:
                try:
                    chat_response = asyncio.run(self.chat_service.process_message(
                        message=query_data["query"],
                        user_id="test_user",
                        session_id="test_session"
                    ))
                    chat_service_result = {
                        "is_restricted": chat_response.get("restriction_info", {}).get("is_restricted", False),
                        "has_multi_stage_info": "multi_stage_validation" in chat_response.get("restriction_info", {}),
                        "response_length": len(chat_response.get("response", "")),
                        "success": True
                    }
                except Exception as e:
                    chat_service_result = {
                        "error": str(e),
                        "success": False
                    }
            
            # ë‹¨ê³„ë³„ ê²°ê³¼ ì •ë¦¬
            stage_results = []
            if self.store_details:
                if isinstance(validation_result, dict) and validation_result.get("stages"):
                    for stage in validation_result["stages"]:
                        # stage expected as dict in improved system
                        stage_results.append({
                            "stage": stage.get("stage"),
                            "result": stage.get("result"),
                            "score": stage.get("score"),
                            "reasoning": stage.get("reasoning")
                        })
                elif hasattr(validation_result, "stages") and validation_result.stages:
                    for stage in validation_result.stages:
                        stage_results.append({
                            "stage": stage.stage.value,
                            "result": stage.result.value,
                            "score": stage.score,
                            "reasoning": stage.reasoning
                        })
            
            return TestResult(
                query=query_data["query"],
                category=query_data["category"],
                subcategory=query_data["subcategory"],
                expected_restricted=query_data["expected_restricted"],
                actual_restricted=actual_restricted,
                is_correct=is_correct,
                confidence=confidence,
                total_score=total_score,
                processing_time=processing_time,
                stage_results=stage_results,
                chat_service_result=chat_service_result
            )
            
        except Exception as e:
            processing_time = time.time() - start_time if 'start_time' in locals() else 0
            return TestResult(
                query=query_data["query"],
                category=query_data["category"],
                subcategory=query_data["subcategory"],
                expected_restricted=query_data["expected_restricted"],
                actual_restricted=False,
                is_correct=False,
                confidence=0.0,
                total_score=0.0,
                processing_time=processing_time,
                error_message=str(e)
            )
    
    def run_batch_test(self, queries: List[Dict[str, Any]], batch_size: int = 100) -> List[TestResult]:
        """ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"ğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì´ {len(queries)}ê°œ ì§ˆì˜, ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        all_results = []
        total_batches = (len(queries) + batch_size - 1) // batch_size
        
        for batch_idx in range(0, len(queries), batch_size):
            batch_queries = queries[batch_idx:batch_idx + batch_size]
            batch_num = batch_idx // batch_size + 1
            
            print(f"ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_queries)}ê°œ ì§ˆì˜)")
            
            batch_start_time = time.time()
            
            # ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_query = {
                    executor.submit(self.process_single_query, query): query 
                    for query in batch_queries
                }
                
                batch_results = []
                for future in as_completed(future_to_query):
                    try:
                        result = future.result()
                        batch_results.append(result)
                    except Exception as e:
                        query = future_to_query[future]
                        print(f"  âŒ ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {query['query'][:50]}... - {e}")
                        batch_results.append(TestResult(
                            query=query["query"],
                            category=query["category"],
                            subcategory=query["subcategory"],
                            expected_restricted=query["expected_restricted"],
                            actual_restricted=False,
                            is_correct=False,
                            confidence=0.0,
                            total_score=0.0,
                            processing_time=0.0,
                            error_message=str(e)
                        ))
            
            batch_time = time.time() - batch_start_time
            batch_correct = sum(1 for r in batch_results if r.is_correct)
            batch_accuracy = batch_correct / len(batch_results) if batch_results else 0
            
            print(f"  âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ: {batch_correct}/{len(batch_results)} ì •í™• ({batch_accuracy:.1%}, {batch_time:.2f}ì´ˆ)")
            
            all_results.extend(batch_results)
        
        return all_results
    
    def run_parallel_test(self, queries: List[Dict[str, Any]]) -> List[TestResult]:
        """ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"ğŸš€ ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì´ {len(queries)}ê°œ ì§ˆì˜, ì›Œì»¤: {self.max_workers})")
        
        # ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬ (Windows í˜¸í™˜: ëª¨ë“ˆ ìˆ˜ì¤€ ì´ˆê¸°í™”ì ì‚¬ìš©)
        from functools import partial
        with ProcessPoolExecutor(max_workers=self.max_workers, initializer=mp_init_worker, initargs=(self.enable_chat,)) as executor:
            dict_results = list(executor.map(mp_process_query_worker, queries))
        
        # dict -> TestResult ë³€í™˜
        results: List[TestResult] = []
        for d in dict_results:
            results.append(TestResult(
                query=d["query"],
                category=d["category"],
                subcategory=d["subcategory"],
                expected_restricted=d["expected_restricted"],
                actual_restricted=d["actual_restricted"],
                is_correct=d["is_correct"],
                confidence=d["confidence"],
                total_score=d["total_score"],
                processing_time=d["processing_time"],
                error_message=d.get("error_message")
            ))
        return results
    
    def run_sequential_test(self, queries: List[Dict[str, Any]]) -> List[TestResult]:
        """ìˆœì°¨ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"ğŸš€ ìˆœì°¨ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì´ {len(queries)}ê°œ ì§ˆì˜)")
        
        results = []
        
        for i, query_data in enumerate(queries):
            if (i + 1) % 100 == 0:
                print(f"ğŸ“Š ì§„í–‰ë¥ : {i + 1}/{len(queries)} ({(i + 1)/len(queries)*100:.1f}%)")
            
            result = self.process_single_query(query_data)
            results.append(result)
        
        return results
    
    def run_massive_test(self, queries: List[Dict[str, Any]], method: str = "batch", batch_size: int = 100) -> List[TestResult]:
        """ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"ğŸ¯ ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹œì‘ - ë°©ë²•: {method}")
        print(f"ğŸ“Š ì´ ì§ˆì˜ ìˆ˜: {len(queries)}")
        
        self.start_time = time.time()
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.initialize_services()
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        if method == "batch":
            results = self.run_batch_test(queries, batch_size=batch_size)
        elif method == "parallel":
            results = self.run_parallel_test(queries)
        elif method == "sequential":
            results = self.run_sequential_test(queries)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í…ŒìŠ¤íŠ¸ ë°©ë²•: {method}")
        
        self.end_time = time.time()
        self.results = results
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {self.end_time - self.start_time:.2f}ì´ˆ")
        
        return results
    
    def generate_summary(self) -> TestSummary:
        """í…ŒìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        if not self.results:
            return TestSummary(
                total_tests=0, correct_predictions=0, incorrect_predictions=0,
                overall_accuracy=0.0, category_accuracies={}, processing_time=0.0,
                error_count=0, average_confidence=0.0, average_score=0.0
            )
        
        # ê°œì¸ ë²•ë¥  ìë¬¸ ì¹´í…Œê³ ë¦¬ ì œì™¸ í”Œë˜ê·¸ (ê¸°ë³¸: ì œì™¸)
        exclude_personal_accuracy = os.getenv("EXCLUDE_PERSONAL_FROM_ACCURACY", "1") == "1"
        filtered_results = [r for r in self.results if not (exclude_personal_accuracy and r.category == "personal_legal_advice")]

        # ê¸°ë³¸ í†µê³„ (í•„í„°ë§ ì ìš©)
        total_tests = len(filtered_results)
        correct_predictions = sum(1 for r in filtered_results if r.is_correct)
        incorrect_predictions = total_tests - correct_predictions
        overall_accuracy = correct_predictions / total_tests if total_tests > 0 else 0.0
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì •í™•ë„
        category_stats = {}
        for result in filtered_results:
            category = result.category
            if category not in category_stats:
                category_stats[category] = {"correct": 0, "total": 0}
            
            category_stats[category]["total"] += 1
            if result.is_correct:
                category_stats[category]["correct"] += 1
        
        category_accuracies = {
            category: stats["correct"] / stats["total"] if stats["total"] > 0 else 0.0
            for category, stats in category_stats.items()
        }
        
        # ê¸°íƒ€ í†µê³„
        error_count = sum(1 for r in filtered_results if r.error_message)
        average_confidence = sum(r.confidence for r in filtered_results) / total_tests if total_tests > 0 else 0.0
        average_score = sum(r.total_score for r in filtered_results) / total_tests if total_tests > 0 else 0.0
        processing_time = self.end_time - self.start_time if self.end_time and self.start_time else 0.0
        
        return TestSummary(
            total_tests=total_tests,
            correct_predictions=correct_predictions,
            incorrect_predictions=incorrect_predictions,
            overall_accuracy=overall_accuracy,
            category_accuracies=category_accuracies,
            processing_time=processing_time,
            error_count=error_count,
            average_confidence=average_confidence,
            average_score=average_score
        )
    
    def save_results(self, results: List[TestResult], summary: TestSummary, filename: str = None) -> str:
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"test_results/massive_test_results_{timestamp}.json"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # ê²°ê³¼ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        results_data = []
        for result in results:
            result_dict = asdict(result)
            if not self.store_details:
                result_dict.pop("stage_results", None)
                result_dict.pop("chat_service_result", None)
            results_data.append(result_dict)
        
        # ìš”ì•½ ë°ì´í„°
        summary_data = asdict(summary)
        
        # ì „ì²´ ë°ì´í„°
        full_data = {
            "metadata": {
                "test_run_at": datetime.now().isoformat(),
                "total_queries": len(results),
                "test_method": "massive_test",
                "processing_time": summary.processing_time
            },
            "summary": summary_data,
            "detailed_results": results_data
        }
        
        # íŒŒì¼ ì €ì¥
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(full_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename
    
    def generate_report(self, summary: TestSummary) -> str:
        """ìƒì„¸ ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("=" * 100)
        report.append("ğŸ¯ ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ")
        report.append("=" * 100)
        
        # ì „ì²´ ê²°ê³¼
        report.append(f"\nğŸ“Š ì „ì²´ ê²°ê³¼:")
        report.append(f"  ì´ í…ŒìŠ¤íŠ¸: {summary.total_tests:,}")
        report.append(f"  ì •í™•í•œ ì˜ˆì¸¡: {summary.correct_predictions:,}")
        report.append(f"  ì˜ëª»ëœ ì˜ˆì¸¡: {summary.incorrect_predictions:,}")
        report.append(f"  ì „ì²´ ì •í™•ë„: {summary.overall_accuracy:.1%}")
        report.append(f"  í…ŒìŠ¤íŠ¸ ì†Œìš” ì‹œê°„: {summary.processing_time:.2f}ì´ˆ")
        report.append(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {summary.processing_time/summary.total_tests*1000:.2f}ms/ì§ˆì˜")
        report.append(f"  ì˜¤ë¥˜ ë°œìƒ: {summary.error_count}ê°œ")
        report.append(f"  í‰ê·  ì‹ ë¢°ë„: {summary.average_confidence:.2f}")
        report.append(f"  í‰ê·  ì ìˆ˜: {summary.average_score:.2f}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ê²°ê³¼
        report.append(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ì •í™•ë„:")
        # í™˜ê²½ í”Œë˜ê·¸: ê°œì¸ ì¹´í…Œê³ ë¦¬ í‘œì‹œ ì—¬ë¶€ ë° ë¹„ì–´ìˆëŠ” ì¹´í…Œê³ ë¦¬ í‘œì‹œ ì—¬ë¶€
        show_personal_in_report = os.getenv("SHOW_PERSONAL_IN_REPORT", "0") == "1"
        show_empty_categories = os.getenv("SHOW_EMPTY_CATEGORIES", "0") == "1"

        # í‘œì‹œí•  ì¹´í…Œê³ ë¦¬ ì§‘í•© êµ¬ì„±
        categories_to_show = set(summary.category_accuracies.keys())

        if show_empty_categories:
            try:
                # ì „ì²´ ì¹´í…Œê³ ë¦¬ ëª©ë¡ í™•ë³´ (ì§ˆì˜ ìƒì„±ê¸°ì˜ ì •ì˜ ì‚¬ìš©)
                from scripts.massive_test_query_generator import MassiveTestQueryGenerator  # ì§€ì—° ì„í¬íŠ¸
                _gen = MassiveTestQueryGenerator()
                categories_to_show.update(_gen.categories.keys())
            except Exception:
                # ì§ˆì˜ ìƒì„±ê¸° ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ, í˜„ì¬ ì¡´ì¬í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë§Œ ì‚¬ìš©
                pass

        # ì •ë ¬í•˜ì—¬ ì¶œë ¥ (ì•ŒíŒŒë²³ìˆœ)
        for category in sorted(categories_to_show):
            if category == "personal_legal_advice" and not show_personal_in_report:
                continue
            accuracy = summary.category_accuracies.get(category, 0.0)
            report.append(f"  {category}: {accuracy:.1%}")
        
        # ì„±ëŠ¥ ë¶„ì„
        report.append(f"\nğŸ“ˆ ì„±ëŠ¥ ë¶„ì„:")
        
        # ë¯¼ê°í•œ ì¹´í…Œê³ ë¦¬ë“¤ì˜ ì •í™•ë„
        sensitive_categories = ["medical_legal_advice", "criminal_case_advice", "illegal_activity_assistance"]
        sensitive_accuracies = [summary.category_accuracies.get(cat, 0.0) for cat in sensitive_categories if cat in summary.category_accuracies]
        if sensitive_accuracies:
            sensitive_avg = sum(sensitive_accuracies) / len(sensitive_accuracies)
            report.append(f"  ë¯¼ê°í•œ ì§ˆë¬¸ ì œí•œ ì •í™•ë„: {sensitive_avg:.1%}")
        
        # ì¼ë°˜ ì •ë³´ í—ˆìš© ì •í™•ë„
        general_categories = ["general_legal_information", "edge_cases"]
        general_accuracies = [summary.category_accuracies.get(cat, 0.0) for cat in general_categories if cat in summary.category_accuracies]
        if general_accuracies:
            general_avg = sum(general_accuracies) / len(general_accuracies)
            report.append(f"  ì¼ë°˜ ì •ë³´ í—ˆìš© ì •í™•ë„: {general_avg:.1%}")
        
        # ì²˜ë¦¬ ì„±ëŠ¥
        queries_per_second = summary.total_tests / summary.processing_time if summary.processing_time > 0 else 0
        report.append(f"  ì²˜ë¦¬ ì„±ëŠ¥: {queries_per_second:.1f} ì§ˆì˜/ì´ˆ")
        
        # ìµœì¢… í‰ê°€
        report.append(f"\nğŸ¯ ìµœì¢… í‰ê°€:")
        if summary.overall_accuracy >= 0.95:
            report.append("  ğŸ† ìš°ìˆ˜: ì‹œìŠ¤í…œì´ ë§¤ìš° ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        elif summary.overall_accuracy >= 0.90:
            report.append("  ğŸ¥‡ ì–‘í˜¸: ì‹œìŠ¤í…œì´ ì˜ ì‘ë™í•˜ê³  ìˆì§€ë§Œ ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif summary.overall_accuracy >= 0.80:
            report.append("  ğŸ¥ˆ ë³´í†µ: ì‹œìŠ¤í…œì´ ì‘ë™í•˜ê³  ìˆì§€ë§Œ ìƒë‹¹í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            report.append("  ğŸ¥‰ ë¯¸í¡: ì‹œìŠ¤í…œ ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤.")
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        report.append(f"\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        
        if summary.overall_accuracy < 0.90:
            report.append("  - ì „ì²´ ì •í™•ë„ê°€ 90% ë¯¸ë§Œì…ë‹ˆë‹¤. ì‹œìŠ¤í…œ íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì •í™•ë„ê°€ ë‚®ì€ ì¹´í…Œê³ ë¦¬ ì‹ë³„
        low_accuracy_categories = [
            category for category, accuracy in summary.category_accuracies.items()
            if accuracy < 0.80
        ]
        
        if low_accuracy_categories:
            report.append(f"  - ì •í™•ë„ê°€ ë‚®ì€ ì¹´í…Œê³ ë¦¬: {', '.join(low_accuracy_categories)}")
            report.append("  - í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ íŒ¨í„´ê³¼ ë¡œì§ì„ ì¬ê²€í† í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        if summary.error_count > 0:
            report.append(f"  - {summary.error_count}ê°œì˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ì²˜ë¦¬ ë¡œì§ì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        if summary.average_confidence < 0.7:
            report.append("  - í‰ê·  ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ í™•ì‹ ë„ë¥¼ ë†’ì´ëŠ” íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        report.append("\n" + "=" * 100)
        
        return "\n".join(report)

def load_test_queries(filename: str) -> List[Dict[str, Any]]:
    """í…ŒìŠ¤íŠ¸ ì§ˆì˜ íŒŒì¼ ë¡œë“œ"""
    print(f"ğŸ“‚ í…ŒìŠ¤íŠ¸ ì§ˆì˜ ë¡œë“œ ì¤‘: {filename}")
    
    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    queries = data.get("queries", [])
    print(f"âœ… {len(queries)}ê°œì˜ ì§ˆì˜ ë¡œë“œ ì™„ë£Œ")
    
    return queries

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # í…ŒìŠ¤íŠ¸ ì§ˆì˜ íŒŒì¼ ê²½ë¡œ (ìƒì„±ëœ íŒŒì¼ ì‚¬ìš©)
        queries_file = "test_results/massive_test_queries_*.json"  # ì‹¤ì œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½ í•„ìš”
        
        # ìµœì‹  íŒŒì¼ ì°¾ê¸°
        import glob
        query_files = glob.glob(queries_file)
        if not query_files:
            print("âŒ í…ŒìŠ¤íŠ¸ ì§ˆì˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì§ˆì˜ ìƒì„±ê¸°ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        latest_file = max(query_files, key=os.path.getctime)
        print(f"ğŸ“ ì‚¬ìš©í•  ì§ˆì˜ íŒŒì¼: {latest_file}")
        
        # ì§ˆì˜ ë¡œë“œ
        queries = load_test_queries(latest_file)
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì´ˆê¸°í™”
        runner = MassiveTestRunner(max_workers=8)  # ì›Œì»¤ ìˆ˜ ì¡°ì • ê°€ëŠ¥
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë°°ì¹˜ ë°©ì‹ ê¶Œì¥)
        results = runner.run_massive_test(queries, method="batch")
        
        # ìš”ì•½ ìƒì„±
        summary = runner.generate_summary()
        
        # ê²°ê³¼ ì €ì¥
        results_file = runner.save_results(results, summary)
        
        # ë³´ê³ ì„œ ìƒì„± ë° ì¶œë ¥
        report = runner.generate_report(summary)
        print("\n" + report)
        
        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        report_file = results_file.replace('.json', '_report.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ {report_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return results, summary, report
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return None, None, None

if __name__ == "__main__":
    main()
