#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
3000ê°œì˜ í…ŒìŠ¤íŠ¸ ì§ˆì˜ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ì—¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
"""

import sys
import os
import json
import time
import glob
from typing import Dict, List, Any, Optional
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ëª¨ë“ˆ import
from scripts.massive_test_query_generator import MassiveTestQueryGenerator
from scripts.massive_test_runner import MassiveTestRunner

class IntegratedMassiveTestSystem:
    """í†µí•© ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self, total_queries: int = 3000, max_workers: int = 8, enable_chat: bool = False, store_details: bool = False, batch_size: int = 200, method: str = "parallel"):
        self.total_queries = total_queries
        self.max_workers = max_workers
        self.enable_chat = enable_chat
        self.store_details = store_details
        self.batch_size = batch_size
        self.method = method
        self.generator = MassiveTestQueryGenerator()
        self.runner = MassiveTestRunner(max_workers=max_workers, enable_chat=enable_chat, store_details=store_details)
        
    def run_complete_test(self) -> Dict[str, Any]:
        """ì™„ì „í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ í†µí•© ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 80)
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ì§ˆì˜ ìƒì„±
        print("\nğŸ“ 1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ì§ˆì˜ ìƒì„±")
        print("-" * 40)
        
        queries = self.generator.generate_massive_test_queries(self.total_queries)
        queries_file = self.generator.save_queries_to_file(queries)
        stats = self.generator.generate_statistics(queries)
        
        print(f"âœ… {len(queries)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì§ˆì˜ ìƒì„± ì™„ë£Œ")
        print(f"ğŸ“ ì§ˆì˜ íŒŒì¼: {queries_file}")
        
        # 2ë‹¨ê³„: ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        print("\nğŸ§ª 2ë‹¨ê³„: ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print("-" * 40)
        
        # ì§ˆì˜ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        queries_data = []
        for query in queries:
            queries_data.append({
                "query": query.query,
                "category": query.category,
                "subcategory": query.subcategory,
                "expected_restricted": query.expected_restricted,
                "difficulty_level": query.difficulty_level,
                "context_type": query.context_type,
                "legal_area": query.legal_area,
                "keywords": query.keywords,
                "description": query.description
            })
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = self.runner.run_massive_test(queries_data, method=self.method, batch_size=self.batch_size)
        summary = self.runner.generate_summary()
        
        print(f"âœ… {len(results)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        # 3ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
        print("\nğŸ“Š 3ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ì €ì¥")
        print("-" * 40)
        
        # ê²°ê³¼ ì €ì¥
        results_file = self.runner.save_results(results, summary)
        
        # ë³´ê³ ì„œ ìƒì„±
        report = self.runner.generate_report(summary)
        
        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        report_file = results_file.replace('.json', '_report.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {results_file}")
        print(f"ğŸ“„ ë³´ê³ ì„œ íŒŒì¼: {report_file}")
        
        # 4ë‹¨ê³„: ìƒì„¸ ë¶„ì„
        print("\nğŸ” 4ë‹¨ê³„: ìƒì„¸ ë¶„ì„")
        print("-" * 40)
        
        detailed_analysis = self._perform_detailed_analysis(results, summary, stats)
        
        # ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì €ì¥
        analysis_file = results_file.replace('.json', '_analysis.json')
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š ìƒì„¸ ë¶„ì„ íŒŒì¼: {analysis_file}")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ìµœì¢… ê²°ê³¼
        final_results = {
            "metadata": {
                "total_queries": self.total_queries,
                "total_time": total_time,
                "queries_file": queries_file,
                "results_file": results_file,
                "report_file": report_file,
                "analysis_file": analysis_file,
                "completed_at": datetime.now().isoformat()
            },
            "generation_stats": stats,
            "test_summary": summary.__dict__,
            "detailed_analysis": detailed_analysis,
            "report": report
        }
        
        print(f"\nğŸ‰ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        return final_results
    
    def _perform_detailed_analysis(self, results: List, summary, generation_stats: Dict) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„ ìˆ˜í–‰"""
        analysis = {
            "performance_metrics": {},
            "category_analysis": {},
            "error_analysis": {},
            "recommendations": []
        }
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        analysis["performance_metrics"] = {
            "queries_per_second": summary.total_tests / summary.processing_time if summary.processing_time > 0 else 0,
            "average_processing_time": summary.processing_time / summary.total_tests if summary.total_tests > 0 else 0,
            "error_rate": summary.error_count / summary.total_tests if summary.total_tests > 0 else 0,
            "confidence_distribution": self._analyze_confidence_distribution(results),
            "score_distribution": self._analyze_score_distribution(results)
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„
        analysis["category_analysis"] = self._analyze_categories(results)
        
        # ì˜¤ë¥˜ ë¶„ì„
        analysis["error_analysis"] = self._analyze_errors(results)
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        analysis["recommendations"] = self._generate_recommendations(summary, analysis)
        
        return analysis
    
    def _analyze_confidence_distribution(self, results: List) -> Dict[str, Any]:
        """ì‹ ë¢°ë„ ë¶„í¬ ë¶„ì„"""
        confidences = [r.confidence for r in results if r.confidence > 0]
        
        if not confidences:
            return {"error": "ì‹ ë¢°ë„ ë°ì´í„° ì—†ìŒ"}
        
        return {
            "min": min(confidences),
            "max": max(confidences),
            "mean": sum(confidences) / len(confidences),
            "median": sorted(confidences)[len(confidences) // 2],
            "high_confidence_count": sum(1 for c in confidences if c >= 0.8),
            "low_confidence_count": sum(1 for c in confidences if c < 0.5)
        }
    
    def _analyze_score_distribution(self, results: List) -> Dict[str, Any]:
        """ì ìˆ˜ ë¶„í¬ ë¶„ì„"""
        scores = [r.total_score for r in results if r.total_score > 0]
        
        if not scores:
            return {"error": "ì ìˆ˜ ë°ì´í„° ì—†ìŒ"}
        
        return {
            "min": min(scores),
            "max": max(scores),
            "mean": sum(scores) / len(scores),
            "median": sorted(scores)[len(scores) // 2],
            "high_score_count": sum(1 for s in scores if s >= 0.8),
            "low_score_count": sum(1 for s in scores if s < 0.3)
        }
    
    def _analyze_categories(self, results: List) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„"""
        category_analysis = {}
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
        categories = {}
        for result in results:
            category = result.category
            if category not in categories:
                categories[category] = []
            categories[category].append(result)
        
        # ê° ì¹´í…Œê³ ë¦¬ ë¶„ì„
        for category, category_results in categories.items():
            total = len(category_results)
            correct = sum(1 for r in category_results if r.is_correct)
            errors = sum(1 for r in category_results if r.error_message)
            
            category_analysis[category] = {
                "total_queries": total,
                "correct_predictions": correct,
                "accuracy": correct / total if total > 0 else 0,
                "error_count": errors,
                "error_rate": errors / total if total > 0 else 0,
                "average_confidence": sum(r.confidence for r in category_results) / total if total > 0 else 0,
                "average_score": sum(r.total_score for r in category_results) / total if total > 0 else 0,
                "average_processing_time": sum(r.processing_time for r in category_results) / total if total > 0 else 0
            }
        
        return category_analysis
    
    def _analyze_errors(self, results: List) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ë¶„ì„"""
        error_results = [r for r in results if r.error_message]
        
        if not error_results:
            return {"error_count": 0, "error_types": {}}
        
        # ì˜¤ë¥˜ ìœ í˜•ë³„ ë¶„ë¥˜
        error_types = {}
        for result in error_results:
            error_type = self._classify_error(result.error_message)
            if error_type not in error_types:
                error_types[error_type] = []
            error_types[error_type].append(result)
        
        # ì˜¤ë¥˜ ìœ í˜•ë³„ í†µê³„
        error_analysis = {
            "error_count": len(error_results),
            "error_rate": len(error_results) / len(results) if results else 0,
            "error_types": {}
        }
        
        for error_type, type_results in error_types.items():
            error_analysis["error_types"][error_type] = {
                "count": len(type_results),
                "percentage": len(type_results) / len(error_results) * 100,
                "categories": list(set(r.category for r in type_results))
            }
        
        return error_analysis
    
    def _classify_error(self, error_message: str) -> str:
        """ì˜¤ë¥˜ ìœ í˜• ë¶„ë¥˜"""
        error_message = error_message.lower()
        
        if "timeout" in error_message or "time" in error_message:
            return "timeout_error"
        elif "memory" in error_message or "out of memory" in error_message:
            return "memory_error"
        elif "connection" in error_message or "network" in error_message:
            return "connection_error"
        elif "validation" in error_message or "invalid" in error_message:
            return "validation_error"
        elif "import" in error_message or "module" in error_message:
            return "import_error"
        else:
            return "unknown_error"
    
    def _generate_recommendations(self, summary, analysis: Dict) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì •í™•ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if summary.overall_accuracy < 0.90:
            recommendations.append("ì „ì²´ ì •í™•ë„ê°€ 90% ë¯¸ë§Œì…ë‹ˆë‹¤. ì‹œìŠ¤í…œ íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if summary.overall_accuracy < 0.80:
            recommendations.append("ì „ì²´ ì •í™•ë„ê°€ 80% ë¯¸ë§Œì…ë‹ˆë‹¤. ì‹œìŠ¤í…œ ì¬ì„¤ê³„ë¥¼ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê¶Œì¥ì‚¬í•­
        low_accuracy_categories = [
            category for category, accuracy in summary.category_accuracies.items()
            if accuracy < 0.80
        ]
        
        if low_accuracy_categories:
            recommendations.append(f"ì •í™•ë„ê°€ ë‚®ì€ ì¹´í…Œê³ ë¦¬ ({', '.join(low_accuracy_categories)})ì˜ íŒ¨í„´ê³¼ ë¡œì§ì„ ì¬ê²€í† í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì˜¤ë¥˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if analysis["error_analysis"]["error_count"] > 0:
            error_rate = analysis["error_analysis"]["error_rate"]
            if error_rate > 0.05:  # 5% ì´ìƒ
                recommendations.append(f"ì˜¤ë¥˜ìœ¨ì´ {error_rate:.1%}ë¡œ ë†’ìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ì²˜ë¦¬ ë¡œì§ì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        performance = analysis["performance_metrics"]
        if performance.get("queries_per_second", 0) < 10:
            recommendations.append("ì²˜ë¦¬ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ë³‘ë ¬ ì²˜ë¦¬ë‚˜ ìºì‹±ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        confidence_dist = analysis["performance_metrics"].get("confidence_distribution", {})
        if confidence_dist.get("low_confidence_count", 0) > confidence_dist.get("high_confidence_count", 0):
            recommendations.append("ë‚®ì€ ì‹ ë¢°ë„ ì§ˆì˜ê°€ ë§ìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ í™•ì‹ ë„ë¥¼ ë†’ì´ëŠ” íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return recommendations

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        print("ğŸ¯ LawFirmAI ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ")
        print("=" * 80)
        
        # í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        # í™˜ê²½ë³€ìˆ˜ TOTAL_QUERIESê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        import os as _os
        _total = int(_os.getenv("TOTAL_QUERIES", "13000"))
        test_system = IntegratedMassiveTestSystem(
            total_queries=_total,
            max_workers=min(os.cpu_count() or 8, 12),
            enable_chat=False,
            store_details=False,
            batch_size=200,
            method="parallel"
        )
        
        # ì™„ì „í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = test_system.run_complete_test()
        
        if results:
            print("\nğŸ“‹ ìµœì¢… ê²°ê³¼ ìš”ì•½:")
            print(f"  ì´ ì§ˆì˜ ìˆ˜: {results['metadata']['total_queries']:,}")
            print(f"  ì „ì²´ ì •í™•ë„: {results['test_summary']['overall_accuracy']:.1%}")
            print(f"  ì´ ì†Œìš” ì‹œê°„: {results['metadata']['total_time']:.2f}ì´ˆ")
            print(f"  ì²˜ë¦¬ ì„±ëŠ¥: {results['test_summary']['total_tests']/results['metadata']['total_time']:.1f} ì§ˆì˜/ì´ˆ")
            
            print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
            print(f"  ì§ˆì˜ íŒŒì¼: {results['metadata']['queries_file']}")
            print(f"  ê²°ê³¼ íŒŒì¼: {results['metadata']['results_file']}")
            print(f"  ë³´ê³ ì„œ íŒŒì¼: {results['metadata']['report_file']}")
            print(f"  ë¶„ì„ íŒŒì¼: {results['metadata']['analysis_file']}")
            
            # ë³´ê³ ì„œ ì¶œë ¥
            print("\n" + "=" * 80)
            print(results['report'])
            
        else:
            print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨")
        
        return results
        
    except Exception as e:
        print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()
