<!-- 0f9c520e-cde8-4420-876f-029ffb4531a0 9121c223-21ea-4aef-9b55-b1a22b585ed7 -->
# ë²•ë¥  ì±—ë´‡ ê°œì„  - 1ë‹¨ê³„ ë° 2ë‹¨ê³„ êµ¬í˜„ ê³„íš

## 1ë‹¨ê³„: ì¦‰ì‹œ ê°œì„  ê°€ëŠ¥í•œ í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„

### Phase 1.1: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ê¸° êµ¬í˜„

**ëª©í‘œ:** ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë²•ë¥ /íŒë¡€ ê²€ìƒ‰ ë¹„ì¤‘ì„ ì¡°ì •

**êµ¬í˜„ íŒŒì¼:**

- `source/services/question_classifier.py` (ì‹ ê·œ)
                                - ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ (ë²•ë¥  ì¡°íšŒ, íŒë¡€ ê²€ìƒ‰, ë²•ì  ì¡°ì–¸, ì ˆì°¨ ì•ˆë‚´ ë“±)
                                - í‚¤ì›Œë“œ ë° íŒ¨í„´ ê¸°ë°˜ ë¶„ë¥˜
                                - ë²•ë¥ /íŒë¡€ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ê²°ì •

**ì£¼ìš” ê¸°ëŠ¥:**

```python
class QuestionClassifier:
    def classify_question(self, query: str) -> QuestionType:
        # "íŒë¡€ ì°¾ì•„ì¤˜" -> QuestionType.PRECEDENT_SEARCH (precedent_weight=0.8)
        # "ë¯¼ë²• ì œ750ì¡°" -> QuestionType.LAW_INQUIRY (law_weight=0.8)
        # "ì†í•´ë°°ìƒ ì²­êµ¬ ë°©ë²•" -> QuestionType.LEGAL_ADVICE (balanced)
```

### Phase 1.2: í†µí•© ê²€ìƒ‰ ì—”ì§„ ê°œì„ 

**ëª©í‘œ:** ë²•ë¥ ê³¼ íŒë¡€ë¥¼ ë™ì  ê°€ì¤‘ì¹˜ë¡œ í†µí•© ê²€ìƒ‰

**ìˆ˜ì • íŒŒì¼:**

- `source/services/hybrid_search_engine.py`
                                - ì§ˆë¬¸ ìœ í˜•ë³„ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì ìš©
                                - ë²•ë¥  ì¸ë±ìŠ¤ì™€ íŒë¡€ ì¸ë±ìŠ¤ í†µí•© ê²€ìƒ‰
                                - ê²°ê³¼ ì¬ë­í‚¹ ë¡œì§ ì¶”ê°€

**êµ¬í˜„ ë‚´ìš©:**

```python
def search_with_question_type(self, query: str, question_type: QuestionType):
    # ë²•ë¥  ê²€ìƒ‰
    law_results = self.search_laws(query, weight=question_type.law_weight)
    
    # íŒë¡€ ê²€ìƒ‰ (ë¯¼ì‚¬ íŒë¡€ ì¸ë±ìŠ¤)
    precedent_results = self.search_precedents(
        query, 
        category='civil',
        weight=question_type.precedent_weight
    )
    
    # í†µí•© ë° ì¬ë­í‚¹
    return self.merge_and_rerank(law_results, precedent_results)
```

### Phase 1.3: íŒë¡€ ì „ìš© ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„

**ëª©í‘œ:** íŒë¡€ ë°ì´í„°ë² ì´ìŠ¤ì™€ ë²¡í„° ì¸ë±ìŠ¤ë¥¼ í™œìš©í•œ ì „ë¬¸ ê²€ìƒ‰

**ì‹ ê·œ íŒŒì¼:**

- `source/services/precedent_search_engine.py`
                                - precedent_cases, precedent_sections í…Œì´ë¸” ê²€ìƒ‰
                                - íŒë¡€ ë²¡í„° ì¸ë±ìŠ¤ í™œìš© (ml_enhanced_ko_sroberta_precedents)
                                - ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ (ë¯¼ì‚¬/í˜•ì‚¬/ê°€ì‚¬)

**ì£¼ìš” ê¸°ëŠ¥:**

```python
class PrecedentSearchEngine:
    def __init__(self):
        self.db = DatabaseManager()
        self.vector_store = LegalVectorStore(
            index_path="data/embeddings/ml_enhanced_ko_sroberta_precedents"
        )
    
    def search_precedents(self, query: str, category: str = 'civil', top_k: int = 5):
        # FTS5 ê²€ìƒ‰ + ë²¡í„° ê²€ìƒ‰
        # íŒì‹œì‚¬í•­, íŒê²°ìš”ì§€ ìš°ì„  ë°˜í™˜
```

### Phase 1.4: ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

**ëª©í‘œ:** ê° ì§ˆë¬¸ ìœ í˜•ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±

**ìˆ˜ì •/ì‹ ê·œ íŒŒì¼:**

- `gradio/prompt_manager.py` (í™•ì¥)
- `source/services/prompt_templates.py` (ì‹ ê·œ)

**êµ¬í˜„ ë‚´ìš©:**

```python
PROMPT_TEMPLATES = {
    "precedent_search": """ë‹¹ì‹ ì€ íŒë¡€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    
ê´€ë ¨ íŒë¡€:
{precedent_list}

ìœ„ íŒë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í•˜ì„¸ìš”:
1. ê°€ì¥ ìœ ì‚¬í•œ íŒë¡€ 3ê°œ ì†Œê°œ (ì‚¬ê±´ë²ˆí˜¸, íŒê²°ìš”ì§€)
2. í•´ë‹¹ ì‚¬ì•ˆì—ì˜ ì ìš© ê°€ëŠ¥ì„±
3. ì‹¤ë¬´ì  ì‹œì‚¬ì 
""",
    
    "law_explanation": """ë‹¹ì‹ ì€ ë²•ë¥  í•´ì„¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
ê´€ë ¨ ë²•ë¥ :
{law_articles}

ìœ„ ë²•ë¥ ì„ ë‹¤ìŒ ìˆœì„œë¡œ ì„¤ëª…í•˜ì„¸ìš”:
1. ë²•ë¥ ì˜ ëª©ì  ë° ì·¨ì§€
2. ì£¼ìš” ë‚´ìš©ì„ ì‰¬ìš´ ë§ë¡œ í’€ì´
3. ì‹¤ì œ ì ìš© ì˜ˆì‹œ
4. ì£¼ì˜ì‚¬í•­
""",
    
    "legal_advice": """ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
ê´€ë ¨ ë²•ë¥  ë° íŒë¡€:
{context}

ë‹¤ìŒ êµ¬ì¡°ë¡œ ì¡°ì–¸í•˜ì„¸ìš”:
1. ìƒí™© ì •ë¦¬
2. ì ìš© ê°€ëŠ¥í•œ ë²•ë¥  (ì¡°ë¬¸ ëª…ì‹œ)
3. ê´€ë ¨ íŒë¡€ (í•µì‹¬ íŒê²°ìš”ì§€)
4. ê¶Œë¦¬ êµ¬ì œ ë°©ë²• (ë‹¨ê³„ë³„)
5. í•„ìš”í•œ ì¦ê±° ìë£Œ
"""
}
```

### Phase 1.5: ì‹ ë¢°ë„ ê¸°ë°˜ ë‹µë³€ ì‹œìŠ¤í…œ

**ëª©í‘œ:** ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ì‹ ë¢°ë„ í‘œì‹œ

**ì‹ ê·œ íŒŒì¼:**

- `source/services/confidence_calculator.py`

**êµ¬í˜„ ë‚´ìš©:**

```python
class ConfidenceCalculator:
    def calculate_confidence(self, query: str, retrieved_docs: List[Dict], answer: str):
        # 1. ê²€ìƒ‰ ê²°ê³¼ ìœ ì‚¬ë„ ì ìˆ˜
        similarity_score = self._calc_similarity_score(retrieved_docs)
        
        # 2. ë²•ë¥ /íŒë¡€ ë§¤ì¹­ ì •í™•ë„
        matching_score = self._calc_matching_score(query, retrieved_docs)
        
        # 3. ë‹µë³€ ê¸¸ì´ ë° êµ¬ì¡° í’ˆì§ˆ
        answer_quality = self._calc_answer_quality(answer)
        
        confidence = (similarity_score * 0.4 + matching_score * 0.4 + answer_quality * 0.2)
        
        return {
            "confidence": confidence,
            "reliability_level": self._get_reliability_level(confidence),
            "warnings": self._generate_warnings(confidence)
        }
    
    def _get_reliability_level(self, confidence: float):
        if confidence >= 0.8:
            return "HIGH"
        elif confidence >= 0.6:
            return "MEDIUM"
        else:
            return "LOW - ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥"
```

### Phase 1.6: FastAPI ì—”ë“œí¬ì¸íŠ¸ í†µí•©

**ëª©í‘œ:** ê°œì„ ëœ ê¸°ëŠ¥ì„ APIë¡œ ì œê³µ

**ìˆ˜ì • íŒŒì¼:**

- `source/api/endpoints.py`

**ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸:**

```python
@api_router.post("/chat/intelligent", response_model=IntelligentChatResponse)
async def intelligent_chat_endpoint(request: IntelligentChatRequest):
    # 1. ì§ˆë¬¸ ë¶„ë¥˜
    question_type = question_classifier.classify(request.message)
    
    # 2. í†µí•© ê²€ìƒ‰
    search_results = hybrid_search_engine.search_with_question_type(
        request.message, question_type
    )
    
    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = prompt_template_manager.get_template(question_type.type)
    context = context_builder.build(search_results, question_type)
    
    # 4. ë‹µë³€ ìƒì„± (Ollama)
    answer = ollama_client.generate(prompt, context)
    
    # 5. ì‹ ë¢°ë„ ê³„ì‚°
    confidence_info = confidence_calculator.calculate(
        request.message, search_results, answer
    )
    
    return IntelligentChatResponse(
        answer=answer,
        question_type=question_type.name,
        confidence=confidence_info,
        sources=self._format_sources(search_results),
        law_sources=[...],
        precedent_sources=[...]
    )
```

## 2ë‹¨ê³„: ë‹¨ê¸° ê°œì„  ê¸°ëŠ¥ êµ¬í˜„

### Phase 2.1: ë²•ë¥  ìš©ì–´ í™•ì¥ ê²€ìƒ‰

**ëª©í‘œ:** ë²•ë¥  ì „ë¬¸ ìš©ì–´ì™€ ì¼ë°˜ ìš©ì–´ ê°„ ë§¤í•‘ìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ

**ì‹ ê·œ íŒŒì¼:**

- `source/services/legal_term_expander.py`
- `data/legal_term_dictionary.json` (ë²•ë¥  ìš©ì–´ ì‚¬ì „)

**êµ¬í˜„ ë‚´ìš©:**

```python
class LegalTermExpander:
    def __init__(self):
        self.term_dict = self._load_legal_terms()
        # "ì†í•´ë°°ìƒ" -> ["ë¶ˆë²•í–‰ìœ„", "ì±„ë¬´ë¶ˆì´í–‰", "ìœ„ìë£Œ", "ë¯¼ë²• ì œ750ì¡°"]
        # "ì„ëŒ€ì°¨" -> ["ì „ì„¸", "ì›”ì„¸", "ë³´ì¦ê¸ˆ", "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•"]
    
    def expand_query(self, query: str):
        # ì›ë³¸ ì¿¼ë¦¬ + ê´€ë ¨ ë²•ë¥  ìš©ì–´ + ê´€ë ¨ ë²•ì¡°ë¬¸
        expanded = self._extract_and_expand_terms(query)
        return {
            "original": query,
            "expanded_terms": expanded,
            "related_laws": self._find_related_laws(expanded)
        }
```

**ë°ì´í„° êµ¬ì¡°:**

```json
{
  "ì†í•´ë°°ìƒ": {
    "synonyms": ["ë°°ìƒ", "ë³´ìƒ", "ìœ„ìë£Œ", "ì†í•´ì „ë³´"],
    "related_terms": ["ë¶ˆë²•í–‰ìœ„", "ì±„ë¬´ë¶ˆì´í–‰", "ê³¼ì‹¤"],
    "related_laws": ["ë¯¼ë²• ì œ750ì¡°", "ë¯¼ë²• ì œ751ì¡°", "ë¯¼ë²• ì œ393ì¡°"],
    "precedent_keywords": ["ì†í•´ë°°ìƒì²­êµ¬ê¶Œ", "ë°°ìƒì±…ì„"]
  }
}
```

### Phase 2.2: ëŒ€í™” ë§¥ë½ ê´€ë¦¬ ì‹œìŠ¤í…œ

**ëª©í‘œ:** ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ì—°ì†ëœ ì§ˆë¬¸ì— ëŒ€ì‘

**ì‹ ê·œ íŒŒì¼:**

- `source/services/conversation_manager.py`
- `source/data/conversation_store.py`

**êµ¬í˜„ ë‚´ìš©:**

```python
class ConversationManager:
    def __init__(self):
        self.sessions = {}  # session_id -> ConversationContext
        self.db = ConversationStore()
    
    def add_turn(self, session_id: str, user_query: str, bot_response: str):
        context = self.sessions.get(session_id, ConversationContext())
        context.add_turn(user_query, bot_response)
        
        # ë²•ë¥  ì—”í‹°í‹° ì¶”ì¶œ ë° ì €ì¥
        entities = self._extract_legal_entities(user_query, bot_response)
        context.update_entities(entities)
        
        self.sessions[session_id] = context
    
    def get_relevant_context(self, session_id: str, current_query: str):
        context = self.sessions.get(session_id)
        if not context:
            return None
        
        # í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì´ì „ ëŒ€í™” ì¶”ì¶œ
        return context.get_relevant_turns(current_query, max_turns=3)
    
    def _extract_legal_entities(self, query: str, response: str):
        # ë²•ë¥ ëª…, ì¡°ë¬¸ë²ˆí˜¸, íŒë¡€ë²ˆí˜¸, ë²•ë¥ ìš©ì–´ ì¶”ì¶œ
        return {
            "laws": self._extract_law_names(query + response),
            "articles": self._extract_article_numbers(query + response),
            "precedents": self._extract_case_numbers(query + response),
            "legal_terms": self._extract_legal_terms(query + response)
        }

class ConversationContext:
    def __init__(self):
        self.turns = []  # [(user_query, bot_response, timestamp), ...]
        self.entities = {
            "laws": set(),
            "articles": set(),
            "precedents": set(),
            "legal_terms": set()
        }
        self.topic_stack = []  # ëŒ€í™” ì£¼ì œ ì¶”ì 
```

### Phase 2.3: ë‹µë³€ êµ¬ì¡°í™” ê°œì„ 

**ëª©í‘œ:** ì¼ê´€ëœ í˜•ì‹ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ ì œê³µ

**ì‹ ê·œ íŒŒì¼:**

- `source/services/answer_formatter.py`

**êµ¬í˜„ ë‚´ìš©:**

```python
class AnswerFormatter:
    def format_answer(self, 
                     raw_answer: str,
                     question_type: QuestionType,
                     sources: Dict[str, List],
                     confidence: ConfidenceInfo):
        
        if question_type == QuestionType.PRECEDENT_SEARCH:
            return self._format_precedent_answer(raw_answer, sources, confidence)
        elif question_type == QuestionType.LAW_EXPLANATION:
            return self._format_law_explanation(raw_answer, sources, confidence)
        else:
            return self._format_general_answer(raw_answer, sources, confidence)
    
    def _format_precedent_answer(self, answer: str, sources: Dict, confidence: ConfidenceInfo):
        formatted = f"""
## ê´€ë ¨ íŒë¡€ ë¶„ì„

{answer}

### ğŸ“‹ ì°¸ê³  íŒë¡€

{self._format_precedent_sources(sources['precedents'])}

### âš–ï¸ ì ìš© ê°€ëŠ¥í•œ ë²•ë¥ 

{self._format_law_sources(sources['laws'])}

### ğŸ’¡ ì‹ ë¢°ë„ ì •ë³´
- ì‹ ë¢°ë„: {confidence.confidence:.1%}
- ìˆ˜ì¤€: {confidence.reliability_level}
{self._format_warnings(confidence.warnings)}

---
ğŸ’¼ ë³¸ ë‹µë³€ì€ ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, ê°œë³„ ì‚¬ì•ˆì— ëŒ€í•œ ë²•ë¥  ìë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤.
êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë³€í˜¸ì‚¬ì™€ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
"""
        return formatted
    
    def _format_precedent_sources(self, precedents: List[Dict]):
        result = []
        for i, prec in enumerate(precedents[:5], 1):
            result.append(f"""
{i}. **{prec['case_name']}** ({prec['case_number']})
   - ë²•ì›: {prec['court']}
   - íŒê²°ì¼: {prec['decision_date']}
   - íŒê²°ìš”ì§€: {prec['summary'][:200]}...
   - ìœ ì‚¬ë„: {prec['similarity']:.1%}
""")
        return "\n".join(result)
```

### Phase 2.4: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ìµœì í™”

**ëª©í‘œ:** ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±

**ìˆ˜ì • íŒŒì¼:**

- `source/services/context_manager.py` (í™•ì¥)

**êµ¬í˜„ ë‚´ìš©:**

```python
class ContextBuilder:
    def build_context_by_question_type(self, 
                                      question_type: QuestionType,
                                      search_results: Dict,
                                      conversation_context: Optional[ConversationContext]):
        
        if question_type == QuestionType.PRECEDENT_SEARCH:
            return self._build_precedent_context(search_results)
        elif question_type == QuestionType.LAW_EXPLANATION:
            return self._build_law_explanation_context(search_results)
        elif question_type == QuestionType.LEGAL_ADVICE:
            return self._build_advice_context(search_results, conversation_context)
    
    def _build_precedent_context(self, results: Dict):
        # íŒë¡€ ì¤‘ì‹¬ ì»¨í…ìŠ¤íŠ¸
        context = "=== ê´€ë ¨ íŒë¡€ ===\n\n"
        
        for prec in results['precedents'][:3]:
            context += f"""
ì‚¬ê±´ë²ˆí˜¸: {prec['case_number']}
ë²•ì›: {prec['court']} / íŒê²°ì¼: {prec['decision_date']}

[íŒì‹œì‚¬í•­]
{prec.get('judgment_summary', '')}

[íŒê²°ìš”ì§€]
{prec.get('judgment_gist', '')}

---
"""
        
        # ê´€ë ¨ ë²•ë¥ ë„ ê°„ëµíˆ í¬í•¨
        if results.get('laws'):
            context += "\n=== ì ìš© ë²•ë¥  ===\n\n"
            for law in results['laws'][:2]:
                context += f"{law['law_name']} {law['article_number']}\n{law['content'][:300]}...\n\n"
        
        return context
    
    def _build_advice_context(self, results: Dict, conv_context: Optional[ConversationContext]):
        # ë²•ë¥  + íŒë¡€ + ëŒ€í™” ë§¥ë½ í†µí•©
        context = ""
        
        # ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ë²•ë¥ /íŒë¡€ ìš°ì„  í¬í•¨
        if conv_context:
            context += "=== ì´ì „ ëŒ€í™” ë§¥ë½ ===\n"
            for turn in conv_context.get_relevant_turns():
                context += f"Q: {turn['query']}\nA: {turn['response'][:200]}...\n\n"
        
        # ë²•ë¥  ìš°ì„ , íŒë¡€ëŠ” ë³´ì¡°
        context += "=== ì ìš© ë²•ë¥  ===\n"
        for law in results['laws'][:3]:
            context += f"{law['law_name']} {law['article_number']}\n{law['content']}\n\n"
        
        context += "=== ì°¸ê³  íŒë¡€ ===\n"
        for prec in results['precedents'][:2]:
            context += f"{prec['case_number']}: {prec['summary']}\n\n"
        
        return context
```

### Phase 2.5: Ollama í†µí•© ë° ì‘ë‹µ ìƒì„± ê°œì„ 

**ëª©í‘œ:** Ollama Qwen2.5:7b ëª¨ë¸ê³¼ ìµœì  í†µí•©

**ì‹ ê·œ/ìˆ˜ì • íŒŒì¼:**

- `source/services/ollama_client.py` (ì‹ ê·œ)
- `source/services/answer_generator.py` (ìˆ˜ì •)

**êµ¬í˜„ ë‚´ìš©:**

```python
class OllamaClient:
    def __init__(self, model_name: str = "qwen2.5:7b"):
        self.model_name = model_name
        self.base_url = "http://localhost:11434"
    
    def generate(self, prompt: str, context: str, temperature: float = 0.7):
        full_prompt = f"""{prompt}

ì»¨í…ìŠ¤íŠ¸:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."""
        
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model_name,
                "prompt": full_prompt,
                "temperature": temperature,
                "stream": False
            }
        )
        
        return response.json()['response']

class ImprovedAnswerGenerator:
    def __init__(self):
        self.ollama = OllamaClient()
        self.formatter = AnswerFormatter()
        self.confidence_calc = ConfidenceCalculator()
    
    def generate_answer(self, 
                       query: str,
                       question_type: QuestionType,
                       context: str,
                       sources: Dict):
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        prompt = PROMPT_TEMPLATES[question_type.template_key]
        
        # Ollamaë¡œ ë‹µë³€ ìƒì„±
        raw_answer = self.ollama.generate(prompt, context)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self.confidence_calc.calculate(query, sources, raw_answer)
        
        # ë‹µë³€ êµ¬ì¡°í™”
        formatted_answer = self.formatter.format_answer(
            raw_answer, question_type, sources, confidence
        )
        
        return {
            "answer": formatted_answer,
            "raw_answer": raw_answer,
            "confidence": confidence,
            "question_type": question_type.name
        }
```

### Phase 2.6: í†µí•© í…ŒìŠ¤íŠ¸ ë° ì—”ë“œí¬ì¸íŠ¸ ìµœì¢… êµ¬ì„±

**ëª©í‘œ:** ëª¨ë“  ê°œì„  ì‚¬í•­ì„ í†µí•©í•˜ì—¬ ìµœì¢… API ì œê³µ

**ìˆ˜ì • íŒŒì¼:**

- `source/api/endpoints.py` (ìµœì¢… í†µí•©)

**ìµœì¢… ì—”ë“œí¬ì¸íŠ¸ êµ¬ì¡°:**

```python
@api_router.post("/chat/v2", response_model=ChatV2Response)
async def chat_v2_endpoint(request: ChatV2Request):
    """ê°œì„ ëœ ë²•ë¥  ì±—ë´‡ API v2"""
    
    # 1. ëŒ€í™” ë§¥ë½ ë¡œë“œ
    conversation_context = None
    if request.session_id:
        conversation_context = conversation_manager.get_relevant_context(
            request.session_id, request.message
        )
    
    # 2. ì§ˆë¬¸ ë¶„ë¥˜
    question_type = question_classifier.classify_question(request.message)
    
    # 3. ë²•ë¥  ìš©ì–´ í™•ì¥
    expanded_query = legal_term_expander.expand_query(request.message)
    
    # 4. í†µí•© ê²€ìƒ‰ (ë²•ë¥  + íŒë¡€)
    search_results = unified_search_engine.search(
        query=expanded_query,
        question_type=question_type,
        top_k=10
    )
    
    # 5. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = context_builder.build_context_by_question_type(
        question_type, search_results, conversation_context
    )
    
    # 6. ë‹µë³€ ìƒì„± (Ollama)
    answer_result = improved_answer_generator.generate_answer(
        query=request.message,
        question_type=question_type,
        context=context,
        sources=search_results
    )
    
    # 7. ëŒ€í™” ë§¥ë½ ì €ì¥
    if request.session_id:
        conversation_manager.add_turn(
            request.session_id,
            request.message,
            answer_result['answer']
        )
    
    return ChatV2Response(
        answer=answer_result['answer'],
        question_type=question_type.name,
        confidence=answer_result['confidence'],
        law_sources=search_results['laws'],
        precedent_sources=search_results['precedents'],
        conversation_context_used=conversation_context is not None,
        expanded_terms=expanded_query['expanded_terms']
    )
```

## êµ¬í˜„ ìˆœì„œ ë° ìš°ì„ ìˆœìœ„

### Week 1: 1ë‹¨ê³„ í•µì‹¬ ê¸°ëŠ¥

1. Phase 1.1: ì§ˆë¬¸ ë¶„ë¥˜ê¸°
2. Phase 1.3: íŒë¡€ ê²€ìƒ‰ ì—”ì§„
3. Phase 1.2: í†µí•© ê²€ìƒ‰ ê°œì„ 

### Week 2: 1ë‹¨ê³„ ì™„ì„±

4. Phase 1.4: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
5. Phase 1.5: ì‹ ë¢°ë„ ì‹œìŠ¤í…œ
6. Phase 1.6: API í†µí•©

### Week 3: 2ë‹¨ê³„ ì‹œì‘

7. Phase 2.1: ë²•ë¥  ìš©ì–´ í™•ì¥
8. Phase 2.5: Ollama í†µí•©

### Week 4: 2ë‹¨ê³„ ì™„ì„±

9. Phase 2.2: ëŒ€í™” ë§¥ë½ ê´€ë¦¬
10. Phase 2.3: ë‹µë³€ êµ¬ì¡°í™”
11. Phase 2.4: ì»¨í…ìŠ¤íŠ¸ ìµœì í™”
12. Phase 2.6: ìµœì¢… í†µí•©

## í•µì‹¬ ê°œì„  íš¨ê³¼

1. **ê²€ìƒ‰ ì •í™•ë„**: ì§ˆë¬¸ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜ë¡œ ë²•ë¥ /íŒë¡€ ê²€ìƒ‰ ìµœì í™”
2. **ë‹µë³€ í’ˆì§ˆ**: êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ì™€ ì‹ ë¢°ë„ í‘œì‹œë¡œ ì‹ ë¢°ì„± í–¥ìƒ
3. **ì‚¬ìš©ì ê²½í—˜**: ëŒ€í™” ë§¥ë½ ìœ ì§€ë¡œ ì—°ì† ì§ˆë¬¸ ëŒ€ì‘
4. **í™•ì¥ì„±**: ë²•ë¥  ìš©ì–´ ì‚¬ì „ìœ¼ë¡œ ì§€ì†ì  ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ 

### To-dos

- [ ] ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ê¸° êµ¬í˜„ (QuestionClassifier)
- [ ] íŒë¡€ ì „ìš© ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„ (PrecedentSearchEngine)
- [ ] í†µí•© ê²€ìƒ‰ ì—”ì§„ ê°œì„  (HybridSearchEngine í™•ì¥)
- [ ] ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€
- [ ] ì‹ ë¢°ë„ ê¸°ë°˜ ë‹µë³€ ì‹œìŠ¤í…œ êµ¬í˜„ (ConfidenceCalculator)
- [ ] FastAPI ì—”ë“œí¬ì¸íŠ¸ í†µí•© (intelligent chat endpoint)
- [ ] ë²•ë¥  ìš©ì–´ í™•ì¥ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„ (LegalTermExpander)
- [ ] Ollama í´ë¼ì´ì–¸íŠ¸ ë° ë‹µë³€ ìƒì„± ê°œì„ 
- [ ] ëŒ€í™” ë§¥ë½ ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„ (ConversationManager)
- [ ] ë‹µë³€ êµ¬ì¡°í™” ê°œì„  (AnswerFormatter)
- [ ] ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ìµœì í™” (ContextBuilder)
- [ ] ìµœì¢… í†µí•© ë° API v2 ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„