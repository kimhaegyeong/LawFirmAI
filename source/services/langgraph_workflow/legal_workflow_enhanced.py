# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ LangGraph Legal Workflow
ë‹µë³€ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ í–¥ìƒëœ ì›Œí¬í”Œë¡œìš° êµ¬í˜„
"""

import logging
import time
from typing import Any, Dict, List

from langchain_community.llms import Ollama
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import END, StateGraph

from ...utils.langgraph_config import LangGraphConfig
from .keyword_mapper import LegalKeywordMapper
from .legal_data_connector import LegalDataConnector
from .performance_optimizer import PerformanceOptimizer
from .prompt_templates import LegalPromptTemplates
from .state_definitions import LegalWorkflowState

logger = logging.getLogger(__name__)

# Mock QuestionType for enhanced workflow
class QuestionType:
    GENERAL_QUESTION = "general_question"
    LAW_INQUIRY = "law_inquiry"
    PRECEDENT_SEARCH = "precedent_search"
    LEGAL_ADVICE = "legal_advice"
    PROCEDURE_GUIDE = "procedure_guide"
    TERM_EXPLANATION = "term_explanation"
    CONTRACT_REVIEW = "contract_review"
    FAMILY_LAW = "family_law"
    CRIMINAL_LAW = "criminal_law"
    CIVIL_LAW = "civil_law"
    LABOR_LAW = "labor_law"
    PROPERTY_LAW = "property_law"
    INTELLECTUAL_PROPERTY = "intellectual_property"
    TAX_LAW = "tax_law"
    CIVIL_PROCEDURE = "civil_procedure"


class EnhancedLegalQuestionWorkflow:
    """ê°œì„ ëœ ë²•ë¥  ì§ˆë¬¸ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°"""

    def __init__(self, config: LangGraphConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.prompt_templates = LegalPromptTemplates()
        self.keyword_mapper = LegalKeywordMapper()
        self.data_connector = LegalDataConnector()
        self.performance_optimizer = PerformanceOptimizer()

        # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self.vector_store = self._initialize_vector_store()

        # LLM ì´ˆê¸°í™”
        self.llm = self._initialize_llm()

        # ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶•
        self.graph = self._build_graph()
        logger.info("EnhancedLegalQuestionWorkflow initialized.")

    def _initialize_vector_store(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
            from ...data.vector_store import LegalVectorStore

            vector_store = LegalVectorStore(
                model_name="jhgan/ko-sroberta-multitask",
                dimension=768,
                index_type="flat",
                enable_quantization=True,
                enable_lazy_loading=True,
                memory_threshold_mb=1500  # ë©”ëª¨ë¦¬ ì„ê³„ê°’ì„ 1500MBë¡œ ë‚®ì¶¤
            )

            # ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            index_paths = [
                "data/embeddings/ml_enhanced_ko_sroberta/ml_enhanced_faiss_index",
                "data/embeddings/ml_enhanced_ko_sroberta_precedents/ml_enhanced_faiss_index",
                "data/embeddings/legal_vector_index"
            ]

            index_loaded = False
            for index_path in index_paths:
                try:
                    if vector_store.load_index(index_path):
                        print(f"Vector store loaded from: {index_path}")
                        index_loaded = True
                        break
                except Exception as e:
                    print(f"Failed to load vector store from {index_path}: {e}")
                    continue

            if not index_loaded:
                print("No vector store loaded, will use database search only")

            return vector_store

        except Exception as e:
            print(f"Failed to initialize vector store: {e}")
            return None

    def _initialize_llm(self):
        """LLM ì´ˆê¸°í™” (Google Gemini ìš°ì„ , Ollama ë°±ì—…)"""
        if self.config.llm_provider == "google":
            try:
                gemini_llm = ChatGoogleGenerativeAI(
                    model=self.config.google_model,
                    temperature=0.3,
                    max_output_tokens=500,  # ë‹µë³€ ê¸¸ì´ ì¦ê°€
                    timeout=30,  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
                    api_key=self.config.google_api_key
                )
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œë¡œ ëª¨ë¸ ë¡œë“œ í™•ì¸ (ì œê±°)
                # test_response = gemini_llm.invoke("ì•ˆë…•í•˜ì„¸ìš”")
                logger.info(f"Initialized Google Gemini LLM: {self.config.google_model}")
                # logger.info(f"Test response: {test_response.content[:50]}...")
                return gemini_llm
            except Exception as e:
                logger.warning(f"Failed to initialize Google Gemini LLM: {e}. Falling back to Ollama.")

        if self.config.llm_provider == "ollama":
            try:
                ollama_llm = Ollama(
                    model=self.config.ollama_model,
                    base_url=self.config.ollama_base_url,
                    temperature=0.3,
                    num_predict=500,  # ë‹µë³€ ê¸¸ì´ ì¦ê°€
                    timeout=30  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
                )
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œë¡œ ëª¨ë¸ ë¡œë“œ í™•ì¸ (ì œê±°)
                # test_response = ollama_llm.invoke("ì•ˆë…•í•˜ì„¸ìš”")
                logger.info(f"Initialized Ollama LLM: {self.config.ollama_model}")
                # logger.info(f"Test response: {test_response[:50]}...")
                return ollama_llm
            except Exception as e:
                logger.warning(f"Failed to initialize Ollama LLM: {e}. Using Mock LLM.")

        # ğŸ†• ê°œì„ ëœ Mock LLM - ê²€ìƒ‰ ê²°ê³¼ í™œìš©
        class ImprovedMockLLM:
            def invoke(self, prompt):
                """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•œ ë²•ë¥  ë‹µë³€ ìƒì„±"""
                # í”„ë¡¬í”„íŠ¸ì—ì„œ ì»¨í…ìŠ¤íŠ¸(ê²€ìƒ‰ ê²°ê³¼) ì¶”ì¶œ
                context = ""
                question = ""

                if "context:" in prompt.lower():
                    parts = prompt.split("context:")
                    if len(parts) > 1:
                        context = parts[1].strip()

                if "question:" in prompt.lower():
                    question_part = prompt.split("question:")[-1]
                    if "context:" in question_part:
                        question = question_part.split("context:")[0].strip()
                    else:
                        question = question_part.strip()

                # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€
                if context and context != "" and len(context) > 10:
                    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì—¬ ë‹µë³€ ìƒì„±
                    return self._generate_response_from_context(question, context)

                # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë‹µë³€
                return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë²•ë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë²•ë¥  ì¡°ë¬¸ì´ë‚˜ êµ¬ì²´ì ì¸ ìƒí™©ì„ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

            def _generate_response_from_context(self, question, context):
                """ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ë‹µë³€ ìƒì„±"""
                # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
                lines = context.split('\n')

                # ì²« ë²ˆì§¸ ì£¼ìš” ë‚´ìš© ì°¾ê¸°
                main_content = ""
                for line in lines:
                    if line.strip() and len(line.strip()) > 20:
                        main_content = line.strip()
                        break

                # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë‹µë³€ ìƒì„±
                if "ìƒì†" in question or "ìœ ì–¸" in question:
                    return f"""ì°¸ê³ í•˜ì‹  ë‚´ìš©ì— ë”°ë¥´ë©´:

{main_content if main_content else 'ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.'}

ì´ì— ëŒ€í•´ ê°„ëµíˆ ì„¤ëª…ë“œë¦¬ë©´, ìƒì†ê³¼ ê´€ë ¨ëœ ë²•ë¥ ì€ ë¯¼ë²•ì— ê·œì •ë˜ì–´ ìˆìœ¼ë©°, ê° ê°€ì¡± êµ¬ì„±ì›ë³„ë¡œ ìƒì†ë¶„ì´ ë‹¤ë¦…ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì¡°ë¬¸ì„ í™•ì¸í•˜ì‹œë©´ ë” ì •í™•í•œ ì •ë³´ë¥¼ ì–»ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

                elif "ì•¼ê°„" in question or "ê·¼ë¬´" in question or "ìˆ˜ë‹¹" in question:
                    return f"""ê·¼ë¡œê¸°ì¤€ë²•ì— ë”°ë¥´ë©´:

{main_content if main_content else 'ì•¼ê°„ê·¼ë¬´ì™€ ê´€ë ¨ëœ ë²•ë¥  ì¡°ë¬¸ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.'}

ì¼ë°˜ì ìœ¼ë¡œ ì•¼ê°„ê·¼ë¬´ëŠ” íŠ¹ì • ì‹œê°„ëŒ€(ë³´í†µ ì˜¤í›„ 10ì‹œ ì´í›„)ì— ìˆ˜í–‰ë˜ëŠ” ê·¼ë¬´ë¥¼ ì˜ë¯¸í•˜ë©°, ì•¼ê°„ìˆ˜ë‹¹ì´ ë³„ë„ë¡œ ì§€ê¸‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì—°ì¥ê·¼ë¬´ì™€ëŠ” ë³„ê°œì˜ ê°œë…ì…ë‹ˆë‹¤."""

                else:
                    # ì¼ë°˜ì ì¸ ë²•ë¥  ë‹µë³€
                    return f"""ë‹¤ìŒê³¼ ê°™ì€ ë²•ë¥  ì •ë³´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤:

{main_content[:300] if main_content else 'ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤'}

ì´ ë‚´ìš©ì´ ë„ì›€ì´ ë˜ì…¨ëŠ”ì§€ í™•ì¸í•´ì£¼ì‹œê³ , ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”.

â€» ì´ ë‹µë³€ì€ ë²•ë¥  ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, êµ¬ì²´ì ì¸ ì‚¬ì•ˆì— ëŒ€í•œ ë²•ë¥  ìë¬¸ì€ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""

            async def ainvoke(self, prompt):
                return self.invoke(prompt)

        logger.warning("No valid LLM provider configured or failed to initialize. Using Improved Mock LLM.")
        return ImprovedMockLLM()

    def _build_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶•"""
        workflow = StateGraph(LegalWorkflowState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("classify_query", self.classify_query)
        workflow.add_node("retrieve_documents", self.retrieve_documents)
        workflow.add_node("generate_answer_enhanced", self.generate_answer_enhanced)
        workflow.add_node("format_response", self.format_response)

        # ì—£ì§€ ì„¤ì •
        workflow.set_entry_point("classify_query")
        workflow.add_edge("classify_query", "retrieve_documents")
        workflow.add_edge("retrieve_documents", "generate_answer_enhanced")
        workflow.add_edge("generate_answer_enhanced", "format_response")
        workflow.add_edge("format_response", END)

        return workflow

    def classify_query(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì§ˆë¬¸ ë¶„ë¥˜ (ê°œì„ ëœ ë²„ì „)"""
        try:
            # ë¡œê¹… ëŒ€ì‹  print ì‚¬ìš© (ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ ì•ˆì „)
            print(f"ğŸ” classify_query ì‹œì‘: query='{state.get('query', 'NOT_FOUND')}'")
            start_time = time.time()

            # ìƒíƒœ ë””ë²„ê¹…
            print(f"classify_query - Received state keys: {list(state.keys())}")
            print(f"classify_query - state['query']: '{state.get('query', 'NOT_FOUND')}'")
            print(f"classify_query - state['user_query']: '{state.get('user_query', 'NOT_FOUND')}'")

            # ìƒíƒœ ì´ˆê¸°í™” (í•„ìš”í•œ í‚¤ë“¤ì´ ì—†ìœ¼ë©´ ì¶”ê°€)
            if "query" not in state:
                state["query"] = ""
            if "errors" not in state:
                state["errors"] = []
            if "query_type" not in state:
                state["query_type"] = QuestionType.GENERAL_QUESTION
            if "confidence" not in state:
                state["confidence"] = 0.0
            if "sources" not in state:
                state["sources"] = []
            if "response" not in state:
                state["response"] = ""
            if "processing_time" not in state:
                state["processing_time"] = 0.0
            if "processing_steps" not in state:
                state["processing_steps"] = []

            # ì›ë³¸ ì¿¼ë¦¬ ë³´ì¡´ (user_queryê°€ ìˆìœ¼ë©´ ì‚¬ìš©)
            original_query = state.get("user_query") or state.get("query", "")
            query = original_query.lower()

            print(f"classify_query - Using query: '{original_query}'")

            # ì›ë³¸ ì¿¼ë¦¬ë¥¼ ìƒíƒœì— ì €ì¥ (ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡)
            state["query"] = original_query
            state["original_query"] = original_query

            # ê°œì„ ëœ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
            if any(k in query for k in ["ê³„ì•½", "ê³„ì•½ì„œ", "ë§¤ë§¤", "ì„ëŒ€", "ë„ê¸‰"]):
                state["query_type"] = QuestionType.CONTRACT_REVIEW
            elif any(k in query for k in ["ì´í˜¼", "ê°€ì¡±", "ìƒì†", "ì–‘ìœ¡", "ì…ì–‘"]):
                state["query_type"] = QuestionType.FAMILY_LAW
            elif any(k in query for k in ["ì ˆë„", "ë²”ì£„", "í˜•ì‚¬", "ì‚¬ê¸°", "í­í–‰", "ê°•ë„", "ì‚´ì¸"]):
                state["query_type"] = QuestionType.CRIMINAL_LAW
            elif any(k in query for k in ["ì†í•´ë°°ìƒ", "ë¯¼ì‚¬", "ë¶ˆë²•í–‰ìœ„", "ì±„ê¶Œ", "ì†Œìœ ê¶Œ"]):
                state["query_type"] = QuestionType.CIVIL_LAW
            elif any(k in query for k in ["í•´ê³ ", "ë…¸ë™", "ì„ê¸ˆ", "ê·¼ë¡œì‹œê°„", "íœ´ê°€", "ì‚°ì—…ì¬í•´"]):
                state["query_type"] = QuestionType.LABOR_LAW
            elif any(k in query for k in ["ë¶€ë™ì‚°", "ë§¤ë§¤", "ë“±ê¸°", "ê³µì‹œ", "í† ì§€"]):
                state["query_type"] = QuestionType.PROPERTY_LAW
            elif any(k in query for k in ["íŠ¹í—ˆ", "ì§€ì ì¬ì‚°ê¶Œ", "ì €ì‘ê¶Œ", "ìƒí‘œ", "ë””ìì¸"]):
                state["query_type"] = QuestionType.INTELLECTUAL_PROPERTY
            elif any(k in query for k in ["ì„¸ê¸ˆ", "ì†Œë“ì„¸", "ë¶€ê°€ê°€ì¹˜ì„¸", "ë²•ì¸ì„¸", "ìƒì†ì„¸", "ê°€ì‚°ì„¸"]):
                state["query_type"] = QuestionType.TAX_LAW
            elif any(k in query for k in ["ì†Œì†¡", "ê´€í• ", "ì¦ê±°", "íŒê²°", "ì§‘í–‰", "ë¯¼ì‚¬ì†Œì†¡"]):
                state["query_type"] = QuestionType.CIVIL_PROCEDURE
            else:
                state["query_type"] = QuestionType.GENERAL_QUESTION

            state["confidence"] = 0.8  # ë¶„ë¥˜ ì‹ ë¢°ë„ í–¥ìƒ
            state["processing_steps"].append(f"ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ (ê°œì„ ): {state['query_type']}")

            processing_time = time.time() - start_time
            state["processing_time"] = state.get("processing_time", 0.0) + processing_time

            print(f"Query classified as {state['query_type']} with confidence {state['confidence']}")

        except Exception as e:
            error_msg = f"ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            state["errors"].append(error_msg)
            state["processing_steps"].append(error_msg)
            print(f"âŒ {error_msg}")

            # ê¸°ë³¸ê°’ ì„¤ì •
            state["query_type"] = QuestionType.GENERAL_QUESTION
            state["confidence"] = 0.3  # ê¸°ë³¸ ì‹ ë¢°ë„ë¥¼ ë‚®ì¶¤

        return state

    def retrieve_documents(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ë¬¸ì„œ ê²€ìƒ‰ (ìºì‹± ì ìš©)"""
        try:
            start_time = time.time()

            # ìƒíƒœ ì´ˆê¸°í™” (ì•ˆì „í•œ ë°©ì‹)
            if "processing_steps" not in state:
                state["processing_steps"] = []
            if "retrieved_docs" not in state:
                state["retrieved_docs"] = []
            if "errors" not in state:
                state["errors"] = []
            if "query" not in state:
                state["query"] = ""
            if "query_type" not in state:
                state["query_type"] = QuestionType.GENERAL_QUESTION
            if "confidence" not in state:
                state["confidence"] = 0.3  # ê¸°ë³¸ ì‹ ë¢°ë„ë¥¼ ë‚®ì¶¤

            # âœ… ê°œì„ ëœ ì¿¼ë¦¬ ì¶”ì¶œ ë¡œì§ (ìˆœì„œ ë³€ê²½)
            query = state.get("user_query") or state.get("query") or state.get("original_query") or ""

            if not query:
                print("âŒ ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                state["retrieved_docs"] = []
                state["processing_steps"].append("ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆì–´ ê²€ìƒ‰ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤")
                return state

            print(f"ğŸ” retrieve_documents ì‹œì‘: query='{query}'")

            query_type = state["query_type"]

            # ìƒíƒœ ë””ë²„ê¹…
            print(f"retrieve_documents - Received state keys: {list(state.keys())}")
            print(f"retrieve_documents - state['query']: '{state.get('query', 'NOT_FOUND')}'")
            print(f"retrieve_documents - state['user_query']: '{state.get('user_query', 'NOT_FOUND')}'")
            print(f"retrieve_documents - state['original_query']: '{state.get('original_query', 'NOT_FOUND')}'")

            # ì¿¼ë¦¬ ë””ë²„ê¹…
            print(f"Document retrieval - Query: '{query}', Type: {query_type}")

            # ìºì‹œì—ì„œ ë¬¸ì„œ í™•ì¸ (ë” ì ê·¹ì ì¸ ìºì‹±)
            cache_key = f"{query}_{query_type}"
            cached_documents = self.performance_optimizer.cache.get_cached_documents(query, query_type)

            if cached_documents:
                state["retrieved_docs"] = cached_documents
                state["processing_steps"].append(f"{len(cached_documents)}ê°œ ìºì‹œëœ ë¬¸ì„œ ì‚¬ìš©")
                print(f"Using cached documents for query: {query[:50]}...")
            else:
                # ë²¡í„° ê²€ìƒ‰ ìš°ì„  ì‹œë„
                documents = []

                # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰ ì‹œë„
                try:
                    if hasattr(self, 'vector_store') and self.vector_store and hasattr(self.vector_store, 'search'):
                        vector_results = self.vector_store.search(query, top_k=5)
                        if vector_results:
                            # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                            for i, result in enumerate(vector_results):
                                doc = {
                                    "id": f"vector_{i}",
                                    "content": result.get("content", str(result)),
                                    "source": "Vector Store",
                                    "relevance_score": result.get("similarity", result.get("score", 0.8)),
                                    "category": query_type
                                }
                                documents.append(doc)
                            print(f"Vector search found {len(documents)} documents")
                except Exception as e:
                    print(f"âš ï¸ Vector search failed: {e}")

                # ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ í•­ìƒ ìˆ˜í–‰ (ì‹¤ì œ ë²•ë¥  ë¬¸ì„œ ì‚¬ìš©)
                try:
                    print(f"ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: query='{query}', query_type='{query_type}'")
                    db_documents = self.data_connector.search_documents(query, query_type, limit=5)
                    print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(db_documents)}ê°œ ë¬¸ì„œ ë°œê²¬")

                    # ì¤‘ë³µ ì œê±°
                    existing_contents = {doc["content"][:100] for doc in documents}
                    for doc in db_documents:
                        if doc.get("content", "")[:100] not in existing_contents:
                            documents.append(doc)
                    print(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ {len(db_documents)}ê°œ ë¬¸ì„œ ì¶”ê°€")
                except Exception as e:
                    print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    import traceback
                    print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

                # ì—¬ì „íˆ ê²°ê³¼ê°€ ë¶€ì¡±í•œ ê²½ìš° ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ì¶”ê°€
                if len(documents) < 3:
                    try:
                        category_docs = self.data_connector.get_document_by_category(query_type, limit=3)
                        existing_contents = {doc["content"][:100] for doc in documents}
                        for doc in category_docs:
                            if doc.get("content", "")[:100] not in existing_contents:
                                documents.append(doc)
                        print(f"Category search added {len(category_docs)} documents")
                    except Exception as e:
                        print(f"Category search failed: {e}")

                state["retrieved_docs"] = documents
                state["processing_steps"].append(f"{len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (ë²¡í„°+DB)")

                # ë¬¸ì„œ ìºì‹±
                self.performance_optimizer.cache.cache_documents(query, query_type, documents)

            processing_time = time.time() - start_time
            state["processing_time"] = state.get("processing_time", 0.0) + processing_time

            print(f"Retrieved {len(state['retrieved_docs'])} documents for query type {query_type}")

        except Exception as e:
            error_msg = f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            state["errors"].append(error_msg)
            state["processing_steps"].append(error_msg)
            print(f"âŒ {error_msg}")

            # í´ë°±: ê¸°ë³¸ ë¬¸ì„œ ì„¤ì •
            state["retrieved_docs"] = [
                {"content": f"'{state['query']}'ì— ëŒ€í•œ ê¸°ë³¸ ë²•ë¥  ì •ë³´ì…ë‹ˆë‹¤.", "source": "Default DB"}
            ]

        return state

    def generate_answer_enhanced(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ê°œì„ ëœ ë‹µë³€ ìƒì„±"""
        # user_queryë¥¼ ë¨¼ì € í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ query ì‚¬ìš©
        query_value = state.get('user_query') or state.get('query', 'NOT_FOUND')
        print(f"ğŸ” generate_answer_enhanced ì‹œì‘: query='{query_value}'")

        # ğŸ†• ìƒì„¸ ìƒíƒœ ë””ë²„ê¹…
        print(f"ğŸ“‹ state í‚¤ ëª©ë¡: {list(state.keys())}")
        print(f"ğŸ“Š retrieved_docs ìœ ë¬´: {'retrieved_docs' in state}")
        print(f"ğŸ“Š retrieved_docs íƒ€ì…: {type(state.get('retrieved_docs', 'NOT_FOUND'))}")
        print(f"ğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(state.get('retrieved_docs', []))}")

        # ğŸ†• retrieved_docs ë‚´ìš© í™•ì¸
        if 'retrieved_docs' in state:
            docs = state['retrieved_docs']
            if isinstance(docs, list):
                print(f"ğŸ“Š retrieved_docs ë¦¬ìŠ¤íŠ¸ ê¸¸ì´: {len(docs)}")
                if docs:
                    print(f"ğŸ“Š ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ: {type(docs[0])} - {list(docs[0].keys()) if isinstance(docs[0], dict) else str(docs[0])[:100]}")
            else:
                print(f"ğŸ“Š retrieved_docs íƒ€ì…: {type(docs)}")

        try:
            start_time = time.time()

            # ğŸ†• ì „ì²´ state ë³´ì¡´ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
            # TypedDictì´ë¯€ë¡œ ì „ì²´ ë”•ì…”ë„ˆë¦¬ë¡œ ì²˜ë¦¬
            updated_state = dict(state)  # ì „ì²´ state ë³µì‚¬

            # í•„ìš”í•œ í•„ë“œë§Œ ì´ˆê¸°í™” (ê¸°ì¡´ ê°’ ìœ ì§€)
            if "query_type" not in updated_state:
                updated_state["query_type"] = QuestionType.GENERAL_QUESTION
            if "query" not in updated_state:
                updated_state["query"] = ""
            if "response" not in updated_state:
                updated_state["response"] = ""
            if "confidence" not in updated_state:
                updated_state["confidence"] = 0.0
            if "sources" not in updated_state:
                updated_state["sources"] = []
            if "processing_steps" not in updated_state:
                updated_state["processing_steps"] = []
            if "errors" not in updated_state:
                updated_state["errors"] = []

            # ğŸ†• retrieved_docsëŠ” ì´ë¯¸ stateì— ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            retrieved_docs = updated_state.get("retrieved_docs", [])

            print(f"ğŸ” generate_answer_enhancedì—ì„œ retrieved_docs í™•ì¸: {len(retrieved_docs)}ê°œ")
            if not retrieved_docs:
                query = updated_state.get("user_query") or updated_state.get("query") or "ì§ˆë¬¸"
                print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
                updated_state["generated_response"] = (
                    f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. "
                    f"ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.\n\n"
                    f"ì´ ì§ˆë¬¸ì€ {updated_state['query_type']} ì˜ì—­ì— í•´ë‹¹í•©ë‹ˆë‹¤. "
                    f"êµ¬ì²´ì ì¸ ì‚¬ì•ˆì— ëŒ€í•œ ì •í™•í•œ ë²•ë¥  ì¡°ì–¸ì€ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
                )
                updated_state["confidence"] = 0.3
                updated_state["processing_steps"] = updated_state.get("processing_steps", [])
                updated_state["processing_steps"].append("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ ë‹µë³€ ì œê³µ")
                return updated_state

            # ğŸ” ë””ë²„ê¹…: ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œì²˜ í™•ì¸
            print(f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´:")
            for i, doc in enumerate(retrieved_docs[:5], 1):  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                source = doc.get("source", "Unknown")
                title = doc.get("title", doc.get("content", "")[:50])
                category = doc.get("category", "Unknown")
                relevance_score = doc.get("relevance_score", 0.0)
                print(f"  [{i}] Source: {source}, Category: {category}, Relevance: {relevance_score:.2f}")
                print(f"      Title: {title[:80]}...")

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n".join([doc.get("content", str(doc)) for doc in retrieved_docs if doc])

            # ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
            original_query = updated_state.get("user_query") or updated_state.get("original_query") or updated_state.get("query")

            # ğŸ” ë””ë²„ê¹…: ì…ë ¥ê°’ í™•ì¸
            print(f"ğŸ” í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë””ë²„ê¹…:")
            print(f"  - original_query: '{original_query}'")
            print(f"  - context ê¸¸ì´: {len(context)}")
            print(f"  - query_type: {updated_state['query_type']}")

            # ì§ˆë¬¸ ìœ í˜•ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
            required_keywords = self.keyword_mapper.get_keywords_for_question(
                original_query, updated_state["query_type"]
            )
            print(f"  - required_keywords: {required_keywords[:5]}")

            # ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
            template = self.prompt_templates.get_template_for_query_type(updated_state["query_type"])

            # ğŸ” ë””ë²„ê¹…: í…œí”Œë¦¿ í™•ì¸
            print(f"  - template íƒ€ì…: {type(template)}")
            print(f"  - template ê¸¸ì´: {len(template) if isinstance(template, str) else 'N/A'}")
            print(f"  - template ì‹œì‘: {str(template)[:100]}...")

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            try:
                # í…œí”Œë¦¿ì´ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                if not isinstance(template, str):
                    print(f"âš ï¸ templateì´ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤. ë³€í™˜í•©ë‹ˆë‹¤.")
                    template = str(template)

                # í”Œë ˆì´ìŠ¤í™€ë” í™•ì¸
                if "{question}" not in template or "{context}" not in template:
                    print(f"âš ï¸ templateì— í•„ìˆ˜ í”Œë ˆì´ìŠ¤í™€ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©")
                    # ê¸°ë³¸ í…œí”Œë¦¿ìœ¼ë¡œ ëŒ€ì²´
                    template = f"""ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë²•ë¥  ìƒë‹´ ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê³  ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

## ì‚¬ìš©ì ì§ˆë¬¸
{{question}}

## ê´€ë ¨ ë²•ë¥  ë¬¸ì„œ
{{context}}

## ë‹µë³€ ì›ì¹™
1. ì¼ìƒì ì¸ ë²•ë¥  ìƒë‹´ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ëŒ€í™”í•˜ì„¸ìš”
2. "~ì…ë‹ˆë‹¤", "ê·€í•˜" ê°™ì€ ê³¼ë„í•˜ê²Œ ê²©ì‹ì ì¸ í‘œí˜„ ëŒ€ì‹ , "~ì˜ˆìš”", "ì§ˆë¬¸í•˜ì‹ " ë“± ìì—°ìŠ¤ëŸ¬ìš´ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”
3. ì§ˆë¬¸ì„ ë‹¤ì‹œ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
4. ì§ˆë¬¸ì˜ ë²”ìœ„ì— ë§ëŠ” ì ì ˆí•œ ì–‘ì˜ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
5. ë¶ˆí•„ìš”í•œ í˜•ì‹(ì œëª©, ë²ˆí˜¸ ë§¤ê¸°ê¸°)ì€ ìµœì†Œí™”í•˜ì„¸ìš”
6. í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ì£¼ìš” ì£¼ì˜ì‚¬í•­ì„ ì œì‹œí•˜ì„¸ìš”
7. ë²•ì  ê·¼ê±°ë¥¼ ëª…ì‹œí•˜ê³  ì‹¤ë¬´ ê¶Œì¥ì‚¬í•­ì„ ì œê³µí•˜ì„¸ìš”

ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì „ë¬¸ ë²•ë¥  ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""

                prompt = template.format(
                    question=original_query,
                    context=context,
                    required_keywords=", ".join(required_keywords[:10]) if "required_keywords" in template else ""
                )
                print(f"  âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± ì„±ê³µ: {len(prompt)} ë¬¸ì")
                print(f"  - í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œ: {prompt[:300]}...")

            except KeyError as e:
                print(f"âŒ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨ - í”Œë ˆì´ìŠ¤í™€ë” ì˜¤ë¥˜: {e}")
                # í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ìˆ˜ë™ìœ¼ë¡œ êµì²´
                prompt = template.replace("{question}", original_query).replace("{context}", context)
                if "{required_keywords}" in prompt:
                    prompt = prompt.replace("{required_keywords}", ", ".join(required_keywords[:10]))
                print(f"  ğŸ”§ ìˆ˜ë™ êµì²´ ì„±ê³µ")
            except Exception as e:
                print(f"âŒ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
                prompt = f"""ì§ˆë¬¸: {original_query}

ê´€ë ¨ ë¬¸ì„œ:
{context}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

            # LLM í˜¸ì¶œ
            print(f"LLM í˜¸ì¶œ ì‹œì‘ - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}")
            response = self._call_llm_with_retry(prompt)
            print(f"LLM ì‘ë‹µ ë°›ìŒ - ì‘ë‹µ ê¸¸ì´: {len(response) if response else 0}")
            print(f"LLM ì‘ë‹µ ë‚´ìš©: {response[:100] if response else 'None'}...")

            # ë‹µë³€ í›„ì²˜ë¦¬ (êµ¬ì¡°í™” ê°•í™”)
            enhanced_response = self._enhance_response_structure(response, required_keywords)
            print(f"êµ¬ì¡°í™”ëœ ì‘ë‹µ ê¸¸ì´: {len(enhanced_response) if enhanced_response else 0}")

            # ëª¨ë“  ì‘ë‹µ í•„ë“œì— ë™ì¼í•œ ê°’ ì„¤ì •
            updated_state["answer"] = enhanced_response
            updated_state["generated_response"] = enhanced_response
            updated_state["response"] = enhanced_response
            updated_state["processing_steps"] = updated_state.get("processing_steps", [])
            updated_state["processing_steps"].append("ê°œì„ ëœ ë‹µë³€ ìƒì„± ì™„ë£Œ")

            # ì‹ ë¢°ë„ ê³„ì‚° (ê°œì„ ëœ ë¡œì§)
            confidence = self._calculate_dynamic_confidence(
                enhanced_response,
                retrieved_docs,
                original_query,
                updated_state["query_type"]
            )
            updated_state["confidence"] = confidence
            updated_state["processing_steps"].append(f"ì‹ ë¢°ë„ ê³„ì‚° ì™„ë£Œ: {confidence:.2f}")

            # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            print(f"ì‘ë‹µ í•„ë“œ ì„¤ì • ì™„ë£Œ:")
            print(f"  - answer ê¸¸ì´: {len(updated_state['answer'])}")
            print(f"  - generated_response ê¸¸ì´: {len(updated_state['generated_response'])}")
            print(f"  - response ê¸¸ì´: {len(updated_state['response'])}")
            print(f"  - confidence: {confidence:.2f}")

            processing_time = time.time() - start_time
            updated_state["processing_time"] = updated_state.get("processing_time", 0.0) + processing_time

            print(f"Enhanced answer generated in {processing_time:.2f}s")

        except Exception as e:
            error_msg = f"ê°œì„ ëœ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            # ìƒíƒœ ì´ˆê¸°í™” í™•ì¸
            if "errors" not in updated_state:
                updated_state["errors"] = []
            if "processing_steps" not in updated_state:
                updated_state["processing_steps"] = []

            updated_state["errors"].append(error_msg)
            updated_state["processing_steps"].append(error_msg)
            print(f"âŒ {error_msg}")

            # ê¸°ë³¸ ë‹µë³€ ì„¤ì •
            updated_state["answer"] = self._generate_fallback_answer(updated_state)

        return updated_state

    def _calculate_dynamic_confidence(self, response: str, retrieved_docs: List[Dict],
                                    query: str, query_type) -> float:
        """ë™ì  ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            base_confidence = 0.5  # âœ… ê¸°ë³¸ ì‹ ë¢°ë„ë¥¼ 0.5ë¡œ ì¦ê°€ (ê¸°ì¡´ 0.2)

            # 1. ì‘ë‹µ ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
            response_length = len(response)
            if response_length > 500:
                length_score = 0.3
            elif response_length > 200:
                length_score = 0.2
            elif response_length > 100:
                length_score = 0.1
            else:
                length_score = 0.0

            # 2. ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜ ê¸°ë°˜ ì ìˆ˜
            doc_count = len(retrieved_docs)
            if doc_count >= 3:
                doc_score = 0.2
            elif doc_count >= 2:
                doc_score = 0.15
            elif doc_count >= 1:
                doc_score = 0.1
            else:
                doc_score = 0.0

            # 3. ì§ˆë¬¸ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
            type_weights = {
                "LEGAL_ADVICE": 0.8,
                "PROCEDURE_GUIDE": 0.7,
                "TERM_EXPLANATION": 0.6,
                "CONTRACT_REVIEW": 0.9,
                "GENERAL_QUESTION": 0.5
            }
            type_score = type_weights.get(str(query_type), 0.5) * 0.2

            # 4. ì‘ë‹µ í’ˆì§ˆ ê¸°ë°˜ ì ìˆ˜ (ë²•ë¥  í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€)
            legal_keywords = ["ë²•ë¥ ", "ì¡°ë¬¸", "íŒë¡€", "ë²•ë ¹", "ê·œì •", "ì†Œì†¡", "ê³„ì•½", "ê¶Œë¦¬", "ì˜ë¬´",
                           "ë¯¼ë²•", "í˜•ë²•", "ìƒë²•", "í—Œë²•", "í–‰ì •", "í˜•ì‚¬", "ë¯¼ì‚¬", "ì´í˜¼", "ìƒì†"]
            keyword_count = sum(1 for keyword in legal_keywords if keyword in response)
            quality_score = min(keyword_count * 0.05, 0.2)

            # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
            final_confidence = base_confidence + length_score + doc_score + type_score + quality_score

            # 0.0 ~ 1.0 ë²”ìœ„ë¡œ ì œí•œ
            return max(0.0, min(1.0, final_confidence))

        except Exception as e:
            print(f"ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.3  # ê¸°ë³¸ê°’

    def _call_llm_with_retry(self, prompt: str, max_retries: int = 3) -> str:
        """LLM í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        print(f"LLM í˜¸ì¶œ ì‹œì‘ - í”„ë¡¬í”„íŠ¸: {prompt[:100]}...")

        for attempt in range(max_retries):
            try:
                print(f"LLM í˜¸ì¶œ ì‹œë„ {attempt + 1}/{max_retries}")

                if hasattr(self.llm, 'invoke'):
                    response = self.llm.invoke(prompt)
                    print(f"LLM ì›ë³¸ ì‘ë‹µ íƒ€ì…: {type(response)}")

                    if hasattr(response, 'content'):
                        result = response.content
                        print(f"LLM ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {result[:100]}...")
                        result = self._clean_llm_response(result)  # âœ… ì‘ë‹µ ì •ë¦¬
                        return result
                    else:
                        result = str(response)
                        print(f"LLM ì‘ë‹µ ë¬¸ìì—´ ë³€í™˜: {result[:100]}...")
                        result = self._clean_llm_response(result)  # âœ… ì‘ë‹µ ì •ë¦¬
                        return result
                else:
                    result = self.llm.invoke(prompt)
                    print(f"LLM ì§ì ‘ í˜¸ì¶œ ê²°ê³¼: {result[:100]}...")
                    result = self._clean_llm_response(result)  # âœ… ì‘ë‹µ ì •ë¦¬
                    return result

            except Exception as e:
                print(f"LLM í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    print(f"LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
                    raise e
                time.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

        print("LLM í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return "LLM í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    def _clean_llm_response(self, result: str) -> str:
        """âœ… LLM ì‘ë‹µì—ì„œ í”„ë¡¬í”„íŠ¸ ì§€ì¹¨ ì œê±°"""
        if not result:
            return result

        # í”„ë¡¬í”„íŠ¸ ì§€ì¹¨ì´ í¬í•¨ëœ ê²½ìš° ì œê±°
        if "## ì‚¬ìš©ì ì§ˆë¬¸" in result and "## ë‹µë³€ ì‘ì„± ì§€ì¹¨" in result:
            # ì§€ì¹¨ ë¶€ë¶„ ì œê±°
            if "## ë‹µë³€ ì‘ì„±" in result:
                parts = result.split("## ë‹µë³€ ì‘ì„±")
                if len(parts) > 1:
                    result = parts[-1].strip()

            # ì¶”ê°€ë¡œ ë‹µë³€ ë¶€ë¶„ë§Œ ë‚¨ê¸°ê¸°
            if "## ë‹µë³€" in result:
                parts = result.split("## ë‹µë³€")
                if len(parts) > 1:
                    result = parts[-1].strip()

        # ë¶ˆí•„ìš”í•œ "ë‹µë³€ ì‘ì„±" ê´€ë ¨ í…ìŠ¤íŠ¸ ì œê±°
        if "ë‹µë³€ ì‘ì„± ì§€ì¹¨" in result:
            result = result.replace("ë‹µë³€ ì‘ì„± ì§€ì¹¨", "").strip()

        return result

    def _enhance_response_structure(self, response: str, required_keywords: List[str]) -> str:
        """ë‹µë³€ êµ¬ì¡°í™” ê°•í™”"""
        # í‚¤ì›Œë“œ í¬í•¨ í™•ì¸ ë° ê°•í™”
        missing_keywords = self.keyword_mapper.get_missing_keywords(response, required_keywords[:5])

        if missing_keywords:
            # ëˆ„ë½ëœ í‚¤ì›Œë“œ ì¶”ê°€
            response += f"\n\n## ì¶”ê°€ ê³ ë ¤ì‚¬í•­\n"
            for keyword in missing_keywords[:3]:  # ìµœëŒ€ 3ê°œë§Œ ì¶”ê°€
                response += f"- {keyword} ê´€ë ¨ ì‚¬í•­ë„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.\n"

        # êµ¬ì¡°í™” ê°•í™”
        if "##" not in response:
            # ì œëª©ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            response = f"## ë‹µë³€\n{response}"

        if not any(marker in response for marker in ["1.", "2.", "3.", "â€¢", "-"]):
            # ëª©ë¡ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            response += "\n\n## ì£¼ìš” í¬ì¸íŠ¸\n- ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ êµ¬ì²´ì ì¸ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."

        return response

    def _generate_fallback_answer(self, state: LegalWorkflowState) -> str:
        """í´ë°± ë‹µë³€ ìƒì„±"""
        query = state["query"]
        query_type = state["query_type"]
        context = "\n".join([doc["content"] for doc in state["retrieved_docs"]])

        return f"""## ë‹µë³€

ì§ˆë¬¸: {query}

ì´ ì§ˆë¬¸ì€ {query_type} ì˜ì—­ì— í•´ë‹¹í•©ë‹ˆë‹¤.

## ê´€ë ¨ ë²•ë¥  ì •ë³´
{context}

## ì£¼ìš” í¬ì¸íŠ¸
1. ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
2. ì •í™•í•œ ë²•ë¥ ì  ì¡°ì–¸ì„ ìœ„í•´ì„œëŠ” ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
3. ê´€ë ¨ ë²•ì¡°ë¬¸ê³¼ íŒë¡€ë¥¼ ì¶”ê°€ë¡œ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

## ì£¼ì˜ì‚¬í•­
- ì´ ë‹µë³€ì€ ì¼ë°˜ì ì¸ ì •ë³´ ì œê³µ ëª©ì ì´ë©°, êµ¬ì²´ì ì¸ ë²•ë¥ ì  ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤.
- ì‹¤ì œ ì‚¬ì•ˆì— ëŒ€í•´ì„œëŠ” ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""

    def format_response(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì‘ë‹µ í¬ë§·íŒ… (ê°œì„ ëœ ë²„ì „)"""
        try:
            start_time = time.time()

            # ìƒíƒœ ë””ë²„ê¹…
            print(f"format_response - Received state keys: {list(state.keys())}")
            print(f"format_response - state['answer']: '{state.get('answer', 'NOT_FOUND')}'")
            print(f"format_response - state['response']: '{state.get('response', 'NOT_FOUND')}'")
            print(f"format_response - state['generated_response']: '{state.get('generated_response', 'NOT_FOUND')}'")

            # ìƒíƒœ ì´ˆê¸°í™”
            if "answer" not in state:
                state["answer"] = state.get("response", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            if "confidence" not in state:
                state["confidence"] = 0.0
            if "retrieved_docs" not in state:
                state["retrieved_docs"] = []
            if "query" not in state:
                state["query"] = ""
            if "query_type" not in state:
                state["query_type"] = QuestionType.GENERAL_QUESTION
            if "response" not in state:
                state["response"] = ""
            if "processing_steps" not in state:
                state["processing_steps"] = []
            if "errors" not in state:
                state["errors"] = []

            # ìµœì¢… ë‹µë³€ ë° ë©”íƒ€ë°ì´í„° ì •ë¦¬
            # generated_responseê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            if state.get("generated_response"):
                final_answer = state["generated_response"]
                print(f"format_response - Using generated_response: '{final_answer[:100]}...'")
            else:
                final_answer = state.get("answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                print(f"format_response - Using answer: '{final_answer[:100]}...'")

            final_confidence = state.get("confidence", 0.0)
            final_sources = [doc.get("source", "Unknown") for doc in state.get("retrieved_docs", [])]

            # ğŸ” ì°¸ì¡° ì¶œì²˜ ì¶”ì¶œ ë° ë‹µë³€ì— ì¶”ê°€
            referenced_sources = []
            for doc in state.get("retrieved_docs", [])[:3]:  # ìƒìœ„ 3ê°œë§Œ
                source = doc.get("source", "Unknown")
                category = doc.get("category", "")
                if source and source not in [s["name"] for s in referenced_sources]:
                    referenced_sources.append({
                        "name": source,
                        "category": category,
                        "relevance": doc.get("relevance_score", 0.0)
                    })

            # ğŸ“š ë‹µë³€ì— ì°¸ì¡° ì¶œì²˜ ì¶”ê°€
            if referenced_sources:
                final_answer += "\n\n## ì°¸ì¡° ì¶œì²˜\n"
                for source_info in referenced_sources:
                    category_str = f" ({source_info['category']})" if source_info.get("category") else ""
                    final_answer += f"- {source_info['name']}{category_str}\n"
                print(f"ğŸ“š ì°¸ì¡° ì¶œì²˜ ì¶”ê°€ë¨: {len(referenced_sources)}ê°œ")

            # í‚¤ì›Œë“œ í¬í•¨ë„ ê³„ì‚° (ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©)
            original_query = state.get("original_query", state["query"])
            required_keywords = self.keyword_mapper.get_keywords_for_question(
                original_query, state["query_type"]
            )
            keyword_coverage = self.keyword_mapper.calculate_keyword_coverage(
                final_answer, required_keywords
            )

            # ì‹ ë¢°ë„ ì¡°ì • (í‚¤ì›Œë“œ í¬í•¨ë„ ë°˜ì˜)
            adjusted_confidence = min(0.9, final_confidence + (keyword_coverage * 0.2))

            # ëª¨ë“  ì‘ë‹µ í•„ë“œì— ìµœì¢… ë‹µë³€ ì„¤ì •
            state["answer"] = final_answer
            state["generated_response"] = final_answer  # generated_response í•„ë“œë„ ì„¤ì •
            state["response"] = final_answer  # response í•„ë“œë„ ì„¤ì •
            state["confidence"] = adjusted_confidence
            state["sources"] = list(set(final_sources))  # ì¤‘ë³µ ì œê±°
            state["legal_references"] = referenced_sources  # ğŸ†• ì°¸ì¡° ì¶œì²˜ ì €ì¥
            state["processing_steps"].append("ì‘ë‹µ í¬ë§·íŒ… ì™„ë£Œ (ê°œì„ )")

            # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            print(f"format_response - ìµœì¢… ì‘ë‹µ í•„ë“œ ì„¤ì •:")
            print(f"  - answer ê¸¸ì´: {len(state['answer'])}")
            print(f"  - generated_response ê¸¸ì´: {len(state['generated_response'])}")
            print(f"  - response ê¸¸ì´: {len(state['response'])}")

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            state["metadata"] = {
                "keyword_coverage": keyword_coverage,
                "required_keywords_count": len(required_keywords),
                "matched_keywords_count": len(required_keywords) - len(
                    self.keyword_mapper.get_missing_keywords(final_answer, required_keywords)
                ),
                "response_length": len(final_answer),
                "query_type": state["query_type"],
                "referenced_sources": [s["name"] for s in referenced_sources],  # ğŸ†• ì°¸ì¡° ì¶œì²˜ ëª©ë¡
                "reference_count": len(referenced_sources)  # ğŸ†• ì°¸ì¡° ì¶œì²˜ ê°œìˆ˜
            }

            processing_time = time.time() - start_time
            state["processing_time"] = state.get("processing_time", 0.0) + processing_time

            print("Enhanced response formatting completed")

        except Exception as e:
            error_msg = f"ì‘ë‹µ í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            # ìƒíƒœ ì´ˆê¸°í™” í™•ì¸
            if "errors" not in state:
                state["errors"] = []
            if "processing_steps" not in state:
                state["processing_steps"] = []

            state["errors"].append(error_msg)
            state["processing_steps"].append(error_msg)
            print(f"âŒ {error_msg}")

        return state
