# -*- coding: utf-8 -*-
"""
í†µí•© ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤
LangGraph ê¸°ë°˜ í†µí•© ë²•ë¥  AI ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤
"""

import asyncio
import logging
import time
from datetime import datetime
from typing import Any, Dict, List, Optional

from langchain_core.messages import AIMessage, HumanMessage
from langgraph.checkpoint.base import Checkpoint
from langgraph.graph import END, StateGraph

from ...utils.langgraph_config import LangGraphConfig, langgraph_config
from ...utils.logger import get_logger
from .checkpoint_manager import CheckpointManager
from .legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
from .state_definitions import LegalWorkflowState

logger = get_logger(__name__)


class IntegratedWorkflowService:
    """LangGraph ê¸°ë°˜ í†µí•© ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤"""

    def __init__(self, config: LangGraphConfig = langgraph_config):
        self.config = config
        self.logger = get_logger(__name__)

        # EnhancedLegalQuestionWorkflow ì§ì ‘ ì‚¬ìš©
        self.workflow_builder = EnhancedLegalQuestionWorkflow(config)

        # ì»´íŒŒì¼ëœ ê·¸ëž˜í”„ ì§ì ‘ ì‚¬ìš©
        self.graph_app = self.workflow_builder.graph.compile()

        # ê·¸ëž˜í”„ ì •ë³´ ë¡œê¹…
        self.logger.info(f"Using EnhancedLegalQuestionWorkflow graph directly")
        self.logger.info(f"Graph nodes: {list(self.workflow_builder.graph.nodes.keys())}")
        self.logger.info(f"Graph edges: {list(self.workflow_builder.graph.edges)}")

        self.logger.info("IntegratedWorkflowService initialized successfully")

    def _build_integrated_workflow(self):
        """í†µí•© ì›Œí¬í”Œë¡œìš° êµ¬ì„± - EnhancedLegalQuestionWorkflow ê·¸ëž˜í”„ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©"""
        try:
            # EnhancedLegalQuestionWorkflowì˜ ê·¸ëž˜í”„ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # ì¶”ê°€ ë…¸ë“œë“¤ì€ í•„ìš”ì‹œ ë‚˜ì¤‘ì— í†µí•©
            self.logger.info("Using EnhancedLegalQuestionWorkflow graph as-is")
            self.logger.info(f"Graph nodes: {list(self.workflow_builder.graph.nodes.keys())}")
            self.logger.info(f"Graph edges: {list(self.workflow_builder.graph.edges)}")

        except Exception as e:
            self.logger.error(f"Failed to build integrated workflow: {e}")
            raise

    async def process_query(self, query: str, context: Optional[str] = None,
                          session_id: Optional[str] = None,
                          user_id: Optional[str] = None) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì¸ ë©”ì„œë“œ"""
        start_time = time.time()

        try:
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = self._create_initial_state(
                query, context, session_id, user_id
            )

            self.logger.info(f"ðŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œìž‘")
            self.logger.info(f"ðŸ“Š ì´ˆê¸° ìƒíƒœ í‚¤: {list(initial_state.keys())}")
            self.logger.info(f"ðŸ“Š ì´ˆê¸° ìƒíƒœ ì¿¼ë¦¬: '{initial_state.get('query', 'NOT_FOUND')}'")

            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            result = await self.graph_app.ainvoke(
                initial_state,
                config={"configurable": {"thread_id": session_id}}
            )

            self.logger.info(f"âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ")

            # ê²°ê³¼ ë””ë²„ê¹…
            self.logger.info(f"LangGraph workflow result keys: {list(result.keys())}")
            self.logger.info(f"Generated response: {result.get('generated_response', 'NOT_FOUND')}")
            self.logger.info(f"Answer: {result.get('answer', 'NOT_FOUND')}")
            self.logger.info(f"Response: {result.get('response', 'NOT_FOUND')}")
            self.logger.info(f"Processing steps: {result.get('processing_steps', [])}")
            self.logger.info(f"Errors: {result.get('errors', [])}")

            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_result = self._format_result(result, start_time)

            self.logger.info(f"Query processed successfully in {time.time() - start_time:.2f}s")
            return formatted_result

        except Exception as e:
            self.logger.error(f"Query processing failed: {e}")
            self.logger.error(f"Error type: {type(e).__name__}")
            import traceback
            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            return self._create_error_result(str(e), start_time)

    def _create_initial_state(self, query: str, context: Optional[str],
                            session_id: Optional[str], user_id: Optional[str]) -> Dict[str, Any]:
        """ì´ˆê¸° ìƒíƒœ ìƒì„± (EnhancedLegalQuestionWorkflowì— ë§žê²Œ ë‹¨ìˆœí™”)"""
        self.logger.info(f"Creating initial state with query: '{query}'")

        initial_state = {
            # ê¸°ë³¸ ì¿¼ë¦¬ ì •ë³´
            "query": query,
            "user_query": query,
            "original_query": query,
            "context": context or "",
            "session_id": session_id or f"session_{int(time.time())}",
            "user_id": user_id or f"user_{int(time.time())}",

            # EnhancedLegalQuestionWorkflowì—ì„œ í•„ìš”í•œ ê¸°ë³¸ í•„ë“œë“¤
            "retrieved_docs": [],
            "query_type": None,
            "confidence": 0.0,
            "sources": [],
            "response": "",
            "answer": "",
            "generated_response": "",
            "processing_time": 0.0,
            "processing_steps": [],
            "errors": []
        }

        self.logger.info(f"Initial state created with query: '{initial_state['query']}'")
        return initial_state

    def _format_result(self, result: Dict[str, Any], start_time: float) -> Dict[str, Any]:
        """ê²°ê³¼ í¬ë§·íŒ…"""
        # ì‘ë‹µ í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„: answer > response > generated_response
        response_text = ""
        if result.get("answer"):
            response_text = result["answer"]
            self.logger.info(f"Using 'answer' field: {response_text[:100]}...")
        elif result.get("response"):
            response_text = result["response"]
            self.logger.info(f"Using 'response' field: {response_text[:100]}...")
        elif result.get("generated_response"):
            response_text = result["generated_response"]
            self.logger.info(f"Using 'generated_response' field: {response_text[:100]}...")

        self.logger.info(f"Final response text: {response_text[:100]}..." if response_text else "No response text found")

        return {
            "response": response_text,
            "confidence": result.get("confidence_score", result.get("confidence", 0.3)),  # ê¸°ë³¸ê°’ì„ 0.3ìœ¼ë¡œ ë³€ê²½
            "sources": result.get("retrieved_documents", result.get("retrieved_docs", [])),
            "workflow_steps": result.get("workflow_steps", []),
            "processing_time": time.time() - start_time,
            "session_id": result.get("session_id", ""),
            "user_id": result.get("user_id", ""),
            "quality_metrics": result.get("quality_metrics", {}),
            "error_messages": result.get("error_messages", []),
            "intermediate_results": result.get("intermediate_results", {}),
            "langgraph_enabled": True
        }

    def _create_error_result(self, error_message: str, start_time: float) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        return {
            "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message}",
            "confidence": 0.0,
            "sources": [],
            "workflow_steps": ["error"],
            "processing_time": time.time() - start_time,
            "session_id": "",
            "user_id": "",
            "quality_metrics": {},
            "error_messages": [error_message],
            "intermediate_results": {},
            "langgraph_enabled": True
        }

    # ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ë©”ì„œë“œë“¤

    async def _validate_input(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ìž…ë ¥ ê²€ì¦ ë…¸ë“œ"""
        try:
            self.logger.info("Validating input...")

            validation_result = {
                "is_valid": True,
                "validation_time": time.time(),
                "message_length": len(state["user_query"]),
                "has_context": bool(state["context"]),
                "session_valid": bool(state["session_id"]),
                "user_valid": bool(state["user_id"])
            }

            # ìž…ë ¥ ê²€ì¦ ë¡œì§
            if not state["user_query"] or len(state["user_query"].strip()) == 0:
                validation_result["is_valid"] = False
                validation_result["error"] = "Empty query"

            if len(state["user_query"]) > 10000:
                validation_result["is_valid"] = False
                validation_result["error"] = "Query too long"

            # ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ ì¶”ê°€
            workflow_steps = state.get("workflow_steps", [])
            workflow_steps.append("input_validation")

            return {
                **state,
                "input_validation": validation_result,
                "workflow_steps": workflow_steps
            }

        except Exception as e:
            self.logger.error(f"Input validation failed: {e}")
            return {
                **state,
                "input_validation": {"is_valid": False, "error": str(e)},
                "error_messages": state.get("error_messages", []) + [str(e)]
            }

    async def _enrich_context(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì»¨í…ìŠ¤íŠ¸ ê°•í™” ë…¸ë“œ"""
        try:
            self.logger.info("Enriching context...")

            # ì»¨í…ìŠ¤íŠ¸ ê°•í™” ë¡œì§
            enriched_context = {
                "timestamp": datetime.now().isoformat(),
                "query_type": "legal_inquiry",
                "domain_hints": self._extract_domain_hints(state["user_query"]),
                "complexity_score": self._calculate_complexity(state["user_query"]),
                "context_enhanced": True
            }

            # ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ ì¶”ê°€
            workflow_steps = state.get("workflow_steps", [])
            workflow_steps.append("context_enrichment")

            return {
                **state,
                "enriched_context": enriched_context,
                "workflow_steps": workflow_steps
            }

        except Exception as e:
            self.logger.error(f"Context enrichment failed: {e}")
            return {
                **state,
                "enriched_context": {"error": str(e)},
                "error_messages": state.get("error_messages", []) + [str(e)]
            }

    async def _coordinate_agents(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ë©€í‹° ì—ì´ì „íŠ¸ ì¡°ì • ë…¸ë“œ"""
        try:
            self.logger.info("Coordinating agents...")

            # ì—ì´ì „íŠ¸ ì¡°ì • ë¡œì§
            coordination_result = {
                "research_agent": {"status": "active", "task": "document_retrieval"},
                "analysis_agent": {"status": "active", "task": "legal_analysis"},
                "review_agent": {"status": "standby", "task": "quality_review"},
                "coordination_time": time.time()
            }

            # ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ ì¶”ê°€
            workflow_steps = state.get("workflow_steps", [])
            workflow_steps.append("multi_agent_coordination")

            return {
                **state,
                "agent_coordination": coordination_result,
                "workflow_steps": workflow_steps
            }

        except Exception as e:
            self.logger.error(f"Agent coordination failed: {e}")
            return {
                **state,
                "agent_coordination": {"error": str(e)},
                "error_messages": state.get("error_messages", []) + [str(e)]
            }

    async def _synthesize_response(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì‘ë‹µ í•©ì„± ë…¸ë“œ"""
        try:
            self.logger.info("Synthesizing response...")

            # ì‘ë‹µ í•©ì„± ë¡œì§
            synthesis_result = {
                "synthesis_method": "multi_agent_collaboration",
                "confidence_score": 0.8,
                "response_generated": True,
                "synthesis_time": time.time()
            }

            # ê¸°ë³¸ ì‘ë‹µ ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ë³µìž¡í•œ ë¡œì§)
            generated_response = f"""ì•ˆë…•í•˜ì„¸ìš”! '{state["user_query"]}'ì— ëŒ€í•œ ë²•ë¥  ìƒë‹´ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ì´ ì§ˆë¬¸ì€ LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ í†µí•´ ì²˜ë¦¬ë˜ì—ˆìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¥¼ ê±°ì³¤ìŠµë‹ˆë‹¤:
- ìž…ë ¥ ê²€ì¦: ì™„ë£Œ
- ì»¨í…ìŠ¤íŠ¸ ê°•í™”: ì™„ë£Œ
- ë©€í‹° ì—ì´ì „íŠ¸ ì¡°ì •: ì™„ë£Œ
- ì‘ë‹µ í•©ì„±: ì™„ë£Œ

êµ¬ì²´ì ì¸ ë²•ë¥  ì¡°ì–¸ì´ í•„ìš”í•˜ì‹œë©´ ë” ìžì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."""

            # ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ ì¶”ê°€
            workflow_steps = state.get("workflow_steps", [])
            workflow_steps.append("response_synthesis")

            return {
                **state,
                "synthesis_result": synthesis_result,
                "generated_response": generated_response,
                "confidence_score": synthesis_result["confidence_score"],
                "workflow_steps": workflow_steps
            }

        except Exception as e:
            self.logger.error(f"Response synthesis failed: {e}")
            return {
                **state,
                "synthesis_result": {"error": str(e)},
                "generated_response": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "error_messages": state.get("error_messages", []) + [str(e)]
            }

    async def _assure_quality(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """í’ˆì§ˆ ë³´ì¦ ë…¸ë“œ"""
        try:
            self.logger.info("Assuring quality...")

            # í’ˆì§ˆ ë³´ì¦ ë¡œì§
            quality_metrics = {
                "response_length": len(state.get("generated_response", "")),
                "confidence_score": state.get("confidence_score", 0.0),
                "workflow_completeness": len(state.get("workflow_steps", [])),
                "error_count": len(state.get("error_messages", [])),
                "quality_score": 0.0
            }

            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            if quality_metrics["response_length"] > 50:
                quality_metrics["quality_score"] += 0.3
            if quality_metrics["confidence_score"] > 0.5:
                quality_metrics["quality_score"] += 0.3
            if quality_metrics["workflow_completeness"] >= 4:
                quality_metrics["quality_score"] += 0.2
            if quality_metrics["error_count"] == 0:
                quality_metrics["quality_score"] += 0.2

            quality_assurance_result = {
                "quality_check_passed": quality_metrics["quality_score"] >= 0.7,
                "quality_metrics": quality_metrics,
                "assurance_time": time.time()
            }

            # ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ ì¶”ê°€
            workflow_steps = state.get("workflow_steps", [])
            workflow_steps.append("quality_assurance")

            return {
                **state,
                "quality_assurance_result": quality_assurance_result,
                "quality_metrics": quality_metrics,
                "workflow_steps": workflow_steps
            }

        except Exception as e:
            self.logger.error(f"Quality assurance failed: {e}")
            return {
                **state,
                "quality_assurance_result": {"error": str(e)},
                "error_messages": state.get("error_messages", []) + [str(e)]
            }

    def _extract_domain_hints(self, query: str) -> List[str]:
        """ë„ë©”ì¸ ížŒíŠ¸ ì¶”ì¶œ"""
        domain_keywords = {
            "civil_law": ["ë¯¼ë²•", "ê³„ì•½", "ì†í•´ë°°ìƒ", "ë¶ˆë²•í–‰ìœ„"],
            "criminal_law": ["í˜•ë²•", "ë²”ì£„", "ì²˜ë²Œ", "í˜•ëŸ‰"],
            "family_law": ["ì´í˜¼", "ìƒì†", "ì–‘ìœ¡ê¶Œ", "ì¹œê¶Œ"],
            "commercial_law": ["ìƒë²•", "íšŒì‚¬", "ì£¼ì‹", "ì´ì‚¬"],
            "labor_law": ["ë…¸ë™ë²•", "ê·¼ë¡œ", "ìž„ê¸ˆ", "í•´ê³ "],
            "real_estate": ["ë¶€ë™ì‚°", "ë§¤ë§¤", "ìž„ëŒ€ì°¨", "ë“±ê¸°"]
        }

        hints = []
        query_lower = query.lower()

        for domain, keywords in domain_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                hints.append(domain)

        return hints

    def _calculate_complexity(self, query: str) -> float:
        """ì§ˆë¬¸ ë³µìž¡ë„ ê³„ì‚°"""
        complexity_score = 0.0

        # ê¸¸ì´ ê¸°ë°˜ ë³µìž¡ë„
        if len(query) > 100:
            complexity_score += 0.3
        elif len(query) > 50:
            complexity_score += 0.2
        else:
            complexity_score += 0.1

        # í‚¤ì›Œë“œ ê¸°ë°˜ ë³µìž¡ë„
        complex_keywords = ["ë³µìž¡", "ë‹¤ì–‘", "ì—¬ëŸ¬", "ì—¬ëŸ¬ê°€ì§€", "ë‹¤ì–‘í•œ", "ë³µí•©"]
        if any(keyword in query for keyword in complex_keywords):
            complexity_score += 0.2

        # ì§ˆë¬¸ ê°œìˆ˜ ê¸°ë°˜ ë³µìž¡ë„
        question_marks = query.count("?")
        if question_marks > 2:
            complexity_score += 0.3
        elif question_marks > 1:
            complexity_score += 0.2
        else:
            complexity_score += 0.1

        return min(complexity_score, 1.0)
