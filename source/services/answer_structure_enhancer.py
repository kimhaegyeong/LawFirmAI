# -*- coding: utf-8 -*-
"""
ë‹µë³€ êµ¬ì¡°í™” í–¥ìƒ ì‹œìŠ¤í…œ
ì§ˆë¬¸ ìœ í˜•ë³„ ë§ì¶¤í˜• ë‹µë³€ êµ¬ì¡° í…œí”Œë¦¿ ì ìš©
"""

import logging
import re
from datetime import datetime
from difflib import SequenceMatcher
from enum import Enum
from typing import Any, Dict, List, Optional

try:
    from langchain_community.llms import Ollama
    from langchain_google_genai import ChatGoogleGenerativeAI
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False
    ChatGoogleGenerativeAI = None
    Ollama = None

from .legal_basis_validator import LegalBasisValidator
from .legal_citation_enhancer import LegalCitationEnhancer

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


class QuestionType(Enum):
    """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
    PRECEDENT_SEARCH = "precedent_search"
    LAW_INQUIRY = "law_inquiry"
    LEGAL_ADVICE = "legal_advice"
    PROCEDURE_GUIDE = "procedure_guide"
    TERM_EXPLANATION = "term_explanation"
    GENERAL_QUESTION = "general_question"
    CONTRACT_REVIEW = "contract_review"
    DIVORCE_PROCEDURE = "divorce_procedure"
    INHERITANCE_PROCEDURE = "inheritance_procedure"
    CRIMINAL_CASE = "criminal_case"
    LABOR_DISPUTE = "labor_dispute"


class AnswerStructureEnhancer:
    """ë‹µë³€ êµ¬ì¡°í™” í–¥ìƒ ì‹œìŠ¤í…œ"""

    def __init__(self, llm=None, max_few_shot_examples: int = 2,
                 enable_few_shot: bool = True, enable_cot: bool = True):
        """
        ì´ˆê¸°í™”

        Args:
            llm: LangChain LLM ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìë™ ì´ˆê¸°í™”)
                - Google Gemini ë˜ëŠ” Ollama ì§€ì›
            max_few_shot_examples: Few-Shot ì˜ˆì‹œ ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 2)
                - í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥
            enable_few_shot: Few-Shot ì˜ˆì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
                - Falseë¡œ ì„¤ì • ì‹œ ì˜ˆì‹œ ì„¹ì…˜ ì œì™¸
            enable_cot: Chain-of-Thought ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
                - Falseë¡œ ì„¤ì • ì‹œ ê°„ë‹¨í•œ Step 1,2,3 ê°€ì´ë“œ ì‚¬ìš©

        Raises:
            FileNotFoundError: Few-Shot ì˜ˆì‹œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° (ê²½ê³ ë§Œ ë°œìƒ)

        Note:
            Few-Shot ì˜ˆì‹œëŠ” data/training/few_shot_examples.json íŒŒì¼ì—ì„œ ë¡œë“œë©ë‹ˆë‹¤.
            ìºì‹±ì´ ì ìš©ë˜ì–´ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œ ì‹œ íŒŒì¼ I/Oê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        """
        # ì„¤ì • ì €ì¥
        self.max_few_shot_examples = max_few_shot_examples
        self.enable_few_shot = enable_few_shot
        self.enable_cot = enable_cot

        # í•˜ë“œì½”ë”©ëœ í…œí”Œë¦¿ ë¡œë“œ
        self.structure_templates = self._load_structure_templates()
        self.quality_indicators = self._load_quality_indicators()

        # Few-Shot ì˜ˆì‹œ ë¡œë“œ (ìºì‹± ì ìš©)
        self._few_shot_examples_cache = None
        self.few_shot_examples = self._load_few_shot_examples() if enable_few_shot else {}

        # ë²•ì  ê·¼ê±° ê°•í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.citation_enhancer = LegalCitationEnhancer()
        self.basis_validator = LegalBasisValidator()

        # LLM ì´ˆê¸°í™” (LLM ê¸°ë°˜ êµ¬ì¡°í™”ë¥¼ ìœ„í•´)
        self.llm = llm or self._initialize_llm()
        self.use_llm = LLM_AVAILABLE and self.llm is not None

    def classify_question_type(self, question: str) -> QuestionType:
        """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ (ê°œì„ ëœ í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„)"""
        try:
            question_lower = question.lower()

            # ë²•ì¡°ë¬¸ íŒ¨í„´ ìš°ì„  ì²´í¬
            if re.search(r'ì œ\d+ì¡°|ì œ\d+í•­|ì œ\d+í˜¸', question):
                # ë²•ë ¹ëª…ê³¼ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸
                law_names = ['ë¯¼ë²•', 'í˜•ë²•', 'ê·¼ë¡œê¸°ì¤€ë²•', 'ìƒë²•', 'í–‰ì •ë²•', 'í—Œë²•', 'íŠ¹í—ˆë²•', 'ë¶€ë™ì‚°ë“±ê¸°ë²•']
                if any(law_name in question_lower for law_name in law_names):
                    return QuestionType.LAW_INQUIRY

            # íŒë¡€ ê´€ë ¨ (íŒ¨í„´ ê¸°ë°˜ ê°œì„ )
            if any(keyword in question_lower for keyword in ['íŒë¡€', 'ëŒ€ë²•ì›', 'ê³ ë“±ë²•ì›', 'ì§€ë°©ë²•ì›', 'íŒê²°']):
                # íŒë¡€ ê²€ìƒ‰ íŒ¨í„´
                precedent_patterns = [
                    r'íŒë¡€ë¥¼?\s+ì°¾ì•„ì£¼ì„¸ìš”',           # "íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”"
                    r'ê´€ë ¨\s+íŒë¡€',                   # "ê´€ë ¨ íŒë¡€"
                    r'ìœ ì‚¬í•œ\s+íŒë¡€',                 # "ìœ ì‚¬í•œ íŒë¡€"
                    r'ìµœê·¼\s+íŒë¡€',                   # "ìµœê·¼ íŒë¡€"
                    r'ëŒ€ë²•ì›\s+íŒë¡€',                 # "ëŒ€ë²•ì› íŒë¡€"
                    r'ê³ ë“±ë²•ì›\s+íŒë¡€',               # "ê³ ë“±ë²•ì› íŒë¡€"
                    r'ì§€ë°©ë²•ì›\s+íŒë¡€',               # "ì§€ë°©ë²•ì› íŒë¡€"
                    r'íŒë¡€\s+ê²€ìƒ‰',                   # "íŒë¡€ ê²€ìƒ‰"
                    r'íŒë¡€\s+ì°¾ê¸°',                   # "íŒë¡€ ì°¾ê¸°"
                ]

                if any(re.search(pattern, question_lower) for pattern in precedent_patterns):
                    return QuestionType.PRECEDENT_SEARCH

                return QuestionType.PRECEDENT_SEARCH

            # ì´í˜¼ ê´€ë ¨ (íŒ¨í„´ ê¸°ë°˜ ê°œì„ )
            if any(keyword in question_lower for keyword in ['ì´í˜¼', 'í˜‘ì˜ì´í˜¼', 'ì¬íŒì´í˜¼', 'ì´í˜¼ì ˆì°¨']):
                # ì´í˜¼ ì ˆì°¨ íŒ¨í„´
                divorce_patterns = [
                    r'ì´í˜¼\s+ì ˆì°¨',                   # "ì´í˜¼ ì ˆì°¨"
                    r'ì´í˜¼\s+ë°©ë²•',                   # "ì´í˜¼ ë°©ë²•"
                    r'ì´í˜¼\s+ì‹ ì²­',                   # "ì´í˜¼ ì‹ ì²­"
                    r'í˜‘ì˜ì´í˜¼\s+ì ˆì°¨',               # "í˜‘ì˜ì´í˜¼ ì ˆì°¨"
                    r'ì¬íŒì´í˜¼\s+ì ˆì°¨',               # "ì¬íŒì´í˜¼ ì ˆì°¨"
                    r'ì´í˜¼\s+ì–´ë–»ê²Œ',                 # "ì´í˜¼ ì–´ë–»ê²Œ"
                    r'ì´í˜¼\s+ì–´ë””ì„œ',                 # "ì´í˜¼ ì–´ë””ì„œ"
                    r'ì´í˜¼\s+ë¹„ìš©',                   # "ì´í˜¼ ë¹„ìš©"
                ]

                if any(re.search(pattern, question_lower) for pattern in divorce_patterns):
                    return QuestionType.DIVORCE_PROCEDURE

                return QuestionType.DIVORCE_PROCEDURE

            # ìƒì† ê´€ë ¨ (íŒ¨í„´ ê¸°ë°˜ ê°œì„ )
            if any(keyword in question_lower for keyword in ['ìƒì†', 'ìœ ì‚°', 'ìƒì†ì¸', 'ìƒì†ì„¸', 'ìœ ì–¸', 'ìƒì†í¬ê¸°']):
                # ìƒì† ì ˆì°¨ íŒ¨í„´
                inheritance_patterns = [
                    r'ìƒì†\s+ì ˆì°¨',                   # "ìƒì† ì ˆì°¨"
                    r'ìƒì†\s+ì‹ ì²­',                   # "ìƒì† ì‹ ì²­"
                    r'ìƒì†\s+ë°©ë²•',                   # "ìƒì† ë°©ë²•"
                    r'ìœ ì‚°\s+ë¶„í• ',                   # "ìœ ì‚° ë¶„í• "
                    r'ìƒì†ì¸\s+í™•ì¸',                 # "ìƒì†ì¸ í™•ì¸"
                    r'ìƒì†ì„¸\s+ì‹ ê³ ',                 # "ìƒì†ì„¸ ì‹ ê³ "
                    r'ìœ ì–¸\s+ê²€ì¸',                   # "ìœ ì–¸ ê²€ì¸"
                    r'ìƒì†í¬ê¸°\s+ì ˆì°¨',               # "ìƒì†í¬ê¸° ì ˆì°¨"
                ]

                if any(re.search(pattern, question_lower) for pattern in inheritance_patterns):
                    return QuestionType.INHERITANCE_PROCEDURE

                return QuestionType.INHERITANCE_PROCEDURE

            # í˜•ì‚¬ ê´€ë ¨ (íŒ¨í„´ ê¸°ë°˜ ê°œì„ )
            if any(keyword in question_lower for keyword in ['ì‚¬ê¸°', 'ì ˆë„', 'ê°•ë„', 'ì‚´ì¸', 'í˜•ì‚¬', 'ë²”ì£„', 'êµ¬ì„±ìš”ê±´']):
                # í˜•ì‚¬ ì‚¬ê±´ íŒ¨í„´
                criminal_patterns = [
                    r'\w+ì£„\s+êµ¬ì„±ìš”ê±´',              # "ì‚¬ê¸°ì£„ êµ¬ì„±ìš”ê±´"
                    r'\w+ì£„\s+ì²˜ë²Œ',                  # "ì‚¬ê¸°ì£„ ì²˜ë²Œ"
                    r'\w+ì£„\s+í˜•ëŸ‰',                  # "ì‚¬ê¸°ì£„ í˜•ëŸ‰"
                    r'\w+ë²”ì£„\s+ì²˜ë²Œ',                # "ì ˆë„ë²”ì£„ ì²˜ë²Œ"
                    r'í˜•ì‚¬\s+ì‚¬ê±´',                  # "í˜•ì‚¬ ì‚¬ê±´"
                    r'ë²”ì£„\s+êµ¬ì„±ìš”ê±´',               # "ë²”ì£„ êµ¬ì„±ìš”ê±´"
                    r'\w+ì‚¬ê±´\s+ëŒ€ì‘',                # "ì‚¬ê¸°ì‚¬ê±´ ëŒ€ì‘"
                ]

                if any(re.search(pattern, question_lower) for pattern in criminal_patterns):
                    return QuestionType.CRIMINAL_CASE

                return QuestionType.CRIMINAL_CASE

            # ë…¸ë™ ê´€ë ¨ (íŒ¨í„´ ê¸°ë°˜ ê°œì„ )
            if any(keyword in question_lower for keyword in ['ë…¸ë™', 'ê·¼ë¡œ', 'ì„ê¸ˆ', 'í•´ê³ ', 'ë¶€ë‹¹í•´ê³ ', 'ì„ê¸ˆì²´ë¶ˆ', 'ê·¼ë¡œì‹œê°„', 'ë…¸ë™ìœ„ì›íšŒ']):
                # ë…¸ë™ ë¶„ìŸ íŒ¨í„´
                labor_patterns = [
                    r'ë…¸ë™\s+ë¶„ìŸ',                   # "ë…¸ë™ ë¶„ìŸ"
                    r'ê·¼ë¡œ\s+ë¶„ìŸ',                   # "ê·¼ë¡œ ë¶„ìŸ"
                    r'ì„ê¸ˆ\s+ì²´ë¶ˆ',                   # "ì„ê¸ˆ ì²´ë¶ˆ"
                    r'ë¶€ë‹¹í•´ê³ \s+êµ¬ì œ',               # "ë¶€ë‹¹í•´ê³  êµ¬ì œ"
                    r'í•´ê³ \s+ëŒ€ì‘',                   # "í•´ê³  ëŒ€ì‘"
                    r'ê·¼ë¡œì‹œê°„\s+ê·œì •',               # "ê·¼ë¡œì‹œê°„ ê·œì •"
                    r'ë…¸ë™ìœ„ì›íšŒ\s+ì‹ ì²­',             # "ë…¸ë™ìœ„ì›íšŒ ì‹ ì²­"
                    r'ì„ê¸ˆ\s+ì§€ê¸‰',                   # "ì„ê¸ˆ ì§€ê¸‰"
                ]

                if any(re.search(pattern, question_lower) for pattern in labor_patterns):
                    return QuestionType.LABOR_DISPUTE

                return QuestionType.LABOR_DISPUTE

            # ë²•ë¥  ìš©ì–´ ì„¤ëª… ê´€ë ¨ (íŒ¨í„´ ê¸°ë°˜ ê°œì„ )
            if any(keyword in question_lower for keyword in ['ì˜ë¯¸', 'ì •ì˜', 'ê°œë…', 'ì„¤ëª…', 'ë¬´ì—‡', 'ëœ»']):
                # ìš©ì–´ ì„¤ëª… íŒ¨í„´ ê°ì§€
                term_patterns = [
                    r'ë¬´ì—‡ì´\s+\w+ì¸ê°€ìš”?',           # "ë¬´ì—‡ì´ ê³„ì•½ì¸ê°€ìš”?"
                    r'ë¬´ì—‡ì´\s+\w+ì¸ê°€\?',           # "ë¬´ì—‡ì´ ê³„ì•½ì¸ê°€?"
                    r'\w+ì˜\s+ì˜ë¯¸ëŠ”?',              # "ê³„ì•½ì˜ ì˜ë¯¸ëŠ”?"
                    r'\w+ì˜\s+ì •ì˜ëŠ”?',              # "ê³„ì•½ì˜ ì •ì˜ëŠ”?"
                    r'\w+ì˜\s+ê°œë…ì€?',              # "ê³„ì•½ì˜ ê°œë…ì€?"
                    r'\w+ì´\s+ë¬´ì—‡ì¸ê°€ìš”?',          # "ê³„ì•½ì´ ë¬´ì—‡ì¸ê°€ìš”?"
                    r'\w+ì´\s+ë¬´ì—‡ì¸ê°€\?',           # "ê³„ì•½ì´ ë¬´ì—‡ì¸ê°€?"
                    r'\w+ë€\s+ë¬´ì—‡ì¸ê°€ìš”?',          # "ê³„ì•½ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
                    r'\w+ë€\s+ë¬´ì—‡ì¸ê°€\?',           # "ê³„ì•½ì´ë€ ë¬´ì—‡ì¸ê°€?"
                    r'\w+ì˜\s+ëœ»ì€?',               # "ê³„ì•½ì˜ ëœ»ì€?"
                    r'\w+ì´\s+ëœ»í•˜ëŠ”\s+ë°”ëŠ”?',       # "ê³„ì•½ì´ ëœ»í•˜ëŠ” ë°”ëŠ”?"
                    r'\w+ì˜\s+ë‚´ìš©ì€?',             # "ê³„ì•½ì˜ ë‚´ìš©ì€?" (ìš©ì–´ ì„¤ëª…)
                    r'\w+ì´\s+ì–´ë–¤\s+ê²ƒì¸ê°€ìš”?',     # "ê³„ì•½ì´ ì–´ë–¤ ê²ƒì¸ê°€ìš”?"
                    r'\w+ì´\s+ì–´ë–¤\s+ê²ƒì¸ê°€\?',      # "ê³„ì•½ì´ ì–´ë–¤ ê²ƒì¸ê°€?"
                    r'\w+ì—\s+ëŒ€í•´\s+ì„¤ëª…í•´ì£¼ì„¸ìš”',   # "ê³„ì•½ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
                    r'\w+ì—\s+ëŒ€í•œ\s+ì„¤ëª…',          # "ê³„ì•½ì— ëŒ€í•œ ì„¤ëª…"
                    r'\w+ì´\s+ë¬´ì—‡ì„\s+ì˜ë¯¸í•˜ë‚˜ìš”?', # "ê³„ì•½ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”?"
                    r'\w+ì´\s+ë¬´ì—‡ì„\s+ì˜ë¯¸í•˜ë‚˜\?',  # "ê³„ì•½ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜?"
                    r'\w+ì´\s+ì–´ë–¤\s+ê²ƒì¸ê°€ìš”?',     # "ê³„ì•½ì´ ì–´ë–¤ ê²ƒì¸ê°€ìš”?" (ì¤‘ë³µ ì œê±°)
                    r'\w+ì´\s+ì–´ë–¤\s+ê²ƒì¸ê°€\?',      # "ê³„ì•½ì´ ì–´ë–¤ ê²ƒì¸ê°€?" (ì¤‘ë³µ ì œê±°)
                ]

                # ìš©ì–´ ì„¤ëª… íŒ¨í„´ ë§¤ì¹­
                if any(re.search(pattern, question_lower) for pattern in term_patterns):
                    return QuestionType.TERM_EXPLANATION

                # ê³„ì•½ì„œ ê²€í†  ì˜ë„ê°€ ëª…í™•í•œ ê²½ìš° (êµ¬ì²´ì  í–‰ë™ í‚¤ì›Œë“œ)
                contract_action_keywords = [
                    'ê³„ì•½ì„œ', 'ì¡°í•­', 'ê²€í† ', 'ìˆ˜ì •', 'ë¶ˆë¦¬í•œ', 'ì‘ì„±', 'ì²´ê²°',
                    'ì„œëª…', 'ê³„ì•½ì„œë¥¼', 'ê³„ì•½ì„œì—', 'ê³„ì•½ì„œì˜', 'ê³„ì•½ì„œê°€',
                    'ê³„ì•½ ì¡°ê±´', 'ê³„ì•½ ì¡°í•­', 'ê³„ì•½ì„œ ê²€í† ',
                    'ê³„ì•½ì„œ ì‘ì„±', 'ê³„ì•½ì„œ ìˆ˜ì •', 'ê³„ì•½ì„œ ì²´ê²°'
                ]

                # "ê³„ì•½ì˜ ë‚´ìš©ì€?" ê°™ì€ ìš©ì–´ ì„¤ëª…ì€ ì œì™¸
                if any(keyword in question_lower for keyword in contract_action_keywords):
                    # ìš©ì–´ ì„¤ëª… íŒ¨í„´ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê³„ì•½ì„œ ê²€í† ë¡œ ë¶„ë¥˜
                    if not any(re.search(pattern, question_lower) for pattern in [
                        r'\w+ì˜\s+ë‚´ìš©ì€?',  # "ê³„ì•½ì˜ ë‚´ìš©ì€?"
                        r'\w+ì´\s+ì–´ë–¤\s+ê²ƒì¸ê°€ìš”?',  # "ê³„ì•½ì´ ì–´ë–¤ ê²ƒì¸ê°€ìš”?"
                    ]):
                        return QuestionType.CONTRACT_REVIEW

                return QuestionType.TERM_EXPLANATION

            # ê³„ì•½ì„œ ê²€í†  ê´€ë ¨ (íŒ¨í„´ ê¸°ë°˜ ê°œì„ )
            if any(keyword in question_lower for keyword in ['ê³„ì•½ì„œ', 'ê³„ì•½', 'ì¡°í•­', 'ê²€í† ', 'ìˆ˜ì •', 'ë¶ˆë¦¬í•œ']):
                # íŒë¡€ í‚¤ì›Œë“œê°€ í•¨ê»˜ ìˆìœ¼ë©´ íŒë¡€ ê²€ìƒ‰ ìš°ì„ 
                if any(keyword in question_lower for keyword in ['íŒë¡€', 'ëŒ€ë²•ì›', 'ê³ ë“±ë²•ì›', 'ì§€ë°©ë²•ì›', 'íŒê²°']):
                    return QuestionType.PRECEDENT_SEARCH

                # ê³„ì•½ì„œ ê²€í†  íŒ¨í„´
                contract_patterns = [
                    r'ê³„ì•½ì„œë¥¼?\s+ê²€í† ',               # "ê³„ì•½ì„œë¥¼ ê²€í† "
                    r'ê³„ì•½ì„œ\s+ê²€í† ',                 # "ê³„ì•½ì„œ ê²€í† "
                    r'ê³„ì•½ì„œë¥¼?\s+ìˆ˜ì •',               # "ê³„ì•½ì„œë¥¼ ìˆ˜ì •"
                    r'ê³„ì•½ì„œ\s+ìˆ˜ì •',                 # "ê³„ì•½ì„œ ìˆ˜ì •"
                    r'ê³„ì•½ì„œë¥¼?\s+ì‘ì„±',               # "ê³„ì•½ì„œë¥¼ ì‘ì„±"
                    r'ê³„ì•½ì„œ\s+ì‘ì„±',                 # "ê³„ì•½ì„œ ì‘ì„±"
                    r'ê³„ì•½ì„œë¥¼?\s+ì²´ê²°',               # "ê³„ì•½ì„œë¥¼ ì²´ê²°"
                    r'ê³„ì•½ì„œ\s+ì²´ê²°',                 # "ê³„ì•½ì„œ ì²´ê²°"
                    r'ê³„ì•½\s+ì¡°í•­',                   # "ê³„ì•½ ì¡°í•­"
                    r'ê³„ì•½\s+ì¡°ê±´',                   # "ê³„ì•½ ì¡°ê±´"
                    r'ê³„ì•½\s+ë‚´ìš©',                   # "ê³„ì•½ ë‚´ìš©"
                    r'ë¶ˆë¦¬í•œ\s+ì¡°í•­',                 # "ë¶ˆë¦¬í•œ ì¡°í•­"
                    r'ê³„ì•½ì„œì˜?\s+ë¬¸ì œì ',             # "ê³„ì•½ì„œì˜ ë¬¸ì œì "
                    r'ê³„ì•½ì„œë¥¼?\s+í™•ì¸',               # "ê³„ì•½ì„œë¥¼ í™•ì¸"
                ]

                if any(re.search(pattern, question_lower) for pattern in contract_patterns):
                    return QuestionType.CONTRACT_REVIEW

                return QuestionType.CONTRACT_REVIEW

            # ë²•ë¥  ìë¬¸ ê´€ë ¨ (íŒ¨í„´ ê¸°ë°˜ ê°œì„ )
            if any(keyword in question_lower for keyword in ['ëŒ€ì‘', 'ê¶Œë¦¬', 'ì˜ë¬´', 'êµ¬ì œ', 'ìƒë‹´', 'ìë¬¸', 'í•´ì•¼', 'ì¡°ì–¸', 'ë„ì›€', 'ì§€ì›']):
                # ë²•ë¥  ìë¬¸ íŒ¨í„´
                advice_patterns = [
                    r'ì–´ë–»ê²Œ\s+ëŒ€ì‘í•´ì•¼',               # "ì–´ë–»ê²Œ ëŒ€ì‘í•´ì•¼"
                    r'ì–´ë–»ê²Œ\s+í•´ì•¼',                  # "ì–´ë–»ê²Œ í•´ì•¼"
                    r'ê¶Œë¦¬\s+êµ¬ì œ',                    # "ê¶Œë¦¬ êµ¬ì œ"
                    r'ì˜ë¬´\s+ì´í–‰',                    # "ì˜ë¬´ ì´í–‰"
                    r'ë²•ë¥ \s+ìƒë‹´',                    # "ë²•ë¥  ìƒë‹´"
                    r'ë²•ë¥ \s+ìë¬¸',                    # "ë²•ë¥  ìë¬¸"
                    r'ë³€í˜¸ì‚¬\s+ìƒë‹´',                  # "ë³€í˜¸ì‚¬ ìƒë‹´"
                    r'ë³€í˜¸ì‚¬\s+ìë¬¸',                  # "ë³€í˜¸ì‚¬ ìë¬¸"
                    r'ë²•ì \s+ëŒ€ì‘',                    # "ë²•ì  ëŒ€ì‘"
                    r'ë²•ì \s+ì¡°ì–¸',                    # "ë²•ì  ì¡°ì–¸"
                    r'ë²•ì \s+ìƒë‹´',                    # "ë²•ì  ìƒë‹´"
                    r'ë²•ì \s+ìë¬¸',                    # "ë²•ì  ìë¬¸"
                    r'ë²•ì \s+ë„ì›€',                    # "ë²•ì  ë„ì›€"
                    r'ë²•ì \s+ì§€ì›',                    # "ë²•ì  ì§€ì›"
                    r'ë²•ì \s+ë³´í˜¸',                    # "ë²•ì  ë³´í˜¸"
                    r'ë²•ì \s+í•´ê²°',                    # "ë²•ì  í•´ê²°"
                    r'ë²•ì \s+êµ¬ì œ',                    # "ë²•ì  êµ¬ì œ"
                    r'í•´ì•¼\s+í• \s+ì¼',                 # "í•´ì•¼ í•  ì¼"
                    r'ì–´ë–¤\s+ì¡°ì¹˜',                   # "ì–´ë–¤ ì¡°ì¹˜"
                    r'ì–´ë–¤\s+ë°©ë²•',                   # "ì–´ë–¤ ë°©ë²•"
                    r'ì¡°ì–¸ì„?\s+êµ¬í•˜ê³ ',               # "ì¡°ì–¸ì„ êµ¬í•˜ê³ "
                    r'ë„ì›€ì´?\s+í•„ìš”',                 # "ë„ì›€ì´ í•„ìš”"
                    r'ì§€ì›ì„?\s+ë°›ê³ ',                 # "ì§€ì›ì„ ë°›ê³ "
                    r'ìƒë‹´ì„?\s+ë°›ê³ ',                 # "ìƒë‹´ì„ ë°›ê³ "
                    r'ìë¬¸ì„?\s+ë°›ê³ ',                 # "ìë¬¸ì„ ë°›ê³ "
                ]

                if any(re.search(pattern, question_lower) for pattern in advice_patterns):
                    return QuestionType.LEGAL_ADVICE

                # ì ˆì°¨ ì•ˆë‚´ í‚¤ì›Œë“œì™€ ì¶©ëŒí•˜ëŠ” ê²½ìš° ë²•ë¥  ìë¬¸ ìš°ì„ 
                if any(keyword in question_lower for keyword in ['ì–´ë–»ê²Œ', 'ë°©ë²•']):
                    # êµ¬ì²´ì ì¸ ë²•ë¥  ìë¬¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë²•ë¥  ìë¬¸ ìš°ì„ 
                    if any(keyword in question_lower for keyword in ['ëŒ€ì‘', 'ê¶Œë¦¬', 'ì˜ë¬´', 'êµ¬ì œ', 'ìƒë‹´', 'ìë¬¸', 'ì¡°ì–¸', 'ë„ì›€', 'ì§€ì›']):
                        return QuestionType.LEGAL_ADVICE

                return QuestionType.LEGAL_ADVICE

            # ì ˆì°¨ ì•ˆë‚´ ê´€ë ¨ (íŒ¨í„´ ê¸°ë°˜ ê°œì„ )
            if any(keyword in question_lower for keyword in ['ì ˆì°¨', 'ì‹ ì²­', 'ì†Œì•¡ì‚¬ê±´', 'ë¯¼ì‚¬ì¡°ì •', 'ì†Œì†¡']):
                # ì ˆì°¨ ì•ˆë‚´ íŒ¨í„´
                procedure_patterns = [
                    r'\w+\s+ì ˆì°¨',                    # "ì†Œì†¡ ì ˆì°¨"
                    r'\w+\s+ì‹ ì²­',                    # "ì†Œì†¡ ì‹ ì²­"
                    r'\w+\s+ë°©ë²•',                    # "ì†Œì†¡ ë°©ë²•"
                    r'ì†Œì•¡ì‚¬ê±´\s+ì ˆì°¨',               # "ì†Œì•¡ì‚¬ê±´ ì ˆì°¨"
                    r'ë¯¼ì‚¬ì¡°ì •\s+ì‹ ì²­',               # "ë¯¼ì‚¬ì¡°ì • ì‹ ì²­"
                    r'ì†Œì†¡\s+ì œê¸°',                    # "ì†Œì†¡ ì œê¸°"
                    r'ì–´ë–»ê²Œ\s+ì‹ ì²­',                 # "ì–´ë–»ê²Œ ì‹ ì²­"
                    r'ì–´ë””ì„œ\s+ì‹ ì²­',                 # "ì–´ë””ì„œ ì‹ ì²­"
                    r'ì‹ ì²­\s+ë°©ë²•',                   # "ì‹ ì²­ ë°©ë²•"
                    r'ì‹ ì²­\s+ì ˆì°¨',                   # "ì‹ ì²­ ì ˆì°¨"
                    r'ì²˜ë¦¬\s+ì ˆì°¨',                   # "ì²˜ë¦¬ ì ˆì°¨"
                    r'ì§„í–‰\s+ì ˆì°¨',                   # "ì§„í–‰ ì ˆì°¨"
                ]

                if any(re.search(pattern, question_lower) for pattern in procedure_patterns):
                    return QuestionType.PROCEDURE_GUIDE

                return QuestionType.PROCEDURE_GUIDE

            # ì¼ë°˜ì ì¸ ë°©ë²•/ì–´ë–»ê²Œ ì§ˆë¬¸ (ë§ˆì§€ë§‰ì— ì²´í¬)
            if any(keyword in question_lower for keyword in ['ì–´ë–»ê²Œ', 'ë°©ë²•', 'í•´ì•¼']):
                # ë‹¤ë¥¸ êµ¬ì²´ì  í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì ˆì°¨ ì•ˆë‚´
                return QuestionType.PROCEDURE_GUIDE

            # ì¼ë°˜ì ì¸ ë„ì›€ ìš”ì²­ (êµ¬ì²´ì  ë²•ë¥  í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°)
            if any(keyword in question_lower for keyword in ['ë„ì›€', 'ì§€ì›', 'í•„ìš”']):
                # êµ¬ì²´ì ì¸ ë²•ë¥  í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì§ˆë¬¸
                if not any(keyword in question_lower for keyword in ['ë²•ë¥ ', 'ë²•ì ', 'ë³€í˜¸ì‚¬', 'ìƒë‹´', 'ìë¬¸', 'ì¡°ì–¸', 'ëŒ€ì‘', 'ê¶Œë¦¬', 'ì˜ë¬´', 'êµ¬ì œ']):
                    return QuestionType.GENERAL_QUESTION

            # ê¸°ë³¸ê°’
            return QuestionType.GENERAL_QUESTION

        except Exception as e:
            logger.error(f"Error in classify_question_type: {e}", exc_info=True)
            return QuestionType.GENERAL_QUESTION

    def _load_structure_templates(self) -> Dict[QuestionType, Dict[str, Any]]:
        """êµ¬ì¡° í…œí”Œë¦¿ ë¡œë“œ"""
        try:
            templates = {}

            # íŒë¡€ ê²€ìƒ‰ í…œí”Œë¦¿
            templates[QuestionType.PRECEDENT_SEARCH] = {
                "title": "íŒë¡€ ê²€ìƒ‰ ê²°ê³¼",
                "sections": [
                    {"name": "ê´€ë ¨ íŒë¡€", "priority": "high", "template": "ë‹¤ìŒê³¼ ê°™ì€ ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:", "content_guide": "íŒë¡€ ë²ˆí˜¸, ì‚¬ê±´ëª…, í•µì‹¬ íŒê²°ìš”ì§€ í¬í•¨", "legal_citations": True},
                    {"name": "íŒë¡€ ë¶„ì„", "priority": "high", "template": "í•´ë‹¹ íŒë¡€ì˜ ì£¼ìš” ìŸì ê³¼ ë²•ì›ì˜ íŒë‹¨:", "content_guide": "ë²•ë¦¬ì  ë¶„ì„ê³¼ ì‹¤ë¬´ì  ì‹œì‚¬ì "},
                    {"name": "ì ìš© ê°€ëŠ¥ì„±", "priority": "medium", "template": "ê·€í•˜ì˜ ì‚¬ì•ˆì—ì˜ ì ìš© ê°€ëŠ¥ì„±:", "content_guide": "ìœ ì‚¬ì ê³¼ ì°¨ì´ì  ë¶„ì„"},
                    {"name": "ì‹¤ë¬´ ì¡°ì–¸", "priority": "medium", "template": "ì‹¤ë¬´ì  ê¶Œì¥ì‚¬í•­:", "content_guide": "êµ¬ì²´ì  í–‰ë™ ë°©ì•ˆ"}
                ]
            }

            # ë²•ë ¹ ë¬¸ì˜ í…œí”Œë¦¿
            templates[QuestionType.LAW_INQUIRY] = {
                "title": "ë²•ë¥  ë¬¸ì˜ ë‹µë³€",
                "sections": [
                    {"name": "ê´€ë ¨ ë²•ë ¹", "priority": "high", "template": "ê´€ë ¨ ë²•ë ¹:", "content_guide": "ì •í™•í•œ ì¡°ë¬¸ ë²ˆí˜¸ì™€ ë‚´ìš©", "legal_citations": True},
                    {"name": "ë²•ë ¹ í•´ì„¤", "priority": "high", "template": "ë²•ë ¹ í•´ì„¤:", "content_guide": "ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì„¤ëª…"},
                    {"name": "ì ìš© ì‚¬ë¡€", "priority": "medium", "template": "ì‹¤ì œ ì ìš© ì‚¬ë¡€:", "content_guide": "êµ¬ì²´ì  ì˜ˆì‹œì™€ ì„¤ëª…"},
                    {"name": "ì£¼ì˜ì‚¬í•­", "priority": "medium", "template": "ì£¼ì˜ì‚¬í•­:", "content_guide": "ë²•ì  ë¦¬ìŠ¤í¬ì™€ ì œí•œì‚¬í•­"}
                ]
            }

            # ë²•ë¥  ìƒë‹´ í…œí”Œë¦¿
            templates[QuestionType.LEGAL_ADVICE] = {
                "title": "ë²•ë¥  ìƒë‹´ ë‹µë³€",
                "sections": [
                    {"name": "ìƒí™© ì •ë¦¬", "priority": "high", "template": "ë§ì”€í•˜ì‹  ìƒí™©ì„ ì •ë¦¬í•˜ë©´:", "content_guide": "í•µì‹¬ ì‚¬ì‹¤ ê´€ê³„ ì •ë¦¬"},
                    {"name": "ë²•ì  ë¶„ì„", "priority": "high", "template": "ë²•ì  ë¶„ì„:", "content_guide": "ì ìš© ë²•ë¥ ê³¼ ë²•ë¦¬ ë¶„ì„", "legal_citations": True},
                    {"name": "ê¶Œë¦¬ êµ¬ì œ ë°©ë²•", "priority": "high", "template": "ê¶Œë¦¬ êµ¬ì œ ë°©ë²•:", "content_guide": "ë‹¨ê³„ë³„ êµ¬ì²´ì  ë°©ì•ˆ"},
                    {"name": "í•„ìš” ì¦ê±°", "priority": "medium", "template": "í•„ìš”í•œ ì¦ê±° ìë£Œ:", "content_guide": "êµ¬ì²´ì  ì¦ê±° ëª©ë¡"},
                    {"name": "ì „ë¬¸ê°€ ìƒë‹´", "priority": "low", "template": "ì „ë¬¸ê°€ ìƒë‹´ ê¶Œìœ :", "content_guide": "ë³€í˜¸ì‚¬ ìƒë‹´ í•„ìš”ì„±"}
                ]
            }

            # ì ˆì°¨ ì•ˆë‚´ í…œí”Œë¦¿
            templates[QuestionType.PROCEDURE_GUIDE] = {
                "title": "ì ˆì°¨ ì•ˆë‚´",
                "sections": [
                    {"name": "ì ˆì°¨ ê°œìš”", "priority": "high", "template": "ì „ì²´ ì ˆì°¨ ê°œìš”:", "content_guide": "ì ˆì°¨ì˜ ì „ì²´ì ì¸ íë¦„"},
                    {"name": "ë‹¨ê³„ë³„ ì ˆì°¨", "priority": "high", "template": "ë‹¨ê³„ë³„ ì ˆì°¨:", "content_guide": "êµ¬ì²´ì  ë‹¨ê³„ë³„ ì„¤ëª…"},
                    {"name": "í•„ìš” ì„œë¥˜", "priority": "high", "template": "í•„ìš”í•œ ì„œë¥˜:", "content_guide": "êµ¬ì²´ì  ì„œë¥˜ ëª©ë¡"},
                    {"name": "ì²˜ë¦¬ ê¸°ê°„", "priority": "medium", "template": "ì²˜ë¦¬ ê¸°ê°„ ë° ë¹„ìš©:", "content_guide": "ì˜ˆìƒ ì†Œìš”ì‹œê°„ê³¼ ë¹„ìš©"},
                    {"name": "ì£¼ì˜ì‚¬í•­", "priority": "medium", "template": "ì£¼ì˜ì‚¬í•­:", "content_guide": "ì ˆì°¨ ì§„í–‰ ì‹œ ì£¼ì˜í•  ì "}
                ]
            }

            # ìš©ì–´ í•´ì„¤ í…œí”Œë¦¿
            templates[QuestionType.TERM_EXPLANATION] = {
                "title": "ë²•ë¥  ìš©ì–´ í•´ì„¤",
                "sections": [
                    {"name": "ìš©ì–´ ì •ì˜", "priority": "high", "template": "ìš©ì–´ ì •ì˜:", "content_guide": "ì •í™•í•œ ë²•ë¥ ì  ì •ì˜"},
                    {"name": "ë²•ì  ê·¼ê±°", "priority": "high", "template": "ë²•ì  ê·¼ê±°:", "content_guide": "ê´€ë ¨ ë²•ì¡°ë¬¸ê³¼ íŒë¡€", "legal_citations": True},
                    {"name": "ì‹¤ì œ ì ìš©", "priority": "medium", "template": "ì‹¤ì œ ì ìš© ì‚¬ë¡€:", "content_guide": "êµ¬ì²´ì  ì ìš© ì˜ˆì‹œ"},
                    {"name": "ê´€ë ¨ ìš©ì–´", "priority": "low", "template": "ê´€ë ¨ ìš©ì–´:", "content_guide": "ë¹„ìŠ·í•˜ê±°ë‚˜ ê´€ë ¨ëœ ìš©ì–´ë“¤"}
                ]
            }

            # ê³„ì•½ì„œ ê²€í†  í…œí”Œë¦¿
            templates[QuestionType.CONTRACT_REVIEW] = {
                "title": "ê³„ì•½ì„œ ê²€í†  ê²°ê³¼",
                "sections": [
                    {"name": "ê³„ì•½ì„œ ë¶„ì„", "priority": "high", "template": "ê³„ì•½ì„œ ì£¼ìš” ë‚´ìš© ë¶„ì„:", "content_guide": "ê³„ì•½ì˜ í•µì‹¬ ì¡°í•­ ë¶„ì„"},
                    {"name": "ë²•ì  ê²€í† ", "priority": "high", "template": "ë²•ì  ê²€í†  ê²°ê³¼:", "content_guide": "ë²•ì  ìœ íš¨ì„±ê³¼ ë¬¸ì œì ", "legal_citations": True},
                    {"name": "ì£¼ì˜ì‚¬í•­", "priority": "high", "template": "ì£¼ì˜í•´ì•¼ í•  ì‚¬í•­:", "content_guide": "ë¶ˆë¦¬í•œ ì¡°í•­ê³¼ ë¦¬ìŠ¤í¬"},
                    {"name": "ê°œì„  ì œì•ˆ", "priority": "medium", "template": "ê°œì„  ì œì•ˆ:", "content_guide": "êµ¬ì²´ì  ìˆ˜ì • ê¶Œì¥ì‚¬í•­"}
                ]
            }

            # ì´í˜¼ ì ˆì°¨ í…œí”Œë¦¿
            templates[QuestionType.DIVORCE_PROCEDURE] = {
                "title": "ì´í˜¼ ì ˆì°¨ ì•ˆë‚´",
                "sections": [
                    {"name": "ì´í˜¼ ë°©ë²•", "priority": "high", "template": "ì´í˜¼ ë°©ë²• ì„ íƒ:", "content_guide": "í˜‘ì˜ì´í˜¼, ì¡°ì •ì´í˜¼, ì¬íŒì´í˜¼ ë¹„êµ"},
                    {"name": "ì ˆì°¨ ë‹¨ê³„", "priority": "high", "template": "êµ¬ì²´ì  ì ˆì°¨:", "content_guide": "ë‹¨ê³„ë³„ ìƒì„¸ ì ˆì°¨"},
                    {"name": "í•„ìš” ì„œë¥˜", "priority": "high", "template": "í•„ìš”í•œ ì„œë¥˜:", "content_guide": "êµ¬ì²´ì  ì„œë¥˜ ëª©ë¡"},
                    {"name": "ì¬ì‚°ë¶„í• ", "priority": "medium", "template": "ì¬ì‚°ë¶„í•  ë° ìœ„ìë£Œ:", "content_guide": "ì¬ì‚°ë¶„í•  ê¸°ì¤€ê³¼ ìœ„ìë£Œ ì‚°ì •"},
                    {"name": "ì–‘ìœ¡ê¶Œ", "priority": "medium", "template": "ì–‘ìœ¡ê¶Œ ë° ë©´ì ‘êµì„­ê¶Œ:", "content_guide": "ìë…€ ì–‘ìœ¡ ê´€ë ¨ ì‚¬í•­"}
                ]
            }

            # ìƒì† ì ˆì°¨ í…œí”Œë¦¿
            templates[QuestionType.INHERITANCE_PROCEDURE] = {
                "title": "ìƒì† ì ˆì°¨ ì•ˆë‚´",
                "sections": [
                    {"name": "ìƒì†ì¸ í™•ì¸", "priority": "high", "template": "ìƒì†ì¸ ë° ìƒì†ë¶„:", "content_guide": "ë²•ì •ìƒì†ì¸ê³¼ ìƒì†ë¶„ ê³„ì‚°"},
                    {"name": "ìƒì† ì ˆì°¨", "priority": "high", "template": "ìƒì† ì ˆì°¨:", "content_guide": "ë‹¨ê³„ë³„ ìƒì† ì ˆì°¨"},
                    {"name": "í•„ìš” ì„œë¥˜", "priority": "high", "template": "í•„ìš”í•œ ì„œë¥˜:", "content_guide": "ìƒì† ê´€ë ¨ ì„œë¥˜ ëª©ë¡"},
                    {"name": "ì„¸ê¸ˆ ë¬¸ì œ", "priority": "medium", "template": "ìƒì†ì„¸ ë° ì¦ì—¬ì„¸:", "content_guide": "ì„¸ê¸ˆ ê´€ë ¨ ì£¼ì˜ì‚¬í•­"},
                    {"name": "ìœ ì–¸ ê²€ì¸", "priority": "low", "template": "ìœ ì–¸ ê²€ì¸ ì ˆì°¨:", "content_guide": "ìœ ì–¸ì´ ìˆëŠ” ê²½ìš° ì ˆì°¨"}
                ]
            }

            # í˜•ì‚¬ ì‚¬ê±´ í…œí”Œë¦¿
            templates[QuestionType.CRIMINAL_CASE] = {
                "title": "í˜•ì‚¬ ì‚¬ê±´ ì•ˆë‚´",
                "sections": [
                    {"name": "ë²”ì£„ ë¶„ì„", "priority": "high", "template": "í•´ë‹¹ ë²”ì£„ì˜ êµ¬ì„±ìš”ê±´:", "content_guide": "ë²”ì£„ ì„±ë¦½ìš”ê±´ ë¶„ì„", "legal_citations": True},
                    {"name": "ë²•ì •í˜•", "priority": "high", "template": "ë²•ì •í˜• ë° í˜•ëŸ‰:", "content_guide": "ì²˜ë²Œ ê¸°ì¤€ê³¼ í˜•ëŸ‰"},
                    {"name": "ìˆ˜ì‚¬ ì ˆì°¨", "priority": "medium", "template": "ìˆ˜ì‚¬ ë° ì¬íŒ ì ˆì°¨:", "content_guide": "ìˆ˜ì‚¬ë¶€í„° ì¬íŒê¹Œì§€ ì ˆì°¨"},
                    {"name": "ë³€í˜¸ì¸ ì¡°ë ¥", "priority": "high", "template": "ë³€í˜¸ì¸ ì¡°ë ¥ê¶Œ:", "content_guide": "ë³€í˜¸ì¸ ì„ ì„ê³¼ ì¡°ë ¥ê¶Œ"},
                    {"name": "êµ¬ì œ ë°©ë²•", "priority": "medium", "template": "ê¶Œë¦¬ êµ¬ì œ ë°©ë²•:", "content_guide": "í•­ì†Œ, ìƒê³  ë“± êµ¬ì œ ì ˆì°¨"}
                ]
            }

            # ë…¸ë™ ë¶„ìŸ í…œí”Œë¦¿
            templates[QuestionType.LABOR_DISPUTE] = {
                "title": "ë…¸ë™ ë¶„ìŸ ì•ˆë‚´",
                "sections": [
                    {"name": "ë¶„ìŸ ë¶„ì„", "priority": "high", "template": "ë…¸ë™ ë¶„ìŸ ë¶„ì„:", "content_guide": "ë¶„ìŸì˜ ì„±ê²©ê³¼ ìŸì "},
                    {"name": "ì ìš© ë²•ë ¹", "priority": "high", "template": "ì ìš© ë²•ë ¹:", "content_guide": "ê·¼ë¡œê¸°ì¤€ë²• ë“± ê´€ë ¨ ë²•ë ¹", "legal_citations": True},
                    {"name": "êµ¬ì œ ì ˆì°¨", "priority": "high", "template": "êµ¬ì œ ì ˆì°¨:", "content_guide": "ë…¸ë™ìœ„ì›íšŒ, ë²•ì› ì ˆì°¨"},
                    {"name": "í•„ìš” ì¦ê±°", "priority": "medium", "template": "í•„ìš”í•œ ì¦ê±°:", "content_guide": "ì„ê¸ˆëŒ€ì¥, ê·¼ë¡œê³„ì•½ì„œ ë“±"},
                    {"name": "ì‹œíš¨ ë¬¸ì œ", "priority": "medium", "template": "ì‹œíš¨ ë° ì œí•œ:", "content_guide": "ì‹ ì²­ ê¸°í•œê³¼ ì œí•œì‚¬í•­"}
                ]
            }

            # ì¼ë°˜ ì§ˆë¬¸ í…œí”Œë¦¿
            templates[QuestionType.GENERAL_QUESTION] = {
                "title": "ë²•ë¥  ì§ˆë¬¸ ë‹µë³€",
                "sections": [
                    {"name": "ì§ˆë¬¸ ë¶„ì„", "priority": "high", "template": "ì§ˆë¬¸ ë‚´ìš© ë¶„ì„:", "content_guide": "ì§ˆë¬¸ì˜ í•µì‹¬ íŒŒì•…"},
                    {"name": "ê´€ë ¨ ë²•ë ¹", "priority": "high", "template": "ê´€ë ¨ ë²•ë ¹:", "content_guide": "ì ìš© ê°€ëŠ¥í•œ ë²•ë ¹", "legal_citations": True},
                    {"name": "ë²•ì  í•´ì„¤", "priority": "medium", "template": "ë²•ì  í•´ì„¤:", "content_guide": "ì‰¬ìš´ ë§ë¡œ ì„¤ëª…"},
                    {"name": "ì‹¤ë¬´ ì¡°ì–¸", "priority": "medium", "template": "ì‹¤ë¬´ì  ì¡°ì–¸:", "content_guide": "êµ¬ì²´ì  í–‰ë™ ë°©ì•ˆ"}
                ]
            }

            return templates

        except Exception as e:
            logger.error(f"Failed to load templates: {e}", exc_info=True)
            return self._get_fallback_templates()

    def _get_fallback_templates(self) -> Dict[QuestionType, Dict[str, Any]]:
        """í´ë°± í…œí”Œë¦¿ ìƒì„±"""
        return {
            QuestionType.GENERAL_QUESTION: {
                "title": "ë²•ë¥  ì§ˆë¬¸ ë‹µë³€",
                "sections": [
                    {
                        "name": "ì§ˆë¬¸ ë¶„ì„",
                        "priority": "high",
                        "template": "ì§ˆë¬¸ ë‚´ìš© ë¶„ì„:",
                        "content_guide": "ì§ˆë¬¸ì˜ í•µì‹¬ íŒŒì•…"
                    },
                    {
                        "name": "ê´€ë ¨ ë²•ë ¹",
                        "priority": "high",
                        "template": "ê´€ë ¨ ë²•ë ¹:",
                        "content_guide": "ì ìš© ê°€ëŠ¥í•œ ë²•ë ¹"
                    },
                    {
                        "name": "ë²•ì  í•´ì„¤",
                        "priority": "medium",
                        "template": "ë²•ì  í•´ì„¤:",
                        "content_guide": "ì‰¬ìš´ ë§ë¡œ ì„¤ëª…"
                    },
                    {
                        "name": "ì‹¤ë¬´ ì¡°ì–¸",
                        "priority": "medium",
                        "template": "ì‹¤ë¬´ì  ì¡°ì–¸:",
                        "content_guide": "êµ¬ì²´ì  í–‰ë™ ë°©ì•ˆ"
                    }
                ]
            }
        }

    def _load_few_shot_examples(self) -> Dict[str, List[Dict[str, Any]]]:
        """
        Few-Shot ì˜ˆì‹œ ë°ì´í„° ë¡œë“œ (ìºì‹± ì ìš©)

        Returns:
            Dict[str, List[Dict[str, Any]]]: ì§ˆë¬¸ ìœ í˜•ë³„ Few-Shot ì˜ˆì‹œ ë°ì´í„°
        """
        # ìºì‹œ í™•ì¸
        if hasattr(self, '_few_shot_examples_cache') and self._few_shot_examples_cache is not None:
            return self._few_shot_examples_cache

        import json
        import os

        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        current_dir = os.path.dirname(os.path.abspath(__file__))
        examples_file = os.path.join(
            current_dir,
            '..',
            '..',
            'data',
            'training',
            'few_shot_examples.json'
        )

        try:
            if os.path.exists(examples_file):
                with open(examples_file, 'r', encoding='utf-8') as f:
                    examples = json.load(f)
                    # ìºì‹œì— ì €ì¥
                    self._few_shot_examples_cache = examples
                    logger.debug(f"Few-shot examples loaded and cached: {len(examples)} question types")
                    return examples
            else:
                # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
                logger.warning(f"Few-shot examples file not found: {examples_file}")
                return {}
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
            logger.warning(f"Failed to load few-shot examples: {e}", exc_info=True)
            return {}

    def _get_few_shot_examples(self, question_type: QuestionType, question: str = "") -> List[Dict[str, Any]]:
        """
        ì§ˆë¬¸ ìœ í˜•ë³„ Few-Shot ì˜ˆì‹œ ë°˜í™˜ (ê²€ì¦ ë° ìœ ì‚¬ë„ ê¸°ë°˜ ì„ íƒ í¬í•¨)

        Args:
            question_type: ì§ˆë¬¸ ìœ í˜• (QuestionType enum)
            question: ì§ˆë¬¸ í…ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ê³„ì‚°ìš©, ì„ íƒì )
                - ì œê³µë˜ë©´ ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì˜ˆì‹œë¥¼ ìš°ì„  ì„ íƒ
                - ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ìˆœì„œëŒ€ë¡œ ë°˜í™˜

        Returns:
            List[Dict[str, Any]]: ì§ˆë¬¸ ìœ í˜•ë³„ Few-Shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
                - ê²€ì¦ í†µê³¼í•œ ì˜ˆì‹œë§Œ í¬í•¨
                - ìµœëŒ€ ê°œìˆ˜: max_few_shot_examples ì„¤ì •ê°’
                - ì§ˆë¬¸ì´ ì œê³µëœ ê²½ìš° ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬

        Note:
            - ê²€ì¦ ì‹¤íŒ¨í•œ ì˜ˆì‹œëŠ” ì œì™¸ë˜ê³  ê²½ê³  ë¡œê¹…
            - ìœ ì‚¬ë„ëŠ” Jaccard ìœ ì‚¬ë„(ë‹¨ì–´ ê¸°ë°˜)ë¡œ ê³„ì‚°
        """
        if not hasattr(self, 'few_shot_examples') or not self.few_shot_examples:
            return []

        # ì§ˆë¬¸ ìœ í˜•ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        question_type_str = question_type.value if isinstance(question_type, QuestionType) else str(question_type)

        # í•´ë‹¹ ì§ˆë¬¸ ìœ í˜•ì˜ ì˜ˆì‹œ ê°€ì ¸ì˜¤ê¸°
        examples = self.few_shot_examples.get(question_type_str, [])

        # ê²€ì¦ í†µê³¼í•œ ì˜ˆì‹œë§Œ í•„í„°ë§ (í’ˆì§ˆ ë©”íŠ¸ë¦­ í¬í•¨)
        valid_examples = []
        for ex in examples:
            if hasattr(self, '_validate_few_shot_example') and self._validate_few_shot_example(ex):
                valid_examples.append(ex)
            elif not hasattr(self, '_validate_few_shot_example'):
                # ê²€ì¦ ë©”ì„œë“œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²€ì¦ë§Œ ìˆ˜í–‰
                if all(key in ex for key in ['question', 'original_answer', 'enhanced_answer', 'improvements']):
                    valid_examples.append(ex)

        # ê²€ì¦ ì‹¤íŒ¨í•œ ì˜ˆì‹œê°€ ìˆìœ¼ë©´ ê²½ê³ 
        if len(valid_examples) < len(examples):
            invalid_count = len(examples) - len(valid_examples)
            logger.warning(f"{question_type_str}: {invalid_count}ê°œ ì˜ˆì‹œê°€ ê²€ì¦ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # ì§ˆë¬¸ì´ ì œê³µë˜ê³  ì˜ˆì‹œê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ìœ ì‚¬ë„ ê¸°ë°˜ ì •ë ¬ ì‹œë„
        if question and len(valid_examples) > 1:
            try:
                if hasattr(self, '_sort_examples_by_similarity'):
                    valid_examples = self._sort_examples_by_similarity(valid_examples, question)
            except Exception as e:
                logger.debug(f"Failed to sort examples by similarity: {e}")

        # ì„¤ì •ëœ ìµœëŒ€ ê°œìˆ˜ê¹Œì§€ë§Œ ë°˜í™˜ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ)
        max_examples = getattr(self, 'max_few_shot_examples', 2)
        return valid_examples[:max_examples]

    def _load_quality_indicators(self) -> Dict[str, List[str]]:
        """í’ˆì§ˆ ì§€í‘œ ë¡œë“œ"""
        return self._get_fallback_quality_indicators()

    def _get_fallback_quality_indicators(self) -> Dict[str, List[str]]:
        """í´ë°± í’ˆì§ˆ ì§€í‘œ"""
        return {
            "legal_accuracy": [
                "ë²•ë ¹", "ì¡°ë¬¸", "ì¡°í•­", "í•­ëª©", "ë²•ì›", "íŒë¡€", "ëŒ€ë²•ì›", "í•˜ê¸‰ì‹¬"
            ],
            "practical_guidance": [
                "êµ¬ì²´ì ", "ì‹¤í–‰", "ë‹¨ê³„ë³„", "ì ˆì°¨", "ë°©ë²•", "ì¡°ì¹˜", "ê¶Œì¥", "ê³ ë ¤"
            ],
            "structure_quality": [
                "##", "###", "**", "1.", "2.", "3.", "â€¢", "-", "ì²«ì§¸", "ë‘˜ì§¸", "ì…‹ì§¸"
            ],
            "completeness": [
                "ë”°ë¼ì„œ", "ê²°ë¡ ì ìœ¼ë¡œ", "ìš”ì•½í•˜ë©´", "ì¢…í•©í•˜ë©´", "íŒë‹¨ì»¨ëŒ€"
            ],
            "risk_management": [
                "ì£¼ì˜", "ì£¼ì˜ì‚¬í•­", "ë¦¬ìŠ¤í¬", "ì œí•œ", "í•œê³„", "ì „ë¬¸ê°€", "ìƒë‹´"
            ]
        }

    def enhance_answer_structure(self, answer: str, question_type: str,
                               question: str = "", domain: str = "general",
                               retrieved_docs: Optional[List[Dict[str, Any]]] = None,
                               legal_references: Optional[List[str]] = None,
                               legal_citations: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """ë‹µë³€ êµ¬ì¡°í™” í–¥ìƒ (ì•ˆì „í•œ ë²„ì „) - ë²•ì  ê·¼ê±° ì •ë³´ í¬í•¨"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not answer or not isinstance(answer, str):
                return {"error": "Invalid answer input"}

            # ë²•ì  ê·¼ê±° ì •ë³´ ì¤€ë¹„ (None ì²´í¬ ë° íƒ€ì… ì•ˆì „ì„± ë³´ì¥)
            retrieved_docs = retrieved_docs if retrieved_docs is not None else []
            legal_references = legal_references if legal_references is not None else []
            legal_citations = legal_citations if legal_citations is not None else []

            # íƒ€ì… ê²€ì¦
            if not isinstance(retrieved_docs, list):
                retrieved_docs = []
            if not isinstance(legal_references, list):
                legal_references = []
            if not isinstance(legal_citations, list):
                legal_citations = []

            # ì§ˆë¬¸ ìœ í˜• ë§¤í•‘
            mapped_question_type = self._map_question_type(question_type, question)

            # LLM ê¸°ë°˜ êµ¬ì¡°í™” ì‹œë„ (ê¶Œì¥)
            if self.use_llm:
                try:
                    return self._enhance_with_llm(
                        answer, question, mapped_question_type,
                        retrieved_docs, legal_references, legal_citations
                    )
                except Exception as e:
                    logger.warning(f"LLM ê¸°ë°˜ êµ¬ì¡°í™” ì‹¤íŒ¨, í…œí”Œë¦¿ ë°©ì‹ìœ¼ë¡œ í´ë°±: {e}", exc_info=True)
                    # í´ë°±: í…œí”Œë¦¿ ê¸°ë°˜ êµ¬ì¡°í™”

            # í…œí”Œë¦¿ ê¸°ë°˜ êµ¬ì¡°í™” (í´ë°±)
            return self._enhance_with_template(
                answer, mapped_question_type, question,
                retrieved_docs, legal_references, legal_citations
            )

        except Exception as e:
            logger.error(f"ë‹µë³€ êµ¬ì¡°í™” í–¥ìƒ ì‹¤íŒ¨: {e}", exc_info=True)
            return {"error": str(e)}

    def _map_question_type(self, question_type: any, question: str) -> QuestionType:
        """ì§ˆë¬¸ ìœ í˜• ë§¤í•‘"""
        try:
            # ëª…ì‹œì  ì§ˆë¬¸ ìœ í˜• ì²˜ë¦¬
            explicit_result = self._handle_explicit_question_type(question_type)
            if explicit_result != QuestionType.GENERAL_QUESTION:
                return explicit_result

            # ì§ˆë¬¸ êµ¬ì¡° ë¶„ì„
            structure_result = self._analyze_question_structure(question)
            if structure_result != QuestionType.GENERAL_QUESTION:
                return structure_result

            # ìµœì¢… í´ë°±
            return QuestionType.GENERAL_QUESTION

        except Exception as e:
            logger.warning(f"Question type mapping failed: {e}", exc_info=True)
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            return self._map_question_type_fallback(question_type, question)

    def _handle_explicit_question_type(self, question_type: any) -> QuestionType:
        """ëª…ì‹œì  ì§ˆë¬¸ ìœ í˜• ì²˜ë¦¬"""
        # QuestionType enumì¸ ê²½ìš° value ì‚¬ìš©
        if isinstance(question_type, QuestionType):
            return question_type

        # ë¬¸ìì—´ì¸ ê²½ìš° ì²˜ë¦¬
        if not question_type or (isinstance(question_type, str) and question_type.lower() == "general"):
            return QuestionType.GENERAL_QUESTION

        # ëª…ì‹œì  ë§¤í•‘
        explicit_mapping = {
            'precedent_search': QuestionType.PRECEDENT_SEARCH,
            'contract_review': QuestionType.CONTRACT_REVIEW,
            'divorce_procedure': QuestionType.DIVORCE_PROCEDURE,
            'inheritance_procedure': QuestionType.INHERITANCE_PROCEDURE,
            'criminal_case': QuestionType.CRIMINAL_CASE,
            'labor_dispute': QuestionType.LABOR_DISPUTE,
            'procedure_guide': QuestionType.PROCEDURE_GUIDE,
            'term_explanation': QuestionType.TERM_EXPLANATION,
            'legal_advice': QuestionType.LEGAL_ADVICE,
            'law_inquiry': QuestionType.LAW_INQUIRY,
            'general_question': QuestionType.GENERAL_QUESTION
        }

        # ë¬¸ìì—´ ë³€í™˜
        if isinstance(question_type, str):
            return explicit_mapping.get(question_type.lower(), QuestionType.GENERAL_QUESTION)

            return QuestionType.GENERAL_QUESTION

    def _initialize_llm(self):
        """LLM ì´ˆê¸°í™”"""
        if not LLM_AVAILABLE:
            return None

        try:
            # LangGraphConfigì—ì„œ LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„ (ìƒëŒ€/ì ˆëŒ€ ê²½ë¡œ ëª¨ë‘ ì§€ì›)
            try:
                from source.utils.langgraph_config import LangGraphConfig
            except ImportError:
                try:
                    from ...utils.langgraph_config import LangGraphConfig
                except ImportError:
                    # ìµœì¢… í´ë°±: sys.pathë¥¼ ì´ìš©í•œ ë™ì  import
                    import os
                    import sys
                    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
                    if project_root not in sys.path:
                        sys.path.insert(0, project_root)
                    from source.utils.langgraph_config import LangGraphConfig

            config = LangGraphConfig.from_env()

            if config.llm_provider == "google" and ChatGoogleGenerativeAI:
                try:
                    llm = ChatGoogleGenerativeAI(
                        model=config.google_model or "gemini-2.5-flash-lite",
                        temperature=0.3,
                        max_output_tokens=4000,
                        timeout=30,
                        api_key=config.google_api_key
                    )
                    logger.info(f"LLM initialized: Google Gemini ({config.google_model})")
                    return llm
                except Exception as e:
                    logger.error(f"Failed to initialize Google Gemini: {e}", exc_info=True)

            if config.llm_provider == "ollama" and Ollama:
                try:
                    llm = Ollama(
                        model=config.ollama_model or "llama2",
                        base_url=config.ollama_base_url or "http://localhost:11434",
                        temperature=0.3,
                        num_predict=4000,
                        timeout=30
                    )
                    logger.info(f"LLM initialized: Ollama ({config.ollama_model})")
                    return llm
                except Exception as e:
                    logger.error(f"Failed to initialize Ollama: {e}", exc_info=True)

        except Exception as e:
            logger.error(f"LLM initialization error: {e}", exc_info=True)

        return None

    def _enhance_with_llm(
        self,
        answer: str,
        question: str,
        question_type: QuestionType,
        retrieved_docs: List[Dict[str, Any]],
        legal_references: List[str],
        legal_citations: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """LLMì„ í™œìš©í•œ êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±"""

        # None ì²´í¬ ë° íƒ€ì… ì•ˆì „ì„± ë³´ì¥
        retrieved_docs = retrieved_docs if retrieved_docs is not None else []
        legal_references = legal_references if legal_references is not None else []
        legal_citations = legal_citations if legal_citations is not None else []

        if not isinstance(retrieved_docs, list):
            retrieved_docs = []
        if not isinstance(legal_references, list):
            legal_references = []
        if not isinstance(legal_citations, list):
            legal_citations = []

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_llm_enhancement_prompt(
            answer, question, question_type,
            retrieved_docs, legal_references, legal_citations
        )

        # LLM í˜¸ì¶œ
        try:
            response = self.llm.invoke(prompt)
            # content ì†ì„±ì´ ìˆìœ¼ë©´ ì‚¬ìš© (ChatModel), ì—†ìœ¼ë©´ ì§ì ‘ ë¬¸ìì—´ ë³€í™˜ (BaseLLM)
            structured_answer = response.content if hasattr(response, 'content') else str(response)
        except Exception:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ì¬ì‹œë„
            structured_answer = str(self.llm.invoke(prompt))

        # LLM ì‘ë‹µ í›„ì²˜ë¦¬ - ì›ë³¸ ë‚´ìš© ë³´ì¡´ ê²€ì¦
        structured_answer = self._post_process_llm_response(
            structured_answer, answer, question_type
        )

        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        quality_metrics = self._calculate_quality_metrics(structured_answer)

        return {
            "original_answer": answer,
            "structured_answer": structured_answer,
            "question_type": question_type.value,
            "template_used": "LLM ê¸°ë°˜ êµ¬ì¡°í™”",
            "method": "llm_based",
            "analysis": {
                "has_title": bool(re.search(r'^#+\s+', structured_answer, re.MULTILINE)),
                "section_count": len(re.findall(r'^###\s+', structured_answer, re.MULTILINE))
            },
            "improvements": [],
            "quality_metrics": quality_metrics,
            "enhancement_timestamp": datetime.now().isoformat()
        }

    def _build_llm_enhancement_prompt(
        self,
        answer: str,
        question: str,
        question_type: QuestionType,
        retrieved_docs: List[Dict[str, Any]],
        legal_references: List[str],
        legal_citations: List[Dict[str, Any]]
    ) -> str:
        """LLM êµ¬ì¡°í™”ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê°œì„ ëœ ë²„ì „)"""

        # None ì²´í¬
        retrieved_docs = retrieved_docs if retrieved_docs is not None else []
        legal_references = legal_references if legal_references is not None else []
        legal_citations = legal_citations if legal_citations is not None else []

        # í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸° (êµ¬ì¡° ê°€ì´ë“œìš©)
        template = self.structure_templates.get(
            question_type,
            self.structure_templates[QuestionType.GENERAL_QUESTION]
        )

        # ë²•ì  ë¬¸ì„œ í¬ë§·íŒ…
        legal_docs_text = self._format_docs_for_prompt(retrieved_docs)

        # ì›ë³¸ ë‹µë³€ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ë‚´ìš© ë³´ì¡´ í™•ì¸ìš©)
        answer_keywords = set()
        if answer:
            keywords = re.findall(r'[\wê°€-í£]{2,}', answer.lower())
            # ë²•ë¥  ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„  ì¶”ì¶œ
            legal_keywords = [kw for kw in keywords if any(term in kw for term in ['ë²•', 'ì¡°', 'í•­', 'íŒë¡€', 'ë²•ì›', 'íŒê²°', 'ì†Œì†¡', 'ê³„ì•½', 'ê¶Œë¦¬', 'ì˜ë¬´'])]
            answer_keywords.update(legal_keywords[:15])
            answer_keywords.update(keywords[:30])  # ì¼ë°˜ í‚¤ì›Œë“œë„ ì¶”ê°€

        keywords_preview = ", ".join(list(answer_keywords)[:10]) if answer_keywords else "ì—†ìŒ"

        # Few-Shot ì˜ˆì‹œ ì„¹ì…˜ ìƒì„± (Phase 1.1: ì„ íƒì  í¬í•¨ - term_explanationì¼ ë•Œë§Œ)
        few_shot_examples_section = ""
        if self.enable_few_shot and question_type == QuestionType.TERM_EXPLANATION:
            few_shot_examples = self._get_few_shot_examples(question_type, question)
            # Phase 1.1: ìµœëŒ€ 1ê°œë§Œ í¬í•¨ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì¶•ì†Œ)
            if few_shot_examples:
                example = few_shot_examples[0]  # ì²« ë²ˆì§¸ ì˜ˆì‹œë§Œ ì‚¬ìš©
                few_shot_examples_section = "\n## ğŸ“š ê°œì„  ì˜ˆì‹œ (ì°¸ê³ ìš©)\n\n"
                few_shot_examples_section += f"""**ì§ˆë¬¸**: {example.get('question', '')}

**ì›ë³¸ ë‹µë³€**: {example.get('original_answer', '')[:200]}...

**ê°œì„ ëœ ë‹µë³€**: {example.get('enhanced_answer', '')[:200]}...

**ì£¼ìš” ê°œì„  ì‚¬í•­**: {', '.join(example.get('improvements', [])[:3])}
"""

        # Chain-of-Thought ì„¹ì…˜ ìƒì„± (ê°œì„  ë°©ì•ˆ 1: ì›ë³¸ í’ˆì§ˆ í‰ê°€ ë‹¨ê³„ ì¶”ê°€)
        chain_of_thought_section = """## ğŸ“ ì‘ì—… ê°€ì´ë“œ

**Step 0: ì›ë³¸ í’ˆì§ˆ í‰ê°€ (í•„ìˆ˜ - ë¨¼ì € ìˆ˜í–‰)**
ì›ë³¸ ë‹µë³€ì„ í‰ê°€í•˜ê³  ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
- [ ] ë²•ì  ì •ë³´ê°€ ì¶©ë¶„í•˜ê³  ì •í™•í•œê°€? (ë²•ì¡°ë¬¸, íŒë¡€, í•´ì„¤)
- [ ] êµ¬ì¡°ê°€ ëª…í™•í•˜ê³  ì½ê¸° ì‰¬ìš´ê°€?
- [ ] ì–´íˆ¬ê°€ ì „ë¬¸ì ì´ê³  ì¼ê´€ëœê°€?
- [ ] êµ¬ì²´ì  ì˜ˆì‹œì™€ ì‹¤ë¬´ ì¡°ì–¸ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?

**í‰ê°€ ê²°ê³¼ì— ë”°ë¥¸ ì‘ì—…**:
- **ì›ë³¸ì´ ì´ë¯¸ ìš°ìˆ˜í•˜ë©´** â†’ ìµœì†Œí•œì˜ í˜•ì‹ ì •ë¦¬ë§Œ ìˆ˜í–‰ (ì¸ì‚¬ë§ ì œê±°, ë¶ˆí•„ìš”í•œ ë°˜ë³µ í†µí•©)
- **ì›ë³¸ì— ê°œì„ ì´ í•„ìš”í•˜ë©´** â†’ ì•„ë˜ ì›ì¹™ì„ ì ìš©í•˜ì—¬ í–¥ìƒ

**Step 1: ì›ë³¸ ì •ë³´ í™•ì¸** (ê°œì„ ì´ í•„ìš”í•œ ê²½ìš°)
- ë²•ì¡°ë¬¸ ë²ˆí˜¸ ë° ë‚´ìš© í™•ì¸
- íŒë¡€ ì •ë³´ í™•ì¸
- ë²•ì  í•´ì„¤ ë° ì‹¤ë¬´ ì¡°ì–¸ í™•ì¸
- êµ¬ì²´ì  ì˜ˆì‹œ í™•ì¸

**Step 2: ê°œì„  ì „ëµ** (ê°œì„ ì´ í•„ìš”í•œ ê²½ìš°)
- ì¸ì‚¬ë§ ì œê±°
- ë¶ˆí•„ìš”í•œ ë°˜ë³µ í†µí•©
- ì–´íˆ¬ í†µì¼ (ì „ë¬¸ì  ì–´ì¡°)

**Step 3: ë‹µë³€ ì‘ì„±**
- ìœ„ í•µì‹¬ ì›ì¹™ì„ ì¤€ìˆ˜í•˜ë©° ë°”ë¡œ í–¥ìƒëœ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”
- ì¶”ë¡  ê³¼ì •ì€ ì‘ì„±í•˜ì§€ ë§ê³  ë°”ë¡œ ë‹µë³€ì„ ì‹œì‘í•˜ì„¸ìš”

"""

        prompt = f"""ë‹¹ì‹ ì€ ë²•ë¥  ë‹µë³€ í’ˆì§ˆ í–¥ìƒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‹µë³€ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ë˜, ì›ë³¸ì˜ ëª¨ë“  ë²•ì  ì •ë³´ì™€ ìƒì„¸í•œ ì„¤ëª…ì„ ë³´ì¡´í•˜ì„¸ìš”.

## ğŸ¯ STEP 0: ì›ë³¸ í’ˆì§ˆ í‰ê°€ (í•„ìˆ˜ - ë¨¼ì € ìˆ˜í–‰)

ë¨¼ì € ì›ë³¸ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”:
- [ ] ë²•ì  ì •ë³´ê°€ ì¶©ë¶„í•˜ê³  ì •í™•í•œê°€? (ë²•ì¡°ë¬¸, íŒë¡€, í•´ì„¤)
- [ ] êµ¬ì¡°ê°€ ëª…í™•í•˜ê³  ì½ê¸° ì‰¬ìš´ê°€?
- [ ] ì–´íˆ¬ê°€ ì „ë¬¸ì ì´ê³  ì¼ê´€ëœê°€?
- [ ] êµ¬ì²´ì  ì˜ˆì‹œì™€ ì‹¤ë¬´ ì¡°ì–¸ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?

**í‰ê°€ ê²°ê³¼ì— ë”°ë¥¸ ì‘ì—…**:
- **ì›ë³¸ì´ ì´ë¯¸ ìš°ìˆ˜í•˜ë©´** â†’ ìµœì†Œí•œì˜ í˜•ì‹ ì •ë¦¬ë§Œ ìˆ˜í–‰í•˜ì„¸ìš” (ì¸ì‚¬ë§ ì œê±°, ë¶ˆí•„ìš”í•œ ë°˜ë³µ í†µí•©)
- **ì›ë³¸ì— ê°œì„ ì´ í•„ìš”í•˜ë©´** â†’ ì•„ë˜ ì›ì¹™ì„ ì ìš©í•˜ì„¸ìš”

## ğŸ¯ í•µì‹¬ ì›ì¹™ (í‰ê°€ í›„ ì ìš©)

1. **ì •ë³´ ë³´ì¡´ ìš°ì„ **: ëª¨ë“  ë²•ì  ì •ë³´(ë²•ì¡°ë¬¸ ë²ˆí˜¸ ë° ë‚´ìš©, íŒë¡€ ì •ë³´, ë²•ì  í•´ì„¤, ì‹¤ë¬´ ì¡°ì–¸, êµ¬ì²´ì  ì˜ˆì‹œ)ë¥¼ ì •í™•íˆ ë³´ì¡´í•˜ì„¸ìš”. ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ê°„ì†Œí™”í•˜ì§€ ë§ˆì„¸ìš”.

2. **ìµœì†Œ ì¹¨ìŠµ ì›ì¹™**: ì›ë³¸ êµ¬ì¡°, ì„¤ëª… ë°©ì‹, ì˜ˆì‹œë¥¼ ìµœëŒ€í•œ ì¡´ì¤‘í•˜ì„¸ìš”. êµ¬ì¡°ê°€ ëª…í™•í•˜ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , ì„¹ì…˜ ì œëª©ì„ í•¨ë¶€ë¡œ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

3. **í˜•ì‹ ê°œì„ ë§Œ**: ì¸ì‚¬ë§ ì œê±°, ë¶ˆí•„ìš”í•œ ë°˜ë³µ í†µí•©, ì–´íˆ¬ í†µì¼(ì „ë¬¸ì  ì–´ì¡°)ë§Œ ìˆ˜í–‰í•˜ì„¸ìš”.

ì›ë³¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ í™•ì¸: {keywords_preview}

{few_shot_examples_section}

{chain_of_thought_section}

## ğŸ“ ì§ˆë¬¸ ì •ë³´

**ì§ˆë¬¸**: {question}
**ì§ˆë¬¸ ìœ í˜•**: {question_type.value}

## ğŸ“„ ì›ë³¸ ë‹µë³€

{answer}

## ğŸ“‹ êµ¬ì¡° ê°€ì´ë“œ (ì°¸ê³ ìš© - ì›ë³¸ êµ¬ì¡° ì¡´ì¤‘ ìš°ì„ )

**ì¤‘ìš”**: ì›ë³¸ ë‹µë³€ì´ ì´ë¯¸ ì˜ êµ¬ì¡°í™”ë˜ì–´ ìˆìœ¼ë©´ ì´ ê°€ì´ë“œë¥¼ ë”°ë¥´ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.

**ì œëª©**: {template.get('title', 'ë²•ë¥  ì§ˆë¬¸ ë‹µë³€')}

**ì„¹ì…˜ êµ¬ì„± (ì°¸ê³ ìš©)**: êµ¬ì¡°ê°€ ë¶ˆëª…í™•í•œ ê²½ìš°ì—ë§Œ ì°¸ê³ í•˜ì„¸ìš”.

"""

        # í…œí”Œë¦¿ ì„¹ì…˜ ì •ë³´ ì¶”ê°€ (ë” ìœ ì—°í•˜ê²Œ)
        sections = template.get('sections', [])
        priority_order = {'high': 1, 'medium': 2, 'low': 3}
        sorted_sections = sorted(sections, key=lambda x: priority_order.get(x.get('priority', 'medium'), 2))

        for i, section in enumerate(sorted_sections, 1):
            priority_marker = {'high': 'ê¶Œì¥', 'medium': 'ì°¸ê³ ', 'low': 'ì„ íƒ'}.get(
                section.get('priority', 'medium'), 'ì°¸ê³ '
            )
            prompt += f"""
{i}. [{priority_marker}] `### {section['name']}`: {section.get('content_guide', '')}
   (ì›ë³¸ì— í•´ë‹¹ ë‚´ìš©ì´ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€)
"""
            if section.get('legal_citations'):
                prompt += "   ë²•ì  ê·¼ê±°ëŠ” ì„¤ëª… ë¬¸ì¥ ë°”ë¡œ ë‹¤ìŒì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨\n"

        # ë²•ì  ë¬¸ì„œ ì •ë³´ (ìˆëŠ” ê²½ìš° - ë³´ì™„ìš©)
        if legal_docs_text and legal_docs_text.strip() != "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.":
            prompt += f"""

## ğŸ” ì°¸ê³ : ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ (ë³´ì™„ìš©)

{legal_docs_text}

**ì‚¬ìš© ê·œì¹™**:
- ì›ë³¸ ë‹µë³€ì— ì´ë¯¸ í¬í•¨ëœ ë‚´ìš©ì´ë©´ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
- ì›ë³¸ì— ë¹ ì§„ ì¤‘ìš”í•œ ë²•ì  ì •ë³´ê°€ ìˆì„ ë•Œë§Œ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•˜ì„¸ìš”
- ë¬¸ì„œ ì¸ìš© ì‹œ "**ì¶œì²˜**: [ë¬¸ì„œëª…]" í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”
"""

        if legal_references:
            refs_text = "\n".join([f"- {ref}" for ref in legal_references[:8]])
            prompt += f"""

## âš–ï¸ ì°¸ê³  ë²•ë ¹ (ë³´ì™„ìš©)

{refs_text}

**ì‚¬ìš© ê·œì¹™**:
- ì›ë³¸ ë‹µë³€ì— ì´ë¯¸ ì–¸ê¸‰ëœ ë²•ë ¹ì´ë©´ ì¤‘ë³µí•˜ì§€ ë§ˆì„¸ìš”
- ì›ë³¸ì— ë¹ ì§„ ì¤‘ìš”í•œ ë²•ë ¹ì´ ìˆì„ ë•Œë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€í•˜ì„¸ìš”
- ì˜ˆ: "ì´ì— ëŒ€í•´ì„œëŠ” **ë¯¼ë²• ì œ111ì¡°**ì—ì„œ ê·œì •í•˜ê³  ìˆìŠµë‹ˆë‹¤."
"""

        if legal_citations:
            citations_text = "\n".join([
                f"- {cite.get('text', cite.get('citation', str(cite)))}"
                for cite in legal_citations[:8]
            ])
            prompt += f"""

## ğŸ“š ì°¸ê³  ë²•ì  ì¸ìš© (ë³´ì™„ìš©)

{citations_text}

**ì‚¬ìš© ê·œì¹™**:
- ì›ë³¸ì— ì´ë¯¸ í¬í•¨ëœ ì¸ìš©ì´ë©´ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
- ì¤‘ìš”í•œ ì¸ìš©ì´ ëˆ„ë½ëœ ê²½ìš°ì—ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€í•˜ì„¸ìš”
- íŒë¡€ë‚˜ ë²•ë ¹ ì¸ìš© ì‹œ ì •í™•í•œ í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”
"""

        prompt += """

## âœ… ìµœì¢… í™•ì¸

**ì‘ì—… ì „ í™•ì¸ì‚¬í•­**:
1. Step 0ì—ì„œ ì›ë³¸ í’ˆì§ˆì„ í‰ê°€í–ˆëŠ”ê°€?
2. ì›ë³¸ì´ ìš°ìˆ˜í•˜ë©´ ìµœì†Œí•œì˜ í˜•ì‹ ì •ë¦¬ë§Œ ìˆ˜í–‰í•˜ëŠ”ê°€?
3. ì›ë³¸ì— ê°œì„ ì´ í•„ìš”í•˜ë©´ ëª¨ë“  ë²•ì  ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ê°œì„ í•˜ëŠ”ê°€?
4. [ ] ëª¨ë“  ë²•ì  ì •ë³´ í¬í•¨? [ ] ì¸ì‚¬ë§ë§Œ ì œê±°? [ ] ì›ë³¸ êµ¬ì¡° ì¡´ì¤‘? [ ] ì–´íˆ¬ í†µì¼?

## ğŸ“ ì¶œë ¥ í˜•ì‹

- ì œëª©: `## ì œëª©` (ì›ë³¸ êµ¬ì¡° ì¡´ì¤‘)
- ì„¹ì…˜: `### ì„¹ì…˜ëª…` (í‘œì‹œ ë¬¸êµ¬ ê¸ˆì§€)
- ê°•ì¡°: `**í…ìŠ¤íŠ¸**`
- ë¦¬ìŠ¤íŠ¸: `- í•­ëª©` ë˜ëŠ” `1. í•­ëª©`

## ğŸ“¤ ì¶œë ¥

Step 0 í‰ê°€ë¥¼ ë¨¼ì € ìˆ˜í–‰í•˜ê³ , í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ìµœì†Œí•œì˜ ê°œì„ ë§Œ ì ìš©í•˜ì„¸ìš”.
ì„¤ëª… ì—†ì´ ë°”ë¡œ í–¥ìƒëœ ë‹µë³€ì„ ì‹œì‘í•˜ì„¸ìš”:

"""

        return prompt

    def _normalize_titles(self, text: str) -> str:
        """ì œëª© ì¤‘ë³µ ì œê±° ë° ì •ê·œí™”"""
        lines = text.split('\n')
        normalized_lines = []
        seen_titles = set()

        for i, line in enumerate(lines):
            # ì œëª© ë¼ì¸ ê°ì§€
            title_match = re.match(r'^(#{1,6})\s+(.+)', line)

            if title_match:
                level = len(title_match.group(1))
                title_text = title_match.group(2).strip()

                # ì´ëª¨ì§€ ì œê±°í•œ ìˆœìˆ˜ ì œëª© í…ìŠ¤íŠ¸ (ê°œì„ : ë” ë§ì€ ì´ëª¨ì§€ ì œê±°)
                clean_title = re.sub(r'[ğŸ“–âš–ï¸ğŸ’¼ğŸ’¡ğŸ“šğŸ“‹â­ğŸ“ŒğŸ”ğŸ’¬ğŸ¯ğŸ“ŠğŸ“ğŸ“„â°ğŸ”—âš ï¸â—âœ…ğŸš¨]+\s*', '', title_text).strip()

                # ë™ì¼í•œ ë ˆë²¨ì˜ ì¤‘ë³µ ì œëª© ì œê±°
                if level == 2:  # ## ì œëª©
                    # ì¤‘ë³µ ì œëª© ì œê±° ë¡œì§ ê°•í™”
                    title_key = clean_title.lower()

                    # "ë‹µë³€", "ë²•ë¥ ì§ˆë¬¸ë‹µë³€" ë“± ìœ ì‚¬ ì œëª© ì²˜ë¦¬
                    is_similar_to_answer = any(pattern in clean_title for pattern in ["ë‹µë³€", "ë²•ë¥ ì§ˆë¬¸ë‹µë³€", "ë²•ë¥  ì§ˆë¬¸ ë‹µë³€"])

                    # ì¤‘ë³µì´ê±°ë‚˜ ìœ ì‚¬í•œ ì œëª© ì œê±°
                    if title_key in seen_titles or (is_similar_to_answer and "ë‹µë³€" in seen_titles):
                        continue  # ì¤‘ë³µ ì œëª© ìŠ¤í‚µ

                    # "ë‹µë³€" ê³„ì—´ ì œëª©ì€ í•˜ë‚˜ë§Œ í—ˆìš©
                    if is_similar_to_answer:
                        seen_titles.add("ë‹µë³€")

                    seen_titles.add(title_key)

                    # ì´ëª¨ì§€ê°€ ìˆìœ¼ë©´ ì œê±°í•˜ê³  ìˆœìˆ˜ ì œëª©ë§Œ ì‚¬ìš©
                    if re.search(r'[ğŸ“–âš–ï¸ğŸ’¼ğŸ’¡ğŸ“šğŸ“‹â­ğŸ“ŒğŸ”ğŸ’¬ğŸ¯ğŸ“ŠğŸ“ğŸ“„â°ğŸ”—âš ï¸â—âœ…ğŸš¨]', title_text):
                        normalized_lines.append(f"## {clean_title}")
                    else:
                        normalized_lines.append(f"## {clean_title}")

                elif level == 3:  # ### ì œëª©
                    # ê°œì„ : "ğŸ’¬ ë‹µë³€" ê°™ì€ í•˜ìœ„ í—¤ë”ê°€ "ë‹µë³€" ê³„ì—´ì´ë©´ ì œê±°
                    is_answer_subtitle = any(pattern in clean_title for pattern in ["ë‹µë³€", "ë²•ë¥ ì§ˆë¬¸ë‹µë³€", "ë²•ë¥  ì§ˆë¬¸ ë‹µë³€"])
                    if is_answer_subtitle and ("ë‹µë³€" in seen_titles or any(st in seen_titles for st in ["ë‹µë³€", "ë²•ë¥ ì§ˆë¬¸ë‹µë³€", "ë²•ë¥  ì§ˆë¬¸ ë‹µë³€"])):
                        continue  # "ë‹µë³€" ê³„ì—´ í•˜ìœ„ í—¤ë”ëŠ” ì œê±°

                    # ì´ëª¨ì§€ ì œê±°
                    if re.search(r'[ğŸ“–âš–ï¸ğŸ’¼ğŸ’¡ğŸ“šğŸ“‹â­ğŸ“ŒğŸ”ğŸ’¬ğŸ¯ğŸ“ŠğŸ“ğŸ“„â°ğŸ”—âš ï¸â—âœ…ğŸš¨]', title_text):
                        normalized_lines.append(f"### {clean_title}")
                    else:
                        normalized_lines.append(line)
                else:
                    normalized_lines.append(line)
            else:
                normalized_lines.append(line)

        return '\n'.join(normalized_lines)

    def _remove_empty_sections(self, text: str) -> str:
        """ë¹ˆ ì„¹ì…˜ ì œê±°"""
        lines = text.split('\n')
        result_lines = []
        current_section_lines = []
        current_section_title = ""
        in_section = False

        i = 0
        while i < len(lines):
            line = lines[i]

            # ì„¹ì…˜ ì‹œì‘ ê°ì§€ (### ë˜ëŠ” ####)
            section_match = re.match(r'^(#{3,4})\s+(.+)', line)

            if section_match:
                # ì´ì „ ì„¹ì…˜ ì²˜ë¦¬
                if in_section:
                    section_content = '\n'.join(current_section_lines)
                    # ë¹ˆ ì„¹ì…˜ì¸ì§€ í™•ì¸
                    if self._validate_section_content(section_content):
                        # ìœ íš¨í•œ ì„¹ì…˜ì´ë©´ ì¶”ê°€
                        result_lines.append(f"### {current_section_title}")
                        result_lines.extend(current_section_lines)

                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                in_section = True
                current_section_title = section_match.group(2).strip()
                current_section_lines = []
                i += 1
                continue

            if in_section:
                current_section_lines.append(line)
            else:
                result_lines.append(line)

            i += 1

        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
        if in_section:
            section_content = '\n'.join(current_section_lines)
            if self._validate_section_content(section_content):
                result_lines.append(f"### {current_section_title}")
                result_lines.extend(current_section_lines)

        return '\n'.join(result_lines)

    def _validate_section_content(self, content: str) -> bool:
        """ì„¹ì…˜ ë‚´ìš©ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        if not content or not content.strip():
            return False

        # ë¹ˆ ë‚´ìš© íŒ¨í„´ í™•ì¸
        empty_patterns = [
            r'^ê´€ë ¨\s*ë²•ë¥ ì„?\s*ì°¾ì„\s*ìˆ˜\s*ì—†ìŠµë‹ˆë‹¤?\.?\s*$',
            r'^ê´€ë ¨\s*ë²•ë¥ \s*ì˜ˆì‹œë¥¼?\s*ì°¾ì„\s*ìˆ˜\s*ì—†ìŠµë‹ˆë‹¤?\.?\s*$',
            r'^ì°¾ì„\s*ìˆ˜\s*ì—†ìŠµë‹ˆë‹¤?\.?\s*$',
            r'^ì•Œ\s*ìˆ˜\s*ì—†ìŠµë‹ˆë‹¤?\.?\s*$',
            r'^ì—†ìŠµë‹ˆë‹¤?\.?\s*$',
            r'^ì •ë³´ë¥¼?\s*ì°¾ì„\s*ìˆ˜\s*ì—†ìŠµë‹ˆë‹¤?\.?\s*$',
            r'^ê´€ë ¨\s*ë²•ë ¹ì„?\s*ì°¾ì„\s*ìˆ˜\s*ì—†ìŠµë‹ˆë‹¤?\.?\s*$',
        ]

        content_clean = content.strip()
        for pattern in empty_patterns:
            if re.match(pattern, content_clean, re.IGNORECASE):
                return False

        # ë„ˆë¬´ ì§§ê³  ì˜ë¯¸ ì—†ëŠ” ë‚´ìš© (50ì ë¯¸ë§Œì´ê³  "ì—†ìŠµë‹ˆë‹¤"ë¡œ ëë‚˜ëŠ” ê²½ìš°)
        if len(content_clean) < 50 and re.search(r'ì—†ìŠµë‹ˆë‹¤?\.?\s*$', content_clean):
            return False

        # ìµœì†Œ ê¸¸ì´ ì²´í¬ (ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì€ ìµœì†Œ 20ì)
        if len(content_clean) < 20:
            return False

        return True

    def _remove_quality_metrics(self, text: str) -> str:
        """í’ˆì§ˆ ì§€í‘œ ë° ì‹ ë¢°ë„ ì •ë³´ ì œê±°"""
        lines = text.split('\n')
        result_lines = []
        skip_section = False

        i = 0
        while i < len(lines):
            line = lines[i]

            # ì‹ ë¢°ë„ ì„¹ì…˜ ì‹œì‘ ê°ì§€
            if re.search(r'ì‹ ë¢°ë„|í’ˆì§ˆ\s*ì ìˆ˜|í’ˆì§ˆ\s*ì§€í‘œ|confidence|quality\s*score', line, re.IGNORECASE):
                # í•´ë‹¹ ì„¹ì…˜ ì „ì²´ ì œê±°
                skip_section = True
                i += 1
                continue

            # ì‹ ë¢°ë„ íŒ¨í„´ì´ í¬í•¨ëœ ë¼ì¸ ì œê±°
            if re.search(r'ğŸŸ .*ì‹ ë¢°ë„|ì‹ ë¢°ë„.*\d+%|ğŸŸ¢.*ì‹ ë¢°ë„|ğŸ”´.*ì‹ ë¢°ë„', line):
                i += 1
                continue

            # "ì‹ ë¢°ë„: XX%" íŒ¨í„´ ì œê±°
            if re.search(r'ì‹ ë¢°ë„\s*:\s*\d+\.?\d*%', line):
                # ë¼ì¸ì—ì„œ ì‹ ë¢°ë„ ë¶€ë¶„ë§Œ ì œê±°
                line = re.sub(r'ì‹ ë¢°ë„\s*:\s*\d+\.?\d*%[^\n]*', '', line)
                line = re.sub(r'\(ì‹ ë¢°ë„[^\)]+\)', '', line)

            # ë©´ì±… ì¡°í•­ ì„¹ì…˜ì€ ìœ ì§€í•˜ë˜ ì‹ ë¢°ë„ ì •ë³´ë§Œ ì œê±°
            if 'ë©´ì±…' in line and 'ì¡°í•­' in line:
                skip_section = False

            if skip_section:
                # ì„¹ì…˜ ëê¹Œì§€ ìŠ¤í‚µ (ë‹¤ìŒ ### ë˜ëŠ” ## ë§Œë‚  ë•Œê¹Œì§€)
                if re.match(r'^#{2,3}\s+', line):
                    skip_section = False
                    result_lines.append(line)
            else:
                # ì‹ ë¢°ë„ ìˆ«ìë§Œ ì œê±°
                line = re.sub(r'\s*ì‹ ë¢°ë„\s*:\s*\d+\.?\d*%', '', line)
                line = re.sub(r'\(ì‹ ë¢°ë„[^\)]+\)', '', line)
                result_lines.append(line)

            i += 1

        return '\n'.join(result_lines)

    def _remove_decorative_emojis(self, text: str) -> str:
        """ì¥ì‹ìš© ì´ëª¨ì§€ ì œê±° (ì„¹ì…˜ëª… ë° ë³¸ë¬¸ì—ì„œ) - ê°œì„ : ëª¨ë“  ì´ëª¨ì§€ ì œê±°"""
        lines = text.split('\n')
        result_lines = []

        # ëª¨ë“  ì´ëª¨ì§€ íŒ¨í„´ (ë” ë§ì€ ì´ëª¨ì§€ í¬í•¨)
        emoji_pattern = r'[ğŸ“–âš–ï¸ğŸ’¼ğŸ’¡ğŸ“šğŸ“‹â­ğŸ“ŒğŸ”ğŸ’¬ğŸ¯ğŸ“ŠğŸ“ğŸ“„â°ğŸ”—âš ï¸â—âœ…ğŸš¨ğŸ“‘ğŸ“ŒğŸ“ğŸ”¬âš¡ğŸŒŸğŸ’«]+\s*'

        for line in lines:
            # ì œëª© ë¼ì¸ì—ì„œ ì´ëª¨ì§€ ì œê±° (### ì œëª© í˜•ì‹)
            title_match = re.match(r'^(#{1,6})\s+(.+)', line)
            if title_match:
                level = title_match.group(1)
                title_text = title_match.group(2)

                # ëª¨ë“  ì´ëª¨ì§€ ì œê±° (ê°œì„ : ì±—ë´‡ ì¹œí™”ì )
                title_text = re.sub(emoji_pattern, '', title_text).strip()
                line = f"{level} {title_text}"
            else:
                # ë³¸ë¬¸ì—ì„œë„ ëª¨ë“  ì´ëª¨ì§€ ì œê±° (ê°œì„ )
                line = re.sub(emoji_pattern, '', line)

            result_lines.append(line)

        return '\n'.join(result_lines)

    def _normalize_structure(self, text: str) -> str:
        """Markdown êµ¬ì¡° ì •ê·œí™”"""
        lines = text.split('\n')
        result_lines = []
        last_level = 0

        for i, line in enumerate(lines):
            title_match = re.match(r'^(#{1,6})\s+(.+)', line)

            if title_match:
                level = len(title_match.group(1))

                # ê³„ì¸µ êµ¬ì¡° ê²€ì¦ ë° ìˆ˜ì •
                if level > 2 and last_level == 0:
                    # ì²« ì œëª©ì´ ###ì´ë©´ ##ë¡œ ë³€ê²½
                    if level == 3:
                        line = f"## {title_match.group(2)}"
                        level = 2

                # ## ë‹¤ìŒì— ë°”ë¡œ #### ì˜¤ëŠ” ê²½ìš° ###ë¡œ ì¡°ì •
                if last_level == 2 and level == 4:
                    line = f"### {title_match.group(2)}"
                    level = 3

                last_level = level

            result_lines.append(line)

        # ë¹ˆ ì¤„ ì •ë¦¬ (ì„¹ì…˜ ì‚¬ì´ì— ë¹ˆ ì¤„ 1ê°œë§Œ)
        result = '\n'.join(result_lines)
        result = re.sub(r'\n{3,}', '\n\n', result)

        return result

    def _post_process_llm_response(
        self,
        structured_answer: str,
        original_answer: str,
        question_type: QuestionType
    ) -> str:
        """LLM ì‘ë‹µ í›„ì²˜ë¦¬ - ì›ë³¸ ë‚´ìš© ë³´ì¡´ ê²€ì¦ ë° ê°œì„  (ê°œì„ ëœ ë²„ì „)"""

        if not structured_answer or not original_answer:
            return structured_answer if structured_answer else original_answer

        try:
            # 1. í†µí•© ì •ë¦¬ í•¨ìˆ˜ ì‚¬ìš©
            structured_answer = self._clean_structured_answer(structured_answer, question_type)

            # 2. ì›ë³¸ì˜ ì¤‘ìš” ë²•ë¥  ì •ë³´ ì¶”ì¶œ ë° ê²€ì¦
            original_lower = original_answer.lower()
            structured_lower = structured_answer.lower()

            # ë²•ì¡°ë¬¸ íŒ¨í„´ (ì œXì¡°, ì œXí•­ ë“±)
            legal_article_patterns = re.findall(r'ì œ\d+ì¡°|ì œ\d+í•­|ì œ\d+í˜¸', original_answer)
            missing_articles = [
                article for article in legal_article_patterns
                if article not in structured_lower
            ]

            # íŒë¡€ íŒ¨í„´
            precedent_patterns = re.findall(
                r'(ëŒ€ë²•ì›|ê³ ë“±ë²•ì›|ì§€ë°©ë²•ì›|ë²•ì›)\s+[\dê°€-í£]+',
                original_answer
            )
            missing_precedents = [
                prec for prec in precedent_patterns
                if prec not in structured_lower
            ]

            # 3. ëˆ„ë½ëœ ì¤‘ìš” ì •ë³´ ë³µì› ì‹œë„ (Phase 3.2: í›„ì²˜ë¦¬ ë¡œì§ ê°œì„ )
            if missing_articles:
                logger.warning(f"ëˆ„ë½ëœ ë²•ì¡°ë¬¸: {missing_articles[:5]}")
                # ì›ë³¸ì—ì„œ í•´ë‹¹ ë²•ì¡°ë¬¸ ë¶€ë¶„ ë³µì› ì‹œë„
                for article in missing_articles[:3]:  # ìµœëŒ€ 3ê°œë§Œ ë³µì›
                    article_pattern = article.replace("ì œ", r"\s*ì œ").replace("ì¡°", r"\s*ì¡°")
                    article_match = re.search(article_pattern, original_answer, re.IGNORECASE)
                    if article_match:
                        # ë²•ì¡°ë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ë‹¨ ì°¾ê¸°
                        start_pos = max(0, article_match.start() - 100)
                        end_pos = min(len(original_answer), article_match.end() + 500)
                        article_context = original_answer[start_pos:end_pos]
                        # êµ¬ì¡°í™”ëœ ë‹µë³€ì— í•´ë‹¹ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ë³µì›
                        if article.lower() not in structured_lower:
                            # êµ¬ì¡°í™”ëœ ë‹µë³€ì˜ ì ì ˆí•œ ìœ„ì¹˜ì— ì‚½ì… (ë²•ì  ê·¼ê±° ì„¹ì…˜ì´ë‚˜ ê´€ë ¨ ì„¹ì…˜)
                            structured_answer = self._restore_missing_content(
                                structured_answer, article_context, article
                            )
                            logger.info(f"ë²•ì¡°ë¬¸ ë³µì› ì‹œë„: {article}")

            if missing_precedents:
                logger.warning(f"ëˆ„ë½ëœ íŒë¡€: {missing_precedents[:3]}")
                # ì›ë³¸ì—ì„œ í•´ë‹¹ íŒë¡€ ë¶€ë¶„ ë³µì› ì‹œë„
                for precedent in missing_precedents[:2]:  # ìµœëŒ€ 2ê°œë§Œ ë³µì›
                    precedent_match = re.search(re.escape(precedent), original_answer, re.IGNORECASE)
                    if precedent_match:
                        start_pos = max(0, precedent_match.start() - 100)
                        end_pos = min(len(original_answer), precedent_match.end() + 300)
                        precedent_context = original_answer[start_pos:end_pos]
                        if precedent.lower() not in structured_lower:
                            structured_answer = self._restore_missing_content(
                                structured_answer, precedent_context, precedent
                            )
                            logger.info(f"íŒë¡€ ë³µì› ì‹œë„: {precedent}")

            # 4. í•µì‹¬ í‚¤ì›Œë“œ ë³´ì¡´ë¥  í™•ì¸
            original_keywords = set(re.findall(r'[\wê°€-í£]{3,}', original_lower))
            # ë²•ë¥  ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
            important_keywords = {
                kw for kw in original_keywords
                if any(term in kw for term in ['ë²•', 'ì¡°', 'í•­', 'íŒë¡€', 'ë²•ì›', 'íŒê²°', 'ê¶Œë¦¬', 'ì˜ë¬´', 'ê³„ì•½', 'ì†Œì†¡'])
            }

            preserved_keywords = {
                kw for kw in important_keywords
                if kw in structured_lower
            }

            preservation_rate = len(preserved_keywords) / len(important_keywords) if important_keywords else 1.0

            # Phase 3.2: í•µì‹¬ í‚¤ì›Œë“œ ë³´ì¡´ë¥ ì´ 70% ë¯¸ë§Œì¼ ë•Œ ì›ë³¸ ë¶€ë¶„ ë³µì› ì‹œë„
            if preservation_rate < 0.7 and important_keywords:
                logger.warning(f"í•µì‹¬ í‚¤ì›Œë“œ ë³´ì¡´ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ ({preservation_rate:.2%})")
                # ëˆ„ë½ëœ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì›ë³¸ ë¬¸ë‹¨ ì°¾ì•„ì„œ ë³µì›
                missing_keywords = important_keywords - preserved_keywords
                for keyword in list(missing_keywords)[:5]:  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œë§Œ ë³µì›
                    # ì›ë³¸ì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ë‹¨ ì°¾ê¸°
                    keyword_pattern = re.escape(keyword)
                    keyword_matches = list(re.finditer(keyword_pattern, original_answer, re.IGNORECASE))
                    if keyword_matches:
                        for match in keyword_matches[:2]:  # ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 2ê°œ ë¬¸ë‹¨
                            start_pos = max(0, match.start() - 200)
                            end_pos = min(len(original_answer), match.end() + 300)
                            keyword_context = original_answer[start_pos:end_pos]
                            # êµ¬ì¡°í™”ëœ ë‹µë³€ì— í•´ë‹¹ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë³µì› ì‹œë„
                            if keyword.lower() not in structured_lower:
                                structured_answer = self._restore_missing_content(
                                    structured_answer, keyword_context, keyword
                                )
                                logger.info(f"í‚¤ì›Œë“œ ë³µì› ì‹œë„: {keyword}")
                                break  # í•œ ë²ˆë§Œ ë³µì›

            return structured_answer.strip()

        except Exception as e:
            logger.error(f"Error in post-processing LLM response: {e}", exc_info=True)
            return structured_answer

    def _clean_structured_answer(self, structured_answer: str, question_type: QuestionType) -> str:
        """êµ¬ì¡°í™”ëœ ë‹µë³€ ìµœì¢… ì •ë¦¬ (í†µí•© í›„ì²˜ë¦¬ í•¨ìˆ˜)"""
        try:
            # 1. ì œëª©ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            if not re.search(r'^##\s+', structured_answer, re.MULTILINE):
                template = self.structure_templates.get(
                    question_type,
                    self.structure_templates[QuestionType.GENERAL_QUESTION]
                )
                title = template.get('title', 'ë²•ë¥  ì§ˆë¬¸ ë‹µë³€')
                structured_answer = f"## {title}\n\n{structured_answer}"

            # 2. ë¶ˆí•„ìš”í•œ ë©”íƒ€ í…ìŠ¤íŠ¸ ì œê±°
            meta_patterns = [
                r'^ìœ„ì˜?\s+.*ì§€ì¹¨ì—?\s+ë”°ë¼.*?\n',
                r'^ë‹¤ìŒê³¼?\s+ê°™ì´.*?\n',
                r'^êµ¬ì¡°í™”ëœ?\s+ë‹µë³€ì€?\s+ë‹¤ìŒê³¼?\s+ê°™ìŠµë‹ˆë‹¤?.*?\n',
            ]
            for pattern in meta_patterns:
                structured_answer = re.sub(pattern, '', structured_answer, flags=re.MULTILINE | re.IGNORECASE)

            # 3. ëŒ€ê´„í˜¸ íŒ¨í„´ ì œê±° (í‘œì‹œ ë¬¸êµ¬ ë“±)
            structured_answer = self._remove_bracket_patterns(structured_answer)

            # 4. ì¹œê·¼í•œ ì–´íˆ¬ ì •ë¦¬
            structured_answer = self._normalize_tone(structured_answer)

            # 5. í’ˆì§ˆ ì§€í‘œ ì œê±°
            structured_answer = self._remove_quality_metrics(structured_answer)

            # 6. ë¹ˆ ì„¹ì…˜ ì œê±°
            structured_answer = self._remove_empty_sections(structured_answer)

            # 7. ì œëª© ì¤‘ë³µ ì œê±° ë° ì •ê·œí™”
            structured_answer = self._normalize_titles(structured_answer)

            # 8. ì´ëª¨ì§€ ì œê±°
            structured_answer = self._remove_decorative_emojis(structured_answer)

            # 9. êµ¬ì¡° ì •ê·œí™”
            structured_answer = self._normalize_structure(structured_answer)

            # 10. ì¤‘ë³µ ì¶œì²˜ ì œê±°
            structured_answer = self._remove_duplicate_sources(structured_answer)

            # 11. ì¤‘ë³µ ë‚´ìš© ì œê±°
            structured_answer = self._remove_duplicate_content(structured_answer)

            # 12. ë¹ˆ ì¤„ ì •ë¦¬ (3ê°œ ì´ìƒ ì—°ì† ë¹ˆ ì¤„ì€ 2ê°œë¡œ)
            structured_answer = re.sub(r'\n{3,}', '\n\n', structured_answer)

            # 13. ì¤‘ë³µ í—¤ë” ì¶”ê°€ ì œê±° (ê°œì„ )
            structured_answer = self._remove_duplicate_headers(structured_answer)

            # 14. ë¹ˆ ì„¹ì…˜ ì¶”ê°€ ì •ë¦¬ (ê°œì„ )
            structured_answer = self._remove_empty_sections_enhanced(structured_answer)

            # 15. ì±—ë´‡ ì¹œí™”ì  êµ¬ì¡°ë¡œ ë³€í™˜ (ê°œì„ )
            structured_answer = self._make_chatbot_friendly(structured_answer)

            return structured_answer.strip()

        except Exception as e:
            logger.error(f"Error in cleaning structured answer: {e}", exc_info=True)
            return structured_answer

    def _remove_duplicate_headers(self, text: str) -> str:
        """ì¤‘ë³µ í—¤ë” ì œê±° (ê°œì„ )"""
        try:
            # ì´ë¯¸ _normalize_titlesì—ì„œ ì²˜ë¦¬ë˜ì§€ë§Œ, ì¶”ê°€ë¡œ ê°•í™”
            lines = text.split('\n')
            result_lines = []
            seen_headers = set()

            for line in lines:
                header_match = re.match(r'^(#{2,3})\s+(.+)', line)
                if header_match:
                    level = len(header_match.group(1))
                    header_text = header_match.group(2).strip()
                    # ì´ëª¨ì§€ ì œê±°
                    clean_header = re.sub(r'[ğŸ“–âš–ï¸ğŸ’¼ğŸ’¡ğŸ“šğŸ“‹â­ğŸ“ŒğŸ”ğŸ’¬ğŸ¯ğŸ“ŠğŸ“ğŸ“„â°ğŸ”—âš ï¸â—âœ…ğŸš¨]+\s*', '', header_text).strip()
                    header_key = f"{level}:{clean_header.lower()}"

                    if header_key in seen_headers:
                        continue  # ì¤‘ë³µ í—¤ë” ì œê±°
                    seen_headers.add(header_key)

                result_lines.append(line)

            return '\n'.join(result_lines)
        except Exception as e:
            logger.error(f"Error removing duplicate headers: {e}")
            return text

    def _remove_empty_sections_enhanced(self, text: str) -> str:
        """ë¹ˆ ì„¹ì…˜ ì œê±° (ê°•í™”ëœ ë²„ì „)"""
        try:
            # _remove_empty_sectionsì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë” ê°•í™”ëœ ë¡œì§
            return self._remove_empty_sections(text)
        except Exception as e:
            logger.error(f"Error removing empty sections enhanced: {e}")
            return text

    def _make_chatbot_friendly(self, text: str) -> str:
        """ì±—ë´‡ ì¹œí™”ì  êµ¬ì¡°ë¡œ ë³€í™˜"""
        try:
            # ìì—°ìŠ¤ëŸ¬ìš´ íë¦„: ì§ˆë¬¸ â†’ í•µì‹¬ ë‹µë³€ â†’ ìƒì„¸ ì„¤ëª… â†’ ì°¸ê³ ì‚¬í•­
            # ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ í—¤ë” ìµœì†Œí™”
            lines = text.split('\n')
            result_lines = []

            # ì—°ì†ëœ í—¤ë” ì¤„ë°”ê¿ˆ ìµœì†Œí™” (í—¤ë” ë°”ë¡œ ë‹¤ìŒì— ë‚´ìš©ì´ ì˜¤ë„ë¡)
            for i, line in enumerate(lines):
                result_lines.append(line)
                # í—¤ë” ë‹¤ìŒì— ë¹ˆ ì¤„ì´ ë‘ ê°œ ì´ìƒì´ë©´ í•˜ë‚˜ë¡œ ì¤„ì„
                if re.match(r'^#{2,3}\s+', line) and i + 1 < len(lines) and lines[i + 1].strip() == '':
                    if i + 2 < len(lines) and lines[i + 2].strip() == '':
                        # ë¹ˆ ì¤„ ë‘ ê°œ ì´ìƒì´ë©´ í•˜ë‚˜ë§Œ ìœ ì§€
                        continue

            text = '\n'.join(result_lines)

            # ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ í—¤ë” ë ˆë²¨ ê°ì†Œ (### -> ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•  ìˆ˜ë„ ìˆì§€ë§Œ, í˜„ì¬ëŠ” ìœ ì§€)
            # ë‹¨, ë„ˆë¬´ ë§ì€ í—¤ë”ëŠ” ì¤„ì„

            return text
        except Exception as e:
            logger.error(f"Error making chatbot friendly: {e}")
            return text

    def _remove_bracket_patterns(self, text: str) -> str:
        """ëŒ€ê´„í˜¸ íŒ¨í„´ ì œê±° (ì˜ˆ: [ì§ˆë¬¸ ë‚´ìš© ë¶„ì„:], [ê´€ë ¨ ë²•ë ¹:])"""
        try:
            if not text:
                return text

            lines = text.split('\n')
            result_lines = []
            prev_line_was_section_title = False

            for line in lines:
                # ì„¹ì…˜ ì œëª© í™•ì¸
                is_section_title = bool(re.match(r'^###\s+', line))

                # ì„¹ì…˜ ì œëª© ë°”ë¡œ ë‹¤ìŒ ì¤„ì— ëŒ€ê´„í˜¸ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° ì œê±°
                if prev_line_was_section_title:
                    # ëŒ€ê´„í˜¸ íŒ¨í„´ ì œê±° (ì˜ˆ: [ì§ˆë¬¸ ë‚´ìš© ë¶„ì„:], [ê´€ë ¨ ë²•ë ¹:])
                    bracket_pattern = re.match(r'^\s*\[[^\]]*:\]\s*$', line)
                    if bracket_pattern:
                        # ì´ ì¤„ì„ ê±´ë„ˆëœ€ (ì¤„ë°”ê¿ˆì€ ìœ ì§€í•˜ê¸° ìœ„í•´ ë¹ˆ ì¤„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
                        prev_line_was_section_title = False
                        continue

                # ì¼ë°˜ ì¤„ì—ì„œ ëŒ€ê´„í˜¸ íŒ¨í„´ í™•ì¸
                bracket_match = re.match(r'^\s*\[[^\]]*:\]\s*$', line)
                if bracket_match:
                    # ëŒ€ê´„í˜¸ íŒ¨í„´ë§Œ ìˆëŠ” ì¤„ì€ ì œê±°
                    continue

                # ëŒ€ê´„í˜¸ íŒ¨í„´ì´ ì•„ë‹Œ ëª¨ë“  ì¤„ì€ ì¶”ê°€
                result_lines.append(line)

                prev_line_was_section_title = is_section_title

            return '\n'.join(result_lines)
        except Exception as e:
            logger.error(f"Error removing bracket patterns: {e}", exc_info=True)
            return text

    def _restore_missing_content(self, structured_answer: str, content: str, identifier: str) -> str:
        """
        ëˆ„ë½ëœ ë‚´ìš©ì„ êµ¬ì¡°í™”ëœ ë‹µë³€ì˜ ì ì ˆí•œ ìœ„ì¹˜ì— ë³µì› (Phase 3.2)

        Args:
            structured_answer: êµ¬ì¡°í™”ëœ ë‹µë³€
            content: ë³µì›í•  ë‚´ìš© (ë¬¸ë‹¨ ë˜ëŠ” ì„¹ì…˜)
            identifier: ë‚´ìš©ì„ ì‹ë³„í•˜ëŠ” í‚¤ì›Œë“œ (ë²•ì¡°ë¬¸, íŒë¡€, í‚¤ì›Œë“œ)

        Returns:
            ë‚´ìš©ì´ ë³µì›ëœ êµ¬ì¡°í™”ëœ ë‹µë³€
        """
        try:
            if not content or not content.strip():
                return structured_answer

            # ë²•ì  ê·¼ê±° ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
            legal_section_keywords = ['ë²•ì  ê·¼ê±°', 'ê´€ë ¨ ë²•ë ¹', 'ë²•ë ¹', 'ë²•ì¡°ë¬¸', 'íŒë¡€', 'ë²•ì  í•´ì„¤']
            lines = structured_answer.split('\n')
            insertion_point = -1

            # ì„¹ì…˜ ì œëª© ë¼ì¸ ì°¾ê¸°
            for i, line in enumerate(lines):
                if re.match(r'^###\s+', line):
                    section_title = re.sub(r'^###\s+', '', line).strip().lower()
                    if any(keyword in section_title for keyword in legal_section_keywords):
                        # í•´ë‹¹ ì„¹ì…˜ ëê¹Œì§€ ì°¾ê¸° (ë‹¤ìŒ ### ë˜ëŠ” ë)
                        insertion_point = i
                        break

            if insertion_point == -1:
                # ë²•ì  ê·¼ê±° ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ëì— ì¶”ê°€
                if structured_answer.strip():
                    structured_answer += f"\n\n### ë²•ì  ê·¼ê±°\n\n{content.strip()}"
                else:
                    structured_answer = f"### ë²•ì  ê·¼ê±°\n\n{content.strip()}"
            else:
                # í•´ë‹¹ ì„¹ì…˜ì— ë‚´ìš© ì¶”ê°€ (ì„¹ì…˜ ëì— ì¶”ê°€)
                next_section = -1
                for i in range(insertion_point + 1, len(lines)):
                    if re.match(r'^###\s+', lines[i]):
                        next_section = i
                        break

                if next_section == -1:
                    # ë‹¤ìŒ ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ëì— ì¶”ê°€
                    lines.append(f"\n{content.strip()}")
                else:
                    # ë‹¤ìŒ ì„¹ì…˜ ì „ì— ì‚½ì…
                    lines.insert(next_section, f"{content.strip()}\n")

                structured_answer = '\n'.join(lines)

            return structured_answer

        except Exception as e:
            logger.warning(f"ë‚´ìš© ë³µì› ì‹¤íŒ¨ ({identifier}): {e}")
            return structured_answer

    def _normalize_tone(self, text: str) -> str:
        """ì¹œê·¼í•œ ì–´íˆ¬ë¥¼ ì „ë¬¸ì ì¸ ì–´íˆ¬ë¡œ ë³€í™˜"""
        try:
            if not text:
                return text

            # ì¹œê·¼í•œ ì–´íˆ¬ íŒ¨í„´ì„ ì „ë¬¸ì  ì–´íˆ¬ë¡œ ë³€í™˜
            replacements = [
                # ì–´ë¯¸ ë³€í™˜ (ì¤„ë°”ê¿ˆ ë³´ì¡´)
                (r'í•´ìš”\.', 'í•©ë‹ˆë‹¤.'),
                (r'ì´ì—ìš”\.', 'ì…ë‹ˆë‹¤.'),
                (r'ì˜ˆìš”\.', 'ì…ë‹ˆë‹¤.'),
                (r'ì•„ìš”\.', 'ìŠµë‹ˆë‹¤.'),
                (r'ì–´ìš”\.', 'ìŠµë‹ˆë‹¤.'),
                (r'í•´ìš”\s+', 'í•©ë‹ˆë‹¤ '),  # ê³µë°±ë§Œ ë§¤ì¹­ (ì¤„ë°”ê¿ˆ ì œì™¸)
                (r'ì´ì—ìš”\s+', 'ì…ë‹ˆë‹¤ '),
                (r'ì˜ˆìš”\s+', 'ì…ë‹ˆë‹¤ '),
                (r'ì•„ìš”\s+', 'ìŠµë‹ˆë‹¤ '),
                (r'ì–´ìš”\s+', 'ìŠµë‹ˆë‹¤ '),

                # ë¶ˆí•„ìš”í•œ ì–´ë¯¸ ë³€í˜•
                (r'ì¢‹ì•„ìš”\.', 'ì¢‹ìŠµë‹ˆë‹¤.'),
                (r'ì¢‹ì•„ìš”\s+', 'ì¢‹ìŠµë‹ˆë‹¤ '),
            ]

            result = text
            for pattern, replacement in replacements:
                result = re.sub(pattern, replacement, result)

            # ë¶ˆí•„ìš”í•œ ëŒ€í™”í˜• ë¬¸êµ¬ ì œê±° (ì¤„ë°”ê¿ˆ ë³´ì¡´)
            # ì¤„ë°”ê¿ˆì„ í¬í•¨í•˜ì§€ ì•ŠëŠ” íŒ¨í„´ ì‚¬ìš©
            lines = result.split('\n')
            result_lines = []

            for line in lines:
                # ì¤„ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ì¤„ë°”ê¿ˆ ë³´ì¡´
                line_processed = line

                # ì¤„ ëì— ìˆëŠ” ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ë§Œ ì œê±° (ì¤„ë°”ê¿ˆ ë³´ì¡´)
                line_processed = re.sub(r'ê¶ê¸ˆí•˜ì‹œêµ°ìš”\.?\s*$', '', line_processed)
                line_processed = re.sub(r'ë§ì”€í•˜ì‹ \s+', 'ì§ˆë¬¸í•˜ì‹  ', line_processed)
                line_processed = re.sub(r'ì—¬ê¸°ì„œ\s+', 'ì—¬ê¸°ì„œ ', line_processed)

                result_lines.append(line_processed)

            result = '\n'.join(result_lines)

            # ë¬¸ì¥ ì‹œì‘ ë¶€ë¶„ì˜ ë¶ˆí•„ìš”í•œ ëŒ€í™”í˜• ë¬¸êµ¬ ì œê±° (ì¤„ë°”ê¿ˆ ë³´ì¡´)
            # ì¤„ë°”ê¿ˆì„ ìœ ì§€í•˜ë©´ì„œ ì• ë¬¸êµ¬ë§Œ ì œê±°
            result = re.sub(r'(^ë¯¼ë²•\s+ì œ\d+ì¡°ì˜\s+ë‚´ìš©ì—\s+ëŒ€í•´\s+)ê¶ê¸ˆí•˜ì‹œêµ°ìš”\.?\s*', r'\1', result, flags=re.MULTILINE)

            return result
        except Exception as e:
            logger.error(f"Error normalizing tone: {e}", exc_info=True)
            return text

    def _remove_duplicate_sources(self, text: str) -> str:
        """ì¤‘ë³µëœ ì¶œì²˜ í‘œì‹œ ì œê±° ë° ì¶œì²˜ í†µí•©"""
        try:
            if not text:
                return text

            # ì¶œì²˜ íŒ¨í„´ ì¶”ì¶œ: **ì¶œì²˜**: [ë‚´ìš©]
            source_pattern = r'\*\*ì¶œì²˜\*\*:\s*([^\n]+)'
            sources = re.findall(source_pattern, text)

            # ì¶œì²˜ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if len(sources) < 2:
                return text

            # ë™ì¼ ì¶œì²˜ í™•ì¸
            unique_sources = {}

            for match in re.finditer(source_pattern, text):
                source_text = match.group(1).strip()
                source_key = source_text.lower()

                if source_key not in unique_sources:
                    unique_sources[source_key] = {
                        'text': source_text,
                        'positions': []
                    }
                unique_sources[source_key]['positions'].append((match.start(), match.end()))

            # ë™ì¼ ì¶œì²˜ê°€ 2íšŒ ì´ìƒ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš°
            result = text
            positions_to_remove = []

            for source_key, source_info in unique_sources.items():
                positions = source_info['positions']
                if len(positions) > 1:
                    # ì²« ë²ˆì§¸ëŠ” ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” ì œê±° ëŒ€ìƒ
                    for start, end in positions[1:]:
                        positions_to_remove.append((start, end))

            # ì—­ìˆœìœ¼ë¡œ ì œê±° (ì¸ë±ìŠ¤ ë³€ê²½ ë°©ì§€)
            for start, end in sorted(positions_to_remove, reverse=True):
                # ì¶œì²˜ ì¤„ ì „ì²´ë¥¼ ì œê±°
                line_start = result.rfind('\n', 0, start) + 1
                line_end = result.find('\n', end)
                if line_end == -1:
                    line_end = len(result)

                # ë¹ˆ ì¤„ë„ í•¨ê»˜ ì œê±°
                prev_newline = result.rfind('\n', 0, line_start - 1) + 1 if line_start > 0 else 0
                next_newline = result.find('\n', line_end)

                # ì•ë’¤ ë¹ˆ ì¤„ í™•ì¸
                if line_start > 0 and result[prev_newline:line_start].strip() == '':
                    line_start = prev_newline
                if next_newline != -1 and result[line_end:next_newline].strip() == '':
                    line_end = next_newline

                result = result[:line_start] + result[line_end:]

            return result
        except Exception as e:
            logger.error(f"Error removing duplicate sources: {e}", exc_info=True)
            return text

    def _remove_duplicate_content(self, text: str, similarity_threshold: float = 0.8) -> str:
        """ì¤‘ë³µ ë‚´ìš© ì œê±° (ë¬¸ë‹¨ ë‹¨ìœ„ ìœ ì‚¬ë„ ê²€ì‚¬)"""
        try:
            if not text:
                return text

            # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ë¹ˆ ì¤„ë¡œ êµ¬ë¶„)
            paragraphs = re.split(r'\n\s*\n', text)

            if len(paragraphs) < 2:
                return text

            # ìœ ì‚¬ë„ ê³„ì‚° ë° ì¤‘ë³µ ì œê±°
            unique_paragraphs = []
            seen_paragraphs = []

            for para in paragraphs:
                if not para or not para.strip():
                    unique_paragraphs.append(para)
                    continue

                # ì œëª©ì´ë‚˜ ì„¹ì…˜ ë§ˆì»¤ëŠ” ì œì™¸
                if re.match(r'^#+\s+', para.strip()):
                    unique_paragraphs.append(para)
                    continue

                # ì„¹ì…˜ ì œëª©ì„ ì œì™¸í•œ ìˆœìˆ˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
                para_lines = para.split('\n')
                para_content_lines = [line for line in para_lines if not re.match(r'^#+\s+', line.strip())]
                para_content = '\n'.join(para_content_lines).strip()

                if not para_content:
                    unique_paragraphs.append(para)
                    continue

                # ê¸°ì¡´ ë¬¸ë‹¨ê³¼ ìœ ì‚¬ë„ ë¹„êµ (ì„¹ì…˜ ì œëª© ì œì™¸í•œ ìˆœìˆ˜ ë‚´ìš©ë§Œ)
                is_duplicate = False
                para_clean = para_content.lower()

                for seen_para in seen_paragraphs:
                    similarity = SequenceMatcher(None, para_clean, seen_para).ratio()
                    if similarity >= similarity_threshold:
                        is_duplicate = True
                        break

                if not is_duplicate:
                    unique_paragraphs.append(para)
                    seen_paragraphs.append(para_clean)

            return '\n\n'.join(unique_paragraphs)
        except Exception as e:
            logger.error(f"Error removing duplicate content: {e}", exc_info=True)
            return text

    def _format_docs_for_prompt(self, retrieved_docs: List[Dict[str, Any]]) -> str:
        """ë²•ë¥  ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        # None ì²´í¬
        if retrieved_docs is None:
            return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        if not retrieved_docs:
            return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        formatted_docs = []
        for i, doc in enumerate(retrieved_docs[:5], 1):  # ìµœëŒ€ 5ê°œ
            if not isinstance(doc, dict):
                continue

            doc_type = doc.get("type", "ë¬¸ì„œ")
            source = doc.get("source", doc.get("title", "ì•Œ ìˆ˜ ì—†ìŒ"))
            content = doc.get("content", doc.get("text", ""))
            score = doc.get("relevance_score", doc.get("score", 0.0))

            # ë‚´ìš© ìš”ì•½ (ìµœëŒ€ 300ì) - None ì²´í¬
            if content is None:
                content = ""
            content_preview = content[:300] + "..." if len(content) > 300 else content

            formatted_docs.append(
                f"{i}. **{doc_type}**: {source}\n"
                f"   - ê´€ë ¨ë„: {score:.2f}\n"
                f"   - ë‚´ìš©: {content_preview}"
            )

        return "\n\n".join(formatted_docs)

    def _enhance_with_template(
        self,
        answer: str,
        mapped_question_type: QuestionType,
        question: str,
        retrieved_docs: List[Dict[str, Any]],
        legal_references: List[str],
        legal_citations: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """í…œí”Œë¦¿ ê¸°ë°˜ êµ¬ì¡°í™” (í´ë°±)"""

        # None ì²´í¬ ë° íƒ€ì… ì•ˆì „ì„± ë³´ì¥
        retrieved_docs = retrieved_docs if retrieved_docs is not None else []
        legal_references = legal_references if legal_references is not None else []
        legal_citations = legal_citations if legal_citations is not None else []

        if not isinstance(retrieved_docs, list):
            retrieved_docs = []
        if not isinstance(legal_references, list):
            legal_references = []
        if not isinstance(legal_citations, list):
            legal_citations = []

        # êµ¬ì¡° í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        template = self.structure_templates.get(mapped_question_type,
                                              self.structure_templates[QuestionType.GENERAL_QUESTION])

        if not template:
            return {"error": "Template not found"}

        # í˜„ì¬ ë‹µë³€ ë¶„ì„
        analysis = self._analyze_current_structure(answer, template)

        # êµ¬ì¡°í™” ê°œì„  ì œì•ˆ (ë²•ì  ê·¼ê±° ì •ë³´ í¬í•¨)
        improvements = self._generate_structure_improvements(
            analysis, template, retrieved_docs, legal_references, legal_citations
        )

        # êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„± (ë²•ì  ê·¼ê±° ì •ë³´ í¬í•¨)
        structured_answer = self._create_structured_answer(
            answer, template, improvements, retrieved_docs, legal_references, legal_citations
        )

        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        quality_metrics = self._calculate_quality_metrics(structured_answer)

        return {
            "original_answer": answer,
            "structured_answer": structured_answer,
            "question_type": mapped_question_type.value,
            "template_used": template.get("title", "Unknown"),
            "method": "template_based",
            "analysis": analysis,
            "improvements": improvements,
            "quality_metrics": quality_metrics,
            "enhancement_timestamp": datetime.now().isoformat()
        }

    # ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ë©”ì„œë“œë“¤ ì œê±°ë¨
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í•˜ë“œì½”ë”©ëœ í‚¤ì›Œë“œ ë§¤ì¹­ ë°©ì‹ ì‚¬ìš©

    def _map_question_type_fallback(self, question_type: any, question: str) -> QuestionType:
        """í´ë°± ì§ˆë¬¸ ìœ í˜• ë§¤í•‘ (ê¸°ì¡´ ë°©ì‹)"""
        question_lower = question.lower()

        # question_typeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        if isinstance(question_type, QuestionType):
            question_type_str = question_type.value if hasattr(question_type, 'value') else str(question_type)
        elif isinstance(question_type, str):
            question_type_str = question_type.lower()
        else:
            question_type_str = str(question_type).lower()

        # í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤í•‘
        if "íŒë¡€" in question or "precedent" in question_type_str:
            return QuestionType.PRECEDENT_SEARCH
        elif "ê³„ì•½ì„œ" in question or "contract" in question_type_str:
            return QuestionType.CONTRACT_REVIEW
        elif "ì´í˜¼" in question or "divorce" in question_type_str:
            return QuestionType.DIVORCE_PROCEDURE
        elif "ìƒì†" in question or "inheritance" in question_type_str:
            return QuestionType.INHERITANCE_PROCEDURE
        elif "ë²”ì£„" in question or "criminal" in question_type_str:
            return QuestionType.CRIMINAL_CASE
        elif "ë…¸ë™" in question or "labor" in question_type_str:
            return QuestionType.LABOR_DISPUTE
        elif "ì ˆì°¨" in question or "procedure" in question_type_str:
            return QuestionType.PROCEDURE_GUIDE
        elif "ìš©ì–´" in question or "term" in question_type_str:
            return QuestionType.TERM_EXPLANATION
        elif "ì¡°ì–¸" in question or "advice" in question_type_str:
            return QuestionType.LEGAL_ADVICE
        elif "ë²•ë¥ " in question or "law" in question_type_str:
            return QuestionType.LAW_INQUIRY
        else:
            return QuestionType.GENERAL_QUESTION

    def _analyze_question_structure(self, question: str) -> QuestionType:
        """ì§ˆë¬¸ êµ¬ì¡° ë¶„ì„ì„ í†µí•œ ìœ í˜• ì¶”ì •"""
        question_lower = question.lower()

        # ì§ˆë¬¸ íŒ¨í„´ ë¶„ì„
        if any(word in question_lower for word in ['ì–´ë–»ê²Œ', 'how', 'ë°©ë²•', 'ì ˆì°¨']):
            return QuestionType.PROCEDURE_GUIDE

        if any(word in question_lower for word in ['ë¬´ì—‡', 'what', 'ì˜ë¯¸', 'ì •ì˜']):
            return QuestionType.TERM_EXPLANATION

        if any(word in question_lower for word in ['ë„ì›€', 'help', 'ì¡°ì–¸', 'advice']):
            return QuestionType.LEGAL_ADVICE

        if any(word in question_lower for word in ['ì°¾', 'search', 'ê²€ìƒ‰', 'ì°¾ì•„']):
            return QuestionType.PRECEDENT_SEARCH

        return QuestionType.GENERAL_QUESTION

    def _analyze_current_structure(self, answer: str, template: Dict[str, Any]) -> Dict[str, Any]:
        """í˜„ì¬ ë‹µë³€ êµ¬ì¡° ë¶„ì„ (ì•ˆì „í•œ ë²„ì „)"""
        analysis = {
            "has_title": False,
            "section_coverage": {},
            "missing_sections": [],
            "structure_score": 0.0,
            "quality_indicators": {}  # ê¸°ë³¸ê°’ ë³´ì¥
        }

        try:
            # ì œëª© ì¡´ì¬ ì—¬ë¶€
            analysis["has_title"] = bool(re.search(r'^#+\s+', answer, re.MULTILINE))

            # ì„¹ì…˜ë³„ í¬í•¨ë„ ë¶„ì„
            sections = template.get("sections", [])
            for section in sections:
                try:
                    section_name = section.get("name", "")
                    section_keywords = self._extract_section_keywords(section)

                    # ì„¹ì…˜ í‚¤ì›Œë“œê°€ ë‹µë³€ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    coverage = self._calculate_section_coverage(answer, section_keywords)
                    analysis["section_coverage"][section_name] = coverage

                    # ëˆ„ë½ëœ ì„¹ì…˜ í™•ì¸
                    if coverage < 0.3:  # 30% ë¯¸ë§Œì´ë©´ ëˆ„ë½ìœ¼ë¡œ ê°„ì£¼
                        analysis["missing_sections"].append(section_name)

                except Exception as e:
                    logger.warning(f"Section analysis error for {section.get('name', 'unknown')}: {e}", exc_info=True)
                    continue

            # êµ¬ì¡° ì ìˆ˜ ê³„ì‚°
            analysis["structure_score"] = self._calculate_structure_score(analysis)

            # í’ˆì§ˆ ì§€í‘œ ë¶„ì„
            analysis["quality_indicators"] = self._analyze_quality_indicators(answer)

        except Exception as e:
            logger.error(f"Structure analysis error: {e}", exc_info=True)
            # ê¸°ë³¸ê°’ ìœ ì§€

        return analysis

    def _extract_section_keywords(self, section: Dict[str, Any]) -> List[str]:
        """ì„¹ì…˜ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            keywords = []

            # None ì²´í¬
            if section is None or not isinstance(section, dict):
                return []

            # ì„¹ì…˜ ì´ë¦„ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            section_name = section.get("name", "")
            if section_name:
                keywords.extend(section_name.split())

            # í…œí”Œë¦¿ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            template_text = section.get("template", "") or ""
            if template_text:
                keywords.extend(re.findall(r'[\wê°€-í£]+', template_text))

            # ë‚´ìš© ê°€ì´ë“œì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            content_guide = section.get("content_guide", "") or ""
            if content_guide:
                keywords.extend(re.findall(r'[\wê°€-í£]+', content_guide))

            return list(set(keywords))  # ì¤‘ë³µ ì œê±°
        except Exception as e:
            logger.error(f"Error in _extract_section_keywords: {e}", exc_info=True)
            return []

    def _calculate_section_coverage(self, answer: str, keywords: List[str]) -> float:
        """ì„¹ì…˜ í¬í•¨ë„ ê³„ì‚° (ì•ˆì „í•œ ë²„ì „)"""
        try:
            if not keywords or len(keywords) == 0:
                return 0.0

            answer_lower = answer.lower()
            matched_keywords = sum(1 for keyword in keywords if keyword.lower() in answer_lower)

            return matched_keywords / len(keywords)

        except Exception as e:
            logger.error(f"Section coverage calculation error: {e}", exc_info=True)
            return 0.0

    def _calculate_structure_score(self, analysis: Dict[str, Any]) -> float:
        """êµ¬ì¡° ì ìˆ˜ ê³„ì‚° (ì•ˆì „í•œ ë²„ì „)"""
        try:
            score = 0.0

            # ì œëª© ì¡´ì¬ ì—¬ë¶€ (20ì )
            if analysis.get("has_title", False):
                score += 0.2

            # ì„¹ì…˜ í¬í•¨ë„ (60ì )
            section_coverage = analysis.get("section_coverage", {})
            if section_coverage:
                section_scores = list(section_coverage.values())
                if section_scores:
                    avg_section_coverage = sum(section_scores) / len(section_scores)
                    score += avg_section_coverage * 0.6

            # í’ˆì§ˆ ì§€í‘œ (20ì )
            quality_indicators = analysis.get("quality_indicators", {})
            if quality_indicators:
                quality_score = sum(quality_indicators.values()) / len(quality_indicators)
                score += quality_score * 0.2

            return min(1.0, score)

        except Exception as e:
            logger.error(f"Structure score calculation error: {e}", exc_info=True)
            return 0.0

    def _analyze_quality_indicators(self, answer: str) -> Dict[str, float]:
        """í’ˆì§ˆ ì§€í‘œ ë¶„ì„ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            answer_lower = answer.lower()
            quality_scores = {}

            for indicator_type, keywords in self.quality_indicators.items():
                if keywords and len(keywords) > 0:
                    matched_keywords = sum(1 for keyword in keywords if keyword.lower() in answer_lower)
                    quality_scores[indicator_type] = matched_keywords / len(keywords)
                else:
                    quality_scores[indicator_type] = 0.0

            return quality_scores

        except Exception as e:
            logger.error(f"Quality indicators analysis error: {e}", exc_info=True)
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {indicator_type: 0.0 for indicator_type in self.quality_indicators.keys()}

    def _generate_structure_improvements(self, analysis: Dict[str, Any],
                                       template: Dict[str, Any],
                                       retrieved_docs: Optional[List[Dict[str, Any]]] = None,
                                       legal_references: Optional[List[str]] = None,
                                       legal_citations: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
        """êµ¬ì¡°í™” ê°œì„  ì œì•ˆ ìƒì„± (ë²•ì  ê·¼ê±° ì •ë³´ í¬í•¨)"""
        improvements = []

        # None ì²´í¬ ë° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        retrieved_docs = retrieved_docs if retrieved_docs is not None else []
        legal_references = legal_references if legal_references is not None else []
        legal_citations = legal_citations if legal_citations is not None else []

        # íƒ€ì… ì•ˆì „ì„± ë³´ì¥
        if not isinstance(retrieved_docs, list):
            retrieved_docs = []
        if not isinstance(legal_references, list):
            legal_references = []
        if not isinstance(legal_citations, list):
            legal_citations = []

        # ì œëª© ì¶”ê°€ ì œì•ˆ
        if not analysis["has_title"]:
            improvements.append({
                "type": "add_title",
                "priority": "high",
                "suggestion": f"ë‹µë³€ì— ì œëª©ì„ ì¶”ê°€í•˜ì„¸ìš”: '{template['title']}'",
                "impact": "ë†’ìŒ"
            })

        # ë²•ì  ê·¼ê±° ì„¹ì…˜ ì¶”ê°€ ì œì•ˆ (ê·¼ê±° ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        if retrieved_docs or legal_references or legal_citations:
            has_legal_basis_section = any(
                section.get("name", "").lower() in ["ë²•ì ê·¼ê±°", "ì°¸ê³ ë²•ë ¹", "íŒë¡€", "legal_basis", "references"]
                for section in analysis.get("found_sections", [])
            )

            if not has_legal_basis_section:
                improvements.append({
                    "type": "add_legal_basis_section",
                    "priority": "high",
                    "suggestion": "ë²•ì  ê·¼ê±° ë° ì°¸ê³  ìë£Œ ì„¹ì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”",
                    "impact": "ë†’ìŒ",
                    "legal_docs_count": len(retrieved_docs),
                    "references_count": len(legal_references),
                    "citations_count": len(legal_citations)
                })

        # ëˆ„ë½ëœ ì„¹ì…˜ ì¶”ê°€ ì œì•ˆ
        for missing_section in analysis["missing_sections"]:
            section_info = next((s for s in template["sections"] if s["name"] == missing_section), None)
            if section_info:
                improvements.append({
                    "type": "add_section",
                    "priority": section_info["priority"],
                    "section_name": missing_section,
                    "suggestion": f"'{missing_section}' ì„¹ì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”",
                    "template": section_info["template"],
                    "content_guide": section_info["content_guide"],
                    "impact": "ì¤‘ê°„" if section_info["priority"] == "medium" else "ë†’ìŒ"
                })

        # í’ˆì§ˆ ì§€í‘œ ê°œì„  ì œì•ˆ
        for indicator_type, score in analysis["quality_indicators"].items():
            if score < 0.5:  # 50% ë¯¸ë§Œì´ë©´ ê°œì„  í•„ìš”
                improvements.append({
                    "type": "improve_quality",
                    "priority": "medium",
                    "indicator_type": indicator_type,
                    "suggestion": f"{indicator_type} ì§€í‘œë¥¼ ê°œì„ í•˜ì„¸ìš”",
                    "current_score": score,
                    "target_score": 0.7,
                    "impact": "ì¤‘ê°„"
                })

        return improvements

    def _create_structured_answer(self, answer: str, template: Dict[str, Any],
                                improvements: List[Dict[str, Any]],
                                retrieved_docs: Optional[List[Dict[str, Any]]] = None,
                                legal_references: Optional[List[str]] = None,
                                legal_citations: Optional[List[Dict[str, Any]]] = None) -> str:
        """êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„± (ë²•ì  ê·¼ê±° ì •ë³´ í¬í•¨)"""
        structured_parts = []

        # None ì²´í¬ ë° íƒ€ì… ì•ˆì „ì„± ë³´ì¥
        retrieved_docs = retrieved_docs if retrieved_docs is not None else []
        legal_references = legal_references if legal_references is not None else []
        legal_citations = legal_citations if legal_citations is not None else []

        if not isinstance(retrieved_docs, list):
            retrieved_docs = []
        if not isinstance(legal_references, list):
            legal_references = []
        if not isinstance(legal_citations, list):
            legal_citations = []

        # ì œëª© ì¶”ê°€
        if not re.search(r'^#+\s+', answer, re.MULTILINE):
            structured_parts.append(f"## {template['title']}")
            structured_parts.append("")

        # ê¸°ì¡´ ë‹µë³€ì„ ì„¹ì…˜ë³„ë¡œ ì¬êµ¬ì„±
        current_answer = answer

        # ì„¹ì…˜ë³„ë¡œ ë‚´ìš© ì¬êµ¬ì„±
        sections = template.get("sections", [])
        if not isinstance(sections, list):
            sections = []

        for section in sections:
            if not section or not isinstance(section, dict):
                continue

            section_name = section.get("name", "ì„¹ì…˜")
            section_template = section.get("template", "")

            # í•´ë‹¹ ì„¹ì…˜ê³¼ ê´€ë ¨ëœ ë‚´ìš© ì¶”ì¶œ
            section_content = self._extract_section_content(current_answer, section)

            # None ì²´í¬
            if section_content is None:
                section_content = ""

            # ë²•ì  ê·¼ê±° ì„¹ì…˜ì¸ ê²½ìš° ê·¼ê±° ì •ë³´ í¬í•¨
            is_legal_basis_section = any(
                keyword in section_name.lower()
                for keyword in ["ë²•ì ê·¼ê±°", "ì°¸ê³ ë²•ë ¹", "íŒë¡€", "legal_basis", "references", "ì¶œì²˜"]
            )

            if is_legal_basis_section and (retrieved_docs or legal_references or legal_citations):
                # ë²•ì  ê·¼ê±° ì •ë³´ë¥¼ ì„¹ì…˜ ë‚´ìš©ì— ì¶”ê°€
                legal_basis_content = self._format_legal_basis_content(
                    retrieved_docs, legal_references, legal_citations
                )
                if legal_basis_content:
                    section_content = f"{section_content}\n\n{legal_basis_content}".strip() if section_content else legal_basis_content

            # ë¹ˆ ì„¹ì…˜ ê²€ì¦ - ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì„¹ì…˜ ìƒì„±í•˜ì§€ ì•ŠìŒ
            if not self._validate_section_content(section_content):
                # í•„ìˆ˜ ì„¹ì…˜(priority: high)ë„ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ìƒì„±í•˜ì§€ ì•ŠìŒ
                # (ì›ë˜ëŠ” ê°€ì´ë“œë§Œ í‘œì‹œí–ˆì§€ë§Œ, ì´ì œëŠ” ì™„ì „íˆ ì œì™¸)
                continue

            # ìœ íš¨í•œ ì„¹ì…˜ë§Œ ì¶”ê°€
            structured_parts.append(f"### {section_name}")
            structured_parts.append(section_template)
            structured_parts.append("")
            structured_parts.append(section_content)
            structured_parts.append("")

            # ê°œì„  ì œì•ˆ ì ìš©
        improvements = improvements if improvements is not None else []
        for improvement in improvements:
            if not improvement or not isinstance(improvement, dict):
                continue

            if improvement.get("type") == "add_section":
                section_name = improvement.get("section_name", "ì„¹ì…˜")
                section_template = improvement.get("template", "")
                content_guide = improvement.get("content_guide", "")

                # ê°€ì´ë“œë§Œ ìˆëŠ” ë¹ˆ ì„¹ì…˜ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ
                if not content_guide or len(content_guide.strip()) < 20:
                    continue

                structured_parts.append(f"### {section_name}")
                structured_parts.append(section_template)
                structured_parts.append("")
                structured_parts.append(f"*{content_guide}*")
                structured_parts.append("")
            elif improvement.get("type") == "add_legal_basis_section":
                # ë²•ì  ê·¼ê±° ì„¹ì…˜ ì¶”ê°€
                legal_basis_content = self._format_legal_basis_content(
                    retrieved_docs, legal_references, legal_citations
                )
                if legal_basis_content:
                    structured_parts.append("### ì°¸ê³  ë²•ë ¹ ë° íŒë¡€")
                    structured_parts.append("")
                    structured_parts.append(legal_basis_content)
                    structured_parts.append("")

        return "\n".join(structured_parts)

    def _filter_relevant_documents(
        self,
        retrieved_docs: List[Dict[str, Any]],
        min_relevance_score: float = 0.3
    ) -> List[Dict[str, Any]]:
        """ê´€ë ¨ì„± ê²€ì¦ ë° í•„í„°ë§"""
        filtered_docs = []

        for doc in retrieved_docs:
            if not isinstance(doc, dict):
                continue

            # ê´€ë ¨ë„ ìŠ¤ì½”ì–´ í™•ì¸
            score = doc.get("relevance_score", doc.get("score", 0.0))

            # ìµœì†Œ ê´€ë ¨ë„ ë¯¸ë§Œì´ë©´ ì œì™¸
            if score < min_relevance_score:
                continue

            # ë¬¸ì„œ íƒ€ì… ê²€ì¦ (ë¹ˆ íƒ€ì… ì œì™¸)
            doc_type = doc.get("type", "").strip()
            if not doc_type:
                continue

            filtered_docs.append(doc)

        # ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        filtered_docs.sort(
            key=lambda x: x.get("relevance_score", x.get("score", 0.0)),
            reverse=True
        )

        return filtered_docs

    def _format_legal_basis_content(
        self,
        retrieved_docs: List[Dict[str, Any]],
        legal_references: List[str],
        legal_citations: List[Dict[str, Any]]
    ) -> str:
        """ë²•ì  ê·¼ê±° ì •ë³´ë¥¼ í¬ë§·íŒ… (ê´€ë ¨ì„± ê²€ì¦ í¬í•¨)"""
        content_parts = []

        # None ì²´í¬ ë° íƒ€ì… ì•ˆì „ì„± ë³´ì¥
        retrieved_docs = retrieved_docs if retrieved_docs is not None else []
        legal_references = legal_references if legal_references is not None else []
        legal_citations = legal_citations if legal_citations is not None else []

        if not isinstance(retrieved_docs, list):
            retrieved_docs = []
        if not isinstance(legal_references, list):
            legal_references = []
        if not isinstance(legal_citations, list):
            legal_citations = []

        # ê´€ë ¨ì„± ê²€ì¦ ë° í•„í„°ë§
        filtered_docs = self._filter_relevant_documents(retrieved_docs, min_relevance_score=0.3)

        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ (ê´€ë ¨ì„± ë†’ì€ ê²ƒë§Œ)
        if filtered_docs:
            content_parts.append("#### ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ")
            for i, doc in enumerate(filtered_docs[:5], 1):  # ìµœëŒ€ 5ê°œ
                if not isinstance(doc, dict):
                    continue

                doc_type = doc.get("type", "ë¬¸ì„œ")
                source = doc.get("source", doc.get("title", "ì•Œ ìˆ˜ ì—†ìŒ"))
                content = doc.get("content", doc.get("text", ""))
                score = doc.get("relevance_score", doc.get("score", 0.0))

                # ë‚´ìš© ìš”ì•½ (ìµœëŒ€ 200ì) - None ì²´í¬
                if content is None:
                    content = ""
                content_preview = content[:200] + "..." if len(content) > 200 else content

                content_parts.append(f"{i}. **{doc_type}**: {source}")
                if score > 0:
                    # ê´€ë ¨ë„ ë‚®ì€ ê²½ìš° í‘œì‹œ
                    if score < 0.5:
                        content_parts.append(f"   - ê´€ë ¨ë„: {score:.2f} (ì°¸ê³ ìš©)")
                    else:
                        content_parts.append(f"   - ê´€ë ¨ë„: {score:.2f}")
                content_parts.append(f"   - ë‚´ìš©: {content_preview}")
                content_parts.append("")

        # ë²•ì  ì°¸ê³  ìë£Œ
        if legal_references:
            content_parts.append("#### ì°¸ê³  ë²•ë ¹")
            for ref in legal_references:
                if ref is not None and ref.strip():
                    content_parts.append(f"- {ref}")
            content_parts.append("")

        # ë²•ì  ì¸ìš©
        if legal_citations:
            content_parts.append("#### ë²•ì  ì¸ìš©")
            for citation in legal_citations:
                if citation is not None:
                    if isinstance(citation, dict):
                        citation_text = citation.get("text", citation.get("citation", str(citation)))
                    else:
                        citation_text = str(citation)

                    if citation_text and citation_text.strip():
                        content_parts.append(f"- {citation_text}")
            content_parts.append("")

        return "\n".join(content_parts).strip()

    def _extract_section_content(self, answer: str, section: Dict[str, Any]) -> str:
        """ì„¹ì…˜ë³„ ë‚´ìš© ì¶”ì¶œ"""
        try:
            # None ì²´í¬
            if answer is None:
                answer = ""
            if section is None or not isinstance(section, dict):
                return ""

            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ
            section_keywords = self._extract_section_keywords(section)

            # None ì²´í¬
            if section_keywords is None:
                section_keywords = []

            # ë¬¸ë‹¨ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë ¨ ë¬¸ë‹¨ ì°¾ê¸°
            paragraphs = answer.split('\n\n')
            relevant_paragraphs = []

            for paragraph in paragraphs:
                if paragraph and any(keyword.lower() in paragraph.lower() for keyword in section_keywords if keyword):
                    relevant_paragraphs.append(paragraph)

            return '\n\n'.join(relevant_paragraphs) if relevant_paragraphs else ""
        except Exception as e:
            logger.error(f"Error in _extract_section_content: {e}", exc_info=True)
            return ""

    def _calculate_quality_metrics(self, structured_answer: str) -> Dict[str, Any]:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° (ì•ˆì „í•œ ë²„ì „)"""
        try:
            metrics = {
                "structure_score": 0.0,
                "completeness_score": 0.0,
                "legal_accuracy_score": 0.0,
                "practical_guidance_score": 0.0,
                "overall_score": 0.0
            }

            if not structured_answer or not isinstance(structured_answer, str):
                return metrics

            # êµ¬ì¡° ì ìˆ˜ (ì„¹ì…˜ ìˆ˜ì™€ ì œëª© ì¡´ì¬ ì—¬ë¶€)
            section_count = len(re.findall(r'^###\s+', structured_answer, re.MULTILINE))
            has_title = bool(re.search(r'^##\s+', structured_answer, re.MULTILINE))
            metrics["structure_score"] = min(1.0, (section_count * 0.2) + (0.2 if has_title else 0))

            # ì™„ì„±ë„ ì ìˆ˜ (í’ˆì§ˆ ì§€í‘œ ê¸°ë°˜)
            quality_indicators = self._analyze_quality_indicators(structured_answer)
            if quality_indicators and len(quality_indicators) > 0:
                metrics["completeness_score"] = sum(quality_indicators.values()) / len(quality_indicators)
            else:
                metrics["completeness_score"] = 0.0

            # ë²•ì  ì •í™•ì„± ì ìˆ˜
            metrics["legal_accuracy_score"] = quality_indicators.get("legal_accuracy", 0.0)

            # ì‹¤ë¬´ ì¡°ì–¸ ì ìˆ˜
            metrics["practical_guidance_score"] = quality_indicators.get("practical_guidance", 0.0)

            # ì „ì²´ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
            metrics["overall_score"] = (
                metrics["structure_score"] * 0.3 +
                metrics["completeness_score"] * 0.3 +
                metrics["legal_accuracy_score"] * 0.2 +
                metrics["practical_guidance_score"] * 0.2
            )

            return metrics

        except Exception as e:
            logger.error(f"Quality metrics calculation error: {e}", exc_info=True)
            return {
                "structure_score": 0.0,
                "completeness_score": 0.0,
                "legal_accuracy_score": 0.0,
                "practical_guidance_score": 0.0,
                "overall_score": 0.0
            }

    def enhance_answer_with_legal_basis(self, answer: str, question_type: QuestionType,
                                       query: str = "") -> Dict[str, Any]:
        """ë²•ì  ê·¼ê±°ë¥¼ í¬í•¨í•œ ë‹µë³€ ê°•í™”"""
        try:
            # 1. ë²•ì  ì¸ìš© ì¶”ì¶œ ë° ê°•í™”
            citation_result = self.citation_enhancer.enhance_text_with_citations(answer)

            # 2. ë²•ì  ê·¼ê±° ê²€ì¦
            validation_result = self.basis_validator.validate_legal_basis(query, answer)

            # 3. êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
            structured_answer = self.create_structured_answer(answer, question_type)

            # 4. ë²•ì  ê·¼ê±° ì„¹ì…˜ ì¶”ê°€
            enhanced_answer = self._add_legal_basis_section(
                structured_answer, citation_result, validation_result
            )

            return {
                "original_answer": answer,
                "enhanced_answer": enhanced_answer,
                "structured_answer": structured_answer,
                "citations": citation_result,
                "validation": validation_result,
                "legal_basis_summary": citation_result.get("legal_basis_summary", {}),
                "confidence": validation_result.confidence,
                "is_legally_sound": validation_result.is_valid
            }

        except Exception as e:
            logger.error(f"Error enhancing answer with legal basis: {e}", exc_info=True)
            return {
                "original_answer": answer,
                "enhanced_answer": answer,
                "structured_answer": answer,
                "citations": {"citations": [], "citation_count": 0},
                "validation": {"is_valid": False, "confidence": 0.0},
                "legal_basis_summary": {},
                "confidence": 0.0,
                "is_legally_sound": False,
                "error": str(e)
            }

    def _add_legal_basis_section(self, structured_answer: str,
                                citation_result: Dict[str, Any],
                                validation_result: Any) -> str:
        """ë²•ì  ê·¼ê±° ì„¹ì…˜ ì¶”ê°€"""
        try:
            legal_basis_section = "\n\n### ğŸ“š ë²•ì  ê·¼ê±°\n\n"

            # ì¸ìš© í†µê³„
            citation_count = citation_result.get("citation_count", 0)
            if citation_count > 0:
                legal_basis_section += f"**ì´ {citation_count}ê°œì˜ ë²•ì  ì¸ìš©ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.**\n\n"

                # ë²•ë ¹ ì¸ìš©
                laws_referenced = citation_result.get("legal_basis_summary", {}).get("laws_referenced", [])
                if laws_referenced:
                    legal_basis_section += "**ê´€ë ¨ ë²•ë ¹:**\n"
                    for law in laws_referenced[:5]:  # ìµœëŒ€ 5ê°œ
                        legal_basis_section += f"- {law['formatted']} (ì‹ ë¢°ë„: {law['confidence']:.2f})\n"
                    legal_basis_section += "\n"

                # íŒë¡€ ì¸ìš©
                precedents_referenced = citation_result.get("legal_basis_summary", {}).get("precedents_referenced", [])
                if precedents_referenced:
                    legal_basis_section += "**ê´€ë ¨ íŒë¡€:**\n"
                    for precedent in precedents_referenced[:5]:  # ìµœëŒ€ 5ê°œ
                        legal_basis_section += f"- {precedent['formatted']} (ì‹ ë¢°ë„: {precedent['confidence']:.2f})\n"
                    legal_basis_section += "\n"

                # ë²•ì› íŒê²°
                court_decisions = citation_result.get("legal_basis_summary", {}).get("court_decisions", [])
                if court_decisions:
                    legal_basis_section += "**ë²•ì› íŒê²°:**\n"
                    for decision in court_decisions[:3]:  # ìµœëŒ€ 3ê°œ
                        legal_basis_section += f"- {decision['formatted']} (ì‹ ë¢°ë„: {decision['confidence']:.2f})\n"
                    legal_basis_section += "\n"
            else:
                legal_basis_section += "**ë²•ì  ì¸ìš©ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**\n\n"

            # ê²€ì¦ ê²°ê³¼
            if hasattr(validation_result, 'confidence'):
                confidence_level = "ë†’ìŒ" if validation_result.confidence >= 0.8 else "ë³´í†µ" if validation_result.confidence >= 0.6 else "ë‚®ìŒ"
                legal_basis_section += f"**ë²•ì  ê·¼ê±° ì‹ ë¢°ë„:** {confidence_level} ({validation_result.confidence:.2f})\n\n"

                if validation_result.is_valid:
                    legal_basis_section += "âœ… **ë²•ì  ê·¼ê±°ê°€ ì¶©ë¶„íˆ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.**\n\n"
                else:
                    legal_basis_section += "âš ï¸ **ë²•ì  ê·¼ê±° ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.**\n\n"

            # ê¶Œì¥ì‚¬í•­
            if hasattr(validation_result, 'recommendations') and validation_result.recommendations:
                legal_basis_section += "**ê°œì„  ê¶Œì¥ì‚¬í•­:**\n"
                for recommendation in validation_result.recommendations[:3]:  # ìµœëŒ€ 3ê°œ
                    legal_basis_section += f"- {recommendation}\n"
                legal_basis_section += "\n"

            # ë©´ì±… ì¡°í•­
            legal_basis_section += "> **ë©´ì±… ì¡°í•­:** ë³¸ ë‹µë³€ì€ ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, ê°œë³„ ì‚¬ì•ˆì— ëŒ€í•œ ë²•ë¥  ìë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë³€í˜¸ì‚¬ì™€ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n"

            return structured_answer + legal_basis_section

        except Exception as e:
            logger.error(f"Error adding legal basis section: {e}", exc_info=True)
            return structured_answer

    def get_legal_citation_statistics(self, text: str) -> Dict[str, Any]:
        """ë²•ì  ì¸ìš© í†µê³„ ì¡°íšŒ"""
        try:
            citation_result = self.citation_enhancer.enhance_text_with_citations(text)

            return {
                "total_citations": citation_result.get("citation_count", 0),
                "citation_types": citation_result.get("citation_stats", {}),
                "confidence_distribution": citation_result.get("confidence_distribution", {}),
                "legal_basis_summary": citation_result.get("legal_basis_summary", {}),
                "enhanced_text": citation_result.get("enhanced_text", text)
            }

        except Exception as e:
            logger.error(f"Error getting citation statistics: {e}", exc_info=True)
            return {
                "total_citations": 0,
                "citation_types": {},
                "confidence_distribution": {},
                "legal_basis_summary": {},
                "enhanced_text": text
            }

    def validate_answer_legal_basis(self, query: str, answer: str) -> Dict[str, Any]:
        """ë‹µë³€ì˜ ë²•ì  ê·¼ê±° ê²€ì¦"""
        try:
            validation_result = self.basis_validator.validate_legal_basis(query, answer)

            return {
                "is_valid": validation_result.is_valid,
                "confidence": validation_result.confidence,
                "validation_details": validation_result.validation_details,
                "legal_sources": validation_result.legal_sources,
                "issues": validation_result.issues,
                "recommendations": validation_result.recommendations
            }

        except Exception as e:
            logger.error(f"Error validating legal basis: {e}", exc_info=True)
            return {
                "is_valid": False,
                "confidence": 0.0,
                "validation_details": [],
                "legal_sources": [],
                "issues": [f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                "recommendations": ["ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”"]
            }

    def reload_templates(self):
        """í…œí”Œë¦¿ ë™ì  ë¦¬ë¡œë“œ"""
        try:
            self.structure_templates = self._load_structure_templates()
            self.quality_indicators = self._load_quality_indicators()
            # ìºì‹œ ë¬´íš¨í™”
            if hasattr(self, '_few_shot_examples_cache'):
                self._few_shot_examples_cache = None
            logger.info("Templates reloaded successfully")
        except Exception as e:
            logger.error(f"Failed to reload templates: {e}", exc_info=True)

    def get_template_info(self, question_type: str) -> Dict[str, Any]:
        """í…œí”Œë¦¿ ì •ë³´ ì¡°íšŒ"""
        try:
            # ì§ˆë¬¸ ìœ í˜• ë§¤í•‘
            try:
                question_type_enum = QuestionType(question_type)
            except ValueError:
                question_type_enum = QuestionType.GENERAL_QUESTION

            template = self.structure_templates.get(question_type_enum)

            if template:
                return {
                    "question_type": question_type,
                    "title": template.get("title", "Unknown"),
                    "section_count": len(template.get("sections", [])),
                    "sections": template.get("sections", []),
                    "source": "hardcoded"
                }
            else:
                return {
                    "question_type": question_type,
                    "title": "Unknown",
                    "section_count": 0,
                    "sections": [],
                    "source": "not_found"
                }
        except Exception as e:
            logger.error(f"Failed to get template info: {e}", exc_info=True)
            return {
                "question_type": question_type,
                "title": "Error",
                "section_count": 0,
                "sections": [],
                "source": "error"
            }

    def create_structured_answer(self, answer: str, question_type: QuestionType) -> str:
        """êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±"""
        try:
            # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
            template = self.structure_templates.get(question_type, {})

            if not template:
                return answer

            # êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
            structured_answer = f"## {template.get('title', 'ë‹µë³€')}\n\n"

            # ê° ì„¹ì…˜ë³„ë¡œ ë‚´ìš© êµ¬ì„±
            sections = template.get('sections', [])
            for section in sections:
                if section.get('priority') == 'high':
                    structured_answer += f"### {section['name']}\n"
                    structured_answer += f"{section['template']}\n\n"
                    structured_answer += f"{answer}\n\n"
                    break  # ì²« ë²ˆì§¸ high priority ì„¹ì…˜ë§Œ ì‚¬ìš©

            return structured_answer

        except Exception as e:
            logger.error(f"Error creating structured answer: {e}", exc_info=True)
            return answer


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
answer_structure_enhancer = AnswerStructureEnhancer()
