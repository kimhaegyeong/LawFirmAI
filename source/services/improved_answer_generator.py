# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸°
Ollama í´ë¼ì´ì–¸íŠ¸ì™€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ í™œìš©í•œ ì§€ëŠ¥í˜• ë‹µë³€ ìƒì„±
"""

import logging
import time
from typing import Dict, Any, Optional, List
from dataclasses import dataclass

from services.ollama_client import OllamaClient, OllamaResponse
from services.prompt_templates import PromptTemplateManager
from services.confidence_calculator import ConfidenceCalculator, ConfidenceInfo
from services.question_classifier import QuestionType, QuestionClassification
from services.answer_formatter import AnswerFormatter, FormattedAnswer
from services.context_builder import ContextBuilder, ContextWindow

logger = logging.getLogger(__name__)


@dataclass
class AnswerResult:
    """ë‹µë³€ ìƒì„± ê²°ê³¼"""
    answer: str
    formatted_answer: Optional[FormattedAnswer]
    raw_answer: str
    confidence: ConfidenceInfo
    question_type: QuestionType
    processing_time: float
    tokens_used: int
    model_info: Dict[str, Any]


class ImprovedAnswerGenerator:
    """ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸°"""
    
    def __init__(self, 
                 ollama_client: Optional[OllamaClient] = None,
                 prompt_template_manager: Optional[PromptTemplateManager] = None,
                 confidence_calculator: Optional[ConfidenceCalculator] = None,
                 answer_formatter: Optional[AnswerFormatter] = None,
                 context_builder: Optional[ContextBuilder] = None):
        """ë‹µë³€ ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.ollama_client = ollama_client or OllamaClient()
        self.prompt_template_manager = prompt_template_manager or PromptTemplateManager()
        self.confidence_calculator = confidence_calculator or ConfidenceCalculator()
        self.answer_formatter = answer_formatter or AnswerFormatter()
        self.context_builder = context_builder or ContextBuilder()
        
        # ë‹µë³€ ìƒì„± ì„¤ì •
        self.generation_config = {
            "max_tokens": 2048,
            "temperature": 0.7,
            "retry_count": 3,
            "retry_delay": 1.0
        }
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹ë³„ ì„¤ì •
        self.question_type_configs = {
            QuestionType.PRECEDENT_SEARCH: {
                "temperature": 0.6,  # ë” ì •í™•í•œ ë‹µë³€
                "max_tokens": 1500
            },
            QuestionType.LAW_INQUIRY: {
                "temperature": 0.5,  # ë§¤ìš° ì •í™•í•œ ë‹µë³€
                "max_tokens": 1200
            },
            QuestionType.LEGAL_ADVICE: {
                "temperature": 0.7,  # ê· í˜•ìž¡ížŒ ë‹µë³€
                "max_tokens": 2000
            },
            QuestionType.PROCEDURE_GUIDE: {
                "temperature": 0.6,  # êµ¬ì¡°í™”ëœ ë‹µë³€
                "max_tokens": 1800
            },
            QuestionType.TERM_EXPLANATION: {
                "temperature": 0.5,  # ì •í™•í•œ ì„¤ëª…
                "max_tokens": 1000
            },
            QuestionType.GENERAL_QUESTION: {
                "temperature": 0.7,  # ì¼ë°˜ì ì¸ ë‹µë³€
                "max_tokens": 1500
            }
        }
    
    def generate_answer(self, 
                       query: str,
                       question_type: QuestionClassification,
                       context: str,
                       sources: Dict[str, List[Dict[str, Any]]],
                       conversation_history: Optional[List[Dict[str, Any]]] = None) -> AnswerResult:
        """
        ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ìž ì§ˆë¬¸
            question_type: ì§ˆë¬¸ ë¶„ë¥˜ ê²°ê³¼
            context: ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            sources: ê²€ìƒ‰ëœ ì†ŒìŠ¤ë“¤
            conversation_history: ëŒ€í™” ì´ë ¥
            
        Returns:
            AnswerResult: ìƒì„±ëœ ë‹µë³€ ê²°ê³¼
        """
        try:
            start_time = time.time()
            self.logger.info(f"Generating answer for query: {query[:100]}...")
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ ì„¤ì • ì ìš©
            config = self.question_type_configs.get(
                question_type.question_type, 
                self.generation_config
            )
            
            # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ìµœì í™”
            context_window = self.context_builder.build_optimized_context(
                query=query,
                question_classification=question_type,
                search_results=sources,
                conversation_history=conversation_history
            )
            
            # ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            optimized_context = self.context_builder.format_context_for_llm(context_window)
            prompt = self._build_enhanced_prompt(query, question_type, optimized_context, sources)
            
            # Ollamaë¡œ ë‹µë³€ ìƒì„± (ìž¬ì‹œë„ ë¡œì§ í¬í•¨)
            raw_answer = self._generate_with_retry(prompt, config)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self.confidence_calculator.calculate_confidence(
                query=query,
                retrieved_docs=sources.get("results", []),
                answer=raw_answer
            )
            
            # ë‹µë³€ í›„ì²˜ë¦¬
            processed_answer = self._post_process_answer(raw_answer, question_type, sources)
            
            # ë‹µë³€ êµ¬ì¡°í™”
            formatted_answer = self.answer_formatter.format_answer(
                raw_answer=raw_answer,
                question_type=question_type.question_type,
                sources=sources,
                confidence=confidence
            )
            
            processing_time = time.time() - start_time
            
            result = AnswerResult(
                answer=processed_answer,
                formatted_answer=formatted_answer,
                raw_answer=raw_answer,
                confidence=confidence,
                question_type=question_type.question_type,
                processing_time=processing_time,
                tokens_used=self._estimate_tokens(raw_answer),
                model_info={"model": self.ollama_client.model_name}
            )
            
            self.logger.info(f"Answer generated successfully: {len(processed_answer)} chars, "
                           f"confidence: {confidence.confidence:.3f}, "
                           f"time: {processing_time:.2f}s")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error generating answer: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë‹µë³€ ë°˜í™˜
            return self._create_fallback_answer(query, question_type, e)
    
    def _build_enhanced_prompt(self, 
                              query: str,
                              question_type: QuestionClassification,
                              context: str,
                              sources: Dict[str, List[Dict[str, Any]]]) -> str:
        """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            context_data = {
                "precedent_list": sources.get("precedent_results", []),
                "law_articles": sources.get("law_results", []),
                "context": sources.get("results", [])
            }
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
            prompt = self.prompt_template_manager.format_prompt(
                question_type=question_type.question_type,
                context_data=context_data,
                user_query=query
            )
            
            # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ê°€ ìžˆìœ¼ë©´ í¬í•¨
            if context:
                prompt += f"\n\nì¶”ê°€ ì»¨í…ìŠ¤íŠ¸:\n{context}"
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹ë³„ ì§€ì‹œì‚¬í•­ ì¶”ê°€
            special_instructions = self._get_special_instructions(question_type.question_type)
            if special_instructions:
                prompt += f"\n\n{special_instructions}"
            
            return prompt
            
        except Exception as e:
            self.logger.error(f"Error building enhanced prompt: {e}")
            return f"ì§ˆë¬¸: {query}\n\nìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ìž‘ì„±í•˜ì„¸ìš”."
    
    def _get_special_instructions(self, question_type: QuestionType) -> str:
        """ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹ë³„ ì§€ì‹œì‚¬í•­"""
        instructions = {
            QuestionType.PRECEDENT_SEARCH: """
íŠ¹ë³„ ì§€ì‹œì‚¬í•­:
- íŒë¡€ì˜ í•µì‹¬ íŒê²°ìš”ì§€ë¥¼ ëª…í™•ížˆ ì œì‹œí•˜ì„¸ìš”
- ì‚¬ê±´ë²ˆí˜¸ì™€ ë²•ì› ì •ë³´ë¥¼ ì •í™•ížˆ ì¸ìš©í•˜ì„¸ìš”
- í•´ë‹¹ íŒë¡€ì˜ ì‹¤ë¬´ì  ì‹œì‚¬ì ì„ ì„¤ëª…í•˜ì„¸ìš”""",
            
            QuestionType.LAW_INQUIRY: """
íŠ¹ë³„ ì§€ì‹œì‚¬í•­:
- ë²•ë¥  ì¡°ë¬¸ì„ ì •í™•ížˆ ì¸ìš©í•˜ì„¸ìš”
- ë²•ë¥ ì˜ ëª©ì ê³¼ ì·¨ì§€ë¥¼ ì„¤ëª…í•˜ì„¸ìš”
- ì‹¤ì œ ì ìš© ì‚¬ë¡€ë¥¼ í¬í•¨í•˜ì„¸ìš”""",
            
            QuestionType.LEGAL_ADVICE: """
íŠ¹ë³„ ì§€ì‹œì‚¬í•­:
- ë‹¨ê³„ë³„ í•´ê²° ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”
- í•„ìš”í•œ ì¦ê±° ìžë£Œë¥¼ ëª…ì‹œí•˜ì„¸ìš”
- ì „ë¬¸ê°€ ìƒë‹´ì˜ í•„ìš”ì„±ì„ ì–¸ê¸‰í•˜ì„¸ìš”""",
            
            QuestionType.PROCEDURE_GUIDE: """
íŠ¹ë³„ ì§€ì‹œì‚¬í•­:
- ì ˆì°¨ë¥¼ ìˆœì„œëŒ€ë¡œ ì„¤ëª…í•˜ì„¸ìš”
- í•„ìš”í•œ ì„œë¥˜ì™€ ë¹„ìš©ì„ ëª…ì‹œí•˜ì„¸ìš”
- ì²˜ë¦¬ ê¸°ê°„ì„ í¬í•¨í•˜ì„¸ìš”""",
            
            QuestionType.TERM_EXPLANATION: """
íŠ¹ë³„ ì§€ì‹œì‚¬í•­:
- ìš©ì–´ì˜ ì •í™•í•œ ì •ì˜ë¥¼ ì œì‹œí•˜ì„¸ìš”
- ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸ì„ ì¸ìš©í•˜ì„¸ìš”
- ì‹¤ì œ ì ìš© ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”"""
        }
        
        return instructions.get(question_type, "")
    
    def _generate_with_retry(self, prompt: str, config: Dict[str, Any]) -> str:
        """ìž¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ ë‹µë³€ ìƒì„±"""
        try:
            for attempt in range(self.generation_config["retry_count"]):
                try:
                    response = self.ollama_client.generate(
                        prompt=prompt,
                        temperature=config.get("temperature", self.generation_config["temperature"]),
                        max_tokens=config.get("max_tokens", self.generation_config["max_tokens"])
                    )
                    
                    if response.response and len(response.response.strip()) > 0:
                        return response.response
                    else:
                        self.logger.warning(f"Empty response on attempt {attempt + 1}")
                        
                except Exception as e:
                    self.logger.warning(f"Generation attempt {attempt + 1} failed: {e}")
                    
                if attempt < self.generation_config["retry_count"] - 1:
                    time.sleep(self.generation_config["retry_delay"])
            
            # ëª¨ë“  ìž¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
        except Exception as e:
            self.logger.error(f"All generation attempts failed: {e}")
            return "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _post_process_answer(self, 
                             raw_answer: str, 
                             question_type: QuestionClassification,
                             sources: Dict[str, List[Dict[str, Any]]]) -> str:
        """ë‹µë³€ í›„ì²˜ë¦¬"""
        try:
            # ê¸°ë³¸ ì •ë¦¬
            processed = raw_answer.strip()
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ í›„ì²˜ë¦¬
            if question_type.question_type == QuestionType.PRECEDENT_SEARCH:
                processed = self._enhance_precedent_answer(processed, sources)
            elif question_type.question_type == QuestionType.LAW_INQUIRY:
                processed = self._enhance_law_answer(processed, sources)
            elif question_type.question_type == QuestionType.LEGAL_ADVICE:
                processed = self._enhance_advice_answer(processed, sources)
            
            # ê³µí†µ í›„ì²˜ë¦¬
            processed = self._add_disclaimer(processed)
            
            return processed
            
        except Exception as e:
            self.logger.error(f"Error in post-processing: {e}")
            return raw_answer
    
    def _enhance_precedent_answer(self, answer: str, sources: Dict[str, List[Dict[str, Any]]]) -> str:
        """íŒë¡€ ë‹µë³€ í–¥ìƒ"""
        try:
            precedents = sources.get("precedent_results", [])
            if precedents:
                answer += f"\n\n**ì°¸ê³  íŒë¡€ ({len(precedents)}ê°œ):**\n"
                for i, prec in enumerate(precedents[:3], 1):
                    answer += f"{i}. {prec.get('case_name', '')} ({prec.get('case_number', '')})\n"
            return answer
        except Exception as e:
            self.logger.error(f"Error enhancing precedent answer: {e}")
            return answer
    
    def _enhance_law_answer(self, answer: str, sources: Dict[str, List[Dict[str, Any]]]) -> str:
        """ë²•ë¥  ë‹µë³€ í–¥ìƒ"""
        try:
            laws = sources.get("law_results", [])
            if laws:
                answer += f"\n\n**ê´€ë ¨ ë²•ë¥  ({len(laws)}ê°œ):**\n"
                for i, law in enumerate(laws[:3], 1):
                    answer += f"{i}. {law.get('law_name', '')} {law.get('article_number', '')}\n"
            return answer
        except Exception as e:
            self.logger.error(f"Error enhancing law answer: {e}")
            return answer
    
    def _enhance_advice_answer(self, answer: str, sources: Dict[str, List[Dict[str, Any]]]) -> str:
        """ì¡°ì–¸ ë‹µë³€ í–¥ìƒ"""
        try:
            laws = sources.get("law_results", [])
            precedents = sources.get("precedent_results", [])
            
            if laws or precedents:
                answer += f"\n\n**ì°¸ê³  ìžë£Œ:**\n"
                if laws:
                    answer += f"- ê´€ë ¨ ë²•ë¥ : {len(laws)}ê°œ\n"
                if precedents:
                    answer += f"- ê´€ë ¨ íŒë¡€: {len(precedents)}ê°œ\n"
            
            return answer
        except Exception as e:
            self.logger.error(f"Error enhancing advice answer: {e}")
            return answer
    
    def _add_disclaimer(self, answer: str) -> str:
        """ë©´ì±… ì¡°í•­ ì¶”ê°€"""
        disclaimer = """

---
ðŸ’¼ **ë©´ì±… ì¡°í•­**
ë³¸ ë‹µë³€ì€ ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, ê°œë³„ ì‚¬ì•ˆì— ëŒ€í•œ ë²•ë¥  ìžë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤.
êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë³€í˜¸ì‚¬ì™€ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëžë‹ˆë‹¤."""
        
        return answer + disclaimer
    
    def _estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì • (ëŒ€ëžµì )"""
        # í•œêµ­ì–´ ê¸°ì¤€ ëŒ€ëžµì ì¸ í† í° ìˆ˜ ì¶”ì •
        return len(text) // 2
    
    def _create_fallback_answer(self, 
                               query: str, 
                               question_type: QuestionClassification,
                               error: Exception) -> AnswerResult:
        """ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë‹µë³€ ìƒì„±"""
        try:
            fallback_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ì˜¤ë¥˜ ë‚´ìš©: {str(error)}

ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ì‹œ ì‹œë„í•´ë³´ì‹œê¸° ë°”ëžë‹ˆë‹¤:
1. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ìž‘ì„±í•´ì£¼ì„¸ìš”
2. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”
3. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìžì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”

ì „ë¬¸ì ì¸ ë²•ë¥  ìƒë‹´ì´ í•„ìš”í•˜ì‹œë©´ ë³€í˜¸ì‚¬ì™€ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëžë‹ˆë‹¤."""
            
            return AnswerResult(
                answer=fallback_answer,
                formatted_answer=None,
                raw_answer=fallback_answer,
                confidence=self.confidence_calculator.calculate_confidence(
                    query, [], fallback_answer
                ),
                question_type=question_type.question_type,
                processing_time=0.0,
                tokens_used=self._estimate_tokens(fallback_answer),
                model_info={"model": "fallback"}
            )
            
        except Exception as e:
            self.logger.error(f"Error creating fallback answer: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨
            return AnswerResult(
                answer="ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                formatted_answer=None,
                raw_answer="ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                confidence=ConfidenceInfo(
                    confidence=0.0,
                    reliability_level="VERY_LOW",
                    similarity_score=0.0,
                    matching_score=0.0,
                    answer_quality=0.0,
                    warnings=["ë‹µë³€ ìƒì„± ì‹¤íŒ¨"],
                    recommendations=["ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ì „ë¬¸ê°€ ìƒë‹´"]
                ),
                question_type=QuestionType.GENERAL_QUESTION,
                processing_time=0.0,
                tokens_used=0,
                model_info={"model": "error"}
            )


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_improved_answer_generator():
    """ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸"""
    generator = ImprovedAnswerGenerator()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_query = "ì†í•´ë°°ìƒ ì²­êµ¬ ë°©ë²•"
    test_question_type = QuestionClassification(
        question_type=QuestionType.LEGAL_ADVICE,
        law_weight=0.5,
        precedent_weight=0.5,
        confidence=0.8,
        keywords=["ì†í•´ë°°ìƒ", "ì²­êµ¬"],
        patterns=[]
    )
    test_context = "ë¯¼ë²• ì œ750ì¡° ë¶ˆë²•í–‰ìœ„ ê´€ë ¨ ì§ˆë¬¸"
    test_sources = {
        "results": [
            {"type": "law", "law_name": "ë¯¼ë²•", "article_number": "ì œ750ì¡°", "similarity": 0.9},
            {"type": "precedent", "case_name": "ì†í•´ë°°ìƒ ì‚¬ê±´", "case_number": "2023ë‹¤12345", "similarity": 0.8}
        ],
        "law_results": [
            {"law_name": "ë¯¼ë²•", "article_number": "ì œ750ì¡°", "content": "ë¶ˆë²•í–‰ìœ„ë¡œ ì¸í•œ ì†í•´ë°°ìƒ"}
        ],
        "precedent_results": [
            {"case_name": "ì†í•´ë°°ìƒ ì‚¬ê±´", "case_number": "2023ë‹¤12345", "summary": "ë¶ˆë²•í–‰ìœ„ ì†í•´ë°°ìƒ"}
        ]
    }
    
    print("=== ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸ ===")
    print(f"ì§ˆë¬¸: {test_query}")
    print(f"ì§ˆë¬¸ ìœ í˜•: {test_question_type.question_type.value}")
    
    try:
        result = generator.generate_answer(
            query=test_query,
            question_type=test_question_type,
            context=test_context,
            sources=test_sources
        )
        
        print(f"\në‹µë³€ ê²°ê³¼:")
        print(f"- ë‹µë³€ ê¸¸ì´: {len(result.answer)} ë¬¸ìž")
        print(f"- êµ¬ì¡°í™”ëœ ë‹µë³€ ê¸¸ì´: {len(result.formatted_answer.formatted_content) if result.formatted_answer else 0} ë¬¸ìž")
        print(f"- ì‹ ë¢°ë„: {result.confidence.confidence:.3f}")
        print(f"- ì‹ ë¢°ë„ ìˆ˜ì¤€: {result.confidence.reliability_level.value}")
        print(f"- ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
        print(f"- ì¶”ì • í† í° ìˆ˜: {result.tokens_used}")
        print(f"- ëª¨ë¸: {result.model_info['model']}")
        
        print(f"\në‹µë³€ ë‚´ìš©:")
        print(result.answer[:500] + "..." if len(result.answer) > 500 else result.answer)
        
        if result.formatted_answer:
            print(f"\nêµ¬ì¡°í™”ëœ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°:")
            print(result.formatted_answer.formatted_content[:500] + "..." if len(result.formatted_answer.formatted_content) > 500 else result.formatted_answer.formatted_content)
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    test_improved_answer_generator()
