# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸°
Ollama í´ë¼ì´ì–¸íŠ¸ì™€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ í™œìš©í•œ ì§€ëŠ¥í˜• ë‹µë³€ ìƒì„±
"""

import logging
import time
from typing import Dict, Any, Optional, List
from dataclasses import dataclass

from .gemini_client import GeminiClient, GeminiResponse
from .prompt_templates import PromptTemplateManager
from .unified_prompt_manager import UnifiedPromptManager, LegalDomain, ModelType
from .prompt_optimizer import PromptOptimizer, PromptPerformanceMetrics
from .semantic_domain_classifier import SemanticDomainClassifier
from .confidence_calculator import ConfidenceCalculator, ConfidenceInfo, ConfidenceLevel
from .question_classifier import QuestionType, QuestionClassification
from .answer_formatter import AnswerFormatter, FormattedAnswer
from .context_builder import ContextBuilder, ContextWindow

logger = logging.getLogger(__name__)


@dataclass
class AnswerResult:
    """ë‹µë³€ ìƒì„± ê²°ê³¼"""
    answer: str
    formatted_answer: Optional[FormattedAnswer]
    raw_answer: str
    confidence: ConfidenceInfo
    question_type: QuestionType
    processing_time: float
    tokens_used: int
    model_info: Dict[str, Any]


class ImprovedAnswerGenerator:
    """ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸°"""
    
    def __init__(self, 
                 gemini_client: Optional[GeminiClient] = None,
                 prompt_template_manager: Optional[PromptTemplateManager] = None,
                 confidence_calculator: Optional[ConfidenceCalculator] = None,
                 answer_formatter: Optional[AnswerFormatter] = None,
                 context_builder: Optional[ContextBuilder] = None,
                 unified_prompt_manager: Optional[UnifiedPromptManager] = None,
                 prompt_optimizer: Optional[PromptOptimizer] = None):
        """ë‹µë³€ ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.gemini_client = gemini_client or GeminiClient()
        self.prompt_template_manager = prompt_template_manager or PromptTemplateManager()
        self.confidence_calculator = confidence_calculator or ConfidenceCalculator()
        self.answer_formatter = answer_formatter or AnswerFormatter()
        self.context_builder = context_builder or ContextBuilder()
        self.unified_prompt_manager = unified_prompt_manager or UnifiedPromptManager()
        self.prompt_optimizer = prompt_optimizer or PromptOptimizer(self.unified_prompt_manager)
        self.semantic_domain_classifier = SemanticDomainClassifier()
        
        # ë‹µë³€ ìƒì„± ì„¤ì •
        self.generation_config = {
            "max_tokens": 2048,
            "temperature": 0.7,
            "retry_count": 3,
            "retry_delay": 1.0
        }
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹ë³„ ì„¤ì •
        self.question_type_configs = {
            QuestionType.PRECEDENT_SEARCH: {
                "temperature": 0.6,  # ë” ì •í™•í•œ ë‹µë³€
                "max_tokens": 1500
            },
            QuestionType.LAW_INQUIRY: {
                "temperature": 0.5,  # ë§¤ìš° ì •í™•í•œ ë‹µë³€
                "max_tokens": 1200
            },
            QuestionType.LEGAL_ADVICE: {
                "temperature": 0.7,  # ê· í˜•ìž¡ížŒ ë‹µë³€
                "max_tokens": 2000
            },
            QuestionType.PROCEDURE_GUIDE: {
                "temperature": 0.6,  # êµ¬ì¡°í™”ëœ ë‹µë³€
                "max_tokens": 1800
            },
            QuestionType.TERM_EXPLANATION: {
                "temperature": 0.5,  # ì •í™•í•œ ì„¤ëª…
                "max_tokens": 1000
            },
            QuestionType.GENERAL_QUESTION: {
                "temperature": 0.7,  # ì¼ë°˜ì ì¸ ë‹µë³€
                "max_tokens": 1500
            }
        }
    
    def generate_answer(self, 
                       query: str,
                       question_type: QuestionClassification,
                       context: str,
                       sources: Dict[str, List[Dict[str, Any]]],
                       conversation_history: Optional[List[Dict[str, Any]]] = None) -> AnswerResult:
        """
        ë‹µë³€ ìƒì„± - ì°¸ê³  ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€
        """
        try:
            start_time = time.time()
            self.logger.info(f"Generating answer for query: {query[:100]}...")
            
            # ì°¸ê³  ë°ì´í„°ê°€ ìžˆëŠ”ì§€ ë¨¼ì € í™•ì¸
            if not self._has_meaningful_sources(sources):
                self.logger.info("No meaningful sources found, returning no-sources response")
                return self._create_no_sources_answer(query, question_type)
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ ì„¤ì • ì ìš©
            config = self.question_type_configs.get(
                question_type.question_type, 
                self.generation_config
            )
            
            # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ìµœì í™”
            context_window = self.context_builder.build_optimized_context(
                query=query,
                question_classification=question_type,
                search_results=sources,
                conversation_history=conversation_history
            )
            
            # ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            optimized_context = self.context_builder.format_context_for_llm(context_window)
            
            # ì˜ë¯¸ ê¸°ë°˜ ë„ë©”ì¸ ë¶„ë¥˜ ì‚¬ìš©
            domain, domain_confidence, domain_reasoning = self.semantic_domain_classifier.classify_domain(
                query, question_type.question_type
            )
            model_type = ModelType.GEMINI  # ê¸°ë³¸ê°’
            
            # ë„ë©”ì¸ ë¶„ë¥˜ ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"Domain classification: {domain.value} (confidence: {domain_confidence:.2f}) - {domain_reasoning}")
            
            prompt = self.unified_prompt_manager.get_optimized_prompt(
                query=query,
                question_type=question_type.question_type,
                domain=domain,
                context=sources,
                model_type=model_type
            )
            
            # Ollamaë¡œ ë‹µë³€ ìƒì„± (ìž¬ì‹œë„ ë¡œì§ í¬í•¨)
            raw_answer = self._generate_with_retry(prompt, config)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            sources_list = []
            if hasattr(sources, '__dict__'):
                # UnifiedSearchResult ê°ì²´ì¸ ê²½ìš° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                sources_list = [{
                    'content': getattr(sources, 'content', ''),
                    'title': getattr(sources, 'title', ''),
                    'source': getattr(sources, 'source', ''),
                    'score': getattr(sources, 'score', 0.0)
                }]
            elif isinstance(sources, dict):
                sources_list = sources.get("results", [])
            elif isinstance(sources, list):
                sources_list = sources
            
            confidence = self.confidence_calculator.calculate_confidence(
                answer=raw_answer,
                sources=sources_list,
                question_type=question_type.question_type.value if hasattr(question_type, 'question_type') else "general"
            )
            
            # ë‹µë³€ í›„ì²˜ë¦¬
            processed_answer = self._post_process_answer(raw_answer, question_type, sources)
            
            # ë‹µë³€ êµ¬ì¡°í™”
            formatted_answer = self.answer_formatter.format_answer(
                raw_answer=raw_answer,
                question_type=question_type.question_type,
                sources=sources,
                confidence=confidence
            )
            
            processing_time = time.time() - start_time
            
            result = AnswerResult(
                answer=processed_answer,
                formatted_answer=formatted_answer,
                raw_answer=raw_answer,
                confidence=confidence,
                question_type=question_type.question_type,
                processing_time=processing_time,
                tokens_used=self._estimate_tokens(raw_answer),
                model_info={"model": self.gemini_client.model_name}
            )
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
            self._record_performance_metrics(
                query=query,
                question_type=question_type,
                domain=domain,
                model_type=model_type,
                response_time=processing_time,
                answer_quality=confidence.confidence,
                context_length=len(str(sources)),
                token_count=self._estimate_tokens(raw_answer),
                domain_confidence=domain_confidence
            )
            
            self.logger.info(f"Answer generated successfully: {len(processed_answer)} chars, "
                           f"confidence: {confidence.confidence:.3f}, "
                           f"time: {processing_time:.2f}s")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error generating answer: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë‹µë³€ ë°˜í™˜
            return self._create_fallback_answer(query, question_type, e)
    
    
    def _record_performance_metrics(self, query: str, question_type: QuestionClassification, 
                                  domain: LegalDomain, model_type: ModelType,
                                  response_time: float, answer_quality: float,
                                  context_length: int, token_count: int,
                                  domain_confidence: float = 0.0) -> None:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡ (ë„ë©”ì¸ ë¶„ë¥˜ ì‹ ë¢°ë„ í¬í•¨)"""
        try:
            # ê³ ìœ  í”„ë¡¬í”„íŠ¸ ID ìƒì„±
            prompt_id = f"{domain.value}_{question_type.question_type.value}_{hash(query) % 10000}"
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìƒì„±
            metrics = PromptPerformanceMetrics(
                prompt_id=prompt_id,
                model_type=model_type,
                domain=domain,
                question_type=question_type.question_type,
                response_time=response_time,
                token_count=token_count,
                context_length=context_length,
                answer_quality_score=answer_quality
            )
            
            # ë„ë©”ì¸ ë¶„ë¥˜ ì‹ ë¢°ë„ ì¶”ê°€ (ë©”íƒ€ë°ì´í„°ë¡œ)
            if hasattr(metrics, 'metadata'):
                metrics.metadata = {
                    "domain_confidence": domain_confidence,
                    "classification_method": "semantic"
                }
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
            self.prompt_optimizer.record_performance(metrics)
            
        except Exception as e:
            self.logger.error(f"Error recording performance metrics: {e}")
    
    def _build_enhanced_prompt(self, 
                              query: str,
                              question_type: QuestionClassification,
                              context: str,
                              sources: Dict[str, List[Dict[str, Any]]]) -> str:
        """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            context_data = {
                "precedent_list": sources.get("precedent_results", []),
                "law_articles": sources.get("law_results", []),
                "context": sources.get("results", [])
            }
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
            prompt = self.prompt_template_manager.format_prompt(
                question_type=question_type.question_type,
                context_data=context_data,
                user_query=query
            )
            
            # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ê°€ ìžˆìœ¼ë©´ í¬í•¨
            if context:
                prompt += f"\n\nì¶”ê°€ ì»¨í…ìŠ¤íŠ¸:\n{context}"
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹ë³„ ì§€ì‹œì‚¬í•­ ì¶”ê°€
            special_instructions = self._get_special_instructions(question_type.question_type)
            if special_instructions:
                prompt += f"\n\n{special_instructions}"
            
            return prompt
            
        except Exception as e:
            self.logger.error(f"Error building enhanced prompt: {e}")
            return f"ì§ˆë¬¸: {query}\n\nìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ìž‘ì„±í•˜ì„¸ìš”."
    
    def _get_special_instructions(self, question_type: QuestionType) -> str:
        """ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹ë³„ ì§€ì‹œì‚¬í•­"""
        instructions = {
            QuestionType.PRECEDENT_SEARCH: """
íŠ¹ë³„ ì§€ì‹œì‚¬í•­:
- íŒë¡€ì˜ í•µì‹¬ íŒê²°ìš”ì§€ë¥¼ ëª…í™•ížˆ ì œì‹œí•˜ì„¸ìš”
- ì‚¬ê±´ë²ˆí˜¸ì™€ ë²•ì› ì •ë³´ë¥¼ ì •í™•ížˆ ì¸ìš©í•˜ì„¸ìš”
- í•´ë‹¹ íŒë¡€ì˜ ì‹¤ë¬´ì  ì‹œì‚¬ì ì„ ì„¤ëª…í•˜ì„¸ìš”
- ì§ˆë¬¸ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš” (ì¶”ê°€ ì§ˆë¬¸ ìš”ì²­ ê¸ˆì§€)""",
            
            QuestionType.LAW_INQUIRY: """
íŠ¹ë³„ ì§€ì‹œì‚¬í•­:
- ë²•ë¥  ì¡°ë¬¸ì„ ì •í™•ížˆ ì¸ìš©í•˜ì„¸ìš”
- ë²•ë¥ ì˜ ëª©ì ê³¼ ì·¨ì§€ë¥¼ ì„¤ëª…í•˜ì„¸ìš”
- ì‹¤ì œ ì ìš© ì‚¬ë¡€ë¥¼ í¬í•¨í•˜ì„¸ìš”
- ì§ˆë¬¸ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš” (ì¶”ê°€ ì§ˆë¬¸ ìš”ì²­ ê¸ˆì§€)""",
            
            QuestionType.LEGAL_ADVICE: """
íŠ¹ë³„ ì§€ì‹œì‚¬í•­:
- ë‹¨ê³„ë³„ í•´ê²° ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”
- í•„ìš”í•œ ì¦ê±° ìžë£Œë¥¼ ëª…ì‹œí•˜ì„¸ìš”
- ì „ë¬¸ê°€ ìƒë‹´ì˜ í•„ìš”ì„±ì„ ì–¸ê¸‰í•˜ì„¸ìš”
- ì§ˆë¬¸ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš” (ì¶”ê°€ ì§ˆë¬¸ ìš”ì²­ ê¸ˆì§€)""",
            
            QuestionType.PROCEDURE_GUIDE: """
íŠ¹ë³„ ì§€ì‹œì‚¬í•­:
- ì ˆì°¨ë¥¼ ìˆœì„œëŒ€ë¡œ ì„¤ëª…í•˜ì„¸ìš”
- í•„ìš”í•œ ì„œë¥˜ì™€ ë¹„ìš©ì„ ëª…ì‹œí•˜ì„¸ìš”
- ì²˜ë¦¬ ê¸°ê°„ì„ í¬í•¨í•˜ì„¸ìš”
- ì§ˆë¬¸ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš” (ì¶”ê°€ ì§ˆë¬¸ ìš”ì²­ ê¸ˆì§€)""",
            
            QuestionType.TERM_EXPLANATION: """
íŠ¹ë³„ ì§€ì‹œì‚¬í•­:
- ìš©ì–´ì˜ ì •í™•í•œ ì •ì˜ë¥¼ ì œì‹œí•˜ì„¸ìš”
- ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸ì„ ì¸ìš©í•˜ì„¸ìš”
- ì‹¤ì œ ì ìš© ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”
- ì§ˆë¬¸ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš” (ì¶”ê°€ ì§ˆë¬¸ ìš”ì²­ ê¸ˆì§€)"""
        }
        
        return instructions.get(question_type, "")
    
    def _generate_with_retry(self, prompt: str, config: Dict[str, Any]) -> str:
        """ìž¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ ë‹µë³€ ìƒì„±"""
        try:
            for attempt in range(self.generation_config["retry_count"]):
                try:
                    response = self.gemini_client.generate(
                        prompt=prompt,
                        temperature=config.get("temperature", self.generation_config["temperature"]),
                        max_tokens=config.get("max_tokens", self.generation_config["max_tokens"])
                    )
                    
                    if response.response and len(response.response.strip()) > 0:
                        return response.response
                    else:
                        self.logger.warning(f"Empty response on attempt {attempt + 1}")
                        
                except Exception as e:
                    self.logger.warning(f"Generation attempt {attempt + 1} failed: {e}")
                    
                if attempt < self.generation_config["retry_count"] - 1:
                    time.sleep(self.generation_config["retry_delay"])
            
            # ëª¨ë“  ìž¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
        except Exception as e:
            self.logger.error(f"All generation attempts failed: {e}")
            return "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _post_process_answer(self, 
                             raw_answer: str, 
                             question_type: QuestionClassification,
                             sources: Dict[str, List[Dict[str, Any]]]) -> str:
        """ë‹µë³€ í›„ì²˜ë¦¬"""
        try:
            # ê¸°ë³¸ ì •ë¦¬
            processed = raw_answer.strip()
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ í›„ì²˜ë¦¬
            if question_type.question_type == QuestionType.PRECEDENT_SEARCH:
                processed = self._enhance_precedent_answer(processed, sources)
            elif question_type.question_type == QuestionType.LAW_INQUIRY:
                processed = self._enhance_law_answer(processed, sources)
            elif question_type.question_type == QuestionType.LEGAL_ADVICE:
                processed = self._enhance_advice_answer(processed, sources)
            
            # ê³µí†µ í›„ì²˜ë¦¬
            processed = self._add_disclaimer(processed)
            
            return processed
            
        except Exception as e:
            self.logger.error(f"Error in post-processing: {e}")
            return raw_answer
    
    def _enhance_precedent_answer(self, answer: str, sources: Dict[str, List[Dict[str, Any]]]) -> str:
        """íŒë¡€ ë‹µë³€ í–¥ìƒ"""
        try:
            precedents = sources.get("precedent_results", [])
            if precedents:
                answer += f"\n\n**ì°¸ê³  íŒë¡€ ({len(precedents)}ê°œ):**\n"
                for i, prec in enumerate(precedents[:3], 1):
                    answer += f"{i}. {prec.get('case_name', '')} ({prec.get('case_number', '')})\n"
            return answer
        except Exception as e:
            self.logger.error(f"Error enhancing precedent answer: {e}")
            return answer
    
    def _enhance_law_answer(self, answer: str, sources: Dict[str, List[Dict[str, Any]]]) -> str:
        """ë²•ë¥  ë‹µë³€ í–¥ìƒ"""
        try:
            laws = sources.get("law_results", [])
            if laws:
                answer += f"\n\n**ê´€ë ¨ ë²•ë¥  ({len(laws)}ê°œ):**\n"
                for i, law in enumerate(laws[:3], 1):
                    answer += f"{i}. {law.get('law_name', '')} {law.get('article_number', '')}\n"
            return answer
        except Exception as e:
            self.logger.error(f"Error enhancing law answer: {e}")
            return answer
    
    def _enhance_advice_answer(self, answer: str, sources: Dict[str, List[Dict[str, Any]]]) -> str:
        """ì¡°ì–¸ ë‹µë³€ í–¥ìƒ"""
        try:
            laws = sources.get("law_results", [])
            precedents = sources.get("precedent_results", [])
            
            if laws or precedents:
                answer += f"\n\n**ì°¸ê³  ìžë£Œ:**\n"
                if laws:
                    answer += f"- ê´€ë ¨ ë²•ë¥ : {len(laws)}ê°œ\n"
                if precedents:
                    answer += f"- ê´€ë ¨ íŒë¡€: {len(precedents)}ê°œ\n"
            
            return answer
        except Exception as e:
            self.logger.error(f"Error enhancing advice answer: {e}")
            return answer
    
    def _add_disclaimer(self, answer: str) -> str:
        """ë©´ì±… ì¡°í•­ ì¶”ê°€"""
        disclaimer = """

---
ðŸ’¼ **ë©´ì±… ì¡°í•­**

ë³¸ ë‹µë³€ì€ ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, ê°œë³„ ì‚¬ì•ˆì— ëŒ€í•œ ë²•ë¥  ìžë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤.
êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë³€í˜¸ì‚¬ì™€ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëžë‹ˆë‹¤."""
        
        return answer + disclaimer
    
    def _estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì • (ëŒ€ëžµì )"""
        # í•œêµ­ì–´ ê¸°ì¤€ ëŒ€ëžµì ì¸ í† í° ìˆ˜ ì¶”ì •
        return len(text) // 2
    
    def _create_fallback_answer(self, 
                               query: str, 
                               question_type: QuestionClassification,
                               error: Exception) -> AnswerResult:
        """ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë‹µë³€ ìƒì„±"""
        try:
            fallback_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ì˜¤ë¥˜ ë‚´ìš©: {str(error)}

ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ì‹œ ì‹œë„í•´ë³´ì‹œê¸° ë°”ëžë‹ˆë‹¤:
1. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ìž‘ì„±í•´ì£¼ì„¸ìš”
2. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”
3. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìžì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”

ì „ë¬¸ì ì¸ ë²•ë¥  ìƒë‹´ì´ í•„ìš”í•˜ì‹œë©´ ë³€í˜¸ì‚¬ì™€ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëžë‹ˆë‹¤."""
            
            return AnswerResult(
                answer=fallback_answer,
                formatted_answer=None,
                raw_answer=fallback_answer,
                confidence=self.confidence_calculator.calculate_confidence(
                    answer=fallback_answer,
                    sources=[],
                    question_type=question_type.question_type.value if hasattr(question_type, 'question_type') else "general"
                ),
                question_type=question_type.question_type,
                processing_time=0.0,
                tokens_used=self._estimate_tokens(fallback_answer),
                model_info={"model": "fallback"}
            )
            
        except Exception as e:
            self.logger.error(f"Error creating fallback answer: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨
            return AnswerResult(
                answer="ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                formatted_answer=None,
                raw_answer="ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                confidence=ConfidenceInfo(
                    confidence=0.0,
                    level=ConfidenceLevel.VERY_LOW,
                    factors={"error": 1.0},
                    explanation="ë‹µë³€ ìƒì„± ì‹¤íŒ¨"
                ),
                question_type=QuestionType.GENERAL_QUESTION,
                processing_time=0.0,
                tokens_used=0,
                model_info={"model": "error"}
            )
    
    def _has_meaningful_sources(self, sources: Dict[str, List[Dict[str, Any]]]) -> bool:
        """ì˜ë¯¸ìžˆëŠ” ì°¸ê³  ë°ì´í„°ê°€ ìžˆëŠ”ì§€ í™•ì¸ - ê°•í™”ëœ ë²„ì „"""
        if not sources:
            return False
        
        # ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
        sources_list = []
        if isinstance(sources, dict):
            sources_list.extend(sources.get("results", []))
            sources_list.extend(sources.get("law_results", []))
            sources_list.extend(sources.get("precedent_results", []))
        elif isinstance(sources, list):
            sources_list = sources
        
        if not sources_list:
            return False
        
        # ìµœì†Œ ê´€ë ¨ë„ ìž„ê³„ê°’ ì„¤ì • (ë” ì—„ê²©í•˜ê²Œ)
        MIN_RELEVANCE_THRESHOLD = 0.4  # 0.3ì—ì„œ 0.4ë¡œ ìƒí–¥
        MIN_CONTENT_LENGTH = 100  # 50ì—ì„œ 100ìœ¼ë¡œ ìƒí–¥
        
        meaningful_sources = []
        for source in sources_list:
            relevance_score = source.get("similarity", source.get("score", 0.0))
            content = source.get("content", "")
            
            # ê´€ë ¨ë„ê°€ ë†’ê³  ë‚´ìš©ì´ ì¶©ë¶„í•œ ì†ŒìŠ¤ë§Œ ìœ íš¨í•œ ê²ƒìœ¼ë¡œ íŒë‹¨
            if relevance_score >= MIN_RELEVANCE_THRESHOLD and len(content.strip()) > MIN_CONTENT_LENGTH:
                meaningful_sources.append(source)
        
        # ì¶”ê°€ ê²€ì¦: ì‹¤ì œ ë²•ë¥  ê´€ë ¨ ë‚´ìš©ì¸ì§€ í™•ì¸
        if meaningful_sources:
            legal_keywords = ["ë²•ë¥ ", "ì¡°ë¬¸", "íŒë¡€", "ë²•ì›", "ë²•ë ¹", "ê·œì •", "ì¡°í•­", "ë²•ì ", "ë²•ë¥ ì "]
            legal_content_count = 0
            
            for source in meaningful_sources:
                content = source.get("content", "").lower()
                if any(keyword in content for keyword in legal_keywords):
                    legal_content_count += 1
            
            # ë²•ë¥  ê´€ë ¨ ë‚´ìš©ì´ ì ˆë°˜ ì´ìƒì´ì–´ì•¼ ìœ íš¨
            return legal_content_count >= len(meaningful_sources) * 0.5
        
        return False
    
    def _create_no_sources_answer(self, query: str, question_type: QuestionClassification) -> AnswerResult:
        """ì°¸ê³  ë°ì´í„°ê°€ ì—†ì„ ë•Œì˜ ë‹µë³€ ìƒì„±"""
        try:
            query_type_value = question_type.question_type.value if hasattr(question_type, 'question_type') else "general"
            
            if query_type_value == "legal_advice":
                no_sources_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë²•ë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
â€¢ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”
â€¢ ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸ì´ë‚˜ íŒë¡€ê°€ ìžˆë‹¤ë©´ í•¨ê»˜ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
â€¢ ì¼ë°˜ì ì¸ ë²•ë¥  ì ˆì°¨ì— ëŒ€í•´ì„œëŠ” ì•ˆë‚´í•´ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤

êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë³€í˜¸ì‚¬ì™€ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëžë‹ˆë‹¤."""
            
            elif query_type_value == "precedent":
                no_sources_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì™€ ê´€ë ¨ëœ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
â€¢ ì‚¬ê±´ë²ˆí˜¸ë‚˜ ë²•ì›ëª…ì„ í¬í•¨í•´ì„œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”
â€¢ ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”
â€¢ ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸ì„ ë¨¼ì € í™•ì¸í•´ë³´ì„¸ìš”

íŒë¡€ ê²€ìƒ‰ì´ ì–´ë ¤ìš°ì‹œë©´ ë²•ì› ë„ì„œê´€ì´ë‚˜ ë²•ë¥  ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ìš©í•´ë³´ì‹œê¸° ë°”ëžë‹ˆë‹¤."""
            
            else:
                no_sources_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
â€¢ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ìž‘ì„±í•´ì£¼ì„¸ìš”
â€¢ ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸ì´ë‚˜ íŒë¡€ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
â€¢ í‚¤ì›Œë“œë¥¼ ë” ëª…í™•í•˜ê²Œ í•´ì£¼ì„¸ìš”

ì¼ë°˜ì ì¸ ë²•ë¥  ìƒì‹ì´ë‚˜ ì ˆì°¨ì— ëŒ€í•´ì„œëŠ” ì•ˆë‚´í•´ë“œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."""
            
            return AnswerResult(
                answer=no_sources_answer,
                formatted_answer=None,
                raw_answer=no_sources_answer,
                confidence=ConfidenceInfo(
                    confidence=0.0,
                    level=ConfidenceLevel.VERY_LOW,
                    factors={"no_sources": 1.0},
                    explanation="ì°¸ê³  ë°ì´í„° ì—†ìŒ"
                ),
                question_type=question_type.question_type,
                processing_time=0.0,
                tokens_used=self._estimate_tokens(no_sources_answer),
                model_info={"model": "no_sources"}
            )
            
        except Exception as e:
            self.logger.error(f"Error creating no-sources answer: {e}")
            return self._create_fallback_answer(query, question_type, e)


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_improved_answer_generator():
    """ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸"""
    generator = ImprovedAnswerGenerator()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_query = "ì†í•´ë°°ìƒ ì²­êµ¬ ë°©ë²•"
    test_question_type = QuestionClassification(
        question_type=QuestionType.LEGAL_ADVICE,
        law_weight=0.5,
        precedent_weight=0.5,
        confidence=0.8,
        keywords=["ì†í•´ë°°ìƒ", "ì²­êµ¬"],
        patterns=[]
    )
    test_context = "ë¯¼ë²• ì œ750ì¡° ë¶ˆë²•í–‰ìœ„ ê´€ë ¨ ì§ˆë¬¸"
    test_sources = {
        "results": [
            {"type": "law", "law_name": "ë¯¼ë²•", "article_number": "ì œ750ì¡°", "similarity": 0.9},
            {"type": "precedent", "case_name": "ì†í•´ë°°ìƒ ì‚¬ê±´", "case_number": "2023ë‹¤12345", "similarity": 0.8}
        ],
        "law_results": [
            {"law_name": "ë¯¼ë²•", "article_number": "ì œ750ì¡°", "content": "ë¶ˆë²•í–‰ìœ„ë¡œ ì¸í•œ ì†í•´ë°°ìƒ"}
        ],
        "precedent_results": [
            {"case_name": "ì†í•´ë°°ìƒ ì‚¬ê±´", "case_number": "2023ë‹¤12345", "summary": "ë¶ˆë²•í–‰ìœ„ ì†í•´ë°°ìƒ"}
        ]
    }
    
    print("=== ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸ ===")
    print(f"ì§ˆë¬¸: {test_query}")
    print(f"ì§ˆë¬¸ ìœ í˜•: {test_question_type.question_type.value}")
    
    try:
        result = generator.generate_answer(
            query=test_query,
            question_type=test_question_type,
            context=test_context,
            sources=test_sources
        )
        
        print(f"\në‹µë³€ ê²°ê³¼:")
        print(f"- ë‹µë³€ ê¸¸ì´: {len(result.answer)} ë¬¸ìž")
        print(f"- êµ¬ì¡°í™”ëœ ë‹µë³€ ê¸¸ì´: {len(result.formatted_answer.formatted_content) if result.formatted_answer else 0} ë¬¸ìž")
        print(f"- ì‹ ë¢°ë„: {result.confidence.confidence:.3f}")
        print(f"- ì‹ ë¢°ë„ ìˆ˜ì¤€: {result.confidence.reliability_level.value}")
        print(f"- ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
        print(f"- ì¶”ì • í† í° ìˆ˜: {result.tokens_used}")
        print(f"- ëª¨ë¸: {result.model_info['model']}")
        
        print(f"\në‹µë³€ ë‚´ìš©:")
        print(result.answer[:500] + "..." if len(result.answer) > 500 else result.answer)
        
        if result.formatted_answer:
            print(f"\nêµ¬ì¡°í™”ëœ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°:")
            print(result.formatted_answer.formatted_content[:500] + "..." if len(result.formatted_answer.formatted_content) > 500 else result.formatted_answer.formatted_content)
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    test_improved_answer_generator()
