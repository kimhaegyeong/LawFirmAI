# -*- coding: utf-8 -*-
"""
Enhanced Law Search Engine
ë²•ë ¹ í…Œì´ë¸”ê³¼ ë²¡í„° ìŠ¤í† ì–´ë¥¼ í™œìš©í•œ í–¥ìƒëœ ì¡°ë¬¸ ê²€ìƒ‰ ì—”ì§„
"""

import re
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ArticleSearchResult:
    """ì¡°ë¬¸ ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    law_name: str
    article_number: str
    article_title: Optional[str] = None
    similarity: float = 1.0
    source: str = "exact_article"
    type: str = "current_law"
    metadata: Dict[str, Any] = None


class EnhancedLawSearchEngine:
    """ë²•ë ¹ í…Œì´ë¸”ê³¼ ë²¡í„° ìŠ¤í† ì–´ë¥¼ í™œìš©í•œ í–¥ìƒëœ ì¡°ë¬¸ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self, db_manager, vector_store, precedent_service=None, hybrid_precedent_service=None):
        self.db_manager = db_manager
        self.vector_store = vector_store
        self.precedent_service = precedent_service
        self.hybrid_precedent_service = hybrid_precedent_service
        self.logger = logging.getLogger(__name__)
        
        # LawContextSearchEngine ì´ˆê¸°í™” (ê´€ë ¨ ì¡°ë¬¸ ê²€ìƒ‰ìš©)
        try:
            from .law_context_search_engine import LawContextSearchEngine
            self.context_search_engine = LawContextSearchEngine(db_manager, vector_store)
        except ImportError as e:
            self.logger.warning(f"LawContextSearchEngine ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.context_search_engine = None
        
        # ì¡°ë¬¸ íŒ¨í„´ ë§¤ì¹­ ê°•í™”
        self.article_patterns = [
            r'(\w+ë²•)\s*ì œ\s*(\d+)ì¡°\s*ì œ\s*(\d+)í•­',  # ë¯¼ë²• ì œ750ì¡° ì œ1í•­
            r'(\w+ë²•)\s*ì œ\s*(\d+)ì¡°',                 # ë¯¼ë²• ì œ750ì¡°
            r'ì œ\s*(\d+)ì¡°\s*ì œ\s*(\d+)í•­',            # ì œ750ì¡° ì œ1í•­
            r'ì œ\s*(\d+)ì¡°',                           # ì œ750ì¡°
            r'(\w+ë²•)\s*(\d+)ì¡°',                     # ë¯¼ë²• 750ì¡°
        ]
        
        # ë²•ë ¹ëª… ë§¤í•‘
        self.law_name_mapping = {
            'ë¯¼ë²•': 'ë¯¼ë²•',
            'í˜•ë²•': 'í˜•ë²•',
            'ìƒë²•': 'ìƒë²•',
            'í–‰ì •ë²•': 'í–‰ì •ë²•',
            'ë¯¼ì‚¬ì†Œì†¡ë²•': 'ë¯¼ì‚¬ì†Œì†¡ë²•',
            'í˜•ì‚¬ì†Œì†¡ë²•': 'í˜•ì‚¬ì†Œì†¡ë²•',
            'ë…¸ë™ë²•': 'ê·¼ë¡œê¸°ì¤€ë²•',
            'ê·¼ë¡œê¸°ì¤€ë²•': 'ê·¼ë¡œê¸°ì¤€ë²•',
            'ê°€ì¡±ë²•': 'ê°€ì¡±ë²•',
            'ë¶€ë™ì‚°ë²•': 'ë¶€ë™ì‚°ë²•'
        }
        
        self.logger.info("Enhanced Law Search Engine ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def search_specific_article(self, query: str) -> Optional[ArticleSearchResult]:
        """íŠ¹ì • ì¡°ë¬¸ ê²€ìƒ‰ (ì •í™•ë„ ìµœìš°ì„ )"""
        try:
            # 1. ì¡°ë¬¸ íŒ¨í„´ ë¶„ì„
            article_info = self._extract_article_info(query)
            
            if not article_info:
                self.logger.debug(f"No article pattern found in query: {query}")
                return None
            
            self.logger.info(f"Extracted article info: {article_info}")
            
            # 2. ì •í™•í•œ ì¡°ë¬¸ ê²€ìƒ‰
            exact_result = await self._search_exact_article(article_info)
            
            if exact_result:
                return ArticleSearchResult(
                    content=exact_result['article_content'],
                    law_name=exact_result['law_name'],
                    article_number=str(exact_result['article_number']),
                    article_title=exact_result.get('article_title', ''),
                    similarity=1.0,
                    source='exact_article',
                    type='current_law',
                    metadata={
                        'law_id': exact_result['law_id'],
                        'article_id': exact_result['article_id'],
                        'is_supplementary': exact_result.get('is_supplementary', False),
                        'parsing_quality_score': exact_result.get('parsing_quality_score', 0.0)
                    }
                )
            
            # 3. ìœ ì‚¬ ì¡°ë¬¸ ê²€ìƒ‰ (íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ)
            return await self._search_similar_article(query, article_info)
            
        except Exception as e:
            self.logger.error(f"Specific article search failed: {e}")
            return None
    
    def _extract_article_info(self, query: str) -> Optional[Dict[str, str]]:
        """ì¿¼ë¦¬ì—ì„œ ë²•ë ¹ëª…ê³¼ ì¡°ë¬¸ë²ˆí˜¸ ì¶”ì¶œ"""
        for pattern in self.article_patterns:
            match = re.search(pattern, query)
            if match:
                groups = match.groups()
                
                if len(groups) == 3:  # ë²•ë ¹ëª… + ì¡°ë¬¸ë²ˆí˜¸ + í•­ë²ˆí˜¸
                    return {
                        'law_name': groups[0],
                        'article_number': groups[1],
                        'paragraph_number': groups[2]
                    }
                elif len(groups) == 2:  # ë²•ë ¹ëª… + ì¡°ë¬¸ë²ˆí˜¸ ë˜ëŠ” ì¡°ë¬¸ë²ˆí˜¸ + í•­ë²ˆí˜¸
                    if groups[0].endswith('ë²•'):
                        return {
                            'law_name': groups[0],
                            'article_number': groups[1]
                        }
                    else:
                        return {
                            'law_name': 'ë¯¼ë²•',  # ê¸°ë³¸ê°’
                            'article_number': groups[0],
                            'paragraph_number': groups[1]
                        }
                elif len(groups) == 1:  # ì¡°ë¬¸ë²ˆí˜¸ë§Œ
                    return {
                        'law_name': 'ë¯¼ë²•',  # ê¸°ë³¸ê°’
                        'article_number': groups[0]
                    }
        
        return None
    
    async def _search_exact_article(self, article_info: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """ì •í™•í•œ ì¡°ë¬¸ ê²€ìƒ‰ (í˜„í–‰ë²•ë ¹ ìš°ì„ )"""
        law_name = article_info['law_name']
        article_number = int(article_info['article_number'])
        
        # 1. í˜„í–‰ë²•ë ¹ ì¡°ë¬¸ì—ì„œ ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„)
        try:
            current_laws_results = self.db_manager.search_current_laws_articles(law_name, article_number)
            if current_laws_results:
                return self._format_current_laws_result(current_laws_results[0])
        except Exception as e:
            self.logger.warning(f"í˜„í–‰ë²•ë ¹ ì¡°ë¬¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # 2. Assembly ì¡°ë¬¸ í…Œì´ë¸”ì—ì„œ ê²€ìƒ‰ (í´ë°±)
        query = """
            SELECT aa.*, al.law_name, al.law_id
            FROM assembly_articles aa
            JOIN assembly_laws al ON aa.law_id = al.law_id
            WHERE al.law_name = ? AND aa.article_number = ?
            ORDER BY aa.parsing_quality_score DESC, aa.word_count DESC
            LIMIT 1
        """
        
        try:
            results = self.db_manager.execute_query(query, (law_name, article_number))
            
            if results:
                result = results[0]
                
                    # í•­ë²ˆí˜¸ê°€ ìˆëŠ” ê²½ìš° í•´ë‹¹ í•­ë§Œ ì¶”ì¶œ
                if 'paragraph_number' in article_info:
                    paragraph_content = self._extract_paragraph_content(
                        result['article_content'], 
                        int(article_info['paragraph_number'])
                    )
                    if paragraph_content:
                        result['article_content'] = paragraph_content
                
                self.logger.info(f"Found exact article: {law_name} ì œ{article_number}ì¡°")
                return result
            
            self.logger.debug(f"No exact article found for {law_name} ì œ{article_number}ì¡°")
            return None
            
        except Exception as e:
            self.logger.error(f"Exact article search failed: {e}")
            return None
    
    async def _search_similar_article(self, query: str, article_info: Dict[str, str]) -> Optional[ArticleSearchResult]:
        """ìœ ì‚¬ ì¡°ë¬¸ ê²€ìƒ‰ (ë²¡í„° ê²€ìƒ‰ í™œìš©)"""
        try:
            # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬í•œ ì¡°ë¬¸ ì°¾ê¸°
            vector_results = self.vector_store.search(query, top_k=5)
            
            # ë²•ë ¹ëª…ê³¼ ì¡°ë¬¸ë²ˆí˜¸ë¡œ í•„í„°ë§
            filtered_results = []
            for result in vector_results:
                metadata = result.get('metadata', {})
                if (metadata.get('law_name') == article_info['law_name'] and
                    str(metadata.get('article_number')) == article_info['article_number']):
                    filtered_results.append(result)
            
            if filtered_results:
                best_result = filtered_results[0]
                return ArticleSearchResult(
                    content=best_result['content'],
                    law_name=article_info['law_name'],
                    article_number=article_info['article_number'],
                    similarity=best_result.get('similarity', 0.8),
                    source='similar_article',
                    type='current_law',
                    metadata=best_result.get('metadata', {})
                )
            
            return None
            
        except Exception as e:
            self.logger.error(f"Similar article search failed: {e}")
            return None
    
    def _format_current_laws_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """í˜„í–‰ë²•ë ¹ ì¡°ë¬¸ ê²°ê³¼ í¬ë§·íŒ…"""
        # ì¡°ë¬¸ ë‚´ìš© êµ¬ì„±
        content_parts = [result['article_content']]
        
        if result.get('paragraph_content'):
            content_parts.append(f"í•­: {result['paragraph_content']}")
        
        if result.get('sub_paragraph_content'):
            content_parts.append(f"í˜¸: {result['sub_paragraph_content']}")
        
        full_content = "\n".join(content_parts)
        
        return {
            'article_content': full_content,  # search_specific_articleì—ì„œ ê¸°ëŒ€í•˜ëŠ” í•„ë“œëª…
            'law_name': result['law_name_korean'],
            'article_number': result['article_number'],
            'article_title': result.get('article_title', ''),
            'law_id': result['law_id'],
            'article_id': result['article_id'],
            'is_supplementary': result.get('is_supplementary', False),
            'parsing_quality_score': result.get('quality_score', 0.9),
            'paragraph_content': result.get('paragraph_content', ''),
            'sub_paragraph_content': result.get('sub_paragraph_content', ''),
            'effective_date': result.get('effective_date', ''),
            'ministry_name': result.get('ministry_name', ''),
            'similarity': 1.0,
            'source': 'current_laws_articles',
            'type': 'current_law',
            'metadata': {
                'article_id': result['article_id'],
                'law_id': result['law_id'],
                'paragraph_number': result.get('paragraph_number'),
                'sub_paragraph_number': result.get('sub_paragraph_number'),
                'quality_score': result.get('quality_score', 0.9),
                'ministry_name': result.get('ministry_name', ''),
                'effective_date': result.get('effective_date', ''),
                'parsing_method': result.get('parsing_method', 'batch_parser')
            }
        }
    
    def _extract_paragraph_content(self, article_content: str, paragraph_number: int) -> Optional[str]:
        """ì¡°ë¬¸ ë‚´ìš©ì—ì„œ íŠ¹ì • í•­ì˜ ë‚´ìš© ì¶”ì¶œ"""
        try:
            # í•­ ë²ˆí˜¸ íŒ¨í„´ ë§¤ì¹­
            patterns = [
                rf'ì œ{paragraph_number}í•­\s*([^ì œ]+?)(?=ì œ\d+í•­|$)',
                rf'{paragraph_number}í•­\s*([^ì œ]+?)(?=ì œ\d+í•­|$)',
                rf'\( {paragraph_number} \)\s*([^\(]+?)(?=\( \d+ \)|$)',
                rf'\({paragraph_number}\)\s*([^\(]+?)(?=\(\d+\)|$)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, article_content, re.DOTALL)
                if match:
                    return match.group(1).strip()
            
            return None
            
        except Exception as e:
            self.logger.error(f"Paragraph extraction failed: {e}")
            return None
    
    async def search_by_keywords(self, keywords: List[str], law_name: str = None) -> List[ArticleSearchResult]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì¡°ë¬¸ ê²€ìƒ‰"""
        try:
            results = []
            
            # í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            search_query = " ".join(keywords)
            
            # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
            vector_results = self.vector_store.search(search_query, top_k=10)
            
            # ë²•ë ¹ëª… í•„í„°ë§ (ì§€ì •ëœ ê²½ìš°)
            for result in vector_results:
                metadata = result.get('metadata', {})
                
                if law_name and metadata.get('law_name') != law_name:
                    continue
                
                results.append(ArticleSearchResult(
                    content=result['content'],
                    law_name=metadata.get('law_name', ''),
                    article_number=str(metadata.get('article_number', '')),
                    article_title=metadata.get('article_title', ''),
                    similarity=result.get('similarity', 0.7),
                    source='keyword_search',
                    type='current_law',
                    metadata=metadata
                ))
            
            return results
            
        except Exception as e:
            self.logger.error(f"Keyword search failed: {e}")
            return []
    
    async def get_article_statistics(self, law_name: str = None) -> Dict[str, Any]:
        """ì¡°ë¬¸ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        try:
            stats = {}
            
            if law_name:
                # íŠ¹ì • ë²•ë ¹ì˜ ì¡°ë¬¸ í†µê³„
                query = """
                    SELECT 
                        COUNT(*) as total_articles,
                        COUNT(CASE WHEN is_supplementary = 0 THEN 1 END) as main_articles,
                        COUNT(CASE WHEN is_supplementary = 1 THEN 1 END) as supplementary_articles,
                        AVG(parsing_quality_score) as avg_quality_score,
                        AVG(word_count) as avg_word_count
                    FROM assembly_articles aa
                    JOIN assembly_laws al ON aa.law_id = al.law_id
                    WHERE al.law_name = ?
                """
                results = self.db_manager.execute_query(query, (law_name,))
            else:
                # ì „ì²´ ì¡°ë¬¸ í†µê³„
                query = """
                    SELECT 
                        COUNT(*) as total_articles,
                        COUNT(CASE WHEN is_supplementary = 0 THEN 1 END) as main_articles,
                        COUNT(CASE WHEN is_supplementary = 1 THEN 1 END) as supplementary_articles,
                        AVG(parsing_quality_score) as avg_quality_score,
                        AVG(word_count) as avg_word_count
                    FROM assembly_articles
                """
                results = self.db_manager.execute_query(query)
            
            if results:
                stats = results[0]
            
            return stats
            
        except Exception as e:
            self.logger.error(f"Article statistics failed: {e}")
            return {}
    
    def _extract_key_elements(self, content: str, law_name: str = "", article_number: str = "") -> List[str]:
        """ì¡°ë¬¸ì—ì„œ í•µì‹¬ êµ¬ì„±ìš”ê±´ ì¶”ì¶œ (ê°œì„ ëœ í†µí•© ë²„ì „)"""
        if not content:
            return ["êµ¬ì„±ìš”ê±´ ì •ë³´ ì—†ìŒ"]
        
        elements = []
        
        try:
            # 1. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•: ì—¬ëŸ¬ ë°©ë²•ë¡  ì¡°í•©
            # NLP ê¸°ë°˜ ì¶”ì¶œ
            nlp_elements = self._extract_key_elements_nlp(content, law_name, article_number)
            elements.extend(nlp_elements)
            
            # íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œ
            pattern_elements = self._extract_key_elements_pattern(content, law_name, article_number)
            elements.extend(pattern_elements)
            
            # ML ê¸°ë°˜ ì¶”ì¶œ (ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ìˆì„ ë•Œë§Œ)
            if len(content) > 50:
                ml_elements = self._extract_key_elements_ml(content, law_name, article_number)
                elements.extend(ml_elements)
            
        except Exception as e:
            self.logger.warning(f"Advanced extraction failed, falling back to basic method: {e}")
            # ê¸°ë³¸ ë°©ë²•ìœ¼ë¡œ í´ë°±
            elements.extend(self._extract_key_elements_basic(content))
        
        # 2. ì¤‘ë³µ ì œê±° ë° ì‹ ë¢°ë„ ê¸°ë°˜ ì •ë ¬
        unique_elements = self._deduplicate_and_rank_elements(elements)
        
        # 3. ìµœì¢… ê²°ê³¼ ë°˜í™˜ (ìµœëŒ€ 5ê°œ)
        return unique_elements[:5]
    
    def _extract_key_elements_basic(self, content: str) -> List[str]:
        """ê¸°ë³¸ êµ¬ì„±ìš”ê±´ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§)"""
        elements = []
        
        # ê³ ì˜/ê³¼ì‹¤ ê´€ë ¨
        if any(word in content for word in ["ê³ ì˜", "ê³¼ì‹¤"]):
            elements.append("ê³ ì˜/ê³¼ì‹¤")
        
        # í–‰ìœ„ ê´€ë ¨
        if any(word in content for word in ["í–‰ìœ„", "ìœ„ë²•", "ë¶ˆë²•"]):
            elements.append("ìœ„ë²•í–‰ìœ„")
        
        # ì†í•´ ê´€ë ¨
        if any(word in content for word in ["ì†í•´", "ë°°ìƒ"]):
            elements.append("ì†í•´ë°œìƒ")
        
        # ì¸ê³¼ê´€ê³„ ê´€ë ¨
        if any(word in content for word in ["ì¸ê³¼", "ê´€ê³„", "ì›ì¸"]):
            elements.append("ì¸ê³¼ê´€ê³„")
        
        # ì±…ì„ ê´€ë ¨
        if any(word in content for word in ["ì±…ì„", "ì˜ë¬´"]):
            elements.append("ë²•ì  ì±…ì„")
        
        # ê¸°ë³¸ êµ¬ì„±ìš”ê±´ì´ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ìš”ì†Œë“¤
        if not elements:
            elements = ["ë²•ì  ìš”ê±´", "ì ìš© ë²”ìœ„", "ë²•ì  íš¨ê³¼"]
        
        return elements
    
    def _extract_key_elements_nlp(self, content: str, law_name: str = "", article_number: str = "") -> List[str]:
        """ìì—°ì–´ì²˜ë¦¬ ê¸°ë°˜ êµ¬ì„±ìš”ê±´ ì¶”ì¶œ"""
        if not content:
            return ["êµ¬ì„±ìš”ê±´ ì •ë³´ ì—†ìŒ"]
        
        elements = []
        
        # 1. ë¬¸ì¥ êµ¬ì¡° ë¶„ì„ì„ í†µí•œ êµ¬ì„±ìš”ê±´ ì¶”ì¶œ
        sentences = self._split_legal_sentences(content)
        
        for sentence in sentences:
            # ì¡°ê±´ë¬¸ íŒ¨í„´ ë¶„ì„
            condition_elements = self._extract_condition_elements(sentence)
            elements.extend(condition_elements)
            
            # íš¨ê³¼ë¬¸ íŒ¨í„´ ë¶„ì„
            effect_elements = self._extract_effect_elements(sentence)
            elements.extend(effect_elements)
        
        # 2. ë²•ë ¹ë³„ íŠ¹í™”ëœ êµ¬ì„±ìš”ê±´ ì¶”ì¶œ
        specialized_elements = self._extract_specialized_elements(content, law_name, article_number)
        elements.extend(specialized_elements)
        
        # 3. ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
        unique_elements = self._prioritize_elements(elements)
        
        return unique_elements[:5]
    
    def _split_legal_sentences(self, content: str) -> List[str]:
        """ë²•ë¥  ë¬¸ì¥ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
        sentences = []
        
        # ë¬¸ì¥ ë íŒ¨í„´
        sentence_endings = ['.', 'ë‹¤.', 'í•œë‹¤.', 'ëœë‹¤.', 'ì•„ì•¼ í•œë‹¤.', 'ì–´ì•¼ í•œë‹¤.']
        
        current_sentence = ""
        for char in content:
            current_sentence += char
            
            # ë¬¸ì¥ ë í™•ì¸
            if any(current_sentence.endswith(ending) for ending in sentence_endings):
                if current_sentence.strip():
                    sentences.append(current_sentence.strip())
                current_sentence = ""
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì²˜ë¦¬
        if current_sentence.strip():
            sentences.append(current_sentence.strip())
        
        return sentences
    
    def _extract_condition_elements(self, sentence: str) -> List[str]:
        """ì¡°ê±´ë¬¸ì—ì„œ êµ¬ì„±ìš”ê±´ ì¶”ì¶œ"""
        elements = []
        
        # ì¡°ê±´ë¬¸ íŒ¨í„´ë“¤
        condition_patterns = [
            r'([ê°€-í£]+(?:ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ))\s*([^ë‹¤]+ë‹¤)',
            r'([ê°€-í£]+(?:ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ))\s*([^ë©´]+ë©´)',
            r'([ê°€-í£]+(?:ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ))\s*([^ë•Œ]+ë•Œ)',
        ]
        
        for pattern in condition_patterns:
            matches = re.findall(pattern, sentence)
            for match in matches:
                condition = match[0].strip()
                if len(condition) > 1 and len(condition) < 10:  # ì ì ˆí•œ ê¸¸ì´ í•„í„°ë§
                    elements.append(f"ì¡°ê±´: {condition}")
        
        return elements
    
    def _extract_effect_elements(self, sentence: str) -> List[str]:
        """íš¨ê³¼ë¬¸ì—ì„œ êµ¬ì„±ìš”ê±´ ì¶”ì¶œ"""
        elements = []
        
        # íš¨ê³¼ë¬¸ íŒ¨í„´ë“¤
        effect_patterns = [
            r'([ê°€-í£]+(?:í•œë‹¤|ëœë‹¤|ì•„ì•¼ í•œë‹¤|ì–´ì•¼ í•œë‹¤))',
            r'([ê°€-í£]+(?:ì˜ë¬´|ì±…ì„|ê¶Œë¦¬|íš¨ë ¥|íš¨ê³¼))',
        ]
        
        for pattern in effect_patterns:
            matches = re.findall(pattern, sentence)
            for match in matches:
                effect = match.strip()
                if len(effect) > 1 and len(effect) < 15:
                    elements.append(f"íš¨ê³¼: {effect}")
        
        return elements
    
    def _extract_specialized_elements(self, content: str, law_name: str, article_number: str) -> List[str]:
        """ë²•ë ¹ë³„ íŠ¹í™”ëœ êµ¬ì„±ìš”ê±´ ì¶”ì¶œ"""
        elements = []
        
        # ë²•ë ¹ë³„ íŠ¹í™” ê·œì¹™
        specialized_rules = {
            "ë¯¼ë²•": {
                "750": ["ê³ ì˜/ê³¼ì‹¤", "ìœ„ë²•í–‰ìœ„", "ì†í•´ë°œìƒ", "ì¸ê³¼ê´€ê³„"],
                "751": ["ì •ì‹ ì  í”¼í•´", "ê³ ì˜/ê³¼ì‹¤", "ìœ„ë²•í–‰ìœ„"],
                "752": ["ìƒëª…ì¹¨í•´", "ì¬ì‚°ì´ì™¸ ì†í•´", "ìœ ì¡±ì˜ í”¼í•´"],
                "753": ["ë¯¸ì„±ë…„ì", "ì±…ì„ëŠ¥ë ¥", "ë²•ì •ëŒ€ë¦¬ì¸"]
            },
            "í˜•ë²•": {
                "250": ["ì‚´ì¸ì˜ ê³ ì˜", "ìƒëª…ì¹¨í•´", "ë²”ì˜"],
                "257": ["ìƒí•´ì˜ ê³ ì˜", "ì‹ ì²´ìƒí•´", "ë²”ì˜"],
                "329": ["ì ˆë„ì˜ ê³ ì˜", "íƒ€ì¸ì¬ë¬¼", "ì ˆì·¨í–‰ìœ„"]
            },
            "ìƒë²•": {
                "382": ["ì´ì‚¬ì˜ ì„ ê´€ì£¼ì˜ì˜ë¬´", "íšŒì‚¬ì´ìµ", "ì¶©ì‹¤ì˜ë¬´"],
                "400": ["ê°ì‚¬ì˜ ë…ë¦½ì„±", "íšŒê³„ê°ì‚¬", "ê°ì‚¬ë³´ê³ ì„œ"]
            }
        }
        
        # í•´ë‹¹ ë²•ë ¹ì˜ íŠ¹í™” ê·œì¹™ ì ìš©
        if law_name in specialized_rules and article_number in specialized_rules[law_name]:
            elements.extend(specialized_rules[law_name][article_number])
        
        # ì¼ë°˜ì ì¸ ë²•ë ¹ë³„ íŠ¹í™” ìš”ì†Œ
        elif law_name in specialized_rules:
            elements.extend(self._extract_general_law_elements(content, law_name))
        
        return elements
    
    def _extract_general_law_elements(self, content: str, law_name: str) -> List[str]:
        """ì¼ë°˜ì ì¸ ë²•ë ¹ë³„ êµ¬ì„±ìš”ê±´ ì¶”ì¶œ"""
        elements = []
        
        law_specific_patterns = {
            "ë¯¼ë²•": [
                (r'[ê°€-í£]+(?:ê¶Œ|ì˜ë¬´)', "ë¯¼ë²•ì  ê¶Œë¦¬/ì˜ë¬´"),
                (r'[ê°€-í£]+(?:ê³„ì•½|í•©ì˜)', "ê³„ì•½ ê´€ë ¨ ìš”ì†Œ"),
                (r'[ê°€-í£]+(?:ì†í•´|ë°°ìƒ)', "ì†í•´ë°°ìƒ ìš”ì†Œ")
            ],
            "í˜•ë²•": [
                (r'[ê°€-í£]+(?:ë²”ì£„|ì²˜ë²Œ)', "ë²”ì£„ êµ¬ì„±ìš”ì†Œ"),
                (r'[ê°€-í£]+(?:ê³ ì˜|ê³¼ì‹¤)', "ì£¼ê´€ì  ìš”ì†Œ"),
                (r'[ê°€-í£]+(?:ë²Œê¸ˆ|ì§•ì—­)', "í˜•ë²Œ ìš”ì†Œ")
            ],
            "ìƒë²•": [
                (r'[ê°€-í£]+(?:íšŒì‚¬|ì£¼ì‹)', "íšŒì‚¬ ê´€ë ¨ ìš”ì†Œ"),
                (r'[ê°€-í£]+(?:ì´ì‚¬|ê°ì‚¬)', "ê¸°ê´€ ê´€ë ¨ ìš”ì†Œ"),
                (r'[ê°€-í£]+(?:ì£¼ì£¼|ì´íšŒ)', "ì£¼ì£¼ ê´€ë ¨ ìš”ì†Œ")
            ]
        }
        
        if law_name in law_specific_patterns:
            for pattern, element_type in law_specific_patterns[law_name]:
                matches = re.findall(pattern, content)
                if matches:
                    elements.append(element_type)
        
        return elements
    
    def _prioritize_elements(self, elements: List[str]) -> List[str]:
        """êµ¬ì„±ìš”ê±´ ìš°ì„ ìˆœìœ„ ì •ë ¬"""
        # ìš°ì„ ìˆœìœ„ ê¸°ì¤€
        priority_keywords = {
            "ê³ ì˜/ê³¼ì‹¤": 10,
            "ìœ„ë²•í–‰ìœ„": 9,
            "ì†í•´ë°œìƒ": 8,
            "ì¸ê³¼ê´€ê³„": 7,
            "ë²•ì  ì±…ì„": 6,
            "ì¡°ê±´": 5,
            "íš¨ê³¼": 4,
            "ê¶Œë¦¬": 3,
            "ì˜ë¬´": 2,
            "ë²”ì£„": 1
        }
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        def get_priority(element):
            for keyword, priority in priority_keywords.items():
                if keyword in element:
                    return priority
            return 0
        
        # ì¤‘ë³µ ì œê±° í›„ ìš°ì„ ìˆœìœ„ ì •ë ¬
        unique_elements = list(dict.fromkeys(elements))
        return sorted(unique_elements, key=get_priority, reverse=True)
    
    def _extract_key_elements_pattern(self, content: str, law_name: str = "", article_number: str = "") -> List[str]:
        """íŒ¨í„´ ê¸°ë°˜ êµ¬ì„±ìš”ê±´ ì¶”ì¶œ ì‹œìŠ¤í…œ"""
        if not content:
            return ["êµ¬ì„±ìš”ê±´ ì •ë³´ ì—†ìŒ"]
        
        elements = []
        
        # 1. ë²•ë¥  ë¬¸ì¥ êµ¬ì¡° íŒ¨í„´ ë¶„ì„
        structural_elements = self._analyze_structural_patterns(content)
        elements.extend(structural_elements)
        
        # 2. ë²•ë¥  ìš©ì–´ íŒ¨í„´ ë§¤ì¹­
        legal_term_elements = self._extract_legal_term_patterns(content)
        elements.extend(legal_term_elements)
        
        # 3. ë¬¸ë²•ì  íŒ¨í„´ ë¶„ì„
        grammatical_elements = self._analyze_grammatical_patterns(content)
        elements.extend(grammatical_elements)
        
        # 4. ë²•ë ¹ë³„ íŠ¹í™” íŒ¨í„´ ì ìš©
        specialized_elements = self._apply_specialized_patterns(content, law_name, article_number)
        elements.extend(specialized_elements)
        
        # 5. ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§ ë° ì •ë ¬
        filtered_elements = self._filter_and_rank_elements(elements)
        
        return filtered_elements[:5]
    
    def _analyze_structural_patterns(self, content: str) -> List[str]:
        """ë²•ë¥  ë¬¸ì¥ì˜ êµ¬ì¡°ì  íŒ¨í„´ ë¶„ì„"""
        elements = []
        
        # ì¡°ê±´-ê²°ê³¼ êµ¬ì¡° íŒ¨í„´
        condition_result_patterns = [
            r'([^ë‹¤]+ë‹¤)\s*([^ë‹¤]+ë‹¤)',  # ì¡°ê±´ë¬¸ + ê²°ê³¼ë¬¸
            r'([^ë©´]+ë©´)\s*([^ë‹¤]+ë‹¤)',  # ì¡°ê±´ë¬¸ + ê²°ê³¼ë¬¸
            r'([^ë•Œ]+ë•Œ)\s*([^ë‹¤]+ë‹¤)',  # ì¡°ê±´ë¬¸ + ê²°ê³¼ë¬¸
        ]
        
        for pattern in condition_result_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match[0]) > 3 and len(match[0]) < 20:
                    elements.append(f"êµ¬ì¡°ì  ì¡°ê±´: {match[0].strip()}")
                if len(match[1]) > 3 and len(match[1]) < 20:
                    elements.append(f"êµ¬ì¡°ì  ê²°ê³¼: {match[1].strip()}")
        
        # í•­ëª©ë³„ êµ¬ì¡° íŒ¨í„´
        item_patterns = [
            r'â‘ \s*([^â‘¡]+)',  # â‘  í•­ëª©
            r'â‘¡\s*([^â‘¢]+)',  # â‘¡ í•­ëª©
            r'â‘¢\s*([^â‘£]+)',   # â‘¢ í•­ëª©
        ]
        
        for pattern in item_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) > 5 and len(match) < 30:
                    elements.append(f"êµ¬ì¡°ì  í•­ëª©: {match.strip()}")
        
        return elements
    
    def _extract_legal_term_patterns(self, content: str) -> List[str]:
        """ë²•ë¥  ìš©ì–´ íŒ¨í„´ ì¶”ì¶œ"""
        elements = []
        
        # ë²•ë¥  ìš©ì–´ íŒ¨í„´ ì •ì˜
        legal_term_patterns = {
            "ì£¼ê´€ì  ìš”ì†Œ": [
                r'[ê°€-í£]*(?:ê³ ì˜|ê³¼ì‹¤|ì˜ë„|ì˜ì‹)[ê°€-í£]*',
                r'[ê°€-í£]*(?:ì¸ì‹|ì˜ˆê²¬|í¬ë§)[ê°€-í£]*'
            ],
            "ê°ê´€ì  ìš”ì†Œ": [
                r'[ê°€-í£]*(?:í–‰ìœ„|ìœ„ë²•|ë¶ˆë²•)[ê°€-í£]*',
                r'[ê°€-í£]*(?:ê²°ê³¼|ì†í•´|í”¼í•´)[ê°€-í£]*'
            ],
            "ë²•ì  íš¨ê³¼": [
                r'[ê°€-í£]*(?:ì±…ì„|ì˜ë¬´|ê¶Œë¦¬)[ê°€-í£]*',
                r'[ê°€-í£]*(?:íš¨ë ¥|íš¨ê³¼|ê²°ê³¼)[ê°€-í£]*'
            ],
            "ì ˆì°¨ì  ìš”ì†Œ": [
                r'[ê°€-í£]*(?:ì ˆì°¨|ë°©ë²•|ê³¼ì •)[ê°€-í£]*',
                r'[ê°€-í£]*(?:ì‹ ì²­|ì œì¶œ|ì²˜ë¦¬)[ê°€-í£]*'
            ]
        }
        
        for category, patterns in legal_term_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, content)
                if matches:
                    # ê°€ì¥ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ìš©ì–´ ì„ íƒ
                    most_common = max(set(matches), key=matches.count)
                    elements.append(f"{category}: {most_common}")
        
        return elements
    
    def _analyze_grammatical_patterns(self, content: str) -> List[str]:
        """ë¬¸ë²•ì  íŒ¨í„´ ë¶„ì„"""
        elements = []
        
        # ì¡°ë™ì‚¬ íŒ¨í„´ ë¶„ì„
        auxiliary_patterns = [
            r'([ê°€-í£]+)\s*(?:ì•„ì•¼|ì–´ì•¼)\s*(?:í•œë‹¤|ëœë‹¤)',  # ì˜ë¬´ í‘œí˜„
            r'([ê°€-í£]+)\s*(?:í• |ë )\s*(?:ìˆ˜|ê²ƒ)\s*(?:ìˆë‹¤|ì—†ë‹¤)',  # ê°€ëŠ¥ì„± í‘œí˜„
            r'([ê°€-í£]+)\s*(?:í•˜ì—¬ì•¼|ë˜ì–´ì•¼)\s*(?:í•œë‹¤|ëœë‹¤)',  # ì˜ë¬´ í‘œí˜„
        ]
        
        for pattern in auxiliary_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) > 2 and len(match) < 15:
                    elements.append(f"ë¬¸ë²•ì  ì˜ë¬´: {match.strip()}")
        
        # ì¡°ê±´ë¬¸ íŒ¨í„´ ë¶„ì„
        conditional_patterns = [
            r'([ê°€-í£]+)\s*(?:ì´ë©´|ê°€ë©´|ìœ¼ë©´|ë©´)\s*([ê°€-í£]+)',  # ì¡°ê±´ë¬¸
            r'([ê°€-í£]+)\s*(?:ì¸|í•œ)\s*(?:ê²½ìš°|ë•Œ)\s*([ê°€-í£]+)',  # ì¡°ê±´ë¬¸
        ]
        
        for pattern in conditional_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match[0]) > 2 and len(match[0]) < 10:
                    elements.append(f"ë¬¸ë²•ì  ì¡°ê±´: {match[0].strip()}")
        
        return elements
    
    def _apply_specialized_patterns(self, content: str, law_name: str, article_number: str) -> List[str]:
        """ë²•ë ¹ë³„ íŠ¹í™” íŒ¨í„´ ì ìš©"""
        elements = []
        
        # ë²•ë ¹ë³„ íŠ¹í™” íŒ¨í„´ ì •ì˜
        specialized_patterns = {
            "ë¯¼ë²•": {
                "ë¶ˆë²•í–‰ìœ„": [
                    r'[ê°€-í£]*(?:ê³ ì˜|ê³¼ì‹¤)[ê°€-í£]*(?:ë¡œ|ìœ¼ë¡œ)\s*[ê°€-í£]*(?:ì¸í•œ|í•œ)\s*[ê°€-í£]*(?:ìœ„ë²•|ë¶ˆë²•)[ê°€-í£]*(?:í–‰ìœ„|í–‰ë™)',
                    r'[ê°€-í£]*(?:íƒ€ì¸|ë‹¤ë¥¸\s*ì‚¬ëŒ)[ê°€-í£]*(?:ì—ê²Œ|ì—)\s*[ê°€-í£]*(?:ì†í•´|í”¼í•´)[ê°€-í£]*(?:ë¥¼|ì„)\s*[ê°€-í£]*(?:ê°€í•˜ëŠ”|ì…íˆëŠ”)'
                ],
                "ê³„ì•½": [
                    r'[ê°€-í£]*(?:ê³„ì•½|í•©ì˜|ì•½ì •)[ê°€-í£]*(?:ì˜|ì´)\s*[ê°€-í£]*(?:ì„±ë¦½|íš¨ë ¥|í•´ì œ)',
                    r'[ê°€-í£]*(?:ë‹¹ì‚¬ì|ê³„ì•½ì)[ê°€-í£]*(?:ê°„|ì‚¬ì´)\s*[ê°€-í£]*(?:í•©ì˜|ì•½ì •)'
                ]
            },
            "í˜•ë²•": {
                "ë²”ì£„": [
                    r'[ê°€-í£]*(?:ë²”ì£„|ì£„)[ê°€-í£]*(?:ë¥¼|ì„)\s*[ê°€-í£]*(?:ë²”í•œ|ì €ì§€ë¥¸)\s*[ê°€-í£]*(?:ì|ì)',
                    r'[ê°€-í£]*(?:ê³ ì˜|ê³¼ì‹¤)[ê°€-í£]*(?:ë¡œ|ìœ¼ë¡œ)\s*[ê°€-í£]*(?:ë²”í•œ|ì €ì§€ë¥¸)'
                ]
            },
            "ìƒë²•": {
                "íšŒì‚¬": [
                    r'[ê°€-í£]*(?:íšŒì‚¬|ë²•ì¸)[ê°€-í£]*(?:ì˜|ì´)\s*[ê°€-í£]*(?:ì´ì‚¬|ê°ì‚¬|ì£¼ì£¼)',
                    r'[ê°€-í£]*(?:ì´ì‚¬|ê°ì‚¬)[ê°€-í£]*(?:ì˜|ì´)\s*[ê°€-í£]*(?:ì˜ë¬´|ì±…ì„)'
                ]
            }
        }
        
        # í•´ë‹¹ ë²•ë ¹ì˜ íŠ¹í™” íŒ¨í„´ ì ìš©
        if law_name in specialized_patterns:
            for category, patterns in specialized_patterns[law_name].items():
                for pattern in patterns:
                    matches = re.findall(pattern, content)
                    if matches:
                        elements.append(f"{law_name} {category}: {matches[0]}")
        
        return elements
    
    def _filter_and_rank_elements(self, elements: List[str]) -> List[str]:
        """êµ¬ì„±ìš”ê±´ í•„í„°ë§ ë° ìˆœìœ„ ê²°ì •"""
        if not elements:
            return ["êµ¬ì„±ìš”ê±´ ì •ë³´ ì—†ìŒ"]
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        element_scores = {}
        for element in elements:
            score = self._calculate_element_confidence(element)
            element_scores[element] = score
        
        # ì‹ ë¢°ë„ ê¸°ì¤€ í•„í„°ë§ (0.3 ì´ìƒ)
        filtered_elements = [
            element for element, score in element_scores.items() 
            if score >= 0.3
        ]
        
        # ì‹ ë¢°ë„ ê¸°ì¤€ ì •ë ¬
        sorted_elements = sorted(
            filtered_elements, 
            key=lambda x: element_scores[x], 
            reverse=True
        )
        
        return sorted_elements
    
    def _calculate_element_confidence(self, element: str) -> float:
        """êµ¬ì„±ìš”ê±´ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜
        high_confidence_keywords = {
            "ê³ ì˜": 0.9, "ê³¼ì‹¤": 0.9, "ìœ„ë²•í–‰ìœ„": 0.8, "ì†í•´": 0.8,
            "ì¸ê³¼ê´€ê³„": 0.7, "ì±…ì„": 0.7, "ì˜ë¬´": 0.6, "ê¶Œë¦¬": 0.6
        }
        
        for keyword, keyword_score in high_confidence_keywords.items():
            if keyword in element:
                score += keyword_score
        
        # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        length = len(element)
        if 5 <= length <= 20:
            score += 0.2
        elif length < 5 or length > 30:
            score -= 0.1
        
        # íŒ¨í„´ ê¸°ë°˜ ì ìˆ˜
        if ":" in element:  # êµ¬ì¡°í™”ëœ íŒ¨í„´
            score += 0.1
        
        # ë²•ë ¹ë³„ íŠ¹í™” ì ìˆ˜
        if any(law in element for law in ["ë¯¼ë²•", "í˜•ë²•", "ìƒë²•"]):
            score += 0.1
        
        # ìµœì¢… ì ìˆ˜ ì •ê·œí™” (0.0 ~ 1.0)
        return min(max(score, 0.0), 1.0)
    
    def _extract_key_elements_ml(self, content: str, law_name: str = "", article_number: str = "") -> List[str]:
        """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ êµ¬ì„±ìš”ê±´ ë¶„ë¥˜ ì‹œìŠ¤í…œ"""
        if not content:
            return ["êµ¬ì„±ìš”ê±´ ì •ë³´ ì—†ìŒ"]
        
        elements = []
        
        # 1. íŠ¹ì„± ì¶”ì¶œ
        features = self._extract_ml_features(content, law_name, article_number)
        
        # 2. êµ¬ì„±ìš”ê±´ ë¶„ë¥˜
        classified_elements = self._classify_legal_elements(features)
        elements.extend(classified_elements)
        
        # 3. ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
        filtered_elements = self._filter_by_confidence(elements)
        
        return filtered_elements[:5]
    
    def _extract_ml_features(self, content: str, law_name: str, article_number: str) -> Dict[str, Any]:
        """ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ íŠ¹ì„± ì¶”ì¶œ"""
        features = {
            "text_length": len(content),
            "sentence_count": len(re.split(r'[.!?]', content)),
            "has_conditions": bool(re.search(r'[ê°€-í£]+(?:ì´ë©´|ê°€ë©´|ìœ¼ë©´|ë©´)', content)),
            "has_effects": bool(re.search(r'[ê°€-í£]+(?:í•œë‹¤|ëœë‹¤|ì•„ì•¼ í•œë‹¤|ì–´ì•¼ í•œë‹¤)', content)),
            "has_penalties": bool(re.search(r'[ê°€-í£]*(?:ë²Œê¸ˆ|ì§•ì—­|ê³¼íƒœë£Œ)', content)),
            "has_deadlines": bool(re.search(r'\d+(?:ì¼|ê°œì›”|ë…„)', content)),
            "has_multiple_items": bool(re.search(r'[â‘ â‘¡â‘¢â‘£â‘¤]', content)),
            "law_type": law_name,
            "article_number": article_number,
            "keyword_density": self._calculate_keyword_density(content),
            "legal_term_count": len(re.findall(r'[ê°€-í£]{2,6}(?:ê¶Œ|ì˜ë¬´|ì±…ì„|íš¨ë ¥)', content)),
            "conditional_structure": self._analyze_conditional_structure(content),
            "effect_structure": self._analyze_effect_structure(content)
        }
        
        return features
    
    def _calculate_keyword_density(self, content: str) -> float:
        """ë²•ë¥  í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°"""
        legal_keywords = [
            "ê³ ì˜", "ê³¼ì‹¤", "ìœ„ë²•", "ë¶ˆë²•", "ì†í•´", "ë°°ìƒ", "ì±…ì„", "ì˜ë¬´",
            "ê¶Œë¦¬", "íš¨ë ¥", "íš¨ê³¼", "ê³„ì•½", "í•©ì˜", "ì†Œìœ ê¶Œ", "ìƒì†", "ì´í˜¼"
        ]
        
        total_words = len(content.split())
        keyword_count = sum(1 for keyword in legal_keywords if keyword in content)
        
        return keyword_count / total_words if total_words > 0 else 0.0
    
    def _analyze_conditional_structure(self, content: str) -> Dict[str, int]:
        """ì¡°ê±´ë¬¸ êµ¬ì¡° ë¶„ì„"""
        conditional_patterns = {
            "if_then": len(re.findall(r'[ê°€-í£]+(?:ì´ë©´|ê°€ë©´|ìœ¼ë©´|ë©´)\s*[ê°€-í£]+', content)),
            "when": len(re.findall(r'[ê°€-í£]+(?:ì¸|í•œ)\s*(?:ê²½ìš°|ë•Œ)\s*[ê°€-í£]+', content)),
            "unless": len(re.findall(r'[ê°€-í£]+(?:ì´|ê°€|ì„|ë¥¼)\s*(?:ì•„ë‹ˆë©´|ì•„ë‹Œ|ì•„ë‹)\s*[ê°€-í£]+', content))
        }
        
        return conditional_patterns
    
    def _analyze_effect_structure(self, content: str) -> Dict[str, int]:
        """íš¨ê³¼ë¬¸ êµ¬ì¡° ë¶„ì„"""
        effect_patterns = {
            "obligation": len(re.findall(r'[ê°€-í£]+(?:ì•„ì•¼|ì–´ì•¼)\s*(?:í•œë‹¤|ëœë‹¤)', content)),
            "prohibition": len(re.findall(r'[ê°€-í£]+(?:í•˜ì—¬ì„œëŠ”|ë˜ì–´ì„œëŠ”)\s*(?:ì•ˆ|ëª»)\s*(?:ëœë‹¤|í•œë‹¤)', content)),
            "permission": len(re.findall(r'[ê°€-í£]+(?:í• |ë )\s*(?:ìˆ˜|ê²ƒ)\s*(?:ìˆë‹¤|ì—†ë‹¤)', content)),
            "consequence": len(re.findall(r'[ê°€-í£]+(?:ì¸|í•œ)\s*(?:ê²½ìš°|ë•Œ)\s*[ê°€-í£]+', content))
        }
        
        return effect_patterns
    
    def _classify_legal_elements(self, features: Dict[str, Any]) -> List[str]:
        """ë²•ë¥  êµ¬ì„±ìš”ê±´ ë¶„ë¥˜"""
        elements = []
        
        # ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ (ì‹¤ì œ ML ëª¨ë¸ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
        if features["has_conditions"] and features["has_effects"]:
            elements.append("ìš”ê±´-íš¨ê³¼ êµ¬ì¡°")
        
        if features["keyword_density"] > 0.1:
            elements.append("ë²•ë¥  ìš©ì–´ ì§‘ì¤‘")
        
        if features["has_penalties"]:
            elements.append("ë²Œì¹™ ì¡°í•­")
        
        if features["has_deadlines"]:
            elements.append("ê¸°í•œ ê´€ë ¨ ì¡°í•­")
        
        if features["has_multiple_items"]:
            elements.append("ë‹¤í•­ ì¡°í•­")
        
        # ë²•ë ¹ë³„ íŠ¹í™” ë¶„ë¥˜
        law_type = features.get("law_type", "")
        if law_type == "ë¯¼ë²•":
            elements.extend(self._classify_civil_law_elements(features))
        elif law_type == "í˜•ë²•":
            elements.extend(self._classify_criminal_law_elements(features))
        elif law_type == "ìƒë²•":
            elements.extend(self._classify_commercial_law_elements(features))
        
        return elements
    
    def _classify_civil_law_elements(self, features: Dict[str, Any]) -> List[str]:
        """ë¯¼ë²• êµ¬ì„±ìš”ê±´ ë¶„ë¥˜"""
        elements = []
        
        if features["legal_term_count"] > 3:
            elements.append("ë¯¼ë²•ì  ê¶Œë¦¬/ì˜ë¬´")
        
        if features["conditional_structure"]["if_then"] > 0:
            elements.append("ë¯¼ë²•ì  ì¡°ê±´")
        
        if features["effect_structure"]["obligation"] > 0:
            elements.append("ë¯¼ë²•ì  ì˜ë¬´")
        
        return elements
    
    def _classify_criminal_law_elements(self, features: Dict[str, Any]) -> List[str]:
        """í˜•ë²• êµ¬ì„±ìš”ê±´ ë¶„ë¥˜"""
        elements = []
        
        if features["has_penalties"]:
            elements.append("í˜•ì‚¬ì  ì²˜ë²Œ")
        
        if features["keyword_density"] > 0.15:
            elements.append("í˜•ì‚¬ì  êµ¬ì„±ìš”ê±´")
        
        return elements
    
    def _classify_commercial_law_elements(self, features: Dict[str, Any]) -> List[str]:
        """ìƒë²• êµ¬ì„±ìš”ê±´ ë¶„ë¥˜"""
        elements = []
        
        if features["has_multiple_items"]:
            elements.append("ìƒë²•ì  ì ˆì°¨")
        
        if features["effect_structure"]["permission"] > 0:
            elements.append("ìƒë²•ì  ê¶Œí•œ")
        
        return elements
    
    def _filter_by_confidence(self, elements: List[str]) -> List[str]:
        """ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§"""
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_scores = {}
        for element in elements:
            score = self._calculate_ml_confidence(element)
            confidence_scores[element] = score
        
        # ì‹ ë¢°ë„ 0.5 ì´ìƒë§Œ í•„í„°ë§
        filtered_elements = [
            element for element, score in confidence_scores.items() 
            if score >= 0.5
        ]
        
        # ì‹ ë¢°ë„ ê¸°ì¤€ ì •ë ¬
        return sorted(filtered_elements, key=lambda x: confidence_scores[x], reverse=True)
    
    def _calculate_ml_confidence(self, element: str) -> float:
        """ML ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        score = 0.0
        
        # êµ¬ì¡°ì  ì‹ ë¢°ë„
        if "êµ¬ì¡°" in element:
            score += 0.3
        
        # ë²•ë ¹ë³„ íŠ¹í™” ì‹ ë¢°ë„
        if any(law in element for law in ["ë¯¼ë²•", "í˜•ë²•", "ìƒë²•"]):
            score += 0.2
        
        # ìš©ì–´ ì‹ ë¢°ë„
        if any(term in element for term in ["ê¶Œë¦¬", "ì˜ë¬´", "ì±…ì„", "íš¨ë ¥"]):
            score += 0.2
        
        # ê¸¸ì´ ì‹ ë¢°ë„
        if 5 <= len(element) <= 20:
            score += 0.1
        
        return min(score, 1.0)
    
    def _deduplicate_and_rank_elements(self, elements: List[str]) -> List[str]:
        """êµ¬ì„±ìš”ê±´ ì¤‘ë³µ ì œê±° ë° ìˆœìœ„ ê²°ì •"""
        if not elements:
            return ["êµ¬ì„±ìš”ê±´ ì •ë³´ ì—†ìŒ"]
        
        # 1. ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        unique_elements = []
        for element in elements:
            is_duplicate = False
            for existing in unique_elements:
                if self._calculate_similarity(element, existing) > 0.7:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_elements.append(element)
        
        # 2. ì‹ ë¢°ë„ ê¸°ë°˜ ìˆœìœ„ ê²°ì •
        element_scores = {}
        for element in unique_elements:
            score = self._calculate_comprehensive_confidence(element)
            element_scores[element] = score
        
        # 3. ì‹ ë¢°ë„ ê¸°ì¤€ ì •ë ¬
        sorted_elements = sorted(
            unique_elements, 
            key=lambda x: element_scores[x], 
            reverse=True
        )
        
        return sorted_elements
    
    def _calculate_similarity(self, element1: str, element2: str) -> float:
        """ë‘ êµ¬ì„±ìš”ê±´ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥)
        words1 = set(element1.split())
        words2 = set(element2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _calculate_comprehensive_confidence(self, element: str) -> float:
        """ì¢…í•©ì ì¸ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # 1. í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜
        high_confidence_keywords = {
            "ê³ ì˜": 0.9, "ê³¼ì‹¤": 0.9, "ìœ„ë²•í–‰ìœ„": 0.8, "ì†í•´": 0.8,
            "ì¸ê³¼ê´€ê³„": 0.7, "ì±…ì„": 0.7, "ì˜ë¬´": 0.6, "ê¶Œë¦¬": 0.6,
            "êµ¬ì¡°ì ": 0.5, "ë¬¸ë²•ì ": 0.4, "ML": 0.3
        }
        
        for keyword, keyword_score in high_confidence_keywords.items():
            if keyword in element:
                score += keyword_score
        
        # 2. ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
        length = len(element)
        if 5 <= length <= 20:
            score += 0.2
        elif length < 5 or length > 30:
            score -= 0.1
        
        # 3. êµ¬ì¡°ì  íŒ¨í„´ ì ìˆ˜
        if ":" in element:  # êµ¬ì¡°í™”ëœ íŒ¨í„´
            score += 0.1
        
        # 4. ë²•ë ¹ë³„ íŠ¹í™” ì ìˆ˜
        if any(law in element for law in ["ë¯¼ë²•", "í˜•ë²•", "ìƒë²•"]):
            score += 0.1
        
        # ìµœì¢… ì ìˆ˜ ì •ê·œí™” (0.0 ~ 1.0)
        return min(max(score, 0.0), 1.0)
    
    async def _format_article_response(self, result: Dict[str, Any], user_query: str = "") -> str:
        """ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ë¬¸ ì‘ë‹µ í¬ë§·íŒ… (ë¹„ë™ê¸°)"""
        law_name = result.get('law_name_korean', '')
        article_number = result.get('article_number', '')
        article_title = result.get('article_title', '')
        article_content = result.get('article_content', '')
        paragraph_content = result.get('paragraph_content', '')
        sub_paragraph_content = result.get('sub_paragraph_content', '')
        effective_date = result.get('effective_date', '')
        ministry_name = result.get('ministry_name', '')
        
        self.logger.info(f"Formatting concise article response for {law_name} ì œ{article_number}ì¡°")
        
        # ì‘ë‹µ ì´ˆê¸°í™” (ì¤‘ë³µ ë°©ì§€)
        response_parts = []
        
        # ì œëª©
        title = f"**{law_name} ì œ{article_number}ì¡°"
        if article_title:
            title += f" ({article_title})"
        title += "**"
        response_parts.append(title)
        response_parts.append("")  # ë¹ˆ ì¤„
        
        # ì¡°ë¬¸ ë‚´ìš© (ì¸ìš©ë¬¸ ìŠ¤íƒ€ì¼)
        main_content = paragraph_content if paragraph_content and paragraph_content.strip() else article_content
        if main_content and main_content.strip():
            response_parts.append(f"> {main_content}")
            response_parts.append("")  # ë¹ˆ ì¤„
        
        # í•µì‹¬ êµ¬ì„±ìš”ê±´ (ê°„ê²°í•˜ê²Œ)
        response_parts.append("**ğŸ“‹ í•µì‹¬ êµ¬ì„±ìš”ê±´**")
        key_elements = self._extract_key_elements(main_content or article_content)
        response_parts.append(f"- {', '.join(key_elements[:3])}")
        response_parts.append("")  # ë¹ˆ ì¤„
        
        # ê´€ë ¨ íŒë¡€ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ - ë¹„ë™ê¸°)
        precedents = []
        self.logger.info(f"íŒë¡€ ê²€ìƒ‰ ì‹œì‘ - í•˜ì´ë¸Œë¦¬ë“œ ì„œë¹„ìŠ¤: {self.hybrid_precedent_service is not None}, DB ì„œë¹„ìŠ¤: {self.precedent_service is not None}")
        
        if self.hybrid_precedent_service:
            # í•˜ì´ë¸Œë¦¬ë“œ ì„œë¹„ìŠ¤ ì‚¬ìš© (DB + API)
            try:
                self.logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ íŒë¡€ ê²€ìƒ‰ í˜¸ì¶œ: {law_name} ì œ{article_number}ì¡°")
                precedents = await self.hybrid_precedent_service.get_related_precedents(
                    law_name, str(article_number), main_content or article_content, limit=3
                )
                self.logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ íŒë¡€ ê²€ìƒ‰ ê²°ê³¼: {len(precedents)}ê°œ")
            except Exception as e:
                self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ íŒë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                # í´ë°±: ê¸°ì¡´ DB ì„œë¹„ìŠ¤ ì‚¬ìš©
                if self.precedent_service:
                    self.logger.info("í´ë°±: DB íŒë¡€ ê²€ìƒ‰ ì‚¬ìš©")
                    precedents = self.precedent_service.get_related_precedents(
                        law_name, str(article_number), main_content or article_content, limit=3
                    )
                    self.logger.info(f"DB íŒë¡€ ê²€ìƒ‰ ê²°ê³¼: {len(precedents)}ê°œ")
        elif self.precedent_service:
            # ê¸°ì¡´ DB ì„œë¹„ìŠ¤ë§Œ ì‚¬ìš©
            self.logger.info(f"DB íŒë¡€ ê²€ìƒ‰ í˜¸ì¶œ: {law_name} ì œ{article_number}ì¡°")
            precedents = self.precedent_service.get_related_precedents(
                law_name, str(article_number), main_content or article_content, limit=3
            )
            self.logger.info(f"DB íŒë¡€ ê²€ìƒ‰ ê²°ê³¼: {len(precedents)}ê°œ")
        else:
            self.logger.warning("íŒë¡€ ê²€ìƒ‰ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        
        if precedents:
            response_parts.append("**âš–ï¸ ì£¼ìš” íŒë¡€**")
            for precedent in precedents:
                source_indicator = "ğŸŒ" if hasattr(precedent, 'source') and precedent.source == "api" else "ğŸ“š"
                response_parts.append(f"- {source_indicator} **{precedent.case_number}**: {precedent.summary}")
            response_parts.append("")  # ë¹ˆ ì¤„
        
        # ì‹¤ë¬´ ì ìš© (í•µì‹¬ë§Œ)
        response_parts.append("**ğŸ’¼ ì‹¤ë¬´ ì ìš©**")
        practical_tips = self._get_concise_practical_tips(law_name, article_number, main_content or article_content)
        for tip in practical_tips[:2]:  # ìµœëŒ€ 2ê°œ
            response_parts.append(f"- {tip}")
        response_parts.append("")  # ë¹ˆ ì¤„
        
        # ê´€ë ¨ ì¡°ë¬¸ (ê°„ë‹¨íˆ)
        related_articles = await self._get_related_articles(law_name, article_number)
        if related_articles:
            response_parts.append("**ğŸ”— ê´€ë ¨ ì¡°ë¬¸**")
            response_parts.append(f"- {', '.join(related_articles[:3])}")
        
        # ìµœì¢… ì‘ë‹µ ì¡°í•© (ì¤‘ë³µ ì œê±°)
        final_response = "\n".join(response_parts)
        return self._validate_response_quality(final_response)
    
    def _validate_response_quality(self, response: str) -> str:
        """ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ """
        if not response:
            return response
        
        # ì¤‘ë³µ ë‚´ìš© ì œê±°
        lines = response.split('\n')
        unique_lines = []
        seen_content = set()
        
        for line in lines:
            content_key = line.strip().lower()
            if content_key and content_key not in seen_content:
                unique_lines.append(line)
                seen_content.add(content_key)
            elif not content_key:  # ë¹ˆ ì¤„ì€ ìœ ì§€
                unique_lines.append(line)
        
        # ì—°ì†ëœ ë¹ˆ ì¤„ ì œê±° (ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ í—ˆìš©)
        cleaned_lines = []
        empty_count = 0
        for line in unique_lines:
            if line.strip() == "":
                empty_count += 1
                if empty_count <= 2:
                    cleaned_lines.append(line)
            else:
                empty_count = 0
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _get_concise_practical_tips(self, law_name: str, article_number: str, content: str) -> List[str]:
        """ê°„ê²°í•œ ì‹¤ë¬´ ì ìš© íŒ ìƒì„±"""
        tips = []
        
        try:
            # ë²•ë ¹ë³„ íŠ¹í™”ëœ ì‹¤ë¬´ íŒ
            if law_name == "ë¯¼ë²•":
                if "750" in str(article_number):  # ë¶ˆë²•í–‰ìœ„
                    tips.extend([
                        "ì…ì¦ì±…ì„: ì›ê³ (í”¼í•´ì)ê°€ ê³¼ì‹¤ ì…ì¦",
                        "ë°°ìƒë²”ìœ„: ë¯¼ë²• ì œ393ì¡° (í†µìƒì†í•´ + íŠ¹ë³„ì†í•´)"
                    ])
                elif "565" in str(article_number):  # ê³„ì•½ê¸ˆ
                    tips.extend([
                        "ê³„ì•½ê¸ˆ í¬ê¸° ì‹œ ê³„ì•½ í•´ì œ ê°€ëŠ¥",
                        "ë°°ì•¡ ìƒí™˜ ì‹œ ë§¤ë„ì¸ë„ í•´ì œ ê°€ëŠ¥"
                    ])
                elif "615" in str(article_number):  # ì„ëŒ€ì°¨
                    tips.extend([
                        "ë³´ì¦ê¸ˆ ë°˜í™˜ ì˜ë¬´: ê³„ì•½ ì¢…ë£Œ ì‹œ",
                        "ì„ì°¨ê¶Œ ë“±ê¸°ëª…ë ¹ìœ¼ë¡œ ëŒ€í•­ë ¥ í™•ë³´"
                    ])
            
            elif law_name == "í˜•ë²•":
                if "250" in str(article_number):  # ì‚´ì¸
                    tips.extend([
                        "êµ¬ì„±ìš”ê±´: ê³ ì˜ + ìƒëª…ì¹¨í•´ + ì¸ê³¼ê´€ê³„",
                        "ë¯¸ìˆ˜ë²”ë„ ì²˜ë²Œ ëŒ€ìƒ"
                    ])
                elif "257" in str(article_number):  # ìƒí•´
                    tips.extend([
                        "ì‹ ì²´ìƒí•´ ì •ë„ì— ë”°ë¼ í˜•ëŸ‰ ì°¨ë“±",
                        "ìƒí•´ì¹˜ì‚¬ëŠ” ê²°ê³¼ì  ê°€ì¤‘ë²”"
                    ])
            
            elif law_name == "ê°€ì¡±ë²•" or "ì´í˜¼" in content:
                tips.extend([
                    "í˜‘ì˜ì´í˜¼: ìˆ™ë ¤ê¸°ê°„ í•„ìˆ˜",
                    "ì¬íŒìƒ ì´í˜¼: ìœ ì±…ì‚¬ìœ  í•„ìš”"
                ])
            
            # ì¼ë°˜ì ì¸ ì‹¤ë¬´ íŒ
            if not tips:
                if "ì†í•´ë°°ìƒ" in content:
                    tips.append("ì†í•´ì•¡ ì‚°ì •: ê°ê´€ì  ì¦ê±° í•„ìš”")
                elif "ê³„ì•½" in content:
                    tips.append("ê³„ì•½ì„œ ì‘ì„±: ëª…í™•í•œ ì¡°ê±´ ê¸°ì¬")
                elif "ì†Œì†¡" in content:
                    tips.append("ì†Œì†¡ ì œê¸°: ê´€í•  ë²•ì› í™•ì¸")
                else:
                    tips.append("ë²•ë¥  ìë¬¸: ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥")
            
            # ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ ë°˜í™˜
            return tips[:2]
            
        except Exception as e:
            self.logger.error(f"ì‹¤ë¬´ íŒ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["ë²•ë¥  ìë¬¸: ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥"]
    
    async def _get_related_articles(self, law_name: str, article_number: str) -> List[str]:
        """ê´€ë ¨ ì¡°ë¬¸ ëª©ë¡ ìƒì„± (DB ê¸°ë°˜)"""
        try:
            # LawContextSearchEngineì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ì¡°ë¬¸ ê²€ìƒ‰
            if self.context_search_engine:
                related_articles = await self.context_search_engine.search_related_articles(
                    law_name, int(article_number), context_range=3
                )
                
                # ê²°ê³¼ í¬ë§·íŒ… (ëŒ€ìƒ ì¡°ë¬¸ ì œì™¸)
                formatted_articles = []
                for article in related_articles:
                    if not article.is_target:  # ëŒ€ìƒ ì¡°ë¬¸ ì œì™¸
                        title_suffix = f"({article.article_title})" if article.article_title else ""
                        formatted_articles.append(f"{law_name} ì œ{article.article_number}ì¡°{title_suffix}")
                
                if formatted_articles:
                    self.logger.info(f"Found {len(formatted_articles)} related articles for {law_name} ì œ{article_number}ì¡°")
                    return formatted_articles[:3]  # ìµœëŒ€ 3ê°œ
            
            # LawContextSearchEngineì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            self.logger.warning(f"LawContextSearchEngine not available for {law_name} ì œ{article_number}ì¡°")
            return []
            
        except Exception as e:
            self.logger.error(f"Related articles search failed: {e}")
            return []
    
