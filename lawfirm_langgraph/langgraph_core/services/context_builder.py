# -*- coding: utf-8 -*-
"""
ì»¨í…ìŠ¤íŠ¸ ë¹Œë” ëª¨ë“ˆ
ê²€ìƒ‰ëœ ë¬¸ì„œë¡œë¶€í„° ë‹µë³€ ìƒì„±ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë¡œì§ì„ ë…ë¦½ ëª¨ë“ˆë¡œ ë¶„ë¦¬
"""

import logging
import re
from typing import Any, Dict, List, Optional

from langgraph_core.data.extractors import DocumentExtractor
from langgraph_core.utils.state_definitions import LegalWorkflowState
from langgraph_core.utils.workflow_utils import WorkflowUtils


class ContextBuilder:
    """
    ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• í´ë˜ìŠ¤

    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì¬ë­í‚¹í•˜ê³  ì„ ë³„í•˜ì—¬, ë‹µë³€ ìƒì„±ì— í•„ìš”í•œ ìµœì ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        semantic_search: Any,
        config: Any,
        logger: Optional[logging.Logger] = None
    ):
        """
        ContextBuilder ì´ˆê¸°í™”

        Args:
            semantic_search: ì˜ë¯¸ì  ê²€ìƒ‰ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
            config: ì„¤ì • ê°ì²´
            logger: ë¡œê±° (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        """
        self.semantic_search = semantic_search
        self.config = config
        self.logger = logger or logging.getLogger(__name__)

    def build_context(self, state: LegalWorkflowState) -> Dict[str, Any]:
        """ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ê¸¸ì´ ì œí•œ ê´€ë¦¬)"""
        max_length = self.config.max_context_length
        context_parts = []
        current_length = 0
        docs_truncated = 0

        retrieved_docs = WorkflowUtils.get_state_value(state, "retrieved_docs", [])
        for doc in retrieved_docs:
            doc_content = doc.get("content", "")
            doc_length = len(doc_content)

            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
            if current_length + doc_length > max_length:
                # ê°€ëŠ¥í•œ ë§Œí¼ë§Œ ì¶”ê°€
                remaining_length = max_length - current_length - 200  # ì—¬ìœ  ê³µê°„
                if remaining_length > 100:  # ìµœì†Œ 100ì
                    truncated_content = doc_content[:remaining_length] + "..."
                    context_parts.append(f"[ë¬¸ì„œ: {doc.get('source', 'unknown')}]\n{truncated_content}")
                    docs_truncated += 1
                    self.logger.warning("Document truncated due to context length limit")
                break

            context_part = f"[ë¬¸ì„œ: {doc.get('source', 'unknown')}]\n{doc_content}"
            context_parts.append(context_part)
            current_length += len(context_part)

        context_text = "\n\n".join(context_parts)

        if docs_truncated > 0:
            self.logger.info(f"Context length management: {current_length}/{max_length} chars, {docs_truncated} docs truncated")

        # structured_documents ìƒì„± (í´ë°± ê²½ë¡œì—ì„œë„ ë¬¸ì„œ í¬í•¨)
        structured_documents = {
            "total_count": len(retrieved_docs),
            "documents": []
        }

        for idx, doc in enumerate(retrieved_docs, 1):
            content = doc.get("content", "") or doc.get("text", "")
            if content and len(content.strip()) >= 10:
                structured_documents["documents"].append({
                    "document_id": idx,
                    "source": doc.get("source", "Unknown"),
                    "relevance_score": doc.get("relevance_score", doc.get("final_weighted_score", 0.0)),
                    "content": content[:2000]  # ìµœëŒ€ 2000ìë¡œ ì œí•œ
                })

        return {
            "context": context_text,
            "structured_documents": structured_documents,
            "document_count": len(structured_documents["documents"]),
            "legal_references": WorkflowUtils.get_state_value(state, "legal_references", []),
            "query_type": WorkflowUtils.get_state_value(state, "query_type", ""),
            "context_length": current_length,
            "docs_included": len(structured_documents["documents"]),
            "docs_truncated": docs_truncated
        }

    def build_intelligent_context(self, state: LegalWorkflowState) -> Dict[str, Any]:
        """ì§€ëŠ¥í˜• ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•"""
        try:
            max_length = self.config.max_context_length

            retrieved_docs = WorkflowUtils.get_state_value(state, "retrieved_docs", [])
            query = WorkflowUtils.get_state_value(state, "query", "")
            query_type = WorkflowUtils.get_state_value(state, "query_type", "")
            extracted_keywords = WorkflowUtils.get_state_value(state, "extracted_keywords", [])

            if not retrieved_docs:
                self.logger.warning("No documents retrieved for context building")
                return {
                    "context": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "legal_references": [],
                    "query_type": query_type,
                    "context_length": 0,
                    "docs_included": 0,
                    "insights": [],
                    "citations": []
                }

            # 1. ë¬¸ì„œ ì¬ë­í‚¹
            reranked_docs = self.rerank_documents_by_relevance(
                retrieved_docs,
                query,
                extracted_keywords
            )

            # 2. ê³ í’ˆì§ˆ ë¬¸ì„œ ì„ ë³„
            high_value_docs = self.select_high_value_documents(
                reranked_docs,
                query,
                min_relevance=0.5,
                max_docs=8
            )

            # 3. ì»¨í…ìŠ¤íŠ¸ ì¡°í•© ìµœì í™”
            optimized_composition = self.optimize_context_composition(
                high_value_docs,
                query,
                max_length
            )

            # 4. í•µì‹¬ ì •ë³´ ì¶”ì¶œ
            key_insights = self.extract_key_insights(high_value_docs, query)

            # 5. ë²•ë¥  ì¸ìš© ì •ë³´ ì¶”ì¶œ
            legal_citations = self.extract_legal_citations(high_value_docs)

            # 6. ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_text = "\n\n".join(optimized_composition["context_parts"])

            # context_partsê°€ ë¹„ì–´ìˆìœ¼ë©´ ìµœì†Œí•œì˜ context ìƒì„± (í´ë°±)
            if not context_text and high_value_docs:
                self.logger.warning(
                    f"âš ï¸ [INTELLIGENT CONTEXT] context_parts is empty, "
                    f"creating minimal context from {len(high_value_docs)} docs"
                )
                # ìµœì†Œí•œì˜ context ìƒì„± (ë¬¸ì„œ ìš”ì•½)
                context_parts = []
                for doc in high_value_docs[:5]:
                    content = doc.get("content", "") or doc.get("text", "") or doc.get("content_text", "")
                    source = doc.get("source", "Unknown")
                    if content and len(content.strip()) > 20:
                        # ë¬¸ì„œ ë‚´ìš© ì¼ë¶€ í¬í•¨
                        content_preview = content[:500]
                        context_parts.append(f"[ë¬¸ì„œ: {source}]\n{content_preview}")

                if context_parts:
                    context_text = "\n\n".join(context_parts)
                    self.logger.info(
                        f"âœ… [INTELLIGENT CONTEXT] Created minimal context with {len(context_parts)} docs "
                        f"({len(context_text)} chars)"
                    )

            # ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
            if key_insights and len(context_text) < max_length - 300:
                insights_text = "\n\n## í•µì‹¬ ì •ë³´\n" + "\n".join([f"- {insight}" for insight in key_insights[:5]])
                if len(context_text) + len(insights_text) < max_length:
                    context_text += insights_text

            # ì¸ìš© ì •ë³´ ì¶”ê°€
            if legal_citations and len(context_text) < max_length - 200:
                citations_text = "\n\n## ë²•ë¥  ì¸ìš©\n" + "\n".join([f"- {cit['text']}" for cit in legal_citations[:5]])
                if len(context_text) + len(citations_text) < max_length:
                    context_text += citations_text

            self.logger.info(
                f"ğŸ§  [INTELLIGENT CONTEXT] Built context with {optimized_composition['docs_included']} docs, "
                f"{len(key_insights)} insights, {len(legal_citations)} citations, "
                f"length: {len(context_text)}/{max_length}"
            )

            # structured_documents ìƒì„± (high_value_docsë¥¼ êµ¬ì¡°í™”)
            structured_documents = {
                "total_count": len(high_value_docs),
                "documents": []
            }

            for idx, doc in enumerate(high_value_docs[:8], 1):
                content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                relevance_score = doc.get("final_weighted_score") or doc.get("relevance_score", 0.0)

                if content and len(content.strip()) >= 10:
                    structured_documents["documents"].append({
                        "document_id": idx,
                        "source": doc.get("source", "Unknown"),
                        "relevance_score": relevance_score,
                        "content": content[:2000]  # ìµœëŒ€ 2000ìë¡œ ì œí•œ
                    })

            return {
                "context": context_text,
                "structured_documents": structured_documents,
                "document_count": len(structured_documents["documents"]),
                "legal_references": [cit["text"] for cit in legal_citations],
                "query_type": query_type,
                "context_length": len(context_text),
                "docs_included": optimized_composition["docs_included"],
                "docs_truncated": optimized_composition["docs_truncated"],
                "insights": key_insights[:5],
                "citations": legal_citations[:5]
            }

        except Exception as e:
            self.logger.error(f"Intelligent context building failed: {e}, falling back to simple context")
            return self.build_context(state)

    def rerank_documents_by_relevance(
        self,
        documents: List[Dict],
        query: str,
        extracted_keywords: List[str]
    ) -> List[Dict]:
        """ë¬¸ì„œ ê´€ë ¨ì„± ì¬ë­í‚¹ - ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ì§ì ‘ ìœ ì‚¬ë„ ì¬ê³„ì‚°"""
        if not documents:
            return documents

        try:
            reranked_docs = []

            for doc in documents:
                doc_content = doc.get("content", "")
                if not doc_content:
                    continue

                # 1. ì˜ë¯¸ì  ìœ ì‚¬ë„ ì¬ê³„ì‚° (semantic_search í™œìš©)
                semantic_score = 0.0
                try:
                    if self.semantic_search:
                        existing_score = doc.get("relevance_score", 0.0) or doc.get("combined_score", 0.0)
                        semantic_score = existing_score

                        # ì§ì ‘ ìœ ì‚¬ë„ ê³„ì‚° ì‹œë„
                        if hasattr(self.semantic_search, '_calculate_semantic_score'):
                            direct_score = self.semantic_search._calculate_semantic_score(query, doc_content)
                            # ê¸°ì¡´ ì ìˆ˜ì™€ ì§ì ‘ ê³„ì‚° ì ìˆ˜ ê°€ì¤‘ í‰ê· 
                            semantic_score = 0.6 * existing_score + 0.4 * direct_score
                except Exception as e:
                    self.logger.debug(f"Semantic score calculation failed: {e}")
                    semantic_score = doc.get("relevance_score", 0.0) or doc.get("combined_score", 0.0)

                # 2. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                keyword_score = 0.0
                if extracted_keywords:
                    content_lower = doc_content.lower()
                    query_lower = query.lower()

                    # ì¶”ì¶œëœ í‚¤ì›Œë“œ ë§¤ì¹­
                    keyword_matches = sum(1 for kw in extracted_keywords if isinstance(kw, str) and kw.lower() in content_lower)
                    keyword_score = min(1.0, keyword_matches / max(1, len(extracted_keywords)))

                    # ì§ˆë¬¸ í‚¤ì›Œë“œ ë§¤ì¹­
                    query_words = set(query_lower.split())
                    content_words = set(content_lower.split())
                    if query_words and content_words:
                        query_match_ratio = len(query_words.intersection(content_words)) / len(query_words)
                        keyword_score = (keyword_score + query_match_ratio) / 2

                # 3. ë²•ë¥  ìš©ì–´ ë§¤ì¹­ ì ìˆ˜
                legal_term_score = 0.0
                if extracted_keywords:
                    # ë²•ë¥  ìš©ì–´ íŒ¨í„´ í™•ì¸
                    legal_patterns = ['ë²•', 'ì¡°', 'ì¡°ë¬¸', 'íŒë¡€', 'ëŒ€ë²•ì›', 'ë²•ì›', 'ê·œì •', 'ë²•ë ¹']
                    content_lower = doc_content.lower()
                    term_matches = sum(1 for pattern in legal_patterns if pattern in content_lower)
                    legal_term_score = min(1.0, term_matches / max(1, len(legal_patterns) // 2))

                # 4. ì¢…í•© ê´€ë ¨ì„± ì ìˆ˜
                existing_combined = doc.get("combined_score", 0.0)

                # ê°€ì¤‘ì¹˜: ì˜ë¯¸ì  50%, í‚¤ì›Œë“œ 30%, ë²•ë¥  ìš©ì–´ 20%
                new_relevance_score = (
                    0.5 * semantic_score +
                    0.3 * keyword_score +
                    0.2 * legal_term_score
                )

                # ê¸°ì¡´ ì ìˆ˜ì™€ ìƒˆ ì ìˆ˜ ê²°í•©
                final_relevance = 0.7 * max(existing_combined, new_relevance_score) + 0.3 * new_relevance_score

                doc["final_relevance_score"] = final_relevance
                doc["query_direct_similarity"] = semantic_score
                doc["keyword_match_score"] = keyword_score
                doc["legal_term_score"] = legal_term_score

                reranked_docs.append(doc)

            # ìµœì¢… ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
            reranked_docs.sort(key=lambda x: x.get("final_relevance_score", 0.0), reverse=True)

            self.logger.info(
                f"ğŸ“Š [RERANK] Re-ranked {len(reranked_docs)} documents. "
                f"Top score: {reranked_docs[0].get('final_relevance_score', 0.0):.3f} if reranked_docs else 0.0"
            )

            return reranked_docs

        except Exception as e:
            self.logger.warning(f"Document reranking failed: {e}, using original order")
            return documents

    def select_high_value_documents(
        self,
        documents: List[Dict],
        query: str,
        min_relevance: float = 0.7,
        max_docs: int = 5
    ) -> List[Dict]:
        """ì •ë³´ ë°€ë„ ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ"""
        if not documents:
            return documents

        try:
            high_value_docs = []

            for doc in documents:
                doc_content = doc.get("content", "")
                if not doc_content or len(doc_content) < 20:
                    continue

                # 1. ë²•ë¥  ì¡°í•­ ì¸ìš© ìˆ˜ ê³„ì‚°
                citation_pattern = r'[ê°€-í£]+ë²•\s*ì œ?\s*\d+\s*ì¡°'
                citations = re.findall(citation_pattern, doc_content)
                citation_count = len(citations)
                citation_score = min(1.0, citation_count / 5.0)

                # 2. í•µì‹¬ ê°œë… ì„¤ëª… ì™„ì„±ë„ í‰ê°€
                query_words = set(query.lower().split())
                content_words = set(doc_content.lower().split())
                explanation_completeness = 0.0
                if query_words and content_words:
                    overlap = len(query_words.intersection(content_words))
                    explanation_completeness = min(1.0, overlap / max(1, len(query_words)))

                sentences = doc_content.split('ã€‚') or doc_content.split('.')
                avg_sentence_length = sum(len(s.strip()) for s in sentences if s.strip()) / max(1, len(sentences))

                descriptive_score_bonus = 0.0
                if 20 <= avg_sentence_length <= 100:
                    descriptive_score_bonus = 0.2
                elif avg_sentence_length > 100:
                    descriptive_score_bonus = 0.1

                explanation_completeness = min(1.0, explanation_completeness + descriptive_score_bonus)

                # 3. ì§ˆë¬¸ í‚¤ì›Œë“œ í¬í•¨ë„
                keyword_coverage = 0.0
                if query_words and content_words:
                    keyword_coverage = len(query_words.intersection(content_words)) / max(1, len(query_words))

                # 4. ì •ë³´ ë°€ë„ ì¢…í•© ì ìˆ˜
                relevance_score = doc.get("final_relevance_score") or doc.get("combined_score", 0.0) or doc.get("relevance_score", 0.0)

                information_density = (
                    0.3 * citation_score +
                    0.3 * explanation_completeness +
                    0.2 * keyword_coverage +
                    0.2 * min(1.0, relevance_score)
                )

                doc["information_density_score"] = information_density
                doc["citation_count"] = citation_count
                doc["explanation_completeness"] = explanation_completeness

                # ê´€ë ¨ì„± ì ìˆ˜ì™€ ì •ë³´ ë°€ë„ ì ìˆ˜ ê°€ì¤‘ í‰ê· 
                combined_value_score = 0.6 * relevance_score + 0.4 * information_density
                doc["combined_value_score"] = combined_value_score

                # ì„ê³„ê°’ ì²´í¬
                if combined_value_score >= min_relevance:
                    high_value_docs.append(doc)

            # combined_value_scoreë¡œ ì •ë ¬
            high_value_docs.sort(key=lambda x: x.get("combined_value_score", 0.0), reverse=True)

            # ìµœëŒ€ ë¬¸ì„œ ìˆ˜ ì œí•œ
            selected_docs = high_value_docs[:max_docs]

            self.logger.info(
                f"ğŸ“š [HIGH VALUE SELECTION] Selected {len(selected_docs)}/{len(documents)} documents. "
                f"Avg density: {sum(d.get('information_density_score', 0.0) for d in selected_docs) / max(1, len(selected_docs)):.3f}"
            )

            return selected_docs

        except Exception as e:
            self.logger.warning(f"High value document selection failed: {e}, using first {max_docs} documents")
            return documents[:max_docs]

    def optimize_context_composition(
        self,
        high_value_docs: List[Dict],
        query: str,
        max_length: int
    ) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ì¡°í•© ìµœì í™” (ê¸¸ì´ ê´€ë¦¬)"""
        try:
            optimized_context = {
                "context_parts": [],
                "citations": [],
                "insights": [],
                "total_length": 0,
                "docs_included": 0,
                "docs_truncated": 0
            }

            current_length = 0
            reserved_space = 500

            for doc in high_value_docs:
                doc_content = doc.get("content", "")
                if not doc_content:
                    continue

                doc_length = len(doc_content)
                doc_source = doc.get("source", "unknown")

                available_space = max_length - current_length - reserved_space

                if available_space <= 100:
                    break

                if doc_length <= available_space:
                    context_part = f"[ë¬¸ì„œ: {doc_source}]\n{doc_content}"
                    optimized_context["context_parts"].append(context_part)
                    current_length += len(context_part)
                    optimized_context["docs_included"] += 1
                elif available_space > 200:
                    sentences = re.split(r'[ã€‚.ï¼!?ï¼Ÿ]\s*', doc_content)
                    included_text = ""

                    for sentence in sentences:
                        sentence = sentence.strip()
                        if len(sentence) < 10:
                            continue

                        if len(included_text) + len(sentence) + 10 <= available_space:
                            included_text += sentence + ". "
                        else:
                            break

                    if included_text:
                        truncated_content = included_text.strip() + "..."
                        context_part = f"[ë¬¸ì„œ: {doc_source}]\n{truncated_content}"
                        optimized_context["context_parts"].append(context_part)
                        current_length += len(context_part)
                        optimized_context["docs_included"] += 1
                        optimized_context["docs_truncated"] += 1
                    else:
                        continue
                else:
                    continue

            optimized_context["total_length"] = current_length

            self.logger.info(
                f"ğŸ“¦ [CONTEXT OPTIMIZATION] Included {optimized_context['docs_included']} docs, "
                f"truncated {optimized_context['docs_truncated']}, "
                f"total length: {current_length}/{max_length} chars"
            )

            return optimized_context

        except Exception as e:
            self.logger.warning(f"Context composition optimization failed: {e}")
            context_parts = []
            current_length = 0

            for doc in high_value_docs[:5]:
                if current_length >= max_length - 500:
                    break
                doc_content = doc.get("content", "")[:500]
                doc_source = doc.get("source", "unknown")
                context_part = f"[ë¬¸ì„œ: {doc_source}]\n{doc_content}"
                context_parts.append(context_part)
                current_length += len(context_part)

            return {
                "context_parts": context_parts,
                "citations": [],
                "insights": [],
                "total_length": current_length,
                "docs_included": len(context_parts),
                "docs_truncated": 0
            }

    def extract_key_insights(
        self,
        documents: List[Dict],
        query: str
    ) -> List[str]:
        """í•µì‹¬ ì •ë³´ ì¶”ì¶œ - ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ"""
        insights = DocumentExtractor.extract_key_insights(documents, query)
        self.logger.debug(f"ğŸ“ [KEY INSIGHTS] Extracted {len(insights)} key insights")
        return insights

    def extract_legal_citations(
        self,
        documents: List[Dict]
    ) -> List[Dict[str, str]]:
        """ë²•ë¥  ì¸ìš© ì •ë³´ ì¶”ì¶œ"""
        citations = DocumentExtractor.extract_legal_citations(documents)
        self.logger.debug(f"âš–ï¸ [LEGAL CITATIONS] Extracted {len(citations)} citations")
        return citations

    def calculate_context_relevance(
        self,
        context: Dict[str, Any],
        query: str
    ) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ê³„ì‚° - ì§ˆë¬¸ê³¼ ê° ë¬¸ì„œì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            context_text = context.get("context", "")
            if not context_text:
                return 0.0

            # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì‹œë„
            try:
                if self.semantic_search and hasattr(self.semantic_search, '_calculate_semantic_score'):
                    relevance_score = self.semantic_search._calculate_semantic_score(query, context_text)
                    return relevance_score
            except Exception as e:
                self.logger.debug(f"Semantic relevance calculation failed: {e}")

            # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„
            query_words = set(query.lower().split())
            context_words = set(context_text.lower().split())

            if not query_words or not context_words:
                return 0.0

            overlap = len(query_words.intersection(context_words))
            relevance = overlap / max(1, len(query_words))

            return min(1.0, relevance)

        except Exception as e:
            self.logger.warning(f"Context relevance calculation failed: {e}")
            return 0.5  # ê¸°ë³¸ê°’

    def calculate_information_coverage(
        self,
        context: Dict[str, Any],
        query: str,
        query_type: str,
        extracted_keywords: List[str]
    ) -> float:
        """ì •ë³´ ì»¤ë²„ë¦¬ì§€ ê³„ì‚° - í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ë„"""
        try:
            context_text = context.get("context", "").lower()
            legal_references = context.get("legal_references", [])
            citations = context.get("citations", [])

            if not context_text and not legal_references and not citations:
                return 0.0

            coverage_scores = []

            # 1. ì¶”ì¶œëœ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€
            if extracted_keywords:
                keyword_matches = sum(1 for kw in extracted_keywords
                                    if isinstance(kw, str) and kw.lower() in context_text)
                keyword_coverage = keyword_matches / max(1, len(extracted_keywords))
                coverage_scores.append(keyword_coverage)

            # 2. ì§ˆë¬¸ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€
            query_words = set(query.lower().split())
            context_words = set(context_text.split())
            if query_words and context_words:
                query_coverage = len(query_words.intersection(context_words)) / max(1, len(query_words))
                coverage_scores.append(query_coverage)

            # 3. ì§ˆë¬¸ ìœ í˜•ë³„ í•„ìˆ˜ ì •ë³´ í¬í•¨ ì—¬ë¶€
            type_coverage = 0.0
            if query_type:
                type_lower = query_type.lower()
                if "precedent" in type_lower or "íŒë¡€" in type_lower:
                    # íŒë¡€ ì •ë³´ í¬í•¨ ì—¬ë¶€
                    precedent_indicators = ["íŒë¡€", "ëŒ€ë²•ì›", "ë²•ì›", "ì„ ê³ ", "íŒê²°"]
                    type_coverage = 1.0 if any(ind in context_text for ind in precedent_indicators) else 0.3
                elif "law" in type_lower or "ë²•ë ¹" in type_lower or "ì¡°ë¬¸" in type_lower:
                    # ë²•ë¥  ì¡°ë¬¸ í¬í•¨ ì—¬ë¶€
                    law_indicators = ["ë²•", "ì¡°", "ì¡°ë¬¸", "ê·œì •"]
                    type_coverage = 1.0 if any(ind in context_text for ind in law_indicators) else 0.3
                elif "advice" in type_lower or "ì¡°ì–¸" in type_lower:
                    # ì‹¤ë¬´ ì¡°ì–¸ í¬í•¨ ì—¬ë¶€
                    advice_indicators = ["í•´ì•¼", "í•´ì•¼", "ê¶Œì¥", "ì£¼ì˜", "ë°©ë²•"]
                    type_coverage = 1.0 if any(ind in context_text for ind in advice_indicators) else 0.5
                else:
                    type_coverage = 0.7  # ì¼ë°˜ ì§ˆë¬¸

            coverage_scores.append(type_coverage)

            # í‰ê·  ì»¤ë²„ë¦¬ì§€
            return sum(coverage_scores) / max(1, len(coverage_scores)) if coverage_scores else 0.0

        except Exception as e:
            self.logger.warning(f"Information coverage calculation failed: {e}")
            return 0.5

    def calculate_context_sufficiency(
        self,
        context: Dict[str, Any],
        query_type: str
    ) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ì¶©ë¶„ì„± í‰ê°€ - ì§ˆë¬¸ ìœ í˜•ë³„ ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡± ì—¬ë¶€"""
        try:
            context_text = context.get("context", "")
            legal_references = context.get("legal_references", [])
            citations = context.get("citations", [])

            # ì§ˆë¬¸ ìœ í˜•ë³„ ìµœì†Œ ìš”êµ¬ì‚¬í•­
            if not query_type:
                return 0.5

            type_lower = query_type.lower()

            # íŒë¡€ ê²€ìƒ‰
            if "precedent" in type_lower or "íŒë¡€" in type_lower:
                # íŒë¡€ ì •ë³´ ìµœì†Œ 1ê°œ
                precedent_count = len([c for c in citations if isinstance(c, dict) and c.get("type") == "precedent"])
                if precedent_count > 0:
                    return 1.0
                elif "íŒë¡€" in context_text or "ëŒ€ë²•ì›" in context_text or "ë²•ì›" in context_text:
                    return 0.7
                else:
                    return 0.3

            # ë²•ë ¹ ì¡°íšŒ
            elif "law" in type_lower or "ë²•ë ¹" in type_lower or "ì¡°ë¬¸" in type_lower:
                # ë²•ë¥  ì¡°ë¬¸ ìµœì†Œ 1ê°œ
                law_citation_count = len([c for c in citations if isinstance(c, dict) and c.get("type") == "law_article"])
                if law_citation_count > 0 or legal_references:
                    return 1.0
                elif "ë²•" in context_text and "ì¡°" in context_text:
                    return 0.7
                else:
                    return 0.3

            # ë²•ë¥  ì¡°ì–¸
            elif "advice" in type_lower or "ì¡°ì–¸" in type_lower:
                # ë²•ë ¹ + ì‹¤ë¬´ ì¡°ì–¸
                has_law = bool(legal_references) or "ë²•" in context_text
                has_advice = any(word in context_text for word in ["í•´ì•¼", "ê¶Œì¥", "ì£¼ì˜", "ë°©ë²•", "ì ˆì°¨"])

                if has_law and has_advice:
                    return 1.0
                elif has_law or has_advice:
                    return 0.6
                else:
                    return 0.3

            # ì¼ë°˜ ì§ˆë¬¸
            else:
                # ìµœì†Œí•œì˜ ì •ë³´ë¼ë„ ìˆì–´ì•¼ í•¨
                if len(context_text) > 100:
                    return 0.8
                elif len(context_text) > 50:
                    return 0.5
                else:
                    return 0.3

        except Exception as e:
            self.logger.warning(f"Context sufficiency calculation failed: {e}")
            return 0.5

    def identify_missing_information(
        self,
        context: Dict[str, Any],
        query: str,
        query_type: str,
        extracted_keywords: List[str]
    ) -> List[str]:
        """ë¶€ì¡±í•œ ì •ë³´ ì‹ë³„"""
        missing = []

        try:
            context_text = context.get("context", "").lower()
            legal_references = context.get("legal_references", [])
            citations = context.get("citations", [])

            # 1. ëˆ„ë½ëœ í‚¤ì›Œë“œ í™•ì¸
            if extracted_keywords:
                for kw in extracted_keywords:
                    if isinstance(kw, str) and kw.lower() not in context_text:
                        missing.append(kw)

            # 2. ì§ˆë¬¸ ìœ í˜•ë³„ í•„ìˆ˜ ì •ë³´ ëˆ„ë½ í™•ì¸
            type_lower = query_type.lower() if query_type else ""

            if "precedent" in type_lower or "íŒë¡€" in type_lower:
                if not any(c.get("type") == "precedent" for c in citations if isinstance(c, dict)):
                    if "íŒë¡€" not in context_text and "ëŒ€ë²•ì›" not in context_text:
                        missing.append("íŒë¡€ ì •ë³´")

            elif "law" in type_lower or "ë²•ë ¹" in type_lower:
                if not legal_references and not any(c.get("type") == "law_article" for c in citations if isinstance(c, dict)):
                    if "ë²•" not in context_text or "ì¡°" not in context_text:
                        missing.append("ë²•ë¥  ì¡°ë¬¸")

            elif "advice" in type_lower or "ì¡°ì–¸" in type_lower:
                if not any(word in context_text for word in ["í•´ì•¼", "ê¶Œì¥", "ì£¼ì˜", "ë°©ë²•", "ì ˆì°¨"]):
                    missing.append("ì‹¤ë¬´ ì¡°ì–¸")

            return missing[:5]  # ìµœëŒ€ 5ê°œ

        except Exception as e:
            self.logger.warning(f"Missing information identification failed: {e}")
            return []
