# -*- coding: utf-8 -*-
"""
ì¿¼ë¦¬ ê°•í™” ëª¨ë“ˆ
ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìµœì í™”í•˜ê³  ê°•í™”í•˜ëŠ” ë¡œì§ì„ ë…ë¦½ ëª¨ë“ˆë¡œ ë¶„ë¦¬
"""

import logging
import re
from typing import Any, Dict, List, Optional

from langgraph_core.data.extractors import DocumentExtractor
from langgraph_core.models.prompt_builders import QueryBuilder
from langgraph_core.utils.prompt_chain_executor import PromptChainExecutor
from langgraph_core.data.response_parsers import QueryParser
from langgraph_core.utils.state_definitions import LegalWorkflowState
from langgraph_core.utils.workflow_constants import WorkflowConstants
from langgraph_core.utils.workflow_utils import WorkflowUtils


class QueryEnhancer:
    """
    ì¿¼ë¦¬ ê°•í™” í´ë˜ìŠ¤

    ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìµœì í™”í•˜ê³  ê°•í™”í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ì™€ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    """

    def __init__(
        self,
        llm: Any,
        llm_fast: Optional[Any],
        term_integrator: Any,
        config: Any,
        logger: Optional[logging.Logger] = None
    ):
        """
        QueryEnhancer ì´ˆê¸°í™”

        Args:
            llm: LLM ì¸ìŠ¤í„´ìŠ¤
            llm_fast: ë¹ ë¥¸ LLM ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒ)
            term_integrator: ë²•ë¥  ìš©ì–´ í†µí•©ê¸°
            config: ì„¤ì • ê°ì²´
            logger: ë¡œê±° (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        """
        self.llm = llm
        self.llm_fast = llm_fast
        self.term_integrator = term_integrator
        self.config = config
        self.logger = logger or logging.getLogger(__name__)

        # ì¿¼ë¦¬ ê°•í™” ìºì‹œ
        self._query_enhancement_cache: Dict[str, Dict[str, Any]] = {}

    def optimize_search_query(
        self,
        query: str,
        query_type: str,
        extracted_keywords: List[str],
        legal_field: str
    ) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” (LLM ê°•í™” í¬í•¨, í´ë°± ì§€ì›)

        Returns:
            {
                "semantic_query": "ì˜ë¯¸ì  ê²€ìƒ‰ìš© ì¿¼ë¦¬",
                "keyword_queries": ["í‚¤ì›Œë“œ query 1", ...],
                "expanded_keywords": ["í™•ì¥ëœ í‚¤ì›Œë“œ", ...],
                "llm_enhanced": bool  # LLM ê°•í™” ì‚¬ìš© ì—¬ë¶€
            }
        """
        # LLM ì¿¼ë¦¬ ê°•í™” ì‹œë„ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
        llm_enhanced = None
        try:
            llm_enhanced = self.enhance_query_with_llm(
                query=query,
                query_type=query_type,
                extracted_keywords=extracted_keywords,
                legal_field=legal_field
            )
        except Exception as e:
            self.logger.debug(f"LLM query enhancement skipped: {e}")

        # LLM ê°•í™” ê²°ê³¼ ì‚¬ìš© ë˜ëŠ” ì›ë³¸ ì‚¬ìš©
        if llm_enhanced and isinstance(llm_enhanced, dict):
            base_query = llm_enhanced.get("optimized_query", query)
            llm_keywords = llm_enhanced.get("expanded_keywords", [])
            llm_variants = llm_enhanced.get("keyword_variants", [])
            llm_used = True
        else:
            # LLM ì‹¤íŒ¨ ì‹œ í´ë°± ê°•í™”: ê¸°ë³¸ ì¿¼ë¦¬ ì •ì œ ë° í‚¤ì›Œë“œ í™•ì¥
            base_query = self.clean_query_for_fallback(query)
            llm_keywords = []
            llm_variants = []
            llm_used = False
            self.logger.info(f"LLM enhancement failed, using enhanced fallback query: '{base_query[:50]}...'")

        # 1. ë²•ë¥  ìš©ì–´ ì •ê·œí™” ë° í™•ì¥ (LLM ê°•í™” ì¿¼ë¦¬ ì‚¬ìš©)
        normalized_terms = self.normalize_legal_terms(base_query, extracted_keywords)

        # 2. ë™ì˜ì–´ ë° ê´€ë ¨ ìš©ì–´ í™•ì¥ (LLM ì‹¤íŒ¨ ì‹œì—ë„ ê°•í™”)
        expanded_terms = self.expand_legal_terms(normalized_terms, legal_field)

        # LLM ì‹¤íŒ¨ ì‹œ ì¶”ê°€ í‚¤ì›Œë“œ í™•ì¥ ì‹œë„
        if not llm_used and extracted_keywords:
            # extracted_keywordsì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì„ íƒ
            core_keywords = [kw for kw in extracted_keywords[:5] if isinstance(kw, str) and len(kw) >= 2]
            expanded_terms.extend(core_keywords)
            expanded_terms = list(set(expanded_terms))[:15]  # ìµœëŒ€ 15ê°œë¡œ ì œí•œ

        # LLM í‚¤ì›Œë“œ ë³‘í•©
        if llm_keywords:
            expanded_terms = list(set(expanded_terms + llm_keywords))

        # 3. ì˜ë¯¸ì  ì¿¼ë¦¬ ìƒì„± (LLM ê°•í™” ì¿¼ë¦¬ ìš°ì„  ì‚¬ìš©)
        semantic_query = self.build_semantic_query(base_query, expanded_terms)

        # semantic_query ê²€ì¦ ë° ìˆ˜ì •
        if not semantic_query or not str(semantic_query).strip():
            self.logger.warning(f"optimize_search_query: semantic_query is empty, using base_query: '{base_query[:50]}...'")
            semantic_query = base_query

        # 4. í‚¤ì›Œë“œ ì¿¼ë¦¬ ìƒì„± (ë²•ë¥  ì¡°í•­, íŒë¡€ ê²€ìƒ‰ìš©)
        keyword_queries = self.build_keyword_queries(base_query, expanded_terms, query_type)

        # keyword_queries ê²€ì¦ ë° ìˆ˜ì •
        if not keyword_queries or len(keyword_queries) == 0:
            self.logger.warning(f"optimize_search_query: keyword_queries is empty, using base_query")
            keyword_queries = [base_query]

        # LLM ë³€í˜• ì¿¼ë¦¬ ì¶”ê°€
        if llm_variants:
            keyword_queries.extend(llm_variants[:3])  # ìµœëŒ€ 3ê°œë§Œ

        result = {
            "semantic_query": semantic_query,
            "keyword_queries": keyword_queries[:5],  # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
            "expanded_keywords": expanded_terms,
            "llm_enhanced": llm_used
        }

        # ìµœì¢… ê²€ì¦ ë¡œê·¸
        self.logger.debug(
            f"optimize_search_query result: "
            f"semantic_query length={len(semantic_query)}, "
            f"keyword_queries count={len(keyword_queries)}"
        )

        return result

    def enhance_query_with_llm(
        self,
        query: str,
        query_type: str,
        extracted_keywords: List[str],
        legal_field: str
    ) -> Optional[Dict[str, Any]]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ê°•í™” (ìºì‹± í¬í•¨)

        Args:
            query: ì›ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬
            query_type: ì§ˆë¬¸ ìœ í˜•
            extracted_keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡
            legal_field: ë²•ë¥  ë¶„ì•¼

        Returns:
            {
                "optimized_query": "ìµœì í™”ëœ ì¿¼ë¦¬",
                "expanded_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...],
                "keyword_variants": ["ë³€í˜• ì¿¼ë¦¬1", "ë³€í˜• ì¿¼ë¦¬2", ...],
                "legal_terms": ["ë²•ë¥  ìš©ì–´1", "ë²•ë¥  ìš©ì–´2", ...],
                "reasoning": "ê°œì„  ì‚¬ìœ "
            } ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        if not self.llm:
            self.logger.debug("LLM not available for query enhancement")
            return None

        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"query_enhance:{query}:{query_type}:{legal_field}"

        # ìºì‹œ í™•ì¸
        if cache_key in self._query_enhancement_cache:
            self.logger.debug(f"Using cached query enhancement for: {query[:50]}")
            return self._query_enhancement_cache[cache_key]

        try:
            # ë°ì´í„° ì •ê·œí™” ë° ê²€ì¦
            normalized_query_type = WorkflowUtils.normalize_query_type_for_prompt(query_type, self.logger)

            # extracted_keywords ê²€ì¦
            if not extracted_keywords or not isinstance(extracted_keywords, list):
                extracted_keywords = []

            # legal_field ê²€ì¦
            if not legal_field or not isinstance(legal_field, str):
                legal_field = ""

            # query ê²€ì¦
            if not query or not isinstance(query, str):
                self.logger.warning("Invalid query provided for enhancement")
                return None

            # ë¡œê¹…: ì „ë‹¬ë˜ëŠ” ë°ì´í„° í™•ì¸
            self.logger.debug(
                f"ğŸ” [QUERY ENHANCEMENT] Building prompt with:\n"
                f"   query: '{query[:50]}...'\n"
                f"   query_type: '{normalized_query_type}'\n"
                f"   extracted_keywords: {len(extracted_keywords)} items\n"
                f"   legal_field: '{legal_field}'"
            )

            # Prompt Chainingì„ ì‚¬ìš©í•œ ì¿¼ë¦¬ ê°•í™”
            enhanced_result = self.enhance_query_with_chain(
                query=query,
                query_type=normalized_query_type,
                extracted_keywords=extracted_keywords,
                legal_field=legal_field
            )

            # ì²´ì¸ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
            if not enhanced_result:
                self.logger.debug("Chain enhancement failed, using fallback")
                prompt = self.build_query_enhancement_prompt(
                    query=query,
                    query_type=normalized_query_type,
                    extracted_keywords=extracted_keywords,
                    legal_field=legal_field
                )

                # LLM í˜¸ì¶œ (ì§§ì€ ì‘ë‹µë§Œ í•„ìš”í•˜ë¯€ë¡œ í† í° ìˆ˜ ì œí•œ)
                try:
                    response = self.llm.invoke(prompt)
                    if isinstance(response, str):
                        llm_output = response
                    elif hasattr(response, 'content'):
                        llm_output = response.content
                    else:
                        self.logger.warning(f"Unexpected LLM response type: {type(response)}")
                        return None
                except Exception as e:
                    self.logger.warning(f"LLM invocation failed: {e}")
                    return None

                # LLM ì‘ë‹µ íŒŒì‹±
                enhanced_result = self.parse_llm_query_enhancement(llm_output)

            if enhanced_result:
                # ê²°ê³¼ ìºì‹±
                self._query_enhancement_cache[cache_key] = enhanced_result
                # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 100ê°œ)
                if len(self._query_enhancement_cache) > 100:
                    # ì˜¤ë˜ëœ í•­ëª© ì œê±° (FIFO)
                    oldest_key = next(iter(self._query_enhancement_cache))
                    del self._query_enhancement_cache[oldest_key]

                self.logger.info(
                    f"âœ… [LLM QUERY ENHANCEMENT] Original: '{query}' â†’ "
                    f"Enhanced: '{enhanced_result.get('optimized_query', query)}'"
                )
            else:
                self.logger.debug("Failed to parse LLM enhancement response")

            return enhanced_result

        except Exception as e:
            self.logger.warning(f"Error in LLM query enhancement: {e}")
            return None

    def enhance_query_with_chain(
        self,
        query: str,
        query_type: str,
        extracted_keywords: List[str],
        legal_field: str
    ) -> Optional[Dict[str, Any]]:
        """
        Prompt Chainingì„ ì‚¬ìš©í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ê°•í™” (ë‹¤ë‹¨ê³„ ì²´ì¸)

        Step 1: ì¿¼ë¦¬ ë¶„ì„ ë° í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        Step 2: í‚¤ì›Œë“œ í™•ì¥ ë° ë³€í˜• ìƒì„± (ë™ì˜ì–´, ê´€ë ¨ ìš©ì–´)
        Step 3: ìµœì í™”ëœ ì¿¼ë¦¬ ìƒì„± (ë²¡í„° ê²€ìƒ‰ìš©, í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)
        Step 4: ê²€ì¦ ë° ê°œì„  ì œì•ˆ

        Returns:
            Optional[Dict[str, Any]]: ê°•í™”ëœ ì¿¼ë¦¬ ê²°ê³¼ ë˜ëŠ” None
        """
        try:
            # PromptChainExecutor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            llm = self.llm_fast if self.llm_fast else self.llm
            chain_executor = PromptChainExecutor(llm, self.logger)

            # ì²´ì¸ ìŠ¤í… ì •ì˜
            chain_steps = []

            # Step 1: ì¿¼ë¦¬ ë¶„ì„ ë° í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            def build_query_analysis_prompt(prev_output, initial_input):
                query_value = initial_input.get("query") if isinstance(initial_input, dict) else query
                query_type_value = initial_input.get("query_type") if isinstance(initial_input, dict) else query_type
                legal_field_value = initial_input.get("legal_field") if isinstance(initial_input, dict) else legal_field

                return f"""ë‹¤ìŒ ë²•ë¥  ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì›ë³¸ ì¿¼ë¦¬: {query_value}
ì§ˆë¬¸ ìœ í˜•: {query_type_value}
ë²•ë¥  ë¶„ì•¼: {legal_field_value if legal_field_value else "ë¯¸ì§€ì •"}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "core_keywords": ["í•µì‹¬ í‚¤ì›Œë“œ1", "í•µì‹¬ í‚¤ì›Œë“œ2", "í•µì‹¬ í‚¤ì›Œë“œ3"],
    "query_intent": "ì¿¼ë¦¬ì˜ ì˜ë„ ì„¤ëª…",
    "key_concepts": ["í•µì‹¬ ë²•ë¥  ê°œë…1", "í•µì‹¬ ë²•ë¥  ê°œë…2"],
    "analysis": "ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼ (í•œêµ­ì–´)"
}}
"""

            chain_steps.append({
                "name": "query_analysis",
                "prompt_builder": build_query_analysis_prompt,
                "input_extractor": lambda prev: prev,
                "output_parser": lambda response, prev: QueryParser.parse_query_analysis_response(response),
                "validator": lambda output: output and isinstance(output, dict) and "core_keywords" in output,
                "required": True
            })

            # Step 2: í‚¤ì›Œë“œ í™•ì¥ ë° ë³€í˜• ìƒì„±
            def build_keyword_expansion_prompt(prev_output, initial_input):
                if not isinstance(prev_output, dict):
                    prev_output = {}

                core_keywords = prev_output.get("core_keywords", [])
                query_intent = prev_output.get("query_intent", "")
                key_concepts = prev_output.get("key_concepts", [])
                query_value = initial_input.get("query") if isinstance(initial_input, dict) else query
                query_type_value = initial_input.get("query_type") if isinstance(initial_input, dict) else query_type
                legal_field_value = initial_input.get("legal_field") if isinstance(initial_input, dict) else legal_field

                return f"""ë‹¤ìŒ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ í™•ì¥í•˜ê¸° ìœ„í•œ í‚¤ì›Œë“œ ë³€í˜•ê³¼ ë™ì˜ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì¿¼ë¦¬: {query_value}
ì§ˆë¬¸ ìœ í˜•: {query_type_value}
ë²•ë¥  ë¶„ì•¼: {legal_field_value if legal_field_value else "ë¯¸ì§€ì •"}
ì¿¼ë¦¬ ì˜ë„: {query_intent}
í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(core_keywords) if core_keywords else "ì—†ìŒ"}
í•µì‹¬ ê°œë…: {', '.join(key_concepts) if key_concepts else "ì—†ìŒ"}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "expanded_keywords": ["í™•ì¥ í‚¤ì›Œë“œ1", "í™•ì¥ í‚¤ì›Œë“œ2", "í™•ì¥ í‚¤ì›Œë“œ3", "í™•ì¥ í‚¤ì›Œë“œ4", "í™•ì¥ í‚¤ì›Œë“œ5"],
    "synonyms": ["ë™ì˜ì–´1", "ë™ì˜ì–´2", "ë™ì˜ì–´3"],
    "related_terms": ["ê´€ë ¨ ìš©ì–´1", "ê´€ë ¨ ìš©ì–´2"],
    "keyword_variants": ["í‚¤ì›Œë“œ ë³€í˜•1", "í‚¤ì›Œë“œ ë³€í˜•2"],
    "reasoning": "í™•ì¥ ê·¼ê±° (í•œêµ­ì–´)"
}}
"""

            chain_steps.append({
                "name": "keyword_expansion",
                "prompt_builder": build_keyword_expansion_prompt,
                "input_extractor": lambda prev: prev,
                "output_parser": lambda response, prev: QueryParser.parse_keyword_expansion_response(response),
                "validator": lambda output: output and isinstance(output, dict) and "expanded_keywords" in output,
                "required": True
            })

            # Step 3: ìµœì í™”ëœ ì¿¼ë¦¬ ìƒì„±
            def build_query_optimization_prompt(prev_output, initial_input):
                if not isinstance(prev_output, dict):
                    prev_output = {}

                expanded_keywords = prev_output.get("expanded_keywords", [])
                synonyms = prev_output.get("synonyms", [])
                related_terms = prev_output.get("related_terms", [])
                keyword_variants = prev_output.get("keyword_variants", [])

                # Step 1 ê²°ê³¼ì—ì„œ core_keywords ê°€ì ¸ì˜¤ê¸°
                core_keywords = []
                if hasattr(chain_executor, 'chain_history'):
                    for step in chain_executor.chain_history:
                        if step.get("step_name") == "query_analysis" and step.get("success"):
                            step_output = step.get("output", {})
                            if isinstance(step_output, dict):
                                core_keywords = step_output.get("core_keywords", [])
                                break

                query_value = initial_input.get("query") if isinstance(initial_input, dict) else query
                query_type_value = initial_input.get("query_type") if isinstance(initial_input, dict) else query_type
                legal_field_value = initial_input.get("legal_field") if isinstance(initial_input, dict) else legal_field

                return f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë²•ë¥  ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì¿¼ë¦¬: {query_value}
ì§ˆë¬¸ ìœ í˜•: {query_type_value}
ë²•ë¥  ë¶„ì•¼: {legal_field_value if legal_field_value else "ë¯¸ì§€ì •"}
í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(core_keywords) if core_keywords else "ì—†ìŒ"}
í™•ì¥ í‚¤ì›Œë“œ: {', '.join(expanded_keywords[:10]) if expanded_keywords else "ì—†ìŒ"}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "optimized_query": "ìµœì í™”ëœ ë©”ì¸ ì¿¼ë¦¬ (ìµœëŒ€ 50ì)",
    "semantic_query": "ë²¡í„° ê²€ìƒ‰ìš© ì¿¼ë¦¬ (ì˜ë¯¸ ì¤‘ì‹¬)",
    "keyword_query": "í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì¿¼ë¦¬ (ì •í™• ë§¤ì¹­ ì¤‘ì‹¬)",
    "legal_terms": ["ë²•ë¥  ì „ë¬¸ ìš©ì–´1", "ë²•ë¥  ì „ë¬¸ ìš©ì–´2"],
    "reasoning": "ìµœì í™” ê·¼ê±° (í•œêµ­ì–´)"
}}
"""

            chain_steps.append({
                "name": "query_optimization",
                "prompt_builder": build_query_optimization_prompt,
                "input_extractor": lambda prev: prev,
                "output_parser": lambda response, prev: QueryParser.parse_query_optimization_response(response),
                "validator": lambda output: output and isinstance(output, dict) and "optimized_query" in output,
                "required": True
            })

            # Step 4: ê²€ì¦ ë° ê°œì„  ì œì•ˆ
            def build_query_validation_prompt(prev_output, initial_input):
                if not isinstance(prev_output, dict):
                    prev_output = {}

                optimized_query = prev_output.get("optimized_query", "")
                query_value = initial_input.get("query") if isinstance(initial_input, dict) else query

                return f"""ë‹¤ìŒ ìµœì í™”ëœ ì¿¼ë¦¬ë¥¼ ê²€ì¦í•˜ê³  ê°œì„  ì œì•ˆì„ í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì¿¼ë¦¬: {query_value}
ìµœì í™”ëœ ì¿¼ë¦¬: {optimized_query}

ë‹¤ìŒ ê´€ì ì—ì„œ ê²€ì¦í•´ì£¼ì„¸ìš”:
1. ì›ë³¸ ì¿¼ë¦¬ì˜ í•µì‹¬ ì˜ë„ê°€ ìœ ì§€ë˜ì—ˆëŠ”ê°€?
2. ê²€ìƒ‰ ì •í™•ë„ê°€ í–¥ìƒë˜ì—ˆëŠ”ê°€?
3. ê²€ìƒ‰ ë²”ìœ„ê°€ ì ì ˆíˆ í™•ì¥ë˜ì—ˆëŠ”ê°€?
4. ë²•ë¥  ì „ë¬¸ì„±ì´ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "is_valid": true | false,
    "quality_score": 0.0-1.0,
    "improvements": ["ê°œì„  ì œì•ˆ1", "ê°œì„  ì œì•ˆ2"],
    "final_reasoning": "ìµœì¢… ê²€ì¦ ê²°ê³¼ ë° ê°œì„  ì‚¬ìœ "
}}
"""

            chain_steps.append({
                "name": "query_validation",
                "prompt_builder": build_query_validation_prompt,
                "input_extractor": lambda prev: prev,
                "output_parser": lambda response, prev: QueryParser.parse_query_validation_response(response),
                "validator": lambda output: output and isinstance(output, dict) and "is_valid" in output,
                "required": False,
            })

            # ì²´ì¸ ì‹¤í–‰
            initial_input_dict = {
                "query": query,
                "query_type": query_type,
                "extracted_keywords": extracted_keywords,
                "legal_field": legal_field
            }

            chain_result = chain_executor.execute_chain(
                chain_steps=chain_steps,
                initial_input=initial_input_dict,
                max_iterations=2,
                stop_on_failure=False
            )

            # ê²°ê³¼ ì¶”ì¶œ ë° í†µí•©
            chain_history = chain_result.get("chain_history", [])

            # Step 1 ê²°ê³¼: ì¿¼ë¦¬ ë¶„ì„
            analysis_result = None
            for step in chain_history:
                if step.get("step_name") == "query_analysis" and step.get("success"):
                    analysis_result = step.get("output", {})
                    break

            # Step 2 ê²°ê³¼: í‚¤ì›Œë“œ í™•ì¥
            expansion_result = None
            for step in chain_history:
                if step.get("step_name") == "keyword_expansion" and step.get("success"):
                    expansion_result = step.get("output", {})
                    break

            # Step 3 ê²°ê³¼: ì¿¼ë¦¬ ìµœì í™”
            optimization_result = None
            for step in chain_history:
                if step.get("step_name") == "query_optimization" and step.get("success"):
                    optimization_result = step.get("output", {})
                    break

            # Step 4 ê²°ê³¼: ê²€ì¦
            validation_result = None
            for step in chain_history:
                if step.get("step_name") == "query_validation" and step.get("success"):
                    validation_result = step.get("output", {})
                    break

            # ê²°ê³¼ í†µí•©
            if not optimization_result or not isinstance(optimization_result, dict):
                return None

            # ìµœì¢… ê²°ê³¼ ìƒì„±
            enhanced_result = {
                "optimized_query": optimization_result.get("optimized_query", ""),
                "expanded_keywords": expansion_result.get("expanded_keywords", []) if expansion_result else [],
                "keyword_variants": [
                    optimization_result.get("semantic_query", ""),
                    optimization_result.get("keyword_query", "")
                ],
                "legal_terms": optimization_result.get("legal_terms", []),
                "reasoning": optimization_result.get("reasoning", "") or (validation_result.get("final_reasoning", "") if validation_result else "")
            }

            # ê²€ì¦ ê²°ê³¼ ë°˜ì˜
            if validation_result and isinstance(validation_result, dict):
                if not validation_result.get("is_valid", True):
                    self.logger.warning(f"Query validation failed: {validation_result.get('final_reasoning', '')}")
                else:
                    quality_score = validation_result.get("quality_score", 0.8)
                    if quality_score < 0.7:
                        self.logger.warning(f"Low quality score: {quality_score}")

            # ì²´ì¸ ì‹¤í–‰ ê²°ê³¼ ë¡œê¹…
            chain_summary = chain_executor.get_chain_summary()
            self.logger.info(
                f"âœ… [QUERY CHAIN] Executed {chain_summary['total_steps']} steps, "
                f"{chain_summary['successful_steps']} successful, "
                f"{chain_summary['failed_steps']} failed, "
                f"Total time: {chain_summary['total_time']:.2f}s"
            )

            return enhanced_result

        except Exception as e:
            self.logger.error(f"âŒ [QUERY CHAIN ERROR] Prompt chain failed: {e}")
            return None

    def build_query_enhancement_prompt(
        self,
        query: str,
        query_type: str,
        extracted_keywords: List[str],
        legal_field: str
    ) -> str:
        """ì¿¼ë¦¬ ê°•í™”ë¥¼ ìœ„í•œ LLM í”„ë¡¬í”„íŠ¸ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        # ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”
        if not query or not isinstance(query, str):
            raise ValueError("query must be a non-empty string")

        if not query_type or not isinstance(query_type, str):
            query_type = "general_question"

        if not isinstance(extracted_keywords, list):
            extracted_keywords = []

        if not isinstance(legal_field, str):
            legal_field = ""

        legal_field_text = legal_field.strip() if legal_field else "ë¯¸ì§€ì •"

        # í‚¤ì›Œë“œ ìƒì„¸ ì •ë³´ êµ¬ì„±
        keywords_info = ""
        if extracted_keywords and len(extracted_keywords) > 0:
            valid_keywords = [kw for kw in extracted_keywords if kw and isinstance(kw, str) and len(kw.strip()) > 0]
            if valid_keywords:
                keywords_info = f"""
**ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡** (ì´ {len(valid_keywords)}ê°œ):
{chr(10).join([f"  - {kw.strip()}" for kw in valid_keywords[:10]])}
"""
                if len(valid_keywords) > 10:
                    keywords_info += f"  ... ì™¸ {len(valid_keywords) - 10}ê°œ\n"
            else:
                keywords_info = "**ì¶”ì¶œëœ í‚¤ì›Œë“œ**: ì—†ìŒ (ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•´ì•¼ í•¨)\n\n**ì£¼ì˜**: ì›ë³¸ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ë²•ë¥  ê²€ìƒ‰ì— í•„ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì‹ë³„í•˜ì„¸ìš”.\n"
        else:
            keywords_info = "**ì¶”ì¶œëœ í‚¤ì›Œë“œ**: ì—†ìŒ (ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•´ì•¼ í•¨)\n\n**ì£¼ì˜**: ì›ë³¸ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ë²•ë¥  ê²€ìƒ‰ì— í•„ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì‹ë³„í•˜ì„¸ìš”.\n"

        # ì§ˆë¬¸ ìœ í˜•ë³„ ìƒì„¸ ê°€ì´ë“œ
        query_type_guides = {
            "precedent_search": {
                "description": "íŒë¡€ ê²€ìƒ‰",
                "search_focus": "ì‚¬ê±´ë²ˆí˜¸, ë²•ì›ëª…, ì‚¬ê±´ëª…, íŒì‹œì‚¬í•­, íŒê²°ìš”ì§€, ê´€ë ¨ ë²•ë ¹",
                "keyword_suggestions": ["íŒë¡€", "ëŒ€ë²•ì›", "ê³ ë“±ë²•ì›", "ì§€ë°©ë²•ì›", "ì‚¬ê±´ë²ˆí˜¸", "íŒê²°", "ì„ ê³ ", "íŒì‹œì‚¬í•­", "íŒê²°ìš”ì§€"],
                "database_fields": "cases.case_number, cases.court, cases.case_type, case_paragraphs.text, cases.announce_date",
                "search_strategy": "ì‚¬ê±´ë²ˆí˜¸ íŒ¨í„´(YYYYë‹¤/ë‚˜XXXXX), ë²•ì›ëª…, ì‚¬ê±´ëª…ì˜ í•µì‹¬ í‚¤ì›Œë“œ, ê´€ë ¨ ë²•ë ¹ëª… ì¡°í•©"
            },
            "law_inquiry": {
                "description": "ë²•ë ¹ ì¡°íšŒ",
                "search_focus": "ë²•ë ¹ëª…, ì¡°ë¬¸ë²ˆí˜¸, ì¡°í•­ ë‚´ìš©, ì‹œí–‰ì¼, ê°œì • ì´ë ¥",
                "keyword_suggestions": ["ë²•ë¥ ", "ë²•ë ¹", "ì¡°í•­", "ì¡°ë¬¸", "ì œXXì¡°", "ì‹œí–‰ë ¹", "ì‹œí–‰ê·œì¹™"],
                "database_fields": "statutes.name, statute_articles.article_no, statute_articles.text, statutes.effective_date",
                "search_strategy": "ë²•ë ¹ëª… + ì¡°ë¬¸ë²ˆí˜¸ ì¡°í•©, ì¡°í•­ì˜ í•µì‹¬ ë²•ë¦¬ ìš©ì–´, ê´€ë ¨ íŒë¡€ì™€ì˜ ì—°ê³„ í‚¤ì›Œë“œ"
            },
            "legal_advice": {
                "description": "ë²•ë¥  ì¡°ì–¸",
                "search_focus": "ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸, ìœ ì‚¬ íŒë¡€, ë²•ë¦¬ í•´ì„, ì‹¤ë¬´ ì ìš©",
                "keyword_suggestions": ["ë²•ë¥ ", "íŒë¡€", "ì¡°ë¬¸", "ë²•ë¦¬", "í•´ì„", "ì ìš©", "ìš”ê±´", "íš¨ë ¥"],
                "database_fields": "statute_articles, case_paragraphs, decision_paragraphs, interpretation_paragraphs",
                "search_strategy": "ë¬¸ì œ ìƒí™©ì˜ í•µì‹¬ ë²•ë¥  ê°œë… + ê´€ë ¨ ì¡°ë¬¸ + ìœ ì‚¬ íŒë¡€ íŒ¨í„´"
            },
            "document_analysis": {
                "description": "ë¬¸ì„œ ë¶„ì„",
                "search_focus": "ë¬¸ì„œ ìœ í˜•, ë²•ì  ê·¼ê±°, ê´€ë ¨ íŒë¡€, ê³„ì•½ ì¡°í•­",
                "keyword_suggestions": ["ê³„ì•½ì„œ", "ë²•ì  ê·¼ê±°", "ê´€ë ¨ íŒë¡€", "ì¡°í•­", "ê³„ì•½ ì¡°í•­", "ì˜ë¬´", "ê¶Œë¦¬"],
                "database_fields": "ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰ ê°€ëŠ¥ (statutes, cases, decisions, interpretations)",
                "search_strategy": "ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ ë²•ë ¹ëª… + ê³„ì•½ ìœ í˜• + ê´€ë ¨ ë²•ë¦¬"
            },
            "general_question": {
                "description": "ì¼ë°˜ ë²•ë¥  ì§ˆë¬¸",
                "search_focus": "ê´€ë ¨ ë²•ë ¹, íŒë¡€, ë²•ë¥  ìš©ì–´, ì‹¤ë¬´ í•´ì„",
                "keyword_suggestions": ["ë²•ë¥ ", "ë²•ë ¹", "íŒë¡€", "ë²•ë¥  ìš©ì–´", "í•´ì„"],
                "database_fields": "ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰",
                "search_strategy": "ì§ˆë¬¸ì˜ í•µì‹¬ ë²•ë¥  ê°œë… + ê´€ë ¨ ë¶„ì•¼ + ì£¼ìš” ìš©ì–´"
            }
        }

        query_guide = query_type_guides.get(query_type, {
            "description": "ì¼ë°˜ ê²€ìƒ‰",
            "search_focus": "ê´€ë ¨ ë²•ë ¹, íŒë¡€, ë²•ë¥  ìš©ì–´",
            "keyword_suggestions": [],
            "database_fields": "ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤",
            "search_strategy": "í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ ê²€ìƒ‰"
        })

        # ë²•ë¥  ë¶„ì•¼ë³„ ì¶”ê°€ ì •ë³´
        field_specific_info = {
            "family": {
                "related_laws": ["ë¯¼ë²• ê°€ì¡±í¸", "ê°€ì¡±ê´€ê³„ì˜ ë“±ë¡ ë“±ì— ê´€í•œ ë²•ë¥ "],
                "key_concepts": ["í˜¼ì¸", "ì´í˜¼", "ì–‘ìœ¡ê¶Œ", "ì¹œê¶Œ", "ìƒì†", "ìœ„ìë£Œ", "ì¬ì‚°ë¶„í• "],
                "common_keywords": ["ë¶€ë¶€", "ê°€ì¡±", "ì´í˜¼", "ìƒì†", "ì¹œì", "ì–‘ìœ¡", "ìœ„ìë£Œ"]
            },
            "civil": {
                "related_laws": ["ë¯¼ë²•", "ë¯¼ì‚¬ì†Œì†¡ë²•"],
                "key_concepts": ["ê³„ì•½", "ë¶ˆë²•í–‰ìœ„", "ì†í•´ë°°ìƒ", "ì±„ê¶Œ", "ì±„ë¬´", "ì†Œìœ ê¶Œ", "ì ìœ "],
                "common_keywords": ["ê³„ì•½", "ì†í•´ë°°ìƒ", "ì±„ê¶Œ", "ì±„ë¬´", "ì†Œìœ ê¶Œ"]
            },
            "criminal": {
                "related_laws": ["í˜•ë²•", "í˜•ì‚¬ì†Œì†¡ë²•"],
                "key_concepts": ["ë²”ì£„", "êµ¬ì„±ìš”ê±´", "í˜•ëŸ‰", "ì²˜ë²Œ", "ê¸°ì†Œ", "ê³µì†Œ"],
                "common_keywords": ["ë²”ì£„", "ì²˜ë²Œ", "í˜•ëŸ‰", "êµ¬ì„±ìš”ê±´", "ê¸°ì†Œ"]
            },
            "labor": {
                "related_laws": ["ê·¼ë¡œê¸°ì¤€ë²•", "ë…¸ë™ì¡°í•©ë²•", "ê³ ìš©ë³´í—˜ë²•"],
                "key_concepts": ["ê·¼ë¡œê³„ì•½", "ì„ê¸ˆ", "ê·¼ë¡œì‹œê°„", "í•´ê³ ", "í‡´ì§ê¸ˆ", "ì‚°ì¬"],
                "common_keywords": ["ê·¼ë¡œ", "ì„ê¸ˆ", "í•´ê³ ", "ë…¸ë™", "ê·¼ë¡œì", "ì‚¬ìš©ì"]
            },
            "corporate": {
                "related_laws": ["ìƒë²•", "ì£¼ì‹íšŒì‚¬ë²•", "ë²•ì¸ì„¸ë²•"],
                "key_concepts": ["íšŒì‚¬", "ì£¼ì£¼", "ì´ì‚¬", "ë²•ì¸", "ìë³¸", "ì´ì‚¬íšŒ"],
                "common_keywords": ["íšŒì‚¬", "ì£¼ì£¼", "ì´ì‚¬", "ë²•ì¸", "ê¸°ì—…"]
            },
            "tax": {
                "related_laws": ["ì†Œë“ì„¸ë²•", "ë²•ì¸ì„¸ë²•", "ë¶€ê°€ê°€ì¹˜ì„¸ë²•"],
                "key_concepts": ["ì†Œë“ì„¸", "ë²•ì¸ì„¸", "ë¶€ê°€ê°€ì¹˜ì„¸", "ê³¼ì„¸", "ê³µì œ", "ì„¸ìœ¨"],
                "common_keywords": ["ì„¸ê¸ˆ", "ê³¼ì„¸", "ì†Œë“ì„¸", "ë²•ì¸ì„¸", "ë¶€ê°€ê°€ì¹˜ì„¸"]
            },
            "intellectual_property": {
                "related_laws": ["íŠ¹í—ˆë²•", "ìƒí‘œë²•", "ì €ì‘ê¶Œë²•", "ë””ìì¸ë³´í˜¸ë²•"],
                "key_concepts": ["íŠ¹í—ˆ", "ìƒí‘œ", "ì €ì‘ê¶Œ", "ë””ìì¸", "ì¹¨í•´", "ë“±ë¡"],
                "common_keywords": ["íŠ¹í—ˆ", "ìƒí‘œ", "ì €ì‘ê¶Œ", "ì§€ì ì¬ì‚°", "ì¹¨í•´"]
            }
        }

        field_info = field_specific_info.get(legal_field, {
            "related_laws": [],
            "key_concepts": [],
            "common_keywords": []
        })

        # ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ì •ë³´
        database_info = """
## ğŸ“Š ê²€ìƒ‰ ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°

### ì£¼ìš” í…Œì´ë¸” ë° í•„ë“œ

**ë²•ë ¹ ë°ì´í„° (statutes, statute_articles)**
- ë²•ë ¹ëª… (statutes.name), ì•½ì¹­ (statutes.abbrv)
- ì¡°ë¬¸ë²ˆí˜¸ (statute_articles.article_no), ì¡°í•­ ë²ˆí˜¸ (clause_no, item_no)
- ì¡°ë¬¸ ë‚´ìš© (statute_articles.text), ì œëª© (statute_articles.heading)
- ì‹œí–‰ì¼ (statutes.effective_date), ê³µí¬ì¼ (statutes.proclamation_date)

**íŒë¡€ ë°ì´í„° (cases, case_paragraphs)**
- ì‚¬ê±´ë²ˆí˜¸ (cases.case_number, í˜•ì‹: YYYYë‹¤/ë‚˜XXXXX)
- ë²•ì›ëª… (cases.court: ëŒ€ë²•ì›, ê³ ë“±ë²•ì›, ì§€ë°©ë²•ì› ë“±)
- ì‚¬ê±´ëª… (cases.casenames)
- ì„ ê³ ì¼ (cases.announce_date)
- íŒë¡€ ë³¸ë¬¸ (case_paragraphs.text)

**ì‹¬ê²°ë¡€ ë°ì´í„° (decisions, decision_paragraphs)**
- ê¸°ê´€ (decisions.org)
- ë¬¸ì„œ ID (decisions.doc_id)
- ê²°ì •ì¼ (decisions.decision_date)
- ì‹¬ê²° ë‚´ìš© (decision_paragraphs.text)

**ìœ ê¶Œí•´ì„ ë°ì´í„° (interpretations, interpretation_paragraphs)**
- ê¸°ê´€ (interpretations.org)
- ë¬¸ì„œ ID (interpretations.doc_id)
- ì œëª© (interpretations.title)
- ì‘ë‹µì¼ (interpretations.response_date)
- í•´ì„ ë‚´ìš© (interpretation_paragraphs.text)

### ê²€ìƒ‰ ë°©ì‹
- **ë²¡í„° ê²€ìƒ‰**: ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ (ë²•ë¥  ì¡°ë¬¸, íŒë¡€ ë³¸ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸)
- **í‚¤ì›Œë“œ ê²€ìƒ‰**: FTS5 ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­ (ë²•ë ¹ëª…, ì¡°ë¬¸ë²ˆí˜¸, ì‚¬ê±´ë²ˆí˜¸ ë“±)
- **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: ë²¡í„° + í‚¤ì›Œë“œ ê²°ê³¼ ë³‘í•© ë° ì¬ë­í‚¹
"""

        prompt = f"""ë‹¹ì‹ ì€ ë²•ë¥  ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë²•ë¥  ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ì— ìµœì í™”í•˜ë„ë¡ ê°œì„ í•´ì£¼ì„¸ìš”.

## ğŸ¯ ì‘ì—… ëª©í‘œ

ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:
1. **ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ**: ë²•ë¥  ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ë” ì •í™•í•˜ê²Œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í‚¤ì›Œë“œ ìµœì í™”
2. **ê²€ìƒ‰ ë²”ìœ„ í™•ì¥**: ë™ì˜ì–´, ê´€ë ¨ ìš©ì–´, ìƒìœ„ ê°œë…ì„ ì¶”ê°€í•˜ì—¬ ê²€ìƒ‰ ëˆ„ë½ ë°©ì§€
3. **ê²€ìƒ‰ íš¨ìœ¨ì„± ì¦ëŒ€**: ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ëª¨ë‘ì— íš¨ê³¼ì ì¸ ì¿¼ë¦¬ ìƒì„±
4. **ë²•ë¥  ì „ë¬¸ì„± ë°˜ì˜**: ë²•ë¥  ë¶„ì•¼ íŠ¹ì„±ê³¼ ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ì „ë¬¸ ìš©ì–´ í™œìš©

{database_info}

## ğŸ“‹ ì…ë ¥ ì •ë³´ (ìƒì„¸)

### ê¸°ë³¸ ì •ë³´
**ì›ë³¸ ì¿¼ë¦¬**: "{query}"
**ì§ˆë¬¸ ìœ í˜•**: {query_type} ({query_guide.get('description', 'ì¼ë°˜ ê²€ìƒ‰')})
**ë²•ë¥  ë¶„ì•¼**: {legal_field_text}

{keywords_info}

### ì§ˆë¬¸ ìœ í˜•ë³„ ê²€ìƒ‰ ì „ëµ
**í˜„ì¬ ì§ˆë¬¸ ìœ í˜•**: {query_guide.get('description', 'ì¼ë°˜ ê²€ìƒ‰')}

**ê²€ìƒ‰ ì´ˆì **: {query_guide.get('search_focus', 'ê´€ë ¨ ë²•ë ¹, íŒë¡€, ë²•ë¥  ìš©ì–´')}

**ê²€ìƒ‰ ì „ëµ**: {query_guide.get('search_strategy', 'í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ ê²€ìƒ‰')}

**ë°ì´í„°ë² ì´ìŠ¤ í•„ë“œ**: {query_guide.get('database_fields', 'ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤')}

**ì¶”ì²œ í‚¤ì›Œë“œ**: {', '.join(query_guide.get('keyword_suggestions', [])[:8])}

### ë²•ë¥  ë¶„ì•¼ë³„ ì •ë³´
{self.format_field_info(legal_field, field_info)}

## ğŸ” ì¿¼ë¦¬ ìµœì í™” ì§€ì¹¨

### 1. ì˜ë¯¸ ë³´ì¡´
- ì›ë³¸ ì¿¼ë¦¬ì˜ í•µì‹¬ ì˜ë„ì™€ ëª©ì ì„ ë°˜ë“œì‹œ ìœ ì§€í•˜ì„¸ìš”
- ì‚¬ìš©ìê°€ ì°¾ê³ ì í•˜ëŠ” ë²•ë¥  ì •ë³´ì˜ ë³¸ì§ˆì„ íŒŒì•…í•˜ì„¸ìš”

### 2. ë²•ë¥  ìš©ì–´ í™•ì¥
- **ë™ì˜ì–´ ì¶”ê°€**: ë²•ë¥  ìš©ì–´ì˜ ë‹¤ì–‘í•œ í‘œí˜„ ì¶”ê°€ (ì˜ˆ: "ê³„ì•½" â†’ "ê³„ì•½ì„œ", "ê³„ì•½ê´€ê³„")
- **ìƒìœ„/í•˜ìœ„ ê°œë…**: ì¼ë°˜ ê°œë…ê³¼ êµ¬ì²´ì  ê°œë… ëª¨ë‘ í¬í•¨ (ì˜ˆ: "ì†í•´ë°°ìƒ" â†’ "ë¶ˆë²•í–‰ìœ„ ì†í•´ë°°ìƒ", "ê³„ì•½ ìœ„ë°˜ ì†í•´ë°°ìƒ")
- **ë²•ë¥  ìš©ì–´ ì •ê·œí™”**: ë²•ë¥ ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê³µì‹ ìš©ì–´ë¡œ ë³€í™˜ (ì˜ˆ: "ì´í˜¼" â†’ "í˜¼ì¸í•´ì†Œ")

### 3. ê²€ìƒ‰ ìµœì í™”
- **ë²¡í„° ê²€ìƒ‰ ìµœì í™”**: ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ê¸° ìœ„í•œ í•µì‹¬ ê°œë… í‚¤ì›Œë“œ í¬í•¨
- **í‚¤ì›Œë“œ ê²€ìƒ‰ ìµœì í™”**: ë²•ë ¹ëª…, ì¡°ë¬¸ë²ˆí˜¸, ì‚¬ê±´ë²ˆí˜¸ ë“± ì •í™•í•œ ë§¤ì¹­ ê°€ëŠ¥í•œ ìš©ì–´ í¬í•¨
- **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: ë‘ ê²€ìƒ‰ ë°©ì‹ ëª¨ë‘ì— íš¨ê³¼ì ì¸ ê· í˜• ì¡íŒ ì¿¼ë¦¬ ìƒì„±

### 4. ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹í™”
- **íŒë¡€ ê²€ìƒ‰**: ì‚¬ê±´ë²ˆí˜¸ íŒ¨í„´, ë²•ì›ëª…, íŒì‹œì‚¬í•­ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
- **ë²•ë ¹ ì¡°íšŒ**: ë²•ë ¹ëª…, ì¡°ë¬¸ë²ˆí˜¸, ì¡°í•­ì˜ í•µì‹¬ ë²•ë¦¬ ìš©ì–´ í¬í•¨
- **ë²•ë¥  ì¡°ì–¸**: ë¬¸ì œ ìƒí™©ì˜ í•µì‹¬ ë²•ë¥  ê°œë… + ê´€ë ¨ ì¡°ë¬¸ + ìœ ì‚¬ íŒë¡€ íŒ¨í„´ ì¡°í•©

### 5. ê°„ê²°ì„± ìœ ì§€
- í•µì‹¬ í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ ìœ ì§€
- ê²€ìƒ‰ì— ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ë‚˜ ì¤‘ë³µ í‘œí˜„ ì œê±°
- ìµœëŒ€ 50ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìœ ì§€

## ğŸ“¤ ì¶œë ¥ í˜•ì‹

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš” (ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥):

```json
{{
    "optimized_query": "ê°œì„ ëœ ê²€ìƒ‰ ì¿¼ë¦¬ (ì›ë³¸ ì˜ë„ ìœ ì§€, ë²•ë¥  ê²€ìƒ‰ ìµœì í™”)",
    "expanded_keywords": ["ê´€ë ¨ í‚¤ì›Œë“œ1", "ê´€ë ¨ í‚¤ì›Œë“œ2", "ê´€ë ¨ í‚¤ì›Œë“œ3", "ê´€ë ¨ í‚¤ì›Œë“œ4", "ê´€ë ¨ í‚¤ì›Œë“œ5"],
    "keyword_variants": ["ê²€ìƒ‰ ë³€í˜• ì¿¼ë¦¬1 (ë²•ë ¹ ê²€ìƒ‰ìš©)", "ê²€ìƒ‰ ë³€í˜• ì¿¼ë¦¬2 (íŒë¡€ ê²€ìƒ‰ìš©)"],
    "legal_terms": ["ë²•ë¥  ì „ë¬¸ ìš©ì–´1", "ë²•ë¥  ì „ë¬¸ ìš©ì–´2"],
    "reasoning": "ê°œì„  ì‚¬ìœ : ì›ë³¸ ì¿¼ë¦¬ë¥¼ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ê°œì„ í–ˆê³ , ì™œ ê·¸ë ‡ê²Œ ê°œì„ í–ˆëŠ”ì§€ ê°„ë‹¨íˆ ì„¤ëª…"
}}
```

### ì¶œë ¥ í•„ë“œ ì„¤ëª…
- **optimized_query**: ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ëª¨ë‘ì— ì‚¬ìš©ë  ë©”ì¸ ì¿¼ë¦¬ (ìµœëŒ€ 50ì ê¶Œì¥)
- **expanded_keywords**: ê²€ìƒ‰ ë²”ìœ„ í™•ì¥ì„ ìœ„í•œ ê´€ë ¨ í‚¤ì›Œë“œ ëª©ë¡ (5-10ê°œ ê¶Œì¥)
- **keyword_variants**: ë‹¤ì–‘í•œ ê²€ìƒ‰ ì‹œë„ë¥¼ ìœ„í•œ ì¿¼ë¦¬ ë³€í˜• (2-3ê°œ ê¶Œì¥, ë²•ë ¹/íŒë¡€ ê²€ìƒ‰ êµ¬ë¶„)
- **legal_terms**: ë²•ë¥  ì „ë¬¸ ìš©ì–´ ëª©ë¡ (ë²•ë¥  ìš©ì–´ ì‚¬ì „ì— ë“±ë¡ë  ìš©ì–´)
- **reasoning**: ê°œì„  ì‚¬ìœ  (50ì ì´ë‚´, ì„ íƒ ì‚¬í•­)

## âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì¶œë ¥ ì „ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
- [ ] optimized_queryê°€ ì›ë³¸ ì¿¼ë¦¬ì˜ í•µì‹¬ ì˜ë„ë¥¼ ìœ ì§€í•˜ëŠ”ê°€?
- [ ] expanded_keywordsì— ë²•ë¥  ë¶„ì•¼ë³„ ê´€ë ¨ ìš©ì–´ê°€ í¬í•¨ë˜ì—ˆëŠ”ê°€?
- [ ] keyword_variantsì— ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” ê²€ìƒ‰ ë³€í˜•ì´ í¬í•¨ë˜ì—ˆëŠ”ê°€?
- [ ] ëª¨ë“  í‚¤ì›Œë“œê°€ í•œê¸€ë¡œ ì‘ì„±ë˜ì—ˆëŠ”ê°€?
- [ ] JSON í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€? (ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥)

## ğŸ“ ì¢‹ì€ ì˜ˆì‹œ

**ì…ë ¥**: "ê³„ì•½ í•´ì§€ ë°©ë²•"
- optimized_query: "ê³„ì•½ í•´ì§€ ìš”ê±´ ë° ì ˆì°¨"
- expanded_keywords: ["ê³„ì•½ í•´ì œ", "í•´ì§€ í†µê³ ", "ì±„ë¬´ë¶ˆì´í–‰", "ì´í–‰ ìµœê³ ", "í•´ì§€ ì˜ì‚¬í‘œì‹œ"]
- keyword_variants: ["ê³„ì•½ í•´ì§€ ìš”ê±´ ë²•ë ¹", "ê³„ì•½ í•´ì§€ íŒë¡€"]

**ì…ë ¥**: "ëŒ€ë²•ì› 2020ë‹¤12345 íŒë¡€"
- optimized_query: "ëŒ€ë²•ì› 2020ë‹¤12345 íŒê²° ìš”ì§€"
- expanded_keywords: ["2020ë‹¤12345", "ëŒ€ë²•ì› íŒë¡€", "íŒê²° ìš”ì§€", "ì‚¬ê±´ë²ˆí˜¸"]
- keyword_variants: ["2020ë‹¤12345", "ëŒ€ë²•ì› 2020ë‹¤12345"]

ì´ì œ ë‹¤ìŒ ì¿¼ë¦¬ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”:
"""

        return prompt

    def format_field_info(self, legal_field: str, field_info: Dict[str, Any]) -> str:
        """ë²•ë¥  ë¶„ì•¼ë³„ ì •ë³´ í¬ë§·íŒ…"""
        if legal_field and field_info.get('related_laws'):
            related_laws = ', '.join(field_info.get('related_laws', [])) if field_info.get('related_laws') else 'ì—†ìŒ'
            key_concepts = ', '.join(field_info.get('key_concepts', [])) if field_info.get('key_concepts') else 'ì—†ìŒ'
            common_keywords = ', '.join(field_info.get('common_keywords', [])) if field_info.get('common_keywords') else 'ì—†ìŒ'

            return f"""
**ê´€ë ¨ ë²•ë ¹**: {related_laws}
**í•µì‹¬ ê°œë…**: {key_concepts}
**ì¼ë°˜ í‚¤ì›Œë“œ**: {common_keywords}
"""
        else:
            return "**ë²•ë¥  ë¶„ì•¼**: ë¯¸ì§€ì • (ì „ì²´ ë²•ë¥  ë¶„ì•¼ ê²€ìƒ‰)"

    def parse_llm_query_enhancement(self, llm_output: str) -> Optional[Dict[str, Any]]:
        """LLM ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            import re

            # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ë‚´ë¶€ ë˜ëŠ” ì§ì ‘ JSON)
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', llm_output, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # ì½”ë“œ ë¸”ë¡ ì—†ì´ JSONë§Œ ìˆëŠ” ê²½ìš°
                json_match = re.search(r'\{.*\}', llm_output, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    self.logger.warning("No JSON found in LLM response")
                    return None

            # JSON íŒŒì‹±
            result = json.loads(json_str)

            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if not result.get("optimized_query"):
                self.logger.warning("LLM response missing 'optimized_query' field")
                return None

            # ê¸°ë³¸ê°’ ì„¤ì •
            enhanced = {
                "optimized_query": result.get("optimized_query", ""),
                "expanded_keywords": result.get("expanded_keywords", []),
                "keyword_variants": result.get("keyword_variants", []),
                "legal_terms": result.get("legal_terms", []),
                "reasoning": result.get("reasoning", "")
            }

            # ìœ íš¨ì„± ê²€ì‚¬
            if not enhanced["optimized_query"] or len(enhanced["optimized_query"]) > 500:
                self.logger.warning("Invalid optimized_query from LLM")
                return None

            return enhanced

        except json.JSONDecodeError as e:
            self.logger.warning(f"Failed to parse JSON from LLM response: {e}")
            return None
        except Exception as e:
            self.logger.warning(f"Error parsing LLM enhancement response: {e}")
            return None

    def normalize_legal_terms(self, query: str, keywords: List[str]) -> List[str]:
        """ë²•ë¥  ìš©ì–´ ì •ê·œí™”"""
        normalized = []
        all_terms = [query] + (keywords if keywords else [])

        for term in all_terms:
            # ë²•ë¥  ìš©ì–´ ì •ê·œí™”
            if isinstance(term, str) and len(term) >= 2:
                # term_integratorë¥¼ ì‚¬ìš©í•œ ì •ê·œí™”
                try:
                    if hasattr(self.term_integrator, 'normalize_term'):
                        normalized_term = self.term_integrator.normalize_term(term)
                        if normalized_term:
                            normalized.append(normalized_term)
                        else:
                            normalized.append(term)
                    else:
                        normalized.append(term)
                except Exception:
                    normalized.append(term)
            elif isinstance(term, str):
                normalized.append(term)

        return list(set(normalized)) if normalized else [query]

    def expand_legal_terms(
        self,
        terms: List[str],
        legal_field: str
    ) -> List[str]:
        """ë²•ë¥  ìš©ì–´ í™•ì¥ (ë™ì˜ì–´, ê´€ë ¨ ìš©ì–´)"""
        expanded = list(terms)

        # ì§€ì›ë˜ëŠ” ë²•ë¥  ë¶„ì•¼ë³„ ê´€ë ¨ ìš©ì–´ ë§¤í•‘ (ë¯¼ì‚¬ë²•, ì§€ì‹ì¬ì‚°ê¶Œë²•, í–‰ì •ë²•, í˜•ì‚¬ë²•ë§Œ)
        field_expansions = {
            "civil": ["ë¯¼ì‚¬", "ê³„ì•½", "ì†í•´ë°°ìƒ", "ì±„ê¶Œ", "ì±„ë¬´"],
            "criminal": ["í˜•ì‚¬", "ë²”ì£„", "ì²˜ë²Œ", "í˜•ëŸ‰"],
            "intellectual_property": ["íŠ¹í—ˆ", "ìƒí‘œ", "ì €ì‘ê¶Œ", "ì§€ì ì¬ì‚°"],
            "administrative": ["í–‰ì •", "í–‰ì •ì²˜ë¶„", "í–‰ì •ì†Œì†¡", "í–‰ì •ì‹¬íŒ"]
        }

        # ê´€ë ¨ ìš©ì–´ ì¶”ê°€
        if legal_field:
            related_terms = field_expansions.get(legal_field, [])
            expanded.extend(related_terms)

        return list(set(expanded))[:15]  # ìµœëŒ€ 15ê°œë¡œ ì œí•œ

    def clean_query_for_fallback(self, query: str) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¿¼ë¦¬ ì •ì œ (í´ë°± ê°•í™”)"""
        if not query or not isinstance(query, str):
            return ""

        # ë¶ˆìš©ì–´ ì œê±° ë° ì •ì œ
        stopwords = ["ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì˜", "ë¡œ", "ìœ¼ë¡œ", "ì™€", "ê³¼", "ë„", "ë§Œ"]
        words = query.split()
        cleaned_words = [w for w in words if w not in stopwords and len(w) >= 2]

        # ì •ì œëœ ì¿¼ë¦¬ ë°˜í™˜ (ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ë°˜í™˜)
        cleaned = " ".join(cleaned_words) if cleaned_words else query
        return cleaned.strip()

    def build_semantic_query(self, query: str, expanded_terms: List[str]) -> str:
        """ì˜ë¯¸ì  ê²€ìƒ‰ìš© ì¿¼ë¦¬ ìƒì„±"""
        return QueryBuilder.build_semantic_query(query, expanded_terms)

    def build_keyword_queries(
        self,
        query: str,
        expanded_terms: List[str],
        query_type: str
    ) -> List[str]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        return QueryBuilder.build_keyword_queries(query, expanded_terms, query_type)

    def determine_search_parameters(
        self,
        query_type: str,
        query_complexity: int,
        keyword_count: int,
        is_retry: bool
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ë™ì  ê²°ì •"""
        base_k = WorkflowConstants.SEMANTIC_SEARCH_K
        base_limit = WorkflowConstants.CATEGORY_SEARCH_LIMIT

        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì¡°ì •
        type_multiplier = {
            "precedent_search": 1.5,  # íŒë¡€ ê²€ìƒ‰: ë” ë§ì€ ê²°ê³¼
            "law_inquiry": 1.3,       # ë²•ë ¹ ì¡°íšŒ: ë” ë§ì€ ê²°ê³¼
            "legal_advice": 1.2,
            "general_question": 1.0
        }
        multiplier = type_multiplier.get(query_type, 1.0)

        # ë³µì¡ë„ì— ë”°ë¥¸ ì¡°ì •
        if query_complexity > 100:
            multiplier += 0.3
        if keyword_count > 10:
            multiplier += 0.2

        # ì¬ì‹œë„ ì‹œ ë” ë§ì€ ê²°ê³¼
        if is_retry:
            multiplier += 0.5

        semantic_k = int(base_k * multiplier)
        keyword_limit = int(base_limit * multiplier)

        # ìœ ì‚¬ë„ ì„ê³„ê°’ ë™ì  ì¡°ì •
        min_relevance = self.config.similarity_threshold
        if query_type == "precedent_search":
            min_relevance = max(0.6, min_relevance - 0.1)  # íŒë¡€ ê²€ìƒ‰: ì™„í™”
        elif query_type == "law_inquiry":
            min_relevance = max(0.65, min_relevance - 0.05)  # ë²•ë ¹ ì¡°íšŒ: ì•½ê°„ ì™„í™”

        return {
            "semantic_k": min(25, semantic_k),  # ìµœëŒ€ 25ê°œ
            "keyword_limit": min(7, keyword_limit),  # ìµœëŒ€ 7ê°œ
            "min_relevance": min_relevance,
            "max_results": int(base_k * multiplier * 1.2),  # ìµœì¢… ê²°ê³¼ ìˆ˜
            "rerank": {
                "top_k": min(20, int(base_k * multiplier)),
                "diversity_weight": 0.3,
                "relevance_weight": 0.7
            }
        }

    def extract_query_relevant_sentences(
        self,
        doc_content: str,
        query: str,
        extracted_keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë¬¸ì¥ ì¶”ì¶œ"""
        return DocumentExtractor.extract_query_relevant_sentences(doc_content, query, extracted_keywords)
