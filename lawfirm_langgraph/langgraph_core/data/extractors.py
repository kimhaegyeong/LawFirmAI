# -*- coding: utf-8 -*-
"""
ì¶”ì¶œ ? í‹¸ë¦¬í‹° ëª¨ë“ˆ
ë¦¬íŒ©? ë§: legal_workflow_enhanced.py?ì„œ ì¶”ì¶œ ë©”ì„œ??ë¶„ë¦¬
"""

import logging
import re
from typing import Any, Dict, List

logger = logging.getLogger(__name__)


class DocumentExtractor:
    """ë¬¸ì„œ ê´€??ì¶”ì¶œ ? í‹¸ë¦¬í‹°"""

    @staticmethod
    def extract_terms_from_documents(docs: List[Dict]) -> List[str]:
        """ë¬¸ì„œ?ì„œ ë²•ë¥  ?©ì–´ ì¶”ì¶œ"""
        all_terms = []
        try:
            for doc in docs:
                content = doc.get("content", "")
                if not content:
                    continue

                korean_terms = re.findall(r'[ê°€-??-9A-Za-z]+', content)
                legal_terms = [
                    term for term in korean_terms
                    if len(term) >= 2 and any('\uac00' <= c <= '\ud7af' for c in term)
                ]
                all_terms.extend(legal_terms)
        except Exception as e:
            logger.warning(f"Failed to extract terms from documents: {e}")

        return all_terms

    @staticmethod
    def extract_key_insights(
        documents: List[Dict],
        query: str
    ) -> List[str]:
        """?µì‹¬ ?•ë³´ ì¶”ì¶œ - ì§ˆë¬¸ê³?ì§ì ‘ ê´€?¨ëœ ?µì‹¬ ë¬¸ì¥ ì¶”ì¶œ"""
        insights = []

        try:
            query_words = set(query.lower().split())

            for doc in documents[:10]:
                doc_content = doc.get("content", "")
                if not doc_content:
                    continue

                sentences = re.split(r'[??ï¼??ï¼?\s*', doc_content)

                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) < 10:
                        continue

                    sentence_words = set(sentence.lower().split())
                    if query_words and sentence_words:
                        overlap = len(query_words.intersection(sentence_words))
                        relevance = overlap / max(1, len(query_words))

                        if relevance >= 0.3:
                            insights.append(sentence)

                            if len(insights) >= 20:
                                break

                if len(insights) >= 20:
                    break

            # ì¤‘ë³µ ?œê±°
            unique_insights = []
            seen_hashes = set()

            for insight in insights:
                insight_hash = hash(insight[:50])
                if insight_hash not in seen_hashes:
                    seen_hashes.add(insight_hash)
                    unique_insights.append(insight)

            return unique_insights[:15]

        except Exception as e:
            logger.warning(f"Key insights extraction failed: {e}")
            return []

    @staticmethod
    def extract_legal_citations(
        documents: List[Dict]
    ) -> List[Dict[str, str]]:
        """ë²•ë¥  ?¸ìš© ?•ë³´ ì¶”ì¶œ"""
        citations = []

        try:
            seen_citations = set()

            citation_pattern = r'([ê°€-??+ë²?\s*??\s*(\d+)\s*ì¡?
            precedent_pattern = r'(?€ë²•ì›|ë²•ì›)\s*(\d{4})[.\s]*(\d{1,2})[.\s]*(\d{1,2})?[.\s]*? ê³ \s*(\d{4}[?¤ë‚˜ë§?\d+)'
            simple_precedent_pattern = r'(?€ë²•ì›|ë²•ì›)\s*(\d{4}[?¤ë‚˜ë§?\d+)'
            law_name_pattern = r'([ê°€-??+ë²?'

            for doc in documents[:10]:
                doc_content = doc.get("content", "")
                doc_source = doc.get("source", "unknown")

                if not doc_content:
                    continue

                # ë²•ë¥  ì¡°í•­ ?¸ìš© ì¶”ì¶œ
                law_matches = re.finditer(citation_pattern, doc_content)
                for match in law_matches:
                    law_name = match.group(1)
                    article_num = match.group(2)
                    citation_key = f"{law_name} ??article_num}ì¡?

                    if citation_key not in seen_citations:
                        seen_citations.add(citation_key)
                        citations.append({
                            "type": "law_article",
                            "text": citation_key,
                            "law_name": law_name,
                            "article_number": article_num,
                            "source": doc_source
                        })

                # ?ë? ?¸ìš© ì¶”ì¶œ
                precedent_matches = re.finditer(precedent_pattern, doc_content)
                for match in precedent_matches:
                    court = match.group(1)
                    case_number = match.group(5) if len(match.groups()) > 4 else None
                    if not case_number:
                        simple_match = re.search(simple_precedent_pattern, match.group(0))
                        if simple_match:
                            case_number = simple_match.group(2) if len(simple_match.groups()) > 1 else None

                    if case_number:
                        citation_key = f"{court} {case_number}"
                        if citation_key not in seen_citations:
                            seen_citations.add(citation_key)
                            citations.append({
                                "type": "precedent",
                                "text": citation_key,
                                "court": court,
                                "case_number": case_number,
                                "source": doc_source
                            })

                # ë²•ë ¹ëª?ì¶”ì¶œ
                law_names = re.findall(law_name_pattern, doc_content)
                for law_name in law_names:
                    if f"{law_name} ?? not in doc_content[:500]:
                        citation_key = law_name
                        if citation_key not in seen_citations and len(law_name) >= 2:
                            seen_citations.add(citation_key)
                            citations.append({
                                "type": "law_name",
                                "text": citation_key,
                                "law_name": law_name,
                                "source": doc_source
                            })

            return citations[:20]

        except Exception as e:
            logger.warning(f"Legal citations extraction failed: {e}")
            return []

    @staticmethod
    def extract_legal_references_from_docs(documents: List[Dict[str, Any]]) -> List[str]:
        """ë¬¸ì„œ?ì„œ ë²•ë¥  ì°¸ì¡° ?•ë³´ ì¶”ì¶œ"""
        legal_references = []

        try:
            citation_pattern = r'[ê°€-??+ë²?s*??\s*\d+\s*ì¡?
            precedent_pattern = r'(?€ë²•ì›|ë²•ì›)\s*(\d{4}[?¤ë‚˜ë§?\d+)'

            for doc in documents[:10]:  # ?ìœ„ 10ê°œë§Œ
                content = doc.get("content", "")
                if not content:
                    continue

                # ë²•ë¥  ì¡°í•­ ?¸ìš© ì¶”ì¶œ
                citations = re.findall(citation_pattern, content)
                legal_references.extend(citations)

                # ?ë? ?¸ìš© ì¶”ì¶œ
                precedents = re.findall(precedent_pattern, content)
                for precedent in precedents:
                    legal_references.append(" ".join(precedent))

            # ì¤‘ë³µ ?œê±°
            legal_references = list(set(legal_references))

        except Exception as e:
            logger.warning(f"Failed to extract legal references: {e}")

        return legal_references[:20]  # ìµœë? 20ê°œë§Œ

    @staticmethod
    def extract_contract_clauses(text: str) -> List[Dict[str, Any]]:
        """ê³„ì•½??ì£¼ìš” ì¡°í•­ ì¶”ì¶œ"""
        clauses = []

        try:
            # ì¡°í•­ ?¨í„´ ë§¤ì¹­
            clause_patterns = {
                "payment": r"(?€ê¸?ê¸ˆì•¡|ì§€ê¸?ê²°ì œ).*?ì¡?,
                "period": r"(ê¸°ê°„|ê¸°í•œ|ë§Œë£Œ).*?ì¡?,
                "termination": r"(?´ì?|?´ì œ|ì¢…ë£Œ).*?ì¡?,
                "liability": r"(ì±…ì„|?í•´ë°°ìƒ|?„ì•½).*?ì¡?,
                "confidentiality": r"(ë¹„ë?|ê¸°ë?|ë³´ì•ˆ).*?ì¡?
            }

            for clause_type, pattern in clause_patterns.items():
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    # ì¡°í•­ ?„ì²´ ì¶”ì¶œ (?œNì¡??•ì‹)
                    article_match = re.search(r'??d+ì¡?^??*', text[match.start():match.start()+500])
                    if article_match:
                        clauses.append({
                            "type": clause_type,
                            "text": article_match.group(0).strip()[:200],
                            "position": match.start()
                        })

            return clauses[:10]  # ?ìœ„ 10ê°œë§Œ

        except Exception as e:
            logger.warning(f"Contract clauses extraction failed: {e}")
            return []

    @staticmethod
    def extract_complaint_elements(text: str) -> List[Dict[str, Any]]:
        """ê³ ì†Œ???”ê±´ ì¶”ì¶œ"""
        elements = []

        try:
            # ê¸°ë³¸ ?”ì†Œ ?¨í„´
            patterns = {
                "parties": r"(?¼ê³ ?Œì¸|?¼í•´??ê°€?´ì)",
                "facts": r"(?¬ì‹¤ê´€ê³?ê²½ìœ„|?´ìš©)",
                "claims": r"(ì²?µ¬|?”êµ¬|ì£¼ì¥)",
            }

            for elem_type, pattern in patterns.items():
                if re.search(pattern, text, re.IGNORECASE):
                    elements.append({
                        "type": elem_type,
                        "found": True
                    })

            return elements

        except Exception as e:
            logger.warning(f"Complaint elements extraction failed: {e}")
            return []

    @staticmethod
    def extract_query_relevant_sentences(
        doc_content: str,
        query: str,
        extracted_keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ?´ìš©?ì„œ ì§ˆë¬¸ê³?ì§ì ‘ ê´€?¨ëœ ë¬¸ì¥ ì¶”ì¶œ"""
        relevant_sentences = []

        if not doc_content:
            return relevant_sentences

        try:
            # ë¬¸ì¥ ë¶„ë¦¬
            sentences = re.split(r'[??ï¼??ï¼?\s*', doc_content)

            query_words = set(query.lower().split())

            for sentence in sentences:
                if not sentence.strip() or len(sentence.strip()) < 10:
                    continue

                sentence_lower = sentence.lower()
                sentence_words = set(sentence_lower.split())

                # ì§ˆë¬¸ ?¤ì›Œ??ë§¤ì¹­ ?ìˆ˜
                query_match = len(query_words.intersection(sentence_words)) / max(1, len(query_words)) if query_words else 0

                # ì¶”ì¶œ???¤ì›Œ??ë§¤ì¹­ ?ìˆ˜
                keyword_matches = sum(1 for kw in extracted_keywords
                                    if isinstance(kw, str) and kw.lower() in sentence_lower)
                keyword_match = keyword_matches / max(1, len(extracted_keywords)) if extracted_keywords else 0

                # ì¢…í•© ê´€?¨ì„± ?ìˆ˜
                relevance_score = (query_match * 0.6 + keyword_match * 0.4)

                if relevance_score > 0.2:  # ?„ê³„ê°?
                    relevant_sentences.append({
                        "sentence": sentence.strip(),
                        "relevance_score": round(relevance_score, 3),
                        "query_match": round(query_match, 3),
                        "keyword_match": round(keyword_match, 3)
                    })

            # ê´€?¨ì„± ?ìˆ˜ë¡??•ë ¬
            relevant_sentences.sort(key=lambda x: x["relevance_score"], reverse=True)

            return relevant_sentences[:5]  # ?ìœ„ 5ê°œë§Œ

        except Exception as e:
            logger.warning(f"Query relevant sentences extraction failed: {e}")
            return []


class ResponseExtractor:
    """?‘ë‹µ ê´€??ì¶”ì¶œ ? í‹¸ë¦¬í‹°"""

    @staticmethod
    def extract_response_content(response) -> str:
        """?‘ë‹µ?ì„œ ?´ìš© ì¶”ì¶œ"""
        try:
            if hasattr(response, 'content'):
                content = response.content
                # contentê°€ ë¬¸ì?´ì¸ì§€ ?•ì¸
                if isinstance(content, dict):
                    content = content.get("content", content.get("answer", str(content)))
                return str(content) if not isinstance(content, str) else content

            # response ?ì²´ë¥?ì²˜ë¦¬
            if isinstance(response, dict):
                return response.get("content", response.get("answer", str(response)))

            return str(response)

        except Exception as e:
            logger.warning(f"Failed to extract response content: {e}")
            return str(response) if response else ""


class QueryExtractor:
    """ì¿¼ë¦¬ ê´€??ì¶”ì¶œ ? í‹¸ë¦¬í‹°"""

    @staticmethod
    def extract_legal_field(query_type: str, query: str) -> str:
        """ë²•ë¥  ë¶„ì•¼ ì¶”ì¶œ"""
        # ?¤ì›Œ??ë§¤í•‘
        field_keywords = {
            "civil": ["ë¯¼ì‚¬", "ê³„ì•½", "?í•´ë°°ìƒ", "?¬ì‚°", "ê³„ì•½??],
            "criminal": ["?•ì‚¬", "ë²”ì£„", "ì²˜ë²Œ", "?•ëŸ‰", "ë²”ì£„??],
            "intellectual_property": ["?¹í—ˆ", "?í‘œ", "?€?‘ê¶Œ", "ì§€?ì¬??],
            "administrative": ["?‰ì •", "?‰ì •ì²˜ë¶„", "?‰ì •?Œì†¡", "?‰ì •?¬íŒ"]
        }

        query_lower = query.lower()
        for field, keywords in field_keywords.items():
            if any(k in query_lower for k in keywords):
                return field

        # ì§ˆë¬¸ ? í˜• ê¸°ë°˜ ?´ë°±
        type_to_field = {
            "precedent_search": "civil",
            "law_inquiry": "civil",
            "procedure_guide": "civil",
            "term_explanation": "civil",
            "legal_advice": "civil"
        }
        return type_to_field.get(query_type, "general")
