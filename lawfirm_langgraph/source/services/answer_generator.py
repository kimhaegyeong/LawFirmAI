# -*- coding: utf-8 -*-
"""
ë‹µë³€ ìƒì„± ëª¨ë“ˆ
LLMì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„± ë° í’ˆì§ˆ ê°œì„  ë¡œì§ì„ ë…ë¦½ ëª¨ë“ˆë¡œ ë¶„ë¦¬
"""

import logging
import time
from typing import Any, Dict, List, Optional

from source.utils.prompt_chain_executor import PromptChainExecutor
from source.data.quality_validators import AnswerValidator
from source.data.response_parsers import AnswerParser
from source.utils.state_definitions import LegalWorkflowState
from source.utils.workflow_constants import WorkflowConstants
from source.utils.workflow_utils import WorkflowUtils


class AnswerGenerator:
    """
    ë‹µë³€ ìƒì„± ë° ê°œì„  í´ë˜ìŠ¤

    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ê³ , Prompt Chainingì„ í†µí•´ í’ˆì§ˆì„ ê°œì„ í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        llm: Any,
        logger: Optional[logging.Logger] = None
    ):
        """
        AnswerGenerator ì´ˆê¸°í™”

        Args:
            llm: LangChain LLM ì¸ìŠ¤í„´ìŠ¤
            logger: ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        """
        self.llm = llm
        self.logger = logger or logging.getLogger(__name__)

    def generate_answer_with_chain(
        self,
        optimized_prompt: str,
        query: str,
        context_dict: Dict[str, Any],
        quality_feedback: Optional[Dict[str, Any]] = None,
        is_retry: bool = False
    ) -> str:
        """
        Prompt Chainingì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„± ë° ê°œì„ 

        Step 1: ì´ˆê¸° ë‹µë³€ ìƒì„±
        Step 2: ë‹µë³€ ê²€ì¦ ë° ë¬¸ì œì  ì¶”ì¶œ
        Step 3: ê°œì„  ì§€ì‹œ ìƒì„± (ë¬¸ì œì ì´ ìˆëŠ” ê²½ìš°)
        Step 4: ê°œì„ ëœ ë‹µë³€ ìƒì„± (Step 3ê°€ ìˆëŠ” ê²½ìš°)
        Step 5: ìµœì¢… ê²€ì¦

        Args:
            optimized_prompt: ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸
            query: ì›ë³¸ ì§ˆë¬¸
            context_dict: ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            quality_feedback: í’ˆì§ˆ í”¼ë“œë°± (ì¬ì‹œë„ ì‹œ ì‚¬ìš©)
            is_retry: ì¬ì‹œë„ ì—¬ë¶€

        Returns:
            ìƒì„±ëœ ë‹µë³€ ë¬¸ìì—´
        """
        try:
            # PromptChainExecutor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            chain_executor = PromptChainExecutor(self.llm, self.logger)

            # ì²´ì¸ ìŠ¤í… ì •ì˜
            chain_steps = []

            # Step 1: ì´ˆê¸° ë‹µë³€ ìƒì„±
            def build_initial_answer_prompt(prev_output, initial_input):
                # ì´ˆê¸° ì…ë ¥ì—ì„œ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
                if isinstance(initial_input, dict):
                    return initial_input.get("prompt", optimized_prompt)
                return optimized_prompt

            chain_steps.append({
                "name": "initial_answer_generation",
                "prompt_builder": build_initial_answer_prompt,
                "input_extractor": lambda prev: prev,  # ì´ˆê¸° ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
                "output_parser": lambda response, prev: WorkflowUtils.normalize_answer(response),
                "validator": lambda output: output and len(output.strip()) > 10,
                "required": True
            })

            # Step 2: ë‹µë³€ ê²€ì¦ ë° ë¬¸ì œì  ì¶”ì¶œ
            def build_validation_prompt(prev_output, initial_input):
                # prev_outputì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (Step 1ì˜ ì¶œë ¥ì´ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ëœ ê²½ìš°) ì²˜ë¦¬
                if isinstance(prev_output, dict):
                    answer = prev_output.get("answer") or prev_output.get("content") or str(prev_output)
                else:
                    answer = prev_output if isinstance(prev_output, str) else str(prev_output)
                validation_criteria = """
ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€ì„ ê²€ì¦í•˜ì„¸ìš”:

1. **ê¸¸ì´**: ìµœì†Œ 50ì ì´ìƒ
2. **ë‚´ìš© ì™„ì„±ë„**: ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ í¬í•¨
3. **ë²•ì  ê·¼ê±°**: ê´€ë ¨ ë²•ë ¹, ì¡°í•­, íŒë¡€ ì¸ìš© ì—¬ë¶€
4. **êµ¬ì¡°**: ëª…í™•í•œ ì„¹ì…˜ê³¼ ë…¼ë¦¬ì  íë¦„
5. **ì¼ê´€ì„±**: ë‹µë³€ ì „ì²´ì˜ ë…¼ë¦¬ì  ì¼ê´€ì„±

ë‹µë³€:
{answer}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê²€ì¦ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”:
{{
    "is_valid": true/false,
    "quality_score": 0.0-1.0,
    "issues": [
        "ë¬¸ì œì  1",
        "ë¬¸ì œì  2"
    ],
    "strengths": [
        "ê°•ì  1",
        "ê°•ì  2"
    ],
    "recommendations": [
        "ê°œì„  ê¶Œê³  1",
        "ê°œì„  ê¶Œê³  2"
    ]
}}
""".format(answer=answer[:2000])  # ë‹µë³€ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ëƒ„
                return validation_criteria

            chain_steps.append({
                "name": "answer_validation",
                "prompt_builder": build_validation_prompt,
                "input_extractor": lambda prev: prev,  # ì´ì „ ë‹¨ê³„ì˜ ë‹µë³€ì„ ê²€ì¦
                "output_parser": lambda response, prev: AnswerParser.parse_validation_response(response),
                "validator": lambda output: output and isinstance(output, dict),
                "required": True,
                "skip_if": lambda prev: not prev or len(str(prev).strip()) < 10  # ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            })

            # Step 3: ê°œì„  ì§€ì‹œ ìƒì„± (ë¬¸ì œì ì´ ìˆëŠ” ê²½ìš°)
            def build_improvement_instructions_prompt(prev_output, initial_input):
                # prev_outputì€ validation_result, initial_inputì—ì„œ ì›ë³¸ ë‹µë³€ ì°¾ê¸°
                validation_result = prev_output
                if not isinstance(validation_result, dict):
                    validation_result = {}

                # ì´ˆê¸° ë‹µë³€ ì°¾ê¸° (ì²´ì¸ íˆìŠ¤í† ë¦¬ì—ì„œ ë˜ëŠ” initial_inputì—ì„œ)
                original_answer = ""
                if isinstance(initial_input, dict):
                    # ì²´ì¸ íˆìŠ¤í† ë¦¬ì—ì„œ ì´ˆê¸° ë‹µë³€ ì°¾ê¸°
                    chain_history = initial_input.get("_chain_history", [])
                    for step in chain_history:
                        if step.get("step_name") == "initial_answer_generation":
                            original_answer = step.get("output", "")
                            break

                if not original_answer:
                    original_answer = str(prev_output) if not isinstance(prev_output, dict) else ""
                if not isinstance(validation_result, dict) or validation_result.get("is_valid", True):
                    return None  # ë¬¸ì œê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°

                issues = validation_result.get("issues", [])
                recommendations = validation_result.get("recommendations", [])
                quality_score = validation_result.get("quality_score", 1.0)

                if quality_score >= 0.75:  # í’ˆì§ˆì´ ì¶©ë¶„íˆ ë†’ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                    return None

                improvement_prompt = f"""
ë‹¤ìŒ ë‹µë³€ì˜ ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ì§€ì‹œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ì›ë³¸ ë‹µë³€**:
{original_answer[:1500]}

**ê²€ì¦ ê²°ê³¼**:
- í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f}/1.0
- ë¬¸ì œì : {', '.join(issues[:5]) if issues else 'ì—†ìŒ'}
- ê¶Œê³ ì‚¬í•­: {', '.join(recommendations[:5]) if recommendations else 'ì—†ìŒ'}

**ê°œì„  ì§€ì‹œ ì‘ì„± ìš”ì²­**:
ìœ„ ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ê°œì„ í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­ì„ ì‘ì„±í•˜ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”:
{{
    "needs_improvement": true,
    "improvement_instructions": [
        "ê°œì„  ì§€ì‹œ 1: êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì„ ì–´ë–»ê²Œ ê°œì„ í• ì§€",
        "ê°œì„  ì§€ì‹œ 2: ..."
    ],
    "preserve_content": [
        "ë³´ì¡´í•  ë‚´ìš© 1",
        "ë³´ì¡´í•  ë‚´ìš© 2"
    ],
    "focus_areas": [
        "ì¤‘ì  ê°œì„  ì˜ì—­ 1",
        "ì¤‘ì  ê°œì„  ì˜ì—­ 2"
    ]
}}
"""
                return improvement_prompt

            chain_steps.append({
                "name": "improvement_instructions",
                "prompt_builder": build_improvement_instructions_prompt,
                "input_extractor": lambda prev: prev,  # ì´ì „ ë‹¨ê³„ ì¶œë ¥ ì‚¬ìš© (validation ê²°ê³¼)
                "output_parser": lambda response, prev: AnswerParser.parse_improvement_instructions(response),
                "validator": lambda output: output is None or (isinstance(output, dict) and output.get("needs_improvement")),
                "required": False,  # ì„ íƒ ë‹¨ê³„ (ë¬¸ì œê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°)
                "skip_if": lambda prev: prev is None or (isinstance(prev, dict) and prev.get("is_valid", True) and prev.get("quality_score", 1.0) >= 0.75)
            })

            # Step 4: ê°œì„ ëœ ë‹µë³€ ìƒì„± (Step 3ê°€ ìˆëŠ” ê²½ìš°)
            def build_improved_answer_prompt(prev_output, initial_input):
                improvement_instructions = prev_output
                if not improvement_instructions or not isinstance(improvement_instructions, dict):
                    return None  # ê°œì„  ì§€ì‹œê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°

                # initial_inputì—ì„œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì°¾ê¸°
                if isinstance(initial_input, dict):
                    original_prompt = initial_input.get("prompt", optimized_prompt)
                else:
                    original_prompt = optimized_prompt
                if not improvement_instructions or not isinstance(improvement_instructions, dict):
                    return None  # ê°œì„  ì§€ì‹œê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°

                if not improvement_instructions.get("needs_improvement", False):
                    return None

                # ì›ë³¸ í”„ë¡¬í”„íŠ¸ì™€ ê°œì„  ì§€ì‹œë¥¼ ê²°í•©
                improvement_text = "\n".join(improvement_instructions.get("improvement_instructions", []))
                preserve_content = "\n".join(improvement_instructions.get("preserve_content", []))

                improved_prompt = f"""{original_prompt}

---

## ğŸ”§ ê°œì„  ìš”ì²­

ìœ„ í”„ë¡¬í”„íŠ¸ë¡œ ìƒì„±í•œ ë‹µë³€ì„ ë‹¤ìŒ ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ê°œì„ í•˜ì„¸ìš”:

**ê°œì„  ì§€ì‹œì‚¬í•­**:
{improvement_text}

**ë³´ì¡´í•  ë‚´ìš©** (ë°˜ë“œì‹œ í¬í•¨):
{preserve_content if preserve_content else "ì›ë³¸ ë‹µë³€ì˜ ëª¨ë“  ë²•ì  ì •ë³´ì™€ ê·¼ê±°"}

**ì¤‘ì  ê°œì„  ì˜ì—­**:
{', '.join(improvement_instructions.get("focus_areas", []))}

ìœ„ ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ë‹µë³€ì„ ê°œì„ í•˜ë˜, ì›ë³¸ ë‹µë³€ì˜ ë²•ì  ê·¼ê±°ì™€ ì •ë³´ëŠ” ë°˜ë“œì‹œ ë³´ì¡´í•˜ì„¸ìš”.
"""
                return improved_prompt

            chain_steps.append({
                "name": "improved_answer_generation",
                "prompt_builder": build_improved_answer_prompt,
                "input_extractor": lambda prev: prev,  # ê°œì„  ì§€ì‹œë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                "output_parser": lambda response, prev: WorkflowUtils.normalize_answer(response),
                "validator": lambda output: output and len(output.strip()) > 10,
                "required": False,  # ì„ íƒ ë‹¨ê³„ (ê°œì„  ì§€ì‹œê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°)
                "skip_if": lambda prev: prev is None or (isinstance(prev, dict) and not prev.get("needs_improvement", False))
            })

            # Step 5: ìµœì¢… ê²€ì¦ (ê°œì„ ëœ ë‹µë³€ì´ ìˆëŠ” ê²½ìš°)
            def build_final_validation_prompt(prev_output, initial_input):
                # prev_outputì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì²˜ë¦¬
                if isinstance(prev_output, dict):
                    answer = prev_output.get("answer") or prev_output.get("content") or str(prev_output)
                else:
                    answer = prev_output if isinstance(prev_output, str) else str(prev_output)

                final_validation_prompt = f"""
ë‹¤ìŒ ë‹µë³€ì˜ ìµœì¢… í’ˆì§ˆì„ ê²€ì¦í•˜ì„¸ìš”:

ë‹µë³€:
{answer[:2000]}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ê²€ì¦ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
1. ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ”ê°€?
2. ë²•ì  ê·¼ê±°ê°€ ì¶©ë¶„í•œê°€?
3. êµ¬ì¡°ì™€ ë…¼ë¦¬ì  íë¦„ì´ ëª…í™•í•œê°€?
4. ê¸¸ì´ê°€ ì ì ˆí•œê°€? (ìµœì†Œ 50ì ì´ìƒ)

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìµœì¢… ê²€ì¦ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”:
{{
    "final_score": 0.0-1.0,
    "meets_quality_threshold": true/false,
    "summary": "ê²€ì¦ ìš”ì•½"
}}
"""
                return final_validation_prompt

            chain_steps.append({
                "name": "final_validation",
                "prompt_builder": build_final_validation_prompt,
                "input_extractor": lambda prev: prev,  # ìµœì¢… ë‹µë³€ ê²€ì¦
                "output_parser": lambda response, prev: AnswerParser.parse_final_validation_response(response),
                "validator": lambda output: output is None or isinstance(output, dict),
                "required": False,  # ì„ íƒ ë‹¨ê³„
                "skip_if": lambda prev: not prev or len(str(prev).strip()) < 10
            })

            # ì²´ì¸ ì‹¤í–‰
            # initial_inputì„ ê° prompt_builderì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì „ë‹¬
            # ê° prompt_builderì˜ ë‘ ë²ˆì§¸ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ë¨
            initial_input_dict = {
                "prompt": optimized_prompt,
                "query": query,
                "context_dict": context_dict,
                "quality_feedback": quality_feedback,
                "is_retry": is_retry
            }

            chain_result = chain_executor.execute_chain(
                chain_steps=chain_steps,
                initial_input=initial_input_dict,
                max_iterations=2,  # ê° ë‹¨ê³„ ìµœëŒ€ 2íšŒ ì¬ì‹œë„
                stop_on_failure=False  # ì¼ë¶€ ë‹¨ê³„ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            )

            # ìµœì¢… ë‹µë³€ ì¶”ì¶œ
            final_output = chain_result.get("final_output")

            # ì²´ì¸ íˆìŠ¤í† ë¦¬ì—ì„œ ë‹µë³€ ì°¾ê¸° (ìš°ì„ ìˆœìœ„: ê°œì„ ëœ ë‹µë³€ > ì´ˆê¸° ë‹µë³€)
            final_answer = ""
            chain_history = chain_result.get("chain_history", [])

            # Step 4 (improved_answer_generation)ì˜ ì¶œë ¥ ì°¾ê¸°
            for step in reversed(chain_history):  # ì—­ìˆœìœ¼ë¡œ ê²€ìƒ‰ (ìµœì‹  ìš°ì„ )
                if step.get("step_name") == "improved_answer_generation" and step.get("success"):
                    output = step.get("output")
                    if isinstance(output, str) and len(output.strip()) > 10:
                        final_answer = output
                        break

            # ê°œì„ ëœ ë‹µë³€ì´ ì—†ìœ¼ë©´ Step 1ì˜ ì¶œë ¥ ì‚¬ìš©
            if not final_answer:
                for step in chain_history:
                    if step.get("step_name") == "initial_answer_generation" and step.get("success"):
                        output = step.get("output")
                        if isinstance(output, str) and len(output.strip()) > 10:
                            final_answer = output
                            break

            # ì—¬ì „íˆ ì—†ìœ¼ë©´ final_outputì—ì„œ ì¶”ì¶œ
            if not final_answer:
                if isinstance(final_output, str):
                    final_answer = final_output
                elif isinstance(final_output, dict):
                    final_answer = final_output.get("improved_answer") or final_output.get("initial_answer") or ""
                else:
                    final_answer = str(final_output) if final_output else ""

            # ì²´ì¸ ì‹¤í–‰ ê²°ê³¼ ë¡œê¹…
            chain_summary = chain_executor.get_chain_summary()
            self.logger.info(
                f"âœ… [PROMPT CHAIN] Executed {chain_summary['total_steps']} steps, "
                f"{chain_summary['successful_steps']} successful, "
                f"{chain_summary['failed_steps']} failed, "
                f"Total time: {chain_summary['total_time']:.2f}s"
            )

            # ìµœì¢… ë‹µë³€ì´ ë¹„ì–´ìˆìœ¼ë©´ í´ë°±
            if not final_answer or len(final_answer.strip()) < 10:
                self.logger.warning("âš ï¸ [CHAIN] Final answer is empty, using fallback")
                # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ë¡œ ë‹¨ìˆœ ìƒì„±
                response = self.call_llm_with_retry(optimized_prompt)
                final_answer = WorkflowUtils.normalize_answer(response)

            return final_answer

        except Exception as e:
            self.logger.error(f"âŒ [CHAIN ERROR] Prompt chain failed: {e}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            response = self.call_llm_with_retry(optimized_prompt)
            return WorkflowUtils.normalize_answer(response)

    def validate_answer_uses_context(
        self,
        answer: str,
        context: Dict[str, Any],
        query: str,
        retrieved_docs: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        ë‹µë³€-ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„ ê²€ì¦ (quality_validators ëª¨ë“ˆ ì‚¬ìš©)

        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            context: ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            query: ì›ë³¸ ì§ˆë¬¸
            retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡

        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result = AnswerValidator.validate_answer_uses_context(
            answer=answer,
            context=context,
            query=query,
            retrieved_docs=retrieved_docs
        )

        # ë¡œê¹…
        self.logger.info(
            f"âœ… [ANSWER-CONTEXT VALIDATION] Coverage: {result.get('coverage_score', 0.0):.2f}, "
            f"Keyword: {result.get('keyword_coverage', 0.0):.2f}, Citation: {result.get('citation_coverage', 0.0):.2f}, "
            f"Uses context: {result.get('uses_context', False)}, Needs regeneration: {result.get('needs_regeneration', False)}"
        )

        return result

    def track_search_to_answer_pipeline(
        self,
        state: LegalWorkflowState
    ) -> Dict[str, Any]:
        """
        ê²€ìƒ‰-ë‹µë³€ íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ ì¶”ì 

        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ

        Returns:
            íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        try:
            metadata = WorkflowUtils.get_state_value(state, "metadata", {})
            if not isinstance(metadata, dict):
                metadata = {}

            search_meta = metadata.get("search", {})
            context_validation = metadata.get("context_validation", {})
            answer_validation = metadata.get("answer_validation", {})

            pipeline_metrics = {
                # ê²€ìƒ‰ í’ˆì§ˆ
                "search_quality": {
                    "doc_count": search_meta.get("final_count", 0),
                    "avg_relevance": search_meta.get("avg_relevance", 0.0),
                    "semantic_results": search_meta.get("semantic_results_count", 0),
                    "keyword_results": search_meta.get("keyword_results_count", 0)
                },
                # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ
                "context_quality": {
                    "relevance_score": context_validation.get("relevance_score", 0.0),
                    "coverage_score": context_validation.get("coverage_score", 0.0),
                    "sufficiency_score": context_validation.get("sufficiency_score", 0.0),
                    "overall_score": context_validation.get("overall_score", 0.0),
                    "docs_included": metadata.get("context_validation", {}).get("docs_included", 0)
                },
                # ë‹µë³€ í’ˆì§ˆ
                "answer_quality": {
                    "uses_context": answer_validation.get("uses_context", False),
                    "coverage_score": answer_validation.get("coverage_score", 0.0),
                    "keyword_coverage": answer_validation.get("keyword_coverage", 0.0),
                    "citation_coverage": answer_validation.get("citation_coverage", 0.0),
                    "citations_found": answer_validation.get("citations_found", 0),
                    "citations_expected": answer_validation.get("citations_expected", 0)
                },
                # íŒŒì´í”„ë¼ì¸ ì¢…í•© ì ìˆ˜
                "pipeline_score": 0.0
            }

            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            search_score = search_meta.get("avg_relevance", 0.5)
            context_score = context_validation.get("overall_score", 0.5)
            answer_score = answer_validation.get("coverage_score", 0.5)

            pipeline_score = (search_score * 0.3 + context_score * 0.3 + answer_score * 0.4)
            pipeline_metrics["pipeline_score"] = pipeline_score

            # ë©”íƒ€ë°ì´í„°ì— ì €ì¥
            metadata["pipeline_metrics"] = pipeline_metrics
            WorkflowUtils.set_state_value(state, "metadata", metadata)

            self.logger.info(
                f"ğŸ“Š [PIPELINE TRACKING] Overall score: {pipeline_score:.2f}, "
                f"Search: {search_score:.2f}, Context: {context_score:.2f}, Answer: {answer_score:.2f}"
            )

            return pipeline_metrics

        except Exception as e:
            self.logger.warning(f"Pipeline tracking failed: {e}")
            return {}

    def call_llm_with_retry(self, prompt: str, max_retries: int = WorkflowConstants.MAX_RETRIES) -> str:
        """
        LLM í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)

        Args:
            prompt: í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            LLM ì‘ë‹µ ë¬¸ìì—´
        """
        for attempt in range(max_retries):
            try:
                response = self.llm.invoke(prompt)
                return WorkflowUtils.extract_response_content(response)
            except Exception as e:
                self.logger.warning(f"LLM í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    raise e
                time.sleep(WorkflowConstants.RETRY_DELAY)

        return "LLM í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    def get_quality_feedback_for_retry(self, state: LegalWorkflowState) -> Dict[str, Any]:
        """
        ì¬ì‹œë„ë¥¼ ìœ„í•œ í’ˆì§ˆ í”¼ë“œë°± ìƒì„±

        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ

        Returns:
            í”¼ë“œë°± ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        quality_meta = WorkflowUtils.get_quality_metadata(state)
        quality_score = quality_meta["quality_score"]

        # ë©”íƒ€ë°ì´í„°ì—ì„œ í’ˆì§ˆ ì²´í¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        metadata = WorkflowUtils.get_state_value(state, "metadata", {})
        quality_metadata = metadata.get("quality_metadata", {})
        quality_checks = quality_metadata.get("quality_checks", {})
        legal_validation = WorkflowUtils.get_state_value(state, "legal_basis_validation", {})

        if not isinstance(legal_validation, dict):
            legal_validation = {}

        feedback = {
            "previous_score": quality_score,
            "failed_checks": [],
            "recommendations": [],
            "retry_strategy": None
        }

        # ì‹¤íŒ¨í•œ ì²´í¬ í™•ì¸
        if not quality_checks.get("has_answer", True):
            feedback["failed_checks"].append("ë‹µë³€ì´ ë¹„ì–´ìˆìŒ")
            feedback["recommendations"].append("ë°˜ë“œì‹œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”")

        if not quality_checks.get("min_length", True):
            answer = WorkflowUtils.normalize_answer(WorkflowUtils.get_state_value(state, "answer", ""))
            current_length = len(answer)
            min_length = WorkflowConstants.MIN_ANSWER_LENGTH_VALIDATION
            feedback["failed_checks"].append(f"ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ ({current_length}ì)")
            feedback["recommendations"].append(f"ìµœì†Œ {min_length}ì ì´ìƒì˜ ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”")

        if not quality_checks.get("has_sources", True):
            feedback["failed_checks"].append("ë²•ë¥  ì†ŒìŠ¤ê°€ ì—†ìŒ")
            feedback["recommendations"].append("ê´€ë ¨ ë²•ë ¹, íŒë¡€, ë²•ë¥  ì¡°í•­ì„ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”")

        if not quality_checks.get("legal_basis_valid", True):
            issues = legal_validation.get("issues", [])
            if isinstance(issues, list):
                issues_str = ", ".join(str(issue) for issue in issues[:3])
            else:
                issues_str = str(issues)[:100]
            feedback["failed_checks"].append("ë²•ë ¹ ê²€ì¦ ì‹¤íŒ¨")
            feedback["recommendations"].extend([
                "ë²•ì  ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”",
                f"ë¬¸ì œì : {issues_str}"
            ])
            feedback["retry_strategy"] = "search"  # ê²€ìƒ‰ ì¬ì‹œë„ ê¶Œì¥

        # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ í”¼ë“œë°±
        if quality_score < 0.4:
            feedback["recommendations"].append("ë‹µë³€ì˜ í’ˆì§ˆì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”")
        elif quality_score < 0.6:
            feedback["recommendations"].append("ë‹µë³€ì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ê°œì„ í•˜ì—¬ ë” ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”")

        # ê¸¸ì´ í”¼ë“œë°±
        answer = WorkflowUtils.normalize_answer(WorkflowUtils.get_state_value(state, "answer", ""))
        if len(answer) < 50:
            feedback["recommendations"].append(
                "ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í¬í•¨í•˜ì—¬ ìƒì„¸íˆ ì‘ì„±í•˜ì„¸ìš”:\n"
                "- ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€\n"
                "- ê´€ë ¨ ë²•ë ¹ ë° ì¡°í•­\n"
                "- ì‹¤ë¬´ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­\n"
                "- ì°¸ê³ í•  ë§Œí•œ íŒë¡€ (ìˆëŠ” ê²½ìš°)"
            )

        return feedback

    def determine_retry_prompt_type(self, quality_feedback: Dict[str, Any]) -> str:
        """
        í”¼ë“œë°± ê¸°ë°˜ ì¬ì‹œë„ í”„ë¡¬í”„íŠ¸ íƒ€ì… ê²°ì •

        Args:
            quality_feedback: í’ˆì§ˆ í”¼ë“œë°± ë”•ì…”ë„ˆë¦¬

        Returns:
            í”„ë¡¬í”„íŠ¸ íƒ€ì… ë¬¸ìì—´
        """
        failed_checks = quality_feedback.get("failed_checks", [])

        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ íƒ€ì…
        base_type = "korean_legal_expert"

        # ë²•ë ¹ ê²€ì¦ ì‹¤íŒ¨ â†’ ë²•ì  ê·¼ê±° ê°•ì¡°
        if any("ë²•ë ¹" in check or "ë²•" in check for check in failed_checks):
            return f"{base_type}_with_legal_basis"
        # ê¸¸ì´ ë¶€ì¡± â†’ ìƒì„¸ ì„¤ëª… ê°•ì¡°
        elif any("ì§§" in check or "ê¸¸ì´" in check for check in failed_checks):
            return f"{base_type}_detailed"
        # ì†ŒìŠ¤ ì—†ìŒ â†’ ì¶œì²˜ ì¸ìš© ê°•ì¡°
        elif any("ì†ŒìŠ¤" in check or "ì¶œì²˜" in check for check in failed_checks):
            return f"{base_type}_with_sources"
        # ì¼ë°˜ ê°œì„ 
        else:
            return f"{base_type}_improved"

    def assess_improvement_potential(
        self,
        quality_score: float,
        quality_checks: Dict[str, bool],
        state: LegalWorkflowState
    ) -> Dict[str, Any]:
        """
        ì¬ì‹œë„ ì‹œ ê°œì„  ê°€ëŠ¥ì„± í‰ê°€

        Args:
            quality_score: í˜„ì¬ í’ˆì§ˆ ì ìˆ˜
            quality_checks: í’ˆì§ˆ ì²´í¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ

        Returns:
            ê°œì„  ê°€ëŠ¥ì„± í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        potential = 0.0
        strategy = None
        reasons = []

        # 1. ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì–´ì„œ ì‹¤íŒ¨í•œ ê²½ìš° â†’ ê²€ìƒ‰ ì¬ì‹œë„ë¡œ ê°œì„  ê°€ëŠ¥ì„± ë†’ìŒ
        retrieved_docs = WorkflowUtils.get_state_value(state, "retrieved_docs", [])
        if len(retrieved_docs) == 0 and not quality_checks.get("has_sources", True):
            potential += 0.4
            strategy = "retry_search"
            reasons.append("ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡±ìœ¼ë¡œ ê²€ìƒ‰ ì¬ì‹œë„ ì‹œ ê°œì„  ê°€ëŠ¥")

        # 2. ë‹µë³€ì´ ì§§ì€ ê²½ìš° â†’ í”„ë¡¬í”„íŠ¸ ê°œì„ ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥ì„± ë†’ìŒ
        answer = WorkflowUtils.normalize_answer(WorkflowUtils.get_state_value(state, "answer", ""))
        if len(answer) < 50 and quality_score > 0.3:
            potential += 0.5
            if strategy is None:
                strategy = "retry_generate"
            reasons.append("ë‹µë³€ ê¸¸ì´ ë¬¸ì œë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œ ê°œì„  ê°€ëŠ¥")

        # 3. ë²•ë ¹ ê²€ì¦ ì‹¤íŒ¨ â†’ ê²€ìƒ‰ ê°œì„ ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥ì„± ë†’ìŒ
        if not quality_checks.get("legal_basis_valid", True):
            potential += 0.6
            strategy = "retry_search"
            reasons.append("ë²•ë ¹ ê²€ì¦ ì‹¤íŒ¨ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì‹œ ê°œì„  ê°€ëŠ¥")

        # 4. í’ˆì§ˆ ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ (0.2 ì´í•˜) â†’ ì¬ì‹œë„ íš¨ê³¼ ë‚®ìŒ
        if quality_score < 0.2:
            potential *= 0.5
            reasons.append("í’ˆì§ˆ ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ì•„ ì¬ì‹œë„ íš¨ê³¼ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŒ")

        return {
            "potential": min(potential, 1.0),
            "strategy": strategy,
            "reasons": reasons
        }

    def generate_fallback_answer(self, state: LegalWorkflowState) -> str:
        """
        í´ë°± ë‹µë³€ ìƒì„±

        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ

        Returns:
            í´ë°± ë‹µë³€ ë¬¸ìì—´
        """
        query = WorkflowUtils.get_state_value(state, "query", "")
        query_type = WorkflowUtils.get_state_value(state, "query_type", "")
        retrieved_docs = WorkflowUtils.get_state_value(state, "retrieved_docs", [])

        # retrieved_docs ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        context_parts = []
        for doc in retrieved_docs:
            if isinstance(doc, dict):
                content = doc.get("content", doc.get("text", str(doc)))
            else:
                content = str(doc)
            if content:
                context_parts.append(content)

        context = "\n".join(context_parts[:5])  # ìµœëŒ€ 5ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©

        return f"""## ë‹µë³€

ì§ˆë¬¸: {query}

ì´ ì§ˆë¬¸ì€ {query_type} ì˜ì—­ì— í•´ë‹¹í•©ë‹ˆë‹¤.

## ê´€ë ¨ ë²•ë¥  ì •ë³´
{context}

## ì£¼ìš” í¬ì¸íŠ¸
1. ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
2. ì •í™•í•œ ë²•ë¥ ì  ì¡°ì–¸ì„ ìœ„í•´ì„œëŠ” ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
3. ê´€ë ¨ ë²•ì¡°ë¬¸ê³¼ íŒë¡€ë¥¼ ì¶”ê°€ë¡œ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

## ì£¼ì˜ì‚¬í•­
- ì´ ë‹µë³€ì€ ì¼ë°˜ì ì¸ ì •ë³´ ì œê³µ ëª©ì ì´ë©°, êµ¬ì²´ì ì¸ ë²•ë¥ ì  ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤.
- ì‹¤ì œ ì‚¬ì•ˆì— ëŒ€í•´ì„œëŠ” ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""
