# -*- coding: utf-8 -*-
"""
Answer Quality Validator
ÎãµÎ≥Ä ÌíàÏßà Í≤ÄÏ¶ù Î°úÏßÅÏùÑ Ï≤òÎ¶¨ÌïòÎäî Í≤ÄÏ¶ùÍ∏∞
"""

import json
import logging
import re
from typing import Any, Dict, List, Optional

try:
    from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
except ImportError:
    from core.workflow.state.state_definitions import LegalWorkflowState
try:
    from lawfirm_langgraph.core.workflow.utils.workflow_constants import WorkflowConstants, QualityThresholds
except ImportError:
    from core.workflow.utils.workflow_constants import WorkflowConstants, QualityThresholds
try:
    from lawfirm_langgraph.core.workflow.utils.workflow_utils import WorkflowUtils
except ImportError:
    from core.workflow.utils.workflow_utils import WorkflowUtils
try:
    from lawfirm_langgraph.core.workflow.state.answer_helpers import parse_answer_with_metadata
except ImportError:
    from core.workflow.state.answer_helpers import parse_answer_with_metadata


class AnswerQualityValidator:
    """ÎãµÎ≥Ä ÌíàÏßà Í≤ÄÏ¶ùÍ∏∞"""

    def __init__(
        self,
        logger,
        validator_llm=None,
        legal_validator=None,
        workflow_validator=None,
        get_state_value_func=None,
        set_state_value_func=None,
        normalize_answer_func=None,
        set_answer_safely_func=None,
        add_step_func=None,
        save_metadata_safely_func=None,
        check_has_sources_func=None
    ):
        self.logger = logger
        self.validator_llm = validator_llm
        self.legal_validator = legal_validator
        self.workflow_validator = workflow_validator
        self._get_state_value_func = get_state_value_func
        self._set_state_value_func = set_state_value_func
        self._normalize_answer_func = normalize_answer_func
        self._set_answer_safely_func = set_answer_safely_func
        self._add_step_func = add_step_func
        self._save_metadata_safely_func = save_metadata_safely_func
        self._check_has_sources_func = check_has_sources_func

    def validate_answer_quality(self, state: LegalWorkflowState) -> bool:
        """ÌíàÏßà Í≤ÄÏ¶ù"""
        answer_raw = self._get_state_value(state, "answer", "")
        normalized_answer = self._normalize_answer(answer_raw)
        if answer_raw != normalized_answer or not isinstance(answer_raw, str):
            self._set_answer_safely(state, normalized_answer)
        answer = normalized_answer
        errors = self._get_state_value(state, "errors", [])
        sources = self._get_state_value(state, "sources", [])

        if not sources or len(sources) == 0:
            retrieved_docs = self._get_state_value(state, "retrieved_docs", [])
            if retrieved_docs and isinstance(retrieved_docs, list) and len(retrieved_docs) > 0:
                sources = []
                for doc in retrieved_docs:
                    if isinstance(doc, dict):
                        source_info = {
                            "source": doc.get("source") or doc.get("title") or doc.get("document_id", ""),
                            "type": doc.get("type") or doc.get("source_type") or "unknown"
                        }
                        if source_info["source"]:
                            sources.append(source_info)

        # üî• Í∞úÏÑ†: ÎãµÎ≥ÄÏóêÏÑú [END]ÏôÄ [metadata] ÏÑπÏÖòÏùÑ Ï†úÍ±∞Ìïú ÏàúÏàò ÎãµÎ≥Ä Î≥∏Î¨∏Îßå Í≤ÄÏ¶ù
        answer_with_metadata = answer if isinstance(answer, str) else str(answer) if answer else ""
        answer_body, extracted_metadata = parse_answer_with_metadata(answer_with_metadata)
        
        # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
        metadata_valid = True
        if extracted_metadata:
            self.logger.debug(f"‚úÖ [VALIDATION] Extracted metadata from answer (document_usage: {len(extracted_metadata.get('document_usage', []))}, coverage: {extracted_metadata.get('coverage', {})})")
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ Í≤ÄÏ¶ù
            document_usage = extracted_metadata.get("document_usage", [])
            coverage = extracted_metadata.get("coverage", {})
            
            # document_usageÍ∞Ä Î¶¨Ïä§Ìä∏Ïù∏ÏßÄ ÌôïÏù∏
            if not isinstance(document_usage, list):
                metadata_valid = False
                self.logger.warning(f"‚ö†Ô∏è [METADATA VALIDATION] document_usage is not a list: {type(document_usage)}")
            
            # coverageÍ∞Ä ÎîïÏÖîÎÑàÎ¶¨Ïù∏ÏßÄ ÌôïÏù∏
            if not isinstance(coverage, dict):
                metadata_valid = False
                self.logger.warning(f"‚ö†Ô∏è [METADATA VALIDATION] coverage is not a dict: {type(coverage)}")
            
            # stateÏóê Ï†ÄÏû•
            if "metadata" not in state:
                state["metadata"] = {}
            state["metadata"]["extracted_metadata"] = extracted_metadata
            state["metadata"]["metadata_valid"] = metadata_valid
        else:
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÎäî Í≤ΩÏö∞Îäî Í≤ΩÍ≥†Îßå (ÌïÑÏàòÎäî ÏïÑÎãò)
            self.logger.debug(f"‚ÑπÔ∏è [VALIDATION] No metadata found in answer (this is acceptable)")
            metadata_valid = True  # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏñ¥ÎèÑ ÎãµÎ≥ÄÏùÄ Ïú†Ìö®Ìï† Ïàò ÏûàÏùå
        
        # ÎãµÎ≥Ä Î≥∏Î¨∏Îßå Í≤ÄÏ¶ùÏóê ÏÇ¨Ïö©
        answer_str_for_check = answer_body

        has_format_errors = self.detect_format_errors(answer_str_for_check)

        has_sources = self._check_has_sources(state, sources)
        source_count = len(sources) if sources and isinstance(sources, list) else 0

        retrieved_docs = self._get_state_value(state, "retrieved_docs", [])
        if retrieved_docs and isinstance(retrieved_docs, list):
            retrieved_docs_count = len(retrieved_docs)
            if source_count == 0 and retrieved_docs_count > 0:
                source_count = retrieved_docs_count
                self.logger.debug(f"üìä [SOURCE COUNT] Using retrieved_docs count: {source_count}")

        specific_case_result = self.detect_specific_case_copy(answer_str_for_check)
        general_principle_result = self._check_general_principle_first(answer_str_for_check)
        structure_result = self._check_answer_structure(answer_str_for_check)

        quality_checks = {
            "has_answer": len(answer_str_for_check) > 0,
            "min_length": len(answer_str_for_check) >= WorkflowConstants.MIN_ANSWER_LENGTH_VALIDATION,
            "no_errors": len(errors) == 0,
            "has_sources": has_sources,
            "no_format_errors": not has_format_errors,
            "no_specific_case_copy": not specific_case_result.get("needs_regeneration", False),
            "general_principle_first": general_principle_result.get("principle_first", False),
            "has_good_structure": structure_result.get("structure_score", 0.0) >= 0.4
        }

        self.logger.info(
            f"üìä [QUALITY CHECKS] Detailed validation:\n"
            f"   has_answer: {quality_checks['has_answer']} (answer length: {len(answer_str_for_check)})\n"
            f"   min_length: {quality_checks['min_length']} (required: {WorkflowConstants.MIN_ANSWER_LENGTH_VALIDATION}, actual: {len(answer_str_for_check)})\n"
            f"   no_errors: {quality_checks['no_errors']} (error count: {len(errors)})\n"
            f"   has_sources: {quality_checks['has_sources']} (source count: {source_count})\n"
            f"   no_format_errors: {quality_checks['no_format_errors']} (format_errors detected: {has_format_errors})\n"
            f"   no_specific_case_copy: {quality_checks['no_specific_case_copy']} (copy_score: {specific_case_result.get('copy_score', 0.0):.2f}, case_numbers: {len(specific_case_result.get('case_numbers', []))}, party_names: {len(specific_case_result.get('party_names', []))})\n"
            f"   general_principle_first: {quality_checks['general_principle_first']} (score: {general_principle_result.get('score', 0.0):.2f})\n"
            f"   has_good_structure: {quality_checks['has_good_structure']} (structure_score: {structure_result.get('structure_score', 0.0):.2f}, missing_sections: {len(structure_result.get('missing_sections', []))})"
        )

        needs_regeneration = specific_case_result.get("needs_regeneration", False)
        if needs_regeneration:
            self.logger.warning(
                f"‚ö†Ô∏è [QUALITY CHECK] Specific case copy detected - needs regeneration:\n"
                f"   copy_score: {specific_case_result.get('copy_score', 0.0):.2f}\n"
                f"   case_numbers: {specific_case_result.get('case_numbers', [])}\n"
                f"   party_names: {specific_case_result.get('party_names', [])}"
            )
            self._set_state_value(state, "needs_regeneration", True)
            self._set_state_value(state, "regeneration_reason", "specific_case_copy")
            state["needs_regeneration"] = True
            state["regeneration_reason"] = "specific_case_copy"
            if "metadata" not in state:
                state["metadata"] = {}
            state["metadata"]["needs_regeneration"] = True
            state["metadata"]["regeneration_reason"] = "specific_case_copy"
            self.logger.info(f"‚úÖ [REGENERATION FLAG] Set needs_regeneration=True in multiple locations")

        if not general_principle_result.get("principle_first", False):
            self.logger.warning(
                f"‚ö†Ô∏è [QUALITY CHECK] General principle not first:\n"
                f"   has_general_principle: {general_principle_result.get('has_general_principle', False)}\n"
                f"   general_principle_position: {general_principle_result.get('general_principle_position', -1)}\n"
                f"   specific_case_position: {general_principle_result.get('specific_case_position', -1)}\n"
                f"   score: {general_principle_result.get('score', 0.0):.2f}"
            )
            if general_principle_result.get("specific_case_position", -1) >= 0 and general_principle_result.get("general_principle_position", -1) < 0:
                self._set_state_value(state, "needs_regeneration", True)
                self._set_state_value(state, "regeneration_reason", "general_principle_not_first")
                state["needs_regeneration"] = True
                state["regeneration_reason"] = "general_principle_not_first"
                if "metadata" not in state:
                    state["metadata"] = {}
                state["metadata"]["needs_regeneration"] = True
                state["metadata"]["regeneration_reason"] = "general_principle_not_first"
                self.logger.info(f"‚úÖ [REGENERATION FLAG] Set needs_regeneration=True (general_principle_not_first) in multiple locations")

        if structure_result.get("structure_score", 0.0) < 0.6:
            self.logger.warning(
                f"‚ö†Ô∏è [QUALITY CHECK] Answer structure score is low:\n"
                f"   structure_score: {structure_result.get('structure_score', 0.0):.2f}\n"
                f"   missing_sections: {structure_result.get('missing_sections', [])}"
            )

        query = self._get_state_value(state, "query", "")
        basic_quality_passed = (
            quality_checks.get("has_answer", False) and
            quality_checks.get("min_length", False) and
            quality_checks.get("no_errors", False) and
            quality_checks.get("has_sources", False)
        )

        temp_passed = sum([quality_checks.get("has_answer", False),
                          quality_checks.get("min_length", False),
                          quality_checks.get("no_errors", False),
                          quality_checks.get("has_sources", False),
                          quality_checks.get("no_format_errors", False)])
        temp_total = len(quality_checks)
        temp_quality_score = temp_passed / temp_total if temp_total > 0 else 0.0

        should_skip_legal_validation = (
            basic_quality_passed and
            temp_quality_score >= 0.8 and
            len(answer_str_for_check) > 200 and
            quality_checks.get("has_sources", False) and
            quality_checks.get("no_format_errors", False)
        )

        if should_skip_legal_validation:
            self.logger.debug(f"Skipping legal validation (answer length: {len(answer_str_for_check)}, has sources: {quality_checks.get('has_sources', False)})")
            self._set_state_value(state, "legal_validity_check", True)
            quality_checks["legal_basis_valid"] = True
        elif self.legal_validator and len(answer_str_for_check) > 0:
            try:
                answer_for_validation = answer if isinstance(answer, str) else answer_str_for_check
                validation_result = self.legal_validator.validate_legal_basis(query, answer_for_validation)
                self._set_state_value(state, "legal_validity_check", validation_result.is_valid)
                self._set_state_value(state, "legal_basis_validation", {
                    "confidence": validation_result.confidence,
                    "issues": validation_result.issues,
                    "recommendations": validation_result.recommendations
                })
                quality_checks["legal_basis_valid"] = validation_result.is_valid
            except Exception as e:
                self.logger.warning(f"Legal validation failed: {e}")
                self._set_state_value(state, "legal_validity_check", True)
                quality_checks["legal_basis_valid"] = True
        else:
            self._set_state_value(state, "legal_validity_check", True)
            quality_checks["legal_basis_valid"] = True

        llm_validation_result = None
        if self.validator_llm and answer_str_for_check and len(answer_str_for_check) > 50:
            try:
                # üî• Í∞úÏÑ†: LLM Í≤ÄÏ¶ùÎèÑ ÎãµÎ≥Ä Î≥∏Î¨∏Îßå ÏÇ¨Ïö© (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†úÏô∏)
                llm_validation_result = self.validate_with_llm(answer_str_for_check, state)
                if llm_validation_result:
                    llm_quality_score = llm_validation_result.get("quality_score", 0.0)
                    llm_needs_regeneration = llm_validation_result.get("needs_regeneration", False)
                    llm_issues = llm_validation_result.get("issues", [])

                    quality_checks["llm_validation_passed"] = llm_quality_score >= 0.7
                    quality_checks["llm_quality_score"] = llm_quality_score

                    if llm_needs_regeneration:
                        self.logger.warning(
                            f"‚ö†Ô∏è [LLM VALIDATION] Regeneration needed: {llm_validation_result.get('regeneration_reason', 'unknown')}\n"
                            f"   quality_score: {llm_quality_score:.2f}\n"
                            f"   issues: {llm_issues}"
                        )
                        self._set_state_value(state, "needs_regeneration", True)
                        self._set_state_value(state, "regeneration_reason", llm_validation_result.get("regeneration_reason", "llm_validation_failed"))
                        state["needs_regeneration"] = True
                        state["regeneration_reason"] = llm_validation_result.get("regeneration_reason", "llm_validation_failed")
                        if "metadata" not in state:
                            state["metadata"] = {}
                        state["metadata"]["needs_regeneration"] = True
                        state["metadata"]["regeneration_reason"] = llm_validation_result.get("regeneration_reason", "llm_validation_failed")
                        state["metadata"]["llm_validation_result"] = llm_validation_result
            except Exception as e:
                self.logger.warning(f"LLM-based validation failed: {e}")

        weighted_scores = {
            "has_answer": 1.0,
            "min_length": 1.0,
            "no_errors": 1.0,
            "has_sources": 1.0,
            "no_format_errors": 1.0,
            "no_specific_case_copy": 1.5,
            "general_principle_first": 1.5,
            "has_good_structure": 1.2,
            "legal_basis_valid": 1.0,
        }

        total_weight = 0.0
        weighted_sum = 0.0

        for check_name, passed in quality_checks.items():
            weight = weighted_scores.get(check_name, 1.0)
            total_weight += weight
            if passed:
                weighted_sum += weight

        quality_score = weighted_sum / total_weight if total_weight > 0 else 0.0
        quality_check_passed = quality_score >= QualityThresholds.QUALITY_PASS_THRESHOLD

        answer = self._get_state_value(state, "answer", "")
        answer_length = len(answer.strip()) if isinstance(answer, str) else 0
        min_length = WorkflowConstants.MIN_ANSWER_LENGTH_VALIDATION

        if quality_score < QualityThresholds.QUALITY_PASS_THRESHOLD or answer_length < min_length:
            needs_regeneration = True
            regeneration_reason = []
            if quality_score < QualityThresholds.QUALITY_PASS_THRESHOLD:
                regeneration_reason.append(f"low_quality_score_{quality_score:.2f}")
            if answer_length < min_length:
                regeneration_reason.append(f"short_answer_{answer_length}chars")

            self._set_state_value(state, "needs_regeneration", True)
            state["needs_regeneration"] = True
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            state["metadata"]["needs_regeneration"] = True
            state["metadata"]["regeneration_reason"] = "_".join(regeneration_reason)
            self.logger.info(
                f"‚úÖ [REGENERATION FLAG] Set needs_regeneration=True (quality_score={quality_score:.2f}, "
                f"answer_length={answer_length}, reason={'_'.join(regeneration_reason)}) in multiple locations"
            )

        self._save_metadata_safely(state, "quality_score", quality_score, save_to_top_level=True)
        self._save_metadata_safely(state, "quality_check_passed", quality_check_passed, save_to_top_level=True)

        if "common" not in state:
            state["common"] = {}
        if "metadata" not in state["common"]:
            state["common"]["metadata"] = {}
        state["common"]["metadata"]["quality_score"] = quality_score
        state["common"]["metadata"]["quality_check_passed"] = quality_check_passed

        state["_quality_score"] = quality_score
        state["_quality_check_passed"] = quality_check_passed

        passed_checks_count = sum(1 for passed in quality_checks.values() if passed)
        total_checks_count = len(quality_checks)

        self.logger.info(
            f"‚úÖ [QUALITY VALIDATION] Final results:\n"
            f"   quality_score: {quality_score:.2f} (threshold: {QualityThresholds.QUALITY_PASS_THRESHOLD})\n"
            f"   quality_check_passed: {quality_check_passed}\n"
            f"   passed_checks: {passed_checks_count}/{total_checks_count}\n"
            f"   weighted_score: {weighted_sum:.2f}/{total_weight:.2f}\n"
            f"   legal_validity: {self._get_state_value(state, 'legal_validity_check', True)}"
        )

        legal_validity = self._get_state_value(state, "legal_validity_check", True)
        self._add_step(state, "ÎãµÎ≥Ä Í≤ÄÏ¶ù",
                     f"ÌíàÏßà: {quality_score:.2f}, Î≤ïÎ†π: {legal_validity}")

        return quality_check_passed

    def validate_with_llm(self, answer: str, state: LegalWorkflowState) -> Dict[str, Any]:
        """LLMÏùÑ ÏÇ¨Ïö©Ìïú ÌíàÏßà Í≤ÄÏ¶ù"""
        if not self.validator_llm or not answer:
            return {}

        query = self._get_state_value(state, "query", "")
        sources = self._get_state_value(state, "sources", [])

        validation_prompt = f"""Îã§Ïùå Î≤ïÎ•† ÎãµÎ≥ÄÏùò ÌíàÏßàÏùÑ Í≤ÄÏ¶ùÌï¥Ï£ºÏÑ∏Ïöî.

ÏßàÎ¨∏: {query}

ÎãµÎ≥Ä:
{answer}

ÏÜåÏä§ Í∞úÏàò: {len(sources) if sources else 0}

Îã§Ïùå Í∏∞Ï§ÄÏúºÎ°ú Í≤ÄÏ¶ùÌï¥Ï£ºÏÑ∏Ïöî:
1. ÎãµÎ≥ÄÏù¥ ÏßàÎ¨∏Ïóê Ï†ÅÏ†àÌûà ÎãµÎ≥ÄÌïòÎäîÍ∞Ä?
2. ÎãµÎ≥ÄÏù¥ Î≤ïÎ•†Ï†ÅÏúºÎ°ú Ï†ïÌôïÌïúÍ∞Ä?
3. ÎãµÎ≥ÄÏù¥ Ï∂©Î∂ÑÌûà ÏÉÅÏÑ∏ÌïúÍ∞Ä?
4. ÎãµÎ≥ÄÏù¥ Íµ¨Ï°∞Ï†ÅÏúºÎ°ú Ïûò Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÎäîÍ∞Ä?
5. ÌäπÏ†ï ÏÇ¨Í±¥Ïùò ÎÇ¥Ïö©Ïù¥ Í∑∏ÎåÄÎ°ú Î≥µÏÇ¨ÎêòÏßÄ ÏïäÏïòÎäîÍ∞Ä?
6. ÏùºÎ∞ò Î≤ïÏ†Å ÏõêÏπôÏù¥ Î®ºÏ†Ä ÏÑ§Î™ÖÎêòÏóàÎäîÍ∞Ä?

Îã§Ïùå JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:
{{
    "is_valid": true/false,
    "quality_score": 0.0-1.0,
    "issues": ["Î¨∏Ï†úÏ†ê1", "Î¨∏Ï†úÏ†ê2"],
    "strengths": ["Í∞ïÏ†ê1", "Í∞ïÏ†ê2"],
    "needs_regeneration": true/false,
    "regeneration_reason": "Ïû¨ÏÉùÏÑ± Ïù¥Ïú† (needs_regenerationÏù¥ trueÏù∏ Í≤ΩÏö∞)"
}}
"""

        try:
            response = self.validator_llm.invoke(validation_prompt)
            response_content = WorkflowUtils.extract_response_content(response)

            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_content, re.DOTALL)
            if json_match:
                validation_result = json.loads(json_match.group(0))
                self.logger.info(
                    f"‚úÖ [LLM VALIDATION] Result: "
                    f"is_valid={validation_result.get('is_valid', False)}, "
                    f"quality_score={validation_result.get('quality_score', 0.0):.2f}, "
                    f"needs_regeneration={validation_result.get('needs_regeneration', False)}"
                )
                return validation_result

            return {}
        except json.JSONDecodeError as e:
            self.logger.warning(f"LLM validation JSON parsing failed: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"LLM validation failed: {e}", exc_info=True)
            return {}

    def detect_format_errors(self, answer: str) -> bool:
        """ÎãµÎ≥ÄÏóêÏÑú ÌòïÏãù Ïò§Î•ò Í∞êÏßÄ"""
        if not answer or not isinstance(answer, str):
            return False

        step_patterns = [
            r'STEP\s*\d+[:Ôºö]',
            r'##\s*STEP\s*\d+',
            r'###\s*STEP\s*\d+',
        ]

        for pattern in step_patterns:
            if re.search(pattern, answer, re.IGNORECASE):
                return True

        evaluation_patterns = [
            r'ÏõêÎ≥∏\s*ÌíàÏßà\s*ÌèâÍ∞Ä',
            r'ÌèâÍ∞Ä\s*Í≤∞Í≥º',
            r'‚Ä¢\s*\[[^\]]*\]\s*Î≤ïÏ†Å\s*Ï†ïÎ≥¥',
            r'Í∞úÏÑ†\s*ÌïÑÏöî',
        ]

        for pattern in evaluation_patterns:
            if re.search(pattern, answer, re.IGNORECASE):
                return True

        return False

    def detect_specific_case_copy(self, answer: str) -> Dict[str, Any]:
        """ÌäπÏ†ï ÏÇ¨Í±¥Ïùò ÎÇ¥Ïö©Ïù¥ Í∑∏ÎåÄÎ°ú Î≥µÏÇ¨ÎêòÏóàÎäîÏßÄ Í∞êÏßÄ"""
        if not answer or not isinstance(answer, str):
            return {
                "has_specific_case": False,
                "case_numbers": [],
                "party_names": [],
                "copy_score": 0.0,
                "needs_regeneration": False
            }

        case_number_patterns = [
            r'\d{4}[Í∞ÄÎÇòÎã§ÎùºÎßàÎ∞îÏÇ¨ÏïÑÏûêÏ∞®Ïπ¥ÌÉÄÌååÌïò]\d+',
            r'\d{4}Í≥†Îã®\d+',
            r'\d{4}Í∞ÄÎã®\d+',
            r'\d{4}ÎÇòÎã®\d+',
            r'Î≤ïÏõê.*?\d{4}[Í∞ÄÎÇòÎã§ÎùºÎßàÎ∞îÏÇ¨ÏïÑÏûêÏ∞®Ïπ¥ÌÉÄÌååÌïò]\d+',
        ]

        case_numbers = []
        for pattern in case_number_patterns:
            matches = re.findall(pattern, answer)
            case_numbers.extend(matches)

        party_patterns = [
            r'ÌîºÍ≥†\s+[Í∞Ä-Ìû£]+',
            r'ÏõêÍ≥†\s+Î≥∏Ïù∏',
            r'Ïù¥\s*ÏÇ¨Í±¥\s*Í∞Å\s*Í≥ÑÏïΩ',
            r'Ïù¥\s*ÏÇ¨Í±¥\s*Í∞Å\s*Í≥ÑÏïΩÏÑú',
        ]

        party_names = []
        for pattern in party_patterns:
            matches = re.findall(pattern, answer)
            party_names.extend(matches)

        copy_score = 0.0
        if case_numbers:
            copy_score += min(0.5, len(case_numbers) * 0.1)
        if party_names:
            copy_score += min(0.5, len(party_names) * 0.1)

        fact_patterns = [
            r'Ïù¥\s*ÏÇ¨Í±¥\s*Í∞Å\s*Í≥ÑÏïΩÏÑú\s*ÏûëÏÑ±\s*ÎãπÏãú',
            r'Ïù¥\s*ÏÇ¨Í±¥\s*Í∞Å\s*Í≥ÑÏïΩ\s*Ï≤¥Í≤∞',
            r'ÌîºÍ≥†\s+[Í∞Ä-Ìû£]+\s*ÎòêÎäî\s*ÌîºÍ≥†\s+[Í∞Ä-Ìû£]+',
        ]

        fact_mentions = 0
        for pattern in fact_patterns:
            if re.search(pattern, answer):
                fact_mentions += 1

        if fact_mentions > 0:
            copy_score += min(0.3, fact_mentions * 0.1)

        needs_regeneration = copy_score >= 0.3 or len(case_numbers) >= 1

        return {
            "has_specific_case": len(case_numbers) > 0 or len(party_names) > 0,
            "case_numbers": list(set(case_numbers)),
            "party_names": list(set(party_names)),
            "copy_score": copy_score,
            "needs_regeneration": needs_regeneration
        }

    def validate_answer_uses_context(
        self,
        answer: str,
        context: Dict[str, Any],
        query: str,
        retrieved_docs: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """ÎãµÎ≥ÄÏù¥ Ïª®ÌÖçÏä§Ìä∏Î•º ÏÇ¨Ïö©ÌïòÎäîÏßÄ Í≤ÄÏ¶ù (ÎûòÌçº)"""
        return {
            "uses_context": True,
            "context_coverage": 0.8,
            "issues": []
        }

    def _get_state_value(self, state: LegalWorkflowState, key: str, default: Any = None) -> Any:
        """StateÏóêÏÑú Í∞í Í∞ÄÏ†∏Ïò§Í∏∞"""
        if self._get_state_value_func:
            return self._get_state_value_func(state, key, default)
        if isinstance(state, dict):
            if key in state:
                return state[key]
        return default

    def _set_state_value(self, state: LegalWorkflowState, key: str, value: Any) -> None:
        """StateÏóê Í∞í ÏÑ§Ï†ï"""
        if self._set_state_value_func:
            self._set_state_value_func(state, key, value)
        elif isinstance(state, dict):
            state[key] = value

    def _normalize_answer(self, answer: Any) -> str:
        """ÎãµÎ≥Ä Ï†ïÍ∑úÌôî"""
        if self._normalize_answer_func:
            return self._normalize_answer_func(answer)
        if isinstance(answer, str):
            return answer.strip()
        return str(answer).strip() if answer else ""

    def _set_answer_safely(self, state: LegalWorkflowState, answer: str) -> None:
        """ÎãµÎ≥Ä ÏïàÏ†ÑÌïòÍ≤å ÏÑ§Ï†ï"""
        if self._set_answer_safely_func:
            self._set_answer_safely_func(state, answer)
        elif isinstance(state, dict):
            state["answer"] = answer

    def _add_step(self, state: LegalWorkflowState, step_name: str, step_info: str) -> None:
        """Îã®Í≥Ñ Ï∂îÍ∞Ä"""
        if self._add_step_func:
            self._add_step_func(state, step_name, step_info)

    def _save_metadata_safely(self, state: LegalWorkflowState, key: str, value: Any, save_to_top_level: bool = False) -> None:
        """Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏïàÏ†ÑÌïòÍ≤å Ï†ÄÏû•"""
        if self._save_metadata_safely_func:
            self._save_metadata_safely_func(state, key, value, save_to_top_level)
        elif isinstance(state, dict):
            if "metadata" not in state:
                state["metadata"] = {}
            state["metadata"][key] = value
            if save_to_top_level:
                state[key] = value

    def _check_has_sources(self, state: LegalWorkflowState, sources: List[Any]) -> bool:
        """ÏÜåÏä§ Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏"""
        if self._check_has_sources_func:
            return self._check_has_sources_func(state, sources)
        return len(sources) > 0 if sources else False

    def _check_general_principle_first(self, answer: str) -> Dict[str, Any]:
        """ÏùºÎ∞ò Î≤ïÏ†Å ÏõêÏπôÏù¥ Î®ºÏ†Ä ÏÑ§Î™ÖÎêòÏóàÎäîÏßÄ Í≤ÄÏ¶ù"""
        if self.workflow_validator:
            return self.workflow_validator.check_general_principle_first(answer)
        return {
            "principle_first": True,
            "has_general_principle": True,
            "score": 1.0
        }

    def _check_answer_structure(self, answer: str) -> Dict[str, Any]:
        """ÎãµÎ≥Ä Íµ¨Ï°∞Í∞Ä Ïò¨Î∞îÎ•∏ÏßÄ Í≤ÄÏ¶ù"""
        if self.workflow_validator:
            return self.workflow_validator.check_answer_structure(answer)
        return {
            "structure_score": 1.0,
            "missing_sections": []
        }

