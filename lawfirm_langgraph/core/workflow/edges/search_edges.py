# -*- coding: utf-8 -*-
"""
Search Edges
ê²€ìƒ‰ ê´€ë ¨ ì—£ì§€ ì •ì˜
"""

import logging
try:
    from lawfirm_langgraph.core.utils.logger import get_logger
except ImportError:
    from core.utils.logger import get_logger
from typing import Optional

from langgraph.graph import StateGraph

try:
    from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
except ImportError:
    from core.workflow.state.state_definitions import LegalWorkflowState


logger = get_logger(__name__)


class SearchEdges:
    """ê²€ìƒ‰ ê´€ë ¨ ì—£ì§€ ë¹Œë”"""
    
    def __init__(
        self,
        should_skip_search_adaptive_func=None,
        should_use_multi_query_agent_func=None,
        logger_instance: Optional[logging.Logger] = None
    ):
        """
        SearchEdges ì´ˆê¸°í™”
        
        Args:
            should_skip_search_adaptive_func: ê²€ìƒ‰ ìŠ¤í‚µ ì—¬ë¶€ ê²°ì • í•¨ìˆ˜
            should_use_multi_query_agent_func: ë©€í‹° ì§ˆì˜ ì—ì´ì „íŠ¸ ì‚¬ìš© ì—¬ë¶€ ê²°ì • í•¨ìˆ˜
            logger_instance: ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
        """
        self.should_skip_search_adaptive_func = should_skip_search_adaptive_func
        self.should_use_multi_query_agent_func = should_use_multi_query_agent_func
        self.logger = logger_instance or logger
    
    def add_search_edges(
        self,
        workflow: StateGraph,
        answer_generation_node: str = "generate_and_validate_answer"
    ) -> None:
        """
        ê²€ìƒ‰ ê´€ë ¨ ì—£ì§€ ì¶”ê°€
        
        Args:
            workflow: StateGraph ì¸ìŠ¤í„´ìŠ¤
            answer_generation_node: ë‹µë³€ ìƒì„± ë…¸ë“œ ì´ë¦„
        """
        # ë¬¸ì„œ ë¶„ì„ í›„ ê²€ìƒ‰ìœ¼ë¡œ
        workflow.add_edge("analyze_document", "expand_keywords")
        
        # ğŸ”¥ ê°œì„ : expand_keywords â†’ classify_complexity_after_keywords â†’ multi_query_search_agent
        # (classification_edges.pyì—ì„œ ì´ë¯¸ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°)
        # í‚¤ì›Œë“œ í™•ì¥ í›„ ë³µì¡ë„ ì¬í‰ê°€ê°€ ë¨¼ì € ì‹¤í–‰ë˜ê³ , ê·¸ í›„ ë©€í‹° ì§ˆì˜ ì—ì´ì „íŠ¸ë¡œ ì´ë™
        # ë”°ë¼ì„œ expand_keywordsì—ì„œ ì§ì ‘ ì—°ê²°í•˜ëŠ” ì—£ì§€ëŠ” ì œê±°ë¨
        
        # ë©€í‹° ì§ˆì˜ ì—ì´ì „íŠ¸ ì‹¤í–‰ í›„ ê²°ê³¼ ì²˜ë¦¬ ë…¸ë“œë¡œ ì—°ê²° (ë³‘í•© ë° ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´)
        # ë…¸ë“œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ë…¸ë“œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¡œ í™•ì¸)
        workflow_nodes = list(workflow.nodes.keys()) if hasattr(workflow, 'nodes') else []
        if "multi_query_search_agent" in workflow_nodes:
            # ğŸ”¥ multi-query ê²°ê³¼ë„ process_search_results_combinedì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ì—°ê²°
            if "process_search_results_combined" in workflow_nodes:
                workflow.add_edge("multi_query_search_agent", "process_search_results_combined")
            else:
                # process_search_results_combinedê°€ ì—†ìœ¼ë©´ prepare_documents_and_termsë¡œ ì§ì ‘ ì—°ê²°
                workflow.add_edge("multi_query_search_agent", "prepare_documents_and_terms")
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ í›„ ì¡°ê±´ë¶€ ê²€ìƒ‰ ì‹¤í–‰
        if self.should_skip_search_adaptive_func:
            workflow.add_conditional_edges(
                "prepare_search_query",
                self.should_skip_search_adaptive_func,
                {
                    "skip": answer_generation_node,
                    "continue": "execute_searches_parallel"
                }
            )
        else:
            workflow.add_edge("prepare_search_query", "execute_searches_parallel")
        
        # ê²€ìƒ‰ ì‹¤í–‰ í›„ ê²°ê³¼ ì²˜ë¦¬
        workflow.add_edge("execute_searches_parallel", "process_search_results_combined")
        
        # ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ í›„ ë¬¸ì„œ ì¤€ë¹„
        workflow.add_edge("process_search_results_combined", "prepare_documents_and_terms")

