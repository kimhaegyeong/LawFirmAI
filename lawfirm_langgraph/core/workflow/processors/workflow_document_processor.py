# -*- coding: utf-8 -*-
"""
ì›Œí¬í”Œë¡œìš° ë¬¸ì„œ ì²˜ë¦¬ í”„ë¡œì„¸ì„œ
ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ì„ íƒ, ì»¨í…ìŠ¤íŠ¸ ë¹Œë”©, í”„ë¡¬í”„íŠ¸ ìµœì í™” ë“±ì„ ë‹´ë‹¹
"""

import logging
import re
import sys
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class WorkflowDocumentProcessor:
    """ì›Œí¬í”Œë¡œìš° ë¬¸ì„œ ì²˜ë¦¬ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, logger: Optional[logging.Logger] = None, query_enhancer=None, semantic_search_engine=None):
        self.logger = logger or logging.getLogger(__name__)
        self.query_enhancer = query_enhancer
        self.semantic_search_engine = semantic_search_engine
    
    def _extract_doc_content(self, doc: Dict[str, Any]) -> str:
        """ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ (ê°•í™”ëœ ë²„ì „)"""
        
        # 1. ê¸°ë³¸ í•„ë“œ í™•ì¸
        content = doc.get("content") or doc.get("text") or doc.get("content_text")
        
        # 2. metadataì—ì„œ í™•ì¸
        if not content:
            metadata = doc.get("metadata", {})
            if isinstance(metadata, dict):
                content = metadata.get("content") or metadata.get("text")
        
        # 3. contentê°€ ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ë³€í™˜ ì‹œë„
        if content and not isinstance(content, str):
            try:
                content = str(content)
            except Exception:
                content = ""
        
        # 4. ë‚´ìš©ì´ ë¹„ì–´ìˆìœ¼ë©´ DBì—ì„œ ë³µì› ì‹œë„
        if not content or len(content.strip()) < 10:
            doc_id = doc.get("id") or doc.get("doc_id") or doc.get("document_id")
            chunk_id = doc.get("chunk_id")
            
            if doc_id or chunk_id:
                try:
                    if self.semantic_search_engine and hasattr(self.semantic_search_engine, '_ensure_text_content'):
                        restored_content = self.semantic_search_engine._ensure_text_content(doc)
                        if restored_content and len(restored_content.strip()) >= 10:
                            content = restored_content
                            doc["content"] = content
                            self.logger.debug(f"âœ… [CONTENT RESTORE] ë¬¸ì„œ ë‚´ìš© ë³µì› ì„±ê³µ: doc_id={doc_id}")
                except Exception as e:
                    self.logger.debug(f"ë¬¸ì„œ ë‚´ìš© ë³µì› ì‹¤íŒ¨: {e}")
        
        # 5. ìµœì¢… ê²€ì¦
        if not content or len(content.strip()) < 10:
            self.logger.warning(
                f"âš ï¸ [CONTENT EXTRACT] ë¬¸ì„œ ë‚´ìš© ë¶€ì¡±: "
                f"doc_id={doc.get('id', 'unknown')}, "
                f"content_len={len(content) if content else 0}, "
                f"keys={list(doc.keys())[:10]}"
            )
        
        return content or ""
    
    def _deduplicate_documents(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì¤‘ë³µ ë¬¸ì„œ ì œê±° (ê°™ì€ content ë˜ëŠ” ê°™ì€ source_idë¥¼ ê°€ì§„ ë¬¸ì„œ)
        """
        seen_content = set()
        seen_source_ids = set()
        deduplicated = []
        
        for doc in documents:
            content = self._extract_doc_content(doc)
            source_id = doc.get("source_id") or doc.get("id") or doc.get("chunk_id")
            
            # content í•´ì‹œë¡œ ì¤‘ë³µ í™•ì¸ (ì²˜ìŒ 500ìë§Œ í•´ì‹œ)
            content_hash = hash(content[:500]) if content else None
            if content_hash and content_hash in seen_content:
                self.logger.debug(f"ì¤‘ë³µ ë¬¸ì„œ ì œê±° (content): source_id={source_id}")
                continue
            
            # source_idë¡œ ì¤‘ë³µ í™•ì¸
            if source_id and source_id in seen_source_ids:
                self.logger.debug(f"ì¤‘ë³µ ë¬¸ì„œ ì œê±° (source_id): source_id={source_id}")
                continue
            
            if content_hash:
                seen_content.add(content_hash)
            if source_id:
                seen_source_ids.add(source_id)
            
            deduplicated.append(doc)
        
        if len(documents) != len(deduplicated):
            self.logger.info(
                f"ì¤‘ë³µ ë¬¸ì„œ ì œê±°: {len(documents)}ê°œ â†’ {len(deduplicated)}ê°œ "
                f"({len(documents) - len(deduplicated)}ê°œ ì œê±°ë¨)"
            )
        
        return deduplicated
    
    def build_prompt_optimized_context(
        self,
        retrieved_docs: List[Dict[str, Any]],
        query: str,
        extracted_keywords: List[str],
        query_type: str,
        legal_field: str,
        select_balanced_documents_func=None,
        extract_query_relevant_sentences_func=None,
        generate_document_based_instructions_func=None
    ) -> Dict[str, Any]:
        """í”„ë¡¬í”„íŠ¸ì— ìµœëŒ€í•œ ë°˜ì˜ë˜ë„ë¡ ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•"""
        try:
            if not retrieved_docs:
                self.logger.warning("build_prompt_optimized_context: retrieved_docs is empty")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }
            
            valid_docs = []
            invalid_docs_count = 0
            
            # ì§ˆì˜ì™€ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ relevance_score ë¡œê¹… (ëª¨ë“  ë¬¸ì„œ)
            self.logger.info(f"ğŸ“Š [RELEVANCE SCORES] ì§ˆì˜: '{query}'")
            self.logger.info(f"ğŸ“Š [RELEVANCE SCORES] ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")
            
            # ê°œì„ : ë™ì  ì„ê³„ê°’ ì¡°ì • (ê²€ìƒ‰ ê²°ê³¼ ì ìˆ˜ ë¶„í¬ ë¶„ì„) - ê°œì„  ë²„ì „
            scores = [doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0) 
                     for doc in retrieved_docs if isinstance(doc, dict)]
            
            # ëª¨ë“  ë¬¸ì„œì˜ ì ìˆ˜ ìƒì„¸ ë¡œê¹…
            doc_scores = []
            for doc in retrieved_docs:
                if not isinstance(doc, dict):
                    continue
                score = doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)
                similarity = doc.get("similarity", 0.0)
                keyword_score = doc.get("keyword_match_score", 0.0)
                doc_id = doc.get("id") or doc.get("doc_id") or doc.get("document_id") or "unknown"
                doc_type = doc.get("type") or doc.get("source_type", "unknown")
                source = doc.get("source", "")[:100] or "unknown"
                content_preview = (doc.get("content", "")[:100] or "").replace("\n", " ")
                doc_scores.append((score, similarity, keyword_score, doc_id, doc_type, source, content_preview, doc))
            
            # ì ìˆ˜ ë¶„í¬ í†µê³„
            if doc_scores:
                scores_only = [s[0] for s in doc_scores]
                avg_score = sum(scores_only) / len(scores_only)
                max_score = max(scores_only)
                min_score = min(scores_only)
                median_score = sorted(scores_only)[len(scores_only) // 2]
                self.logger.info(
                    f"ğŸ“Š [SCORE STATS] í‰ê· ={avg_score:.3f}, ìµœëŒ€={max_score:.3f}, ìµœì†Œ={min_score:.3f}, ì¤‘ì•™ê°’={median_score:.3f}"
                )
                
                # ëª¨ë“  ë¬¸ì„œì˜ ì ìˆ˜ ìƒì„¸ ë¡œê¹… (ì •ë ¬ëœ ìˆœì„œ)
                doc_scores_sorted = sorted(doc_scores, key=lambda x: x[0], reverse=True)
                self.logger.info(f"ğŸ“Š [ALL DOCS SCORES] ëª¨ë“  {len(doc_scores_sorted)}ê°œ ë¬¸ì„œì˜ relevance_score:")
                for i, (score, similarity, keyword_score, doc_id, doc_type, source, content_preview, doc) in enumerate(doc_scores_sorted, 1):
                    self.logger.info(
                        f"   {i}. final_score={score:.3f}, similarity={similarity:.3f}, keyword={keyword_score:.3f}, "
                        f"type={doc_type}, id={doc_id[:50]}, source={source}, "
                        f"content_preview={content_preview}"
                    )
            
            # avg_scoreë¥¼ ì™¸ë¶€ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë¯¸ë¦¬ ì •ì˜
            avg_score = 0.0
            if scores:
                import statistics
                avg_score = sum(scores) / len(scores)
                max_score = max(scores)
                min_score = min(scores)
                score_range = max_score - min_score
                
                # í‘œì¤€í¸ì°¨ ê³„ì‚° (ë” ì •êµí•œ ë¶„í¬ ë¶„ì„)
                try:
                    std_dev = statistics.stdev(scores) if len(scores) > 1 else 0.0
                except Exception:
                    std_dev = 0.0
                
                # ë¶„ìœ„ìˆ˜ ê³„ì‚° (25%, 50%, 75%)
                sorted_scores = sorted(scores)
                q25_idx = int(len(sorted_scores) * 0.25)
                q50_idx = int(len(sorted_scores) * 0.50)
                q75_idx = int(len(sorted_scores) * 0.75)
                q25 = sorted_scores[q25_idx] if q25_idx < len(sorted_scores) else min_score
                q50 = sorted_scores[q50_idx] if q50_idx < len(sorted_scores) else avg_score
                q75 = sorted_scores[q75_idx] if q75_idx < len(sorted_scores) else max_score
                
                # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì— ë”°ë¥¸ ì„ê³„ê°’ ì¡°ì •
                num_results = len(retrieved_docs)
                if num_results < 5:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ë§¤ìš° ì ìœ¼ë©´ ì„ê³„ê°’ì„ í¬ê²Œ ì™„í™”
                    threshold_adjustment = -0.15
                elif num_results < 10:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì ìœ¼ë©´ ì„ê³„ê°’ì„ ì™„í™”
                    threshold_adjustment = -0.10
                elif num_results < 20:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ë³´í†µì´ë©´ ì•½ê°„ ì™„í™”
                    threshold_adjustment = -0.05
                else:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ì¡°ì • ì—†ìŒ
                    threshold_adjustment = 0.0
                
                # ì ìˆ˜ ë¶„í¬ì— ë”°ë¼ ë™ì  ì„ê³„ê°’ ê³„ì‚° (ê°œì„ ëœ ë¡œì§)
                # ì‹¤ì œ ì ìˆ˜ ë²”ìœ„ë¥¼ ê³ ë ¤í•˜ì—¬ thresholdë¥¼ ë” ë‚®ê²Œ ì„¤ì •
                # avg_scoreê°€ ë‚®ìœ¼ë©´(0.2 ë¯¸ë§Œ) ì„ê³„ê°’ì„ ë” ë‚®ì¶¤
                if avg_score < 0.20:
                    # í‰ê·  ì ìˆ˜ê°€ ë§¤ìš° ë‚®ìœ¼ë©´ ìµœì†Œê°’ ê¸°ì¤€ìœ¼ë¡œ ë§¤ìš° ë‚®ê²Œ ì„¤ì •
                    # ìµœì†Œê°’ì˜ 95% ì´ìƒì„ í¬í•¨í•˜ë„ë¡ (ê±°ì˜ ëª¨ë“  ë¬¸ì„œ í¬í•¨)
                    dynamic_threshold = max(0.10, min_score * 0.95 + threshold_adjustment)
                    self.logger.info(f"ğŸ“Š [LOW SCORE] Average score is very low ({avg_score:.3f}), using minimum-based threshold: {dynamic_threshold:.3f}")
                elif score_range < 0.15:
                    # ì ìˆ˜ê°€ ë§¤ìš° ë¹„ìŠ·í•˜ë©´ ìµœì†Œê°’ ê¸°ì¤€ìœ¼ë¡œ ë‚®ì¶¤ (ìµœì†Œê°’ì˜ 90% ì´ìƒ)
                    dynamic_threshold = max(0.12, min_score * 0.90 + threshold_adjustment)
                elif score_range < 0.25:
                    # ì ìˆ˜ê°€ ë¹„ìŠ·í•˜ë©´ 25% ë¶„ìœ„ìˆ˜ ê¸°ì¤€ (ë” ë‚®ê²Œ)
                    dynamic_threshold = max(0.15, q25 - 0.05 + threshold_adjustment)
                elif score_range < 0.4:
                    # ì ìˆ˜ ì°¨ì´ê°€ ì¤‘ê°„ì´ë©´ í‰ê·  ê¸°ì¤€ (í‘œì¤€í¸ì°¨ ê³ ë ¤, ë” ë‚®ê²Œ)
                    if std_dev > 0.1:
                        # ë¶„ì‚°ì´ í¬ë©´ í‰ê·  - í‘œì¤€í¸ì°¨ * 1.5 (ë” ì™„í™”)
                        dynamic_threshold = max(0.15, avg_score - std_dev * 1.5 + threshold_adjustment)
                    else:
                        # ë¶„ì‚°ì´ ì‘ìœ¼ë©´ í‰ê·  - 0.10 (ë” ì™„í™”)
                        dynamic_threshold = max(0.15, avg_score - 0.10 + threshold_adjustment)
                else:
                    # ì ìˆ˜ ì°¨ì´ê°€ í¬ë©´ ì¤‘ìœ„ìˆ˜ ê¸°ì¤€ (ì´ìƒì¹˜ ì˜í–¥ ìµœì†Œí™”, ë” ë‚®ê²Œ)
                    dynamic_threshold = max(0.20, q50 - 0.05 + threshold_adjustment)
                
                threshold_msg = (
                    f"ğŸ“Š [DYNAMIC THRESHOLD] avg={avg_score:.3f}, "
                    f"std={std_dev:.3f}, range={score_range:.3f}, "
                    f"q25={q25:.3f}, q50={q50:.3f}, q75={q75:.3f}, "
                    f"num_results={num_results}, threshold={dynamic_threshold:.3f}"
                )
                print(threshold_msg, flush=True, file=sys.stdout)
                self.logger.info(threshold_msg)
            else:
                dynamic_threshold = 0.35
            
            # ê°œì„  1, 4: ë¬¸ì„œ íƒ€ì…ë³„ í•„í„°ë§ ê¸°ì¤€ ì°¨ë“±í™” (ë™ì  ì„ê³„ê°’ ì ìš© - ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ )
            # ì‹¤ì œ ì ìˆ˜ ë²”ìœ„ë¥¼ ê³ ë ¤í•˜ì—¬ ë” ì™„í™”ëœ ê¸°ì¤€ ì ìš©
            # avg_scoreê°€ ë‚®ìœ¼ë©´(0.2 ë¯¸ë§Œ) ëª¨ë“  íƒ€ì…ì˜ ê¸°ì¤€ì„ ë” ë‚®ì¶¤
            if avg_score < 0.20:
                # í‰ê·  ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ëª¨ë“  íƒ€ì…ì˜ ê¸°ì¤€ì„ ë§¤ìš° ë‚®ê²Œ ì„¤ì •
                min_relevance_score_semantic = max(0.10, dynamic_threshold - 0.05)
                min_relevance_score_keyword = max(0.10, dynamic_threshold - 0.05)
                min_relevance_score_statute_article = max(0.08, dynamic_threshold - 0.12)
                min_relevance_score_precedent = max(0.10, dynamic_threshold - 0.05)
                min_relevance_score_general = max(0.12, dynamic_threshold - 0.08)
                self.logger.info(f"ğŸ“Š [LOW SCORE FILTER] Using relaxed thresholds due to low average score ({avg_score:.3f})")
            else:
                # í‰ê·  ì ìˆ˜ê°€ ì •ìƒì´ë©´ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
                min_relevance_score_semantic = max(0.15, dynamic_threshold - 0.05)
                min_relevance_score_keyword = max(0.15, dynamic_threshold - 0.05)
                min_relevance_score_statute_article = max(0.10, dynamic_threshold - 0.10)
                min_relevance_score_precedent = max(0.15, dynamic_threshold - 0.05)
                min_relevance_score_general = max(0.20, dynamic_threshold)
            
            # ê°œì„  7: ì§ˆë¬¸ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
            query_lower = query.lower()
            query_keywords = []
            for keyword in extracted_keywords:
                if keyword and len(keyword) > 1:
                    query_keywords.append(keyword.lower())
            
            # ê°œì„  2.2: ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ ë° ê²€ì¦ (ê°•í™”ëœ ë²„ì „)
            valid_docs_for_prompt = []
            for doc in retrieved_docs:
                if not isinstance(doc, dict):
                    invalid_docs_count += 1
                    continue
                
                # _extract_doc_content ì‚¬ìš© (ê°•í™”ëœ ë‚´ìš© ì¶”ì¶œ)
                content = self._extract_doc_content(doc)
                
                # ìµœì†Œ ê¸¸ì´ ê²€ì¦
                if content and len(content.strip()) >= 10:
                    valid_docs_for_prompt.append({
                        **doc,
                        "content": content  # í™•ì‹¤íˆ content í•„ë“œ ì„¤ì •
                    })
                else:
                    self.logger.warning(
                        f"âš ï¸ [PROMPT BUILD] ë¬¸ì„œ ì œì™¸ (ë‚´ìš© ë¶€ì¡±): "
                        f"doc_id={doc.get('id', 'unknown')}, "
                        f"content_len={len(content) if content else 0}"
                    )
                    invalid_docs_count += 1
            
            # ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê²½ê³  ë° í´ë°±
            if not valid_docs_for_prompt:
                self.logger.error(
                    f"âŒ [PROMPT BUILD] ìœ íš¨í•œ ë¬¸ì„œ ì—†ìŒ: "
                    f"retrieved_docs={len(retrieved_docs)}, "
                    f"valid_docs=0"
                )
                # í´ë°±: ì›ë³¸ ë¬¸ì„œì—ì„œ ìµœì†Œí•œì˜ ë‚´ìš©ì´ë¼ë„ ì¶”ì¶œ
                for doc in retrieved_docs[:5]:  # ìµœëŒ€ 5ê°œë§Œ ì‹œë„
                    if not isinstance(doc, dict):
                        continue
                    content = str(doc.get("content", "")) + str(doc.get("text", ""))
                    if len(content.strip()) >= 5:  # ìµœì†Œ ê¸¸ì´ ì™„í™”
                        valid_docs_for_prompt.append({**doc, "content": content})
            
            if not valid_docs_for_prompt:
                self.logger.error(
                    f"âŒ [PROMPT BUILD] í´ë°± í›„ì—ë„ ìœ íš¨í•œ ë¬¸ì„œ ì—†ìŒ"
                )
                return {
                    "prompt_optimized_text": f"ì§ˆë¬¸: {query}\n\nì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }
            
            # valid_docs_for_promptë¥¼ ì‚¬ìš©í•˜ì—¬ í•„í„°ë§ ë° ì ìˆ˜ ê²€ì¦ ì§„í–‰
            retrieved_docs = valid_docs_for_prompt  # ê²€ì¦ëœ ë¬¸ì„œ ì‚¬ìš©
            
            for doc in retrieved_docs:
                content = doc.get("content", "")
                if not content or len(content.strip()) < 10:
                    invalid_docs_count += 1
                    continue
                
                search_type = doc.get("search_type", "semantic")
                relevance_score = doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)
                keyword_match_score = doc.get("keyword_match_score", 0.0)
                matched_keywords = doc.get("matched_keywords", [])
                has_keyword_match = keyword_match_score > 0.0 or len(matched_keywords) > 0
                
                # ë¬¸ì„œ íƒ€ì… ë° ì†ŒìŠ¤ íƒ€ì… ì •ì˜ (doc_type ì˜¤ë¥˜ ìˆ˜ì •)
                doc_type = doc.get("type") or doc.get("source_type", "unknown")
                source_type = doc.get("source_type") or doc.get("type", "unknown")
                is_legal_doc = (
                    "ë²•" in content[:200] or
                    "ì¡°ë¬¸" in content[:200] or
                    "íŒë¡€" in content[:200] or
                    "ëŒ€ë²•ì›" in content[:200] or
                    doc_type in ["statute_article", "case_paragraph", "decision_paragraph", "interpretation_paragraph"] or
                    source_type in ["statute_article", "case_paragraph", "decision_paragraph", "interpretation_paragraph"]
                )
                
                # ê°œì„  6: í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§ (ë”ìš± ì™„í™”)
                if keyword_match_score == 0.0 and not matched_keywords:
                    content_lower = content.lower()
                    has_query_keyword = False
                    for qkw in query_keywords:
                        if qkw in content_lower:
                            has_query_keyword = True
                            break
                    
                    # ê´€ë ¨ì„± ì„ê³„ê°’ ë”ìš± ì™„í™” (ë²•ë ¹/íŒë¡€ ë¬¸ì„œëŠ” 0.30, ê¸°íƒ€ëŠ” 0.40)
                    relevance_threshold = 0.30 if is_legal_doc else 0.40
                    if not has_query_keyword and relevance_score < relevance_threshold:
                        invalid_docs_count += 1
                        self.logger.debug(
                            f"Document filtered: no keyword match and low relevance "
                            f"(relevance: {relevance_score:.3f}, threshold: {relevance_threshold}, source: {doc.get('source', 'Unknown')})"
                        )
                        continue
                
                # ë¬¸ì„œ íƒ€ì… í™•ì¸
                is_statute_article = (
                    doc_type == "statute_article" or 
                    source_type == "statute_article" or
                    "statute_article" in doc_type or
                    "statute_article" in source_type or
                    doc.get("direct_match", False) or
                    search_type == "direct_statute"
                )
                is_precedent = (
                    doc_type == "precedent" or
                    source_type == "precedent" or
                    "precedent" in doc_type or
                    "precedent" in source_type or
                    "case_paragraph" in doc_type or
                    "case_paragraph" in source_type or
                    "íŒë¡€" in content[:200] or
                    "ëŒ€ë²•ì›" in content[:200]
                )
                # is_legal_docëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨
                
                # ê°œì„ : ë²•ë¥  ì¡°ë¬¸ í•„í„°ë§ ì˜ˆì™¸ (ìš°ì„ ìˆœìœ„ 2) - ë²•ë¥  ì¡°ë¬¸ì€ ê´€ë ¨ë„ì™€ ë¬´ê´€í•˜ê²Œ í¬í•¨
                if is_statute_article:
                    # ë²•ë¥  ì¡°ë¬¸ì€ í•­ìƒ í¬í•¨ (ê´€ë ¨ë„ ì ìˆ˜ ë¬´ì‹œ)
                    print(f"[STATUTE EXCEPTION] ë²•ë¥  ì¡°ë¬¸ í¬í•¨ (ê´€ë ¨ë„ ë¬´ì‹œ): source={doc.get('source', 'Unknown')}, relevance={relevance_score:.3f}", flush=True, file=sys.stdout)
                    self.logger.debug(
                        f"âœ… [STATUTE EXCEPTION] ë²•ë¥  ì¡°ë¬¸ í¬í•¨ (ê´€ë ¨ë„ ë¬´ì‹œ): "
                        f"source={doc.get('source', 'Unknown')}, relevance={relevance_score:.3f}"
                    )
                    valid_docs.append(doc)
                    continue
                
                # ê°œì„  4: ë¬¸ì„œ íƒ€ì…ë³„ í•„í„°ë§ ê¸°ì¤€ ì°¨ë“±í™” (í‚¤ì›Œë“œ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ì™„í™”)
                if is_precedent:
                    min_score = min_relevance_score_precedent
                elif search_type == "keyword" and has_keyword_match:
                    min_score = min_relevance_score_keyword
                elif search_type == "semantic":
                    min_score = min_relevance_score_semantic
                else:
                    min_score = min_relevance_score_general
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ê¸°ì¤€ì„ ë” ì™„í™” (ê²€ìƒ‰ í’ˆì§ˆ ê°œì„  - avg_scoreê°€ ë‚®ìœ¼ë©´ ë” ì™„í™”)
                if has_keyword_match or has_query_keyword:
                    if avg_score < 0.25:
                        # í‰ê·  ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ë§¤ìš° ì™„í™”ëœ ê¸°ì¤€ ì‚¬ìš© (0.20 â†’ 0.25ë¡œ í™•ì¥)
                        min_score = max(0.08, min_score - 0.15)
                    else:
                        min_score = max(0.15, min_score - 0.10)
                
                # ì²« ë²ˆì§¸ í•„í„°ë§(í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ì„ ë•Œ)ì„ í†µê³¼í•œ ê²½ìš°, ë‘ ë²ˆì§¸ í•„í„°ë§ì€ ë” ì™„í™”
                if not has_keyword_match and not has_query_keyword:
                    # relevance_score >= 0.30 ì¡°ê±´ ì œê±° (avg_scoreê°€ ë‚®ìœ¼ë©´ ëª¨ë“  ë¬¸ì„œì— ì ìš©)
                    if avg_score < 0.25:
                        # í‰ê·  ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ë§¤ìš° ì™„í™”ëœ ê¸°ì¤€ ì‚¬ìš© (0.20 â†’ 0.25ë¡œ í™•ì¥)
                        min_score = max(0.10, min_score - 0.20)
                    elif relevance_score >= 0.30:
                        # í‰ê·  ì ìˆ˜ê°€ ì •ìƒì´ê³  relevance_scoreê°€ ë†’ìœ¼ë©´ ê¸°ì¡´ ë¡œì§
                        min_score = max(0.20, min_score - 0.15)
                
                if relevance_score < min_score:
                    invalid_docs_count += 1
                    self.logger.debug(
                        f"Document filtered: relevance score too low ({relevance_score:.3f} < {min_score:.3f}) "
                        f"(source: {doc.get('source', 'Unknown')}, type: {search_type}, doc_type: {doc_type}, "
                        f"has_keyword: {has_keyword_match or has_query_keyword})"
                    )
                    continue
                
                valid_docs.append(doc)
            
            if invalid_docs_count > 0:
                self.logger.warning(
                    f"build_prompt_optimized_context: Filtered {invalid_docs_count} invalid documents "
                    f"(no content, content too short, or relevance < threshold). Valid docs: {len(valid_docs)}"
                )
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì ì„ ë•Œ í•„í„°ë§ ê¸°ì¤€ ì™„í™”í•˜ì—¬ ìµœì†Œ ë¬¸ì„œ ìˆ˜ ë³´ì¥ (ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ )
            if not valid_docs and retrieved_docs:
                self.logger.warning(
                    f"build_prompt_optimized_context: No valid documents after filtering. "
                    f"Relaxing criteria to ensure minimum documents (total retrieved: {len(retrieved_docs)})"
                )
                
                # relevance_score ë¶„í¬ ë¶„ì„
                relevance_scores = []
                for doc in retrieved_docs:
                    if isinstance(doc, dict):
                        score = doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)
                        relevance_scores.append(score)
                
                if relevance_scores:
                    min_rel_score = min(relevance_scores)
                    max_rel_score = max(relevance_scores)
                    avg_rel_score = sum(relevance_scores) / len(relevance_scores)
                    
                    # ë¶„í¬ì— ë”°ë¼ ë™ì ìœ¼ë¡œ relaxed_min_score ì„¤ì •
                    if avg_rel_score < 0.20:
                        # í‰ê· ì´ ë§¤ìš° ë‚®ìœ¼ë©´ ìµœì†Œê°’ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
                        relaxed_min_score = max(0.05, min_rel_score * 0.90)
                    elif avg_rel_score < 0.30:
                        # í‰ê· ì´ ë‚®ìœ¼ë©´ í‰ê· ì˜ 80% ê¸°ì¤€
                        relaxed_min_score = max(0.08, avg_rel_score * 0.80)
                    else:
                        # í‰ê· ì´ ì •ìƒì´ë©´ ê¸°ì¡´ ë¡œì§
                        relaxed_min_score = 0.10
                    
                    self.logger.info(
                        f"ğŸ“Š [RELAXED FILTER] Score distribution - min={min_rel_score:.3f}, "
                        f"max={max_rel_score:.3f}, avg={avg_rel_score:.3f}, "
                        f"relaxed_threshold={relaxed_min_score:.3f}"
                    )
                else:
                    relaxed_min_score = 0.10
                
                # í•„í„°ë§ ê¸°ì¤€ì„ ë§¤ìš° ì™„í™”í•˜ì—¬ ì¬ì‹œë„
                for doc in retrieved_docs:
                    if not isinstance(doc, dict):
                        continue
                    
                    content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                    if not content or len(content.strip()) < 5:
                        continue
                    
                    relevance_score = doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)
                    if relevance_score >= relaxed_min_score:
                        valid_docs.append(doc)
                        if len(valid_docs) >= 5:  # ìµœì†Œ 5ê°œê¹Œì§€ëŠ” ë³´ì¥ (3ê°œ â†’ 5ê°œë¡œ ì¦ê°€)
                            break
                
                if valid_docs:
                    self.logger.info(
                        f"âœ… build_prompt_optimized_context: Relaxed criteria applied. "
                        f"Found {len(valid_docs)} documents with relaxed threshold ({relaxed_min_score:.3f})"
                    )
            
            if not valid_docs:
                self.logger.error("build_prompt_optimized_context: No valid documents with content found even after relaxing criteria")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }
            
            # ê°œì„ : Keyword Coverage ê¸°ë°˜ í•„í„°ë§ (Phase 1) - ë¬¸ì„œ ì†ì‹¤ ë°©ì§€ (ë” ì™„í™”)
            docs_before_filter = len(valid_docs)
            if extracted_keywords:
                # ë™ì  ì„ê³„ê°’ ê³„ì‚° (ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì— ë”°ë¼ ì¡°ì •) - ë§¤ìš° ì™„í™”ëœ ê¸°ì¤€
                num_valid_docs = len(valid_docs)
                if num_valid_docs >= 10:
                    min_coverage = 0.1  # ê°œì„ : 0.2 â†’ 0.1ë¡œ ë” ì™„í™”
                elif num_valid_docs >= 5:
                    min_coverage = 0.05  # ê°œì„ : 0.1 â†’ 0.05ë¡œ ë” ì™„í™”
                else:
                    min_coverage = 0.0  # ê°œì„ : 0.05 â†’ 0.0ìœ¼ë¡œ ì™„ì „ ì™„í™” (ê²°ê³¼ê°€ ì ìœ¼ë©´ í•„í„°ë§ ì•ˆ í•¨)
                
                # ë¬¸ì„œê°€ 10ê°œ ì´í•˜ì¸ ê²½ìš° í•„í„°ë§ ê±´ë„ˆë›°ê¸° (ë¬¸ì„œ ì†ì‹¤ ë°©ì§€)
                if num_valid_docs <= 10:
                    self.logger.debug(
                        f"ğŸ” [KEYWORD FILTERING] Skipping keyword coverage filter "
                        f"(documents={num_valid_docs} <= 10, preventing document loss)"
                    )
                else:
                    valid_docs = self.filter_by_keyword_coverage(
                        valid_docs,
                        extracted_keywords,
                        min_coverage=min_coverage
                    )
                
                # ë¬¸ì„œ ì†ì‹¤ ë¡œê¹…
                docs_after_filter = len(valid_docs)
                if docs_after_filter < docs_before_filter:
                    lost_count = docs_before_filter - docs_after_filter
                    self.logger.warning(
                        f"âš ï¸ [DOCUMENT LOSS] filter_by_keyword_coverage: {lost_count} documents lost "
                        f"({docs_before_filter} â†’ {docs_after_filter}, min_coverage={min_coverage})"
                    )
            
            # textToSQL ê²°ê³¼ì™€ ë²¡í„° ì„ë² ë”© ê²°ê³¼ ë¶„ë¦¬
            text2sql_docs = []
            vector_docs = []
            seen_ids = set()
            
            for doc in valid_docs:
                doc_id = doc.get("id") or doc.get("document_id") or doc.get("doc_id") or str(doc.get("source", ""))
                if doc_id in seen_ids:
                    continue
                seen_ids.add(doc_id)
                
                # textToSQL ê²°ê³¼ íŒë³„
                search_type = doc.get("search_type", "")
                direct_match = doc.get("direct_match", False)
                is_text2sql = (
                    search_type == "text2sql" or
                    search_type == "direct_statute" or
                    direct_match is True or
                    (doc.get("type") == "statute_article" and doc.get("statute_name") and doc.get("article_no"))
                )
                
                if is_text2sql:
                    text2sql_docs.append(doc)
                else:
                    vector_docs.append(doc)
            
            # ìš°ì„ ìˆœìœ„ 5: ë²¡í„° ê²°ê³¼ ê´€ë ¨ì„± ì ìˆ˜ ë™ì  ì„ê³„ê°’ ì ìš©
            # ìš°ì„ ìˆœìœ„ 7: ì„±ëŠ¥ ìµœì í™” - ì ìˆ˜ ê³„ì‚° ìºì‹±
            if vector_docs:
                # ì ìˆ˜ ê³„ì‚°ì„ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ì—¬ ì¬ì‚¬ìš©
                doc_scores = []
                for doc in vector_docs:
                    score = doc.get("final_weighted_score", doc.get("relevance_score", 0.0))
                    doc_scores.append((doc, score))
                
                scores = [score for _, score in doc_scores]
                avg_score = sum(scores) / len(scores) if scores else 0.0
                max_score = max(scores) if scores else 0.0
                min_score = min(scores) if scores else 0.0
                
                # ë™ì  ì„ê³„ê°’ ê³„ì‚°: í‰ê·  ì ìˆ˜ì˜ 80% ë˜ëŠ” ìµœì†Œ 0.60
                dynamic_threshold = max(0.60, min(0.75, avg_score * 0.8))
                
                # ì ìˆ˜ ë¶„í¬ê°€ ë‚®ìœ¼ë©´ ì„ê³„ê°’ ì™„í™”
                if avg_score < 0.70:
                    dynamic_threshold = max(0.50, avg_score * 0.7)
                
                # statute_article íƒ€ì…ì€ ë” ë‚®ì€ ì„ê³„ê°’ ì ìš©
                statute_docs = [d for d in vector_docs if (d.get("type") == "statute_article" or d.get("source_type") == "statute_article")]
                if statute_docs:
                    statute_threshold = max(0.40, dynamic_threshold * 0.8)
                else:
                    statute_threshold = dynamic_threshold
            else:
                dynamic_threshold = 0.75
                statute_threshold = 0.60
            
            # ìš°ì„ ìˆœìœ„ 6: ì„±ëŠ¥ ìµœì í™” - ê²€ì¦ ê²°ê³¼ ìºì‹± ë° ë°°ì¹˜ ì²˜ë¦¬
            filtered_vector_docs = []
            validation_cache = {}  # doc_id -> validation_result
            
            for doc, score in doc_scores:
                doc_type = doc.get("type") or doc.get("source_type", "")
                
                # íƒ€ì…ë³„ ì°¨ë“± ì„ê³„ê°’ ì ìš©
                threshold = statute_threshold if doc_type == "statute_article" else dynamic_threshold
                
                if score >= threshold:
                    # ìš°ì„ ìˆœìœ„ 6: ê²€ì¦ ê²°ê³¼ ìºì‹± (ë™ì¼ ë¬¸ì„œ ì¬ê²€ì¦ ë°©ì§€)
                    doc_id = doc.get("id") or doc.get("doc_id") or str(doc.get("source", ""))
                    if doc_id in validation_cache:
                        if validation_cache[doc_id]:
                            filtered_vector_docs.append(doc)
                        continue
                    
                    # ìš°ì„ ìˆœìœ„ 2: ë©”íƒ€ë°ì´í„° ê²€ì¦
                    metadata_valid = self._validate_document_metadata(doc)
                    if not metadata_valid:
                        validation_cache[doc_id] = False
                        continue
                    
                    # ìš°ì„ ìˆœìœ„ 3: ë‚´ìš© í’ˆì§ˆ ê²€ì¦
                    content = doc.get("content") or doc.get("text", "")
                    content_valid = self._validate_document_content_quality(doc, content)
                    if not content_valid:
                        validation_cache[doc_id] = False
                        continue
                    
                    # ìš°ì„ ìˆœìœ„ 3: ì¶œì²˜ ì‹ ë¢°ë„ ê²€ì¦
                    source_valid = self._validate_document_source_reliability(doc)
                    if not source_valid:
                        validation_cache[doc_id] = False
                        continue
                    
                    # ëª¨ë“  ê²€ì¦ í†µê³¼
                    validation_cache[doc_id] = True
                    filtered_vector_docs.append(doc)
            
            # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ì„ê³„ê°’ì„ ì ì§„ì ìœ¼ë¡œ ë‚®ì¶¤
            min_docs_needed = 3
            if len(filtered_vector_docs) < min_docs_needed and len(vector_docs) >= min_docs_needed:
                # ì„ê³„ê°’ì„ 0.1ì”© ë‚®ì¶°ê°€ë©° ì¬ì‹œë„
                for relaxed_threshold in [dynamic_threshold - 0.1, dynamic_threshold - 0.2, 0.30]:
                    if len(filtered_vector_docs) >= min_docs_needed:
                        break
                    for doc in vector_docs:
                        if doc in filtered_vector_docs:
                            continue
                        score = doc.get("final_weighted_score", doc.get("relevance_score", 0.0))
                        if score >= relaxed_threshold:
                            # ê²€ì¦ì€ ì™„í™”ëœ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰
                            content = doc.get("content") or doc.get("text", "")
                            if content and len(content.strip()) >= 5:  # ìµœì†Œ ê¸¸ì´ë§Œ í™•ì¸
                                filtered_vector_docs.append(doc)
                                if len(filtered_vector_docs) >= min_docs_needed:
                                    break
                if len(filtered_vector_docs) < min_docs_needed:
                    self.logger.warning(
                        f"âš ï¸ [VECTOR FILTER] ìµœì†Œ ë¬¸ì„œ ìˆ˜ ë¯¸ë‹¬: {len(filtered_vector_docs)}ê°œ (ëª©í‘œ: {min_docs_needed}ê°œ)"
                    )
            
            # ìš°ì„ ìˆœìœ„ 7: ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ê°œì„  - ìƒì„¸ í•„í„°ë§ í†µê³„
            if len(filtered_vector_docs) < len(vector_docs):
                filtered_count = len(vector_docs) - len(filtered_vector_docs)
                filter_reasons = {
                    "threshold": 0,
                    "metadata": 0,
                    "content": 0,
                    "source": 0
                }
                
                # í•„í„°ë§ ì‚¬ìœ ë³„ í†µê³„ (ê°„ë‹¨í•œ ì¶”ì •)
                for doc, score in doc_scores:
                    if doc not in filtered_vector_docs:
                        doc_type = doc.get("type") or doc.get("source_type", "")
                        threshold = statute_threshold if doc_type == "statute_article" else dynamic_threshold
                        if score < threshold:
                            filter_reasons["threshold"] += 1
                        else:
                            # ê²€ì¦ ì‹¤íŒ¨ ì›ì¸ ì¶”ì •
                            doc_id = doc.get("id") or doc.get("doc_id") or str(doc.get("source", ""))
                            if doc_id in validation_cache and not validation_cache[doc_id]:
                                # ì–´ë–¤ ê²€ì¦ì´ ì‹¤íŒ¨í–ˆëŠ”ì§€ í™•ì¸ (ê°„ë‹¨í•œ ì¶”ì •)
                                if not self._validate_document_metadata(doc):
                                    filter_reasons["metadata"] += 1
                                elif not self._validate_document_content_quality(doc, doc.get("content") or doc.get("text", "")):
                                    filter_reasons["content"] += 1
                                elif not self._validate_document_source_reliability(doc):
                                    filter_reasons["source"] += 1
                
                self.logger.info(
                    f"ğŸ”€ [VECTOR FILTER] ê´€ë ¨ì„± ì ìˆ˜ í•„í„°ë§: "
                    f"{len(vector_docs)}ê°œ â†’ {len(filtered_vector_docs)}ê°œ "
                    f"(ë™ì  ì„ê³„ê°’: {dynamic_threshold:.2f}, statute: {statute_threshold:.2f})"
                )
                self.logger.info(
                    f"ğŸ“Š [FILTER STATS] í•„í„°ë§ ì‚¬ìœ ë³„ í†µê³„: "
                    f"ì„ê³„ê°’={filter_reasons['threshold']}, "
                    f"ë©”íƒ€ë°ì´í„°={filter_reasons['metadata']}, "
                    f"ë‚´ìš©={filter_reasons['content']}, "
                    f"ì¶œì²˜={filter_reasons['source']}"
                )
            
            # ë²¡í„° ì„ë² ë”© ê²°ê³¼ë§Œ ì¬ë­í‚¹
            sorted_vector_docs = sorted(
                filtered_vector_docs,
                key=lambda x: (
                    x.get("final_weighted_score", x.get("relevance_score", 0.0)),
                    x.get("similarity", 0.0),
                    x.get("keyword_match_score", 0.0)
                ),
                reverse=True
            )
            
            # textToSQL ê²°ê³¼ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í¬í•¨
            max_docs_for_prompt = 10
            text2sql_count = len(text2sql_docs)
            max_vector_docs = max(0, max_docs_for_prompt - text2sql_count)
            
            # ë²¡í„° ê²°ê³¼ ì„ íƒ (ê´€ë ¨ì„± ìš°ì„ )
            if select_balanced_documents_func and sorted_vector_docs:
                selected_vector_docs = select_balanced_documents_func(
                    sorted_vector_docs, max_docs=max_vector_docs
                )
            else:
                selected_vector_docs = self.select_balanced_documents_relevance_first(
                    sorted_vector_docs, 
                    query=query,
                    extracted_keywords=extracted_keywords,
                    query_type=query_type,
                    max_docs=max_vector_docs
                ) if sorted_vector_docs else []
            
            if not selected_vector_docs and sorted_vector_docs:
                selected_vector_docs = sorted_vector_docs[:max_vector_docs]
            
            # textToSQL ê²°ê³¼ + ì¬ë­í‚¹ëœ ë²¡í„° ê²°ê³¼ ê²°í•©
            sorted_docs = text2sql_docs + selected_vector_docs
            
            self.logger.info(
                f"ğŸ“‹ [FINAL DOCS] textToSQL: {len(text2sql_docs)}ê°œ, "
                f"ë²¡í„°(ì¬ë­í‚¹): {len(selected_vector_docs)}ê°œ, "
                f"ì´: {len(sorted_docs)}ê°œ"
            )
            
            # ìš°ì„ ìˆœìœ„ 7: ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ê°œì„  - ë¬¸ì„œ ì†ì‹¤ ìƒì„¸ ë¶„ì„
            if len(sorted_docs) < len(valid_docs):
                lost_count = len(valid_docs) - len(sorted_docs)
                loss_ratio = lost_count / len(valid_docs) if valid_docs else 0.0
                
                # ì†ì‹¤ëœ ë¬¸ì„œì˜ ì ìˆ˜ ë¶„í¬ ë¶„ì„
                lost_docs = [doc for doc in valid_docs if doc not in sorted_docs]
                if lost_docs:
                    lost_scores = [
                        doc.get("final_weighted_score", doc.get("relevance_score", 0.0))
                        for doc in lost_docs
                    ]
                    avg_lost_score = sum(lost_scores) / len(lost_scores) if lost_scores else 0.0
                    min_lost_score = min(lost_scores) if lost_scores else 0.0
                    max_lost_score = max(lost_scores) if lost_scores else 0.0
                    
                    self.logger.warning(
                        f"âš ï¸ [DOCUMENT LOSS] select_balanced_documents: {lost_count} documents lost "
                        f"({len(valid_docs)} â†’ {len(sorted_docs)}, max_docs={max_docs_for_prompt}, "
                        f"loss_ratio={loss_ratio:.1%})"
                    )
                    self.logger.info(
                        f"ğŸ“Š [LOST DOCS STATS] ì†ì‹¤ëœ ë¬¸ì„œ ì ìˆ˜ ë¶„í¬: "
                        f"í‰ê· ={avg_lost_score:.3f}, ìµœëŒ€={max_lost_score:.3f}, ìµœì†Œ={min_lost_score:.3f}"
                    )
                else:
                    self.logger.warning(
                        f"âš ï¸ [DOCUMENT LOSS] select_balanced_documents: {lost_count} documents lost "
                        f"({len(valid_docs)} â†’ {len(sorted_docs)}, max_docs={max_docs_for_prompt})"
                    )
            
            # ìš°ì„ ìˆœìœ„ 1 ê°œì„ : ë¹ˆ ë¬¸ì„œ ì²˜ë¦¬ - ì›ë³¸ retrieved_docsì—ì„œ ìƒìœ„ ë¬¸ì„œ ì„ íƒ
            if not sorted_docs:
                self.logger.warning("âš ï¸ [EMPTY DOCS] build_prompt_optimized_context: sorted_docs is empty after filtering")
                # Fallback: ì›ë³¸ valid_docsì—ì„œ ìƒìœ„ ë¬¸ì„œ ì„ íƒ
                if valid_docs:
                    # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë¬¸ì„œ ì„ íƒ
                    fallback_docs = sorted(
                        valid_docs,
                        key=lambda x: (
                            x.get("final_weighted_score", x.get("relevance_score", 0.0)),
                            x.get("similarity", 0.0),
                            x.get("keyword_match_score", 0.0)
                        ),
                        reverse=True
                    )[:max_docs_for_prompt]
                    sorted_docs = fallback_docs
                    self.logger.info(
                        f"ğŸ“‹ [FALLBACK] ì›ë³¸ ë¬¸ì„œì—ì„œ {len(sorted_docs)}ê°œ ì„ íƒ (fallback)"
                    )
                else:
                    self.logger.error("build_prompt_optimized_context: valid_docsë„ ë¹„ì–´ìˆìŒ")
                    return {
                        "prompt_optimized_text": "",
                        "structured_documents": {},
                        "document_count": 0,
                        "total_context_length": 0
                    }
            
            # ì¤‘ë³µ ë¬¸ì„œ ì œê±°
            sorted_docs = self._deduplicate_documents(sorted_docs)
            
            if generate_document_based_instructions_func:
                document_instructions = generate_document_based_instructions_func(
                    documents=sorted_docs,
                    query=query,
                    query_type=query_type
                )
            else:
                document_instructions = self.generate_document_based_instructions(
                    documents=sorted_docs,
                    query=query,
                    query_type=query_type
                )
            
            prompt_section = f"""## ë‹µë³€ ìƒì„± ì§€ì‹œì‚¬í•­

{document_instructions}

## ì°¸ê³  ë¬¸ì„œ ëª©ë¡

ë‹¤ìŒ {len(sorted_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
ê° ë¬¸ì„œëŠ” ê´€ë ¨ì„± ì ìˆ˜ì™€ í•µì‹¬ ë‚´ìš©ì´ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

"""
            
            for idx, doc in enumerate(sorted_docs, 1):
                relevance_score = doc.get("final_weighted_score") or doc.get("relevance_score", 0.0)
                source = doc.get("source", "Unknown")
                content = doc.get("content", "")
                original_content_length = len(content)  # ì›ë³¸ ë¬¸ì„œ ê¸¸ì´ ì €ì¥
                
                if extract_query_relevant_sentences_func:
                    relevant_sentences = extract_query_relevant_sentences_func(
                        doc_content=content,
                        query=query,
                        extracted_keywords=extracted_keywords
                    )
                elif self.query_enhancer:
                    relevant_sentences = self.query_enhancer.extract_query_relevant_sentences(
                        content, query, extracted_keywords
                    )
                else:
                    relevant_sentences = []
                
                doc_section = f"""
### ë¬¸ì„œ {idx}: {source} (ê´€ë ¨ì„± ì ìˆ˜: {relevance_score:.2f})

**í•µì‹¬ ë‚´ìš©:**
"""
                
                if relevant_sentences:
                    doc_section += "\n".join([
                        f"- [ì¤‘ìš”] {sent['sentence']}"
                        for sent in relevant_sentences[:3]
                    ])
                    doc_section += "\n\n"
                
                # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™”: í† í° ìˆ˜ ê¸°ë°˜ ë™ì  ì¡°ì •
                # í•œê¸€ ê¸°ì¤€ ëŒ€ëµ 1í† í° = 2-3ì, ì˜ì–´ ê¸°ì¤€ 1í† í° = 4ì
                # ì•ˆì „í•˜ê²Œ 1í† í° = 2.5ìë¡œ ê³„ì‚°
                max_tokens_per_doc = 600  # ë¬¸ì„œë‹¹ ìµœëŒ€ í† í° ìˆ˜
                max_content_length = int(max_tokens_per_doc * 2.5)  # ì•½ 1500ì
                
                # ì§ˆë¬¸ íƒ€ì…ë³„ ë™ì  ì¡°ì •
                if query_type == "law_inquiry":
                    max_tokens_per_doc = 800  # ë²•ë ¹ ì¡°íšŒ: ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ í—ˆìš©
                    max_content_length = int(max_tokens_per_doc * 2.5)  # ì•½ 2000ì
                elif query_type == "complex_question":
                    max_tokens_per_doc = 1000  # ë³µì¡í•œ ì§ˆë¬¸: ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ í—ˆìš©
                    max_content_length = int(max_tokens_per_doc * 2.5)  # ì•½ 2500ì
                
                # ìŠ¤ë§ˆíŠ¸ ë¬¸ì„œ ì¶•ì•½ (ê¸´ ë¬¸ì„œ ì²˜ë¦¬)
                is_truncated = False
                if len(content) > max_content_length:
                    is_truncated = True
                    content = self._smart_truncate_long_document(
                        content=content,
                        doc_type=doc.get("type", ""),
                        query=query,
                        extracted_keywords=extracted_keywords,
                        max_length=max_content_length,
                        relevant_sentences=relevant_sentences,
                        metadata=doc.get("metadata", {})
                    )
                
                # í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ê°œì„  (í•µì‹¬ ë‚´ìš© + ì „ì²´ ìš”ì•½)
                if is_truncated:
                    doc_section += f"""**í•µì‹¬ ë‚´ìš© (ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë¶€ë¶„):**
{content}

**ë¬¸ì„œ ì •ë³´:**
- ì „ì²´ ë¬¸ì„œ ê¸¸ì´: {original_content_length:,}ì
- ì¶”ì¶œëœ í•µì‹¬ ë‚´ìš©: {len(content):,}ì
- ì¶•ì•½ ë¹„ìœ¨: {len(content)/original_content_length*100:.1f}%

---
"""
                else:
                    doc_section += f"""**ì „ì²´ ë‚´ìš©:**
{content}

---
"""
                
                prompt_section += doc_section
            
            prompt_section += """
## ë¬¸ì„œ ì¸ìš© ê·œì¹™

ë‹µë³€ì—ì„œ ìœ„ ë¬¸ì„œë¥¼ ì¸ìš©í•  ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ ëª…ì‹œí•˜ì„¸ìš”:
- "ë¬¸ì„œ {0}ì— ë”°ë¥´ë©´..." ë˜ëŠ” "[{0}] ì¸ìš© ë‚´ìš©"
- ê° ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œ

## ì¤‘ìš” ì‚¬í•­

- ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”
- ë¬¸ì„œì—ì„œ ì¶”ë¡ í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì¼ê´€ëœ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”
""".format("n")
            
            content_validation = {
                "has_document_content": False,
                "total_content_length": 0,
                "documents_with_content": 0
            }
            
            # ê°œì„  2.2: ìµœì¢… ê²€ì¦ ê°•í™” (ë” ì •í™•í•œ ê²€ì¦)
            for doc in sorted_docs:
                content = self._extract_doc_content(doc)
                if content and len(content.strip()) >= 10:
                    # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë¬¸ì„œ ë‚´ìš© í¬í•¨ ì—¬ë¶€ í™•ì¸
                    content_preview = content[:200]  # ë” ê¸´ í”„ë¦¬ë·°ë¡œ í™•ì¸
                    content_middle = content[len(content)//2:len(content)//2+200] if len(content) > 400 else ""
                    doc_id = doc.get("id") or doc.get("doc_id") or doc.get("document_id", "")
                    source = doc.get("source", "") or doc.get("title", "")
                    
                    # í”„ë¡¬í”„íŠ¸ì— ë¬¸ì„œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    has_content = (
                        content_preview in prompt_section or
                        (content_middle and content_middle in prompt_section) or
                        (doc_id and f"ë¬¸ì„œ {doc_id}" in prompt_section) or
                        (source and source in prompt_section and len(content.strip()) > 0)
                    )
                    
                    if has_content:
                        content_validation["has_document_content"] = True
                        content_validation["total_content_length"] += len(content)
                        content_validation["documents_with_content"] += 1
            
            # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê²€ì¦
            if len(prompt_section.strip()) < 100:
                self.logger.error(
                    f"âŒ [PROMPT BUILD] í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ: "
                    f"length={len(prompt_section)}, "
                    f"valid_docs={len(sorted_docs)}"
                )
            
            # ë¬¸ì„œ ë‚´ìš© í¬í•¨ ì—¬ë¶€ í™•ì¸
            has_document_content = any(
                len(self._extract_doc_content(doc).strip()) >= 10 
                for doc in sorted_docs
            )
            
            # ë¬¸ì„œ ë‚´ìš©ì´ ì—†ì„ ë•Œ ì¬êµ¬ì„± ì‹œë„
            if not content_validation["has_document_content"] and len(sorted_docs) > 0:
                self.logger.error(
                    f"âŒ [PROMPT BUILD] í”„ë¡¬í”„íŠ¸ì— ë¬¸ì„œ ë‚´ìš© ì—†ìŒ: "
                    f"valid_docs={len(sorted_docs)}, "
                    f"prompt_length={len(prompt_section)}"
                )
                
                # ì¬êµ¬ì„± ì‹œë„: ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ ì¶”ê°€
                self.logger.warning(
                    f"âš ï¸ [PROMPT BUILD] ë¬¸ì„œ ë‚´ìš© ì¬êµ¬ì„± ì‹œë„ ì¤‘..."
                )
                
                # ë¬¸ì„œ ë‚´ìš©ì´ ìˆëŠ” ë¬¸ì„œë§Œ í•„í„°ë§
                docs_with_content = [
                    doc for doc in sorted_docs 
                    if len(self._extract_doc_content(doc).strip()) >= 10
                ]
                
                if docs_with_content:
                    # í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±: ë¬¸ì„œ ë‚´ìš© ì§ì ‘ ì¶”ê°€
                    reconstructed_section = "\n\n## ì°¸ê³  ë¬¸ì„œ ë‚´ìš©\n\n"
                    for idx, doc in enumerate(docs_with_content[:5], 1):
                        content = self._extract_doc_content(doc)
                        if content and len(content.strip()) >= 10:
                            source = doc.get("source", "") or doc.get("title", "") or f"ë¬¸ì„œ {idx}"
                            doc_id = doc.get("id") or doc.get("doc_id") or doc.get("document_id", f"doc_{idx}")
                            
                            reconstructed_section += f"### ë¬¸ì„œ {idx}: {source} (ID: {doc_id})\n\n"
                            reconstructed_section += f"{content[:2000]}\n\n"  # ìµœëŒ€ 2000ì
                            reconstructed_section += "---\n\n"
                            
                            content_validation["has_document_content"] = True
                            content_validation["total_content_length"] += len(content)
                            content_validation["documents_with_content"] += 1
                    
                    # ì¬êµ¬ì„±ëœ ì„¹ì…˜ì„ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ (ì—¬ëŸ¬ ìœ„ì¹˜ ì‹œë„)
                    if "## ë¬¸ì„œ ì¸ìš© ê·œì¹™" in prompt_section:
                        prompt_section = prompt_section.replace(
                            "## ë¬¸ì„œ ì¸ìš© ê·œì¹™",
                            reconstructed_section + "## ë¬¸ì„œ ì¸ìš© ê·œì¹™"
                        )
                    elif "## ì°¸ê³  ë¬¸ì„œ" in prompt_section:
                        prompt_section = prompt_section.replace(
                            "## ì°¸ê³  ë¬¸ì„œ",
                            reconstructed_section + "## ì°¸ê³  ë¬¸ì„œ"
                        )
                    elif "## ê²€ìƒ‰ëœ ë¬¸ì„œ" in prompt_section:
                        prompt_section = prompt_section.replace(
                            "## ê²€ìƒ‰ëœ ë¬¸ì„œ",
                            reconstructed_section + "## ê²€ìƒ‰ëœ ë¬¸ì„œ"
                        )
                    else:
                        # ë¬¸ì„œ ì¸ìš© ê·œì¹™ì´ ì—†ìœ¼ë©´ í”„ë¡¬í”„íŠ¸ ëì— ì¶”ê°€
                        prompt_section = prompt_section + "\n\n" + reconstructed_section
                    
                    if content_validation["has_document_content"]:
                        self.logger.info(
                            f"âœ… [PROMPT BUILD] ë¬¸ì„œ ë‚´ìš© ì¬êµ¬ì„± ì„±ê³µ: "
                            f"{content_validation['documents_with_content']}ê°œ ë¬¸ì„œ ì¶”ê°€ë¨, "
                            f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt_section)}ì"
                        )
                    else:
                        self.logger.warning(
                            f"âš ï¸ [PROMPT BUILD] ë¬¸ì„œ ë‚´ìš© ì¬êµ¬ì„± ì‹¤íŒ¨: "
                            f"ë¬¸ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ"
                        )
                else:
                    self.logger.warning(
                        f"âš ï¸ [PROMPT BUILD] ì¬êµ¬ì„±í•  ë¬¸ì„œ ì—†ìŒ: "
                        f"ëª¨ë“  ë¬¸ì„œì˜ ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ"
                    )
            else:
                self.logger.info(
                    f"âœ… [PROMPT BUILD] í”„ë¡¬í”„íŠ¸ì— ë¬¸ì„œ ë‚´ìš© í¬í•¨ë¨: "
                    f"{content_validation['documents_with_content']}ê°œ ë¬¸ì„œ, "
                    f"ì´ ë‚´ìš© ê¸¸ì´: {content_validation.get('total_content_length', 0)}ì, "
                    f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt_section)}ì"
                )
            
            if not content_validation["has_document_content"] and len(sorted_docs) > 0:
                self.logger.warning(
                    f"âš ï¸ [PROMPT BUILD] Content validation failed, but returning prompt anyway "
                    f"(may contain instructions only without actual document content)"
                )
            
            # ê°œì„  10: í”„ë¡¬í”„íŠ¸ ìƒì„± í›„ ìµœì¢… ê²€ì¦
            final_validation = self._validate_final_documents(
                sorted_docs=sorted_docs,
                query=query,
                extracted_keywords=extracted_keywords,
                query_type=query_type
            )
            
            if final_validation.get("low_relevance_warning"):
                self.logger.warning(
                    f"build_prompt_optimized_context: {final_validation['low_relevance_warning']} "
                    f"(low_relevance_count: {final_validation.get('low_relevance_count', 0)})"
                )
            
            return {
                "prompt_optimized_text": prompt_section,
                "structured_documents": {
                    "total_count": len(sorted_docs),
                    "documents": [{
                        "document_id": idx,
                        "source": doc.get("source", "Unknown"),
                        "relevance_score": doc.get("final_weighted_score") or doc.get("relevance_score", 0.0),
                        "content": (doc.get("content") or doc.get("text") or doc.get("content_text", ""))[:2000]
                    } for idx, doc in enumerate(sorted_docs, 1)]
                },
                "document_count": len(sorted_docs),
                "total_context_length": len(prompt_section),
                "content_validation": content_validation,
                "final_validation": final_validation
            }
        
        except Exception as e:
            self.logger.error(f"Prompt optimized context building failed: {e}", exc_info=True)
            return {
                "prompt_optimized_text": "",
                "structured_documents": {},
                "document_count": 0,
                "total_context_length": 0
            }
    
    def _smart_truncate_long_document(
        self,
        content: str,
        doc_type: str,
        query: str,
        extracted_keywords: List[str],
        max_length: int,
        relevant_sentences: List[Dict[str, Any]] = None,
        metadata: Dict[str, Any] = None
    ) -> str:
        """ê¸´ ë¬¸ì„œë¥¼ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ì¶•ì•½ (ìš°ì„ ìˆœìœ„ 1)"""
        if not content or len(content) <= max_length:
            return content
        
        doc_type_lower = doc_type.lower() if doc_type else ""
        metadata = metadata or {}
        
        # ë¬¸ì„œ íƒ€ì…ë³„ ìµœì í™”
        if "case" in doc_type_lower or "precedent" in doc_type_lower:
            return self._extract_precedent_key_parts(
                content=content,
                metadata=metadata,
                query=query,
                keywords=extracted_keywords,
                max_length=max_length,
                relevant_sentences=relevant_sentences
            )
        elif "statute" in doc_type_lower:
            return self._extract_statute_key_parts(
                content=content,
                metadata=metadata,
                query=query,
                keywords=extracted_keywords,
                max_length=max_length,
                relevant_sentences=relevant_sentences
            )
        elif "interpretation" in doc_type_lower or "decision" in doc_type_lower:
            return self._extract_interpretation_key_parts(
                content=content,
                metadata=metadata,
                query=query,
                keywords=extracted_keywords,
                max_length=max_length,
                relevant_sentences=relevant_sentences
            )
        else:
            return self._extract_general_key_parts(
                content=content,
                query=query,
                keywords=extracted_keywords,
                max_length=max_length,
                relevant_sentences=relevant_sentences
            )
    
    def _extract_precedent_key_parts(
        self,
        content: str,
        metadata: Dict[str, Any],
        query: str,
        keywords: List[str],
        max_length: int,
        relevant_sentences: List[Dict[str, Any]] = None
    ) -> str:
        """íŒë¡€ í•µì‹¬ ë¶€ë¶„ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ 2)"""
        parts = []
        remaining_length = max_length
        
        # 1. íŒì‹œì‚¬í•­ (ìµœìš°ì„ , ìµœëŒ€ 400ì)
        holding_text = None
        if metadata.get("case_holding"):
            holding_text = metadata["case_holding"][:400]
        elif "íŒì‹œì‚¬í•­" in content:
            holding_match = re.search(r'íŒì‹œì‚¬í•­[:\s]*(.+?)(?=\n|íŒê²°ìš”ì§€|$)', content, re.DOTALL)
            if holding_match:
                holding_text = holding_match.group(1).strip()[:400]
        
        if holding_text and len(holding_text) <= remaining_length:
            parts.append(f"**íŒì‹œì‚¬í•­**: {holding_text}")
            remaining_length -= len(holding_text) + 20
        
        # 2. íŒê²°ìš”ì§€ (ìµœëŒ€ 400ì)
        reasoning_text = None
        if metadata.get("case_reasoning"):
            reasoning_text = metadata["case_reasoning"][:400]
        elif "íŒê²°ìš”ì§€" in content:
            reasoning_match = re.search(r'íŒê²°ìš”ì§€[:\s]*(.+?)(?=\n|$)', content, re.DOTALL)
            if reasoning_match:
                reasoning_text = reasoning_match.group(1).strip()[:400]
        
        if reasoning_text and len(reasoning_text) <= remaining_length:
            parts.append(f"**íŒê²°ìš”ì§€**: {reasoning_text}")
            remaining_length -= len(reasoning_text) + 20
        
        # 3. ê´€ë ¨ ë¬¸ì¥ (í‚¤ì›Œë“œ í¬í•¨, ìµœëŒ€ 3ê°œ)
        if relevant_sentences:
            relevant_list = []
            for sent in relevant_sentences[:3]:
                sent_text = sent.get("sentence", "")[:300]
                if sent_text and len(sent_text) <= remaining_length - 50:
                    relevant_list.append(f"- {sent_text}")
                    remaining_length -= len(sent_text) + 10
            
            if relevant_list:
                parts.append(f"**ê´€ë ¨ ë¬¸ì¥**:\n" + "\n".join(relevant_list))
        
        # 4. í‚¤ì›Œë“œ ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ (ë‚¨ì€ ê³µê°„ í™œìš©)
        if remaining_length > 200 and keywords:
            keyword_contexts = self._extract_keyword_contexts(content, keywords, remaining_length)
            if keyword_contexts:
                parts.append(f"**í‚¤ì›Œë“œ ê´€ë ¨ ë¬¸ë§¥**:\n{keyword_contexts}")
        
        result = "\n\n".join(parts)
        
        # ìµœì¢… ê¸¸ì´ í™•ì¸
        if len(result) > max_length:
            # ë¹„ìœ¨ì— ë§ì¶° ì¶•ì•½
            ratio = max_length / len(result)
            result = "\n\n".join([
                part[:int(len(part) * ratio)] + ("..." if len(part) > int(len(part) * ratio) else "")
                for part in parts
            ])
        
        return result[:max_length] if len(result) > max_length else result
    
    def _extract_statute_key_parts(
        self,
        content: str,
        metadata: Dict[str, Any],
        query: str,
        keywords: List[str],
        max_length: int,
        relevant_sentences: List[Dict[str, Any]] = None
    ) -> str:
        """ë²•ë ¹ í•µì‹¬ ë¶€ë¶„ ì¶”ì¶œ"""
        parts = []
        remaining_length = max_length
        
        # 1. ì¡°ë¬¸ë²ˆí˜¸ ì •ë³´
        article_no = metadata.get("article_no") or metadata.get("article_number")
        if article_no:
            parts.append(f"**ì¡°ë¬¸ë²ˆí˜¸**: ì œ{article_no}ì¡°")
            remaining_length -= 50
        
        # 2. ì œëª©/í—¤ë”©
        heading = metadata.get("heading") or metadata.get("title")
        if heading and len(heading) <= remaining_length:
            parts.append(f"**ì œëª©**: {heading[:200]}")
            remaining_length -= len(heading) + 20
        
        # 3. ê´€ë ¨ ë¬¸ì¥ (ìµœìš°ì„ )
        if relevant_sentences:
            relevant_list = []
            for sent in relevant_sentences[:5]:
                sent_text = sent.get("sentence", "")[:400]
                if sent_text and len(sent_text) <= remaining_length - 50:
                    relevant_list.append(f"- {sent_text}")
                    remaining_length -= len(sent_text) + 10
            
            if relevant_list:
                parts.append(f"**ê´€ë ¨ ì¡°ë¬¸ ë‚´ìš©**:\n" + "\n".join(relevant_list))
        
        # 4. í‚¤ì›Œë“œ ì£¼ë³€ ë¬¸ë§¥
        if remaining_length > 200 and keywords:
            keyword_contexts = self._extract_keyword_contexts(content, keywords, remaining_length)
            if keyword_contexts:
                parts.append(f"**ê´€ë ¨ ë¬¸ë§¥**:\n{keyword_contexts}")
        
        result = "\n\n".join(parts)
        return result[:max_length] if len(result) > max_length else result
    
    def _extract_interpretation_key_parts(
        self,
        content: str,
        metadata: Dict[str, Any],
        query: str,
        keywords: List[str],
        max_length: int,
        relevant_sentences: List[Dict[str, Any]] = None
    ) -> str:
        """í•´ì„ë¡€/ê²°ì •ë¡€ í•µì‹¬ ë¶€ë¶„ ì¶”ì¶œ"""
        parts = []
        remaining_length = max_length
        
        # 1. ì œëª©
        title = metadata.get("title") or metadata.get("heading")
        if title and len(title) <= remaining_length:
            parts.append(f"**ì œëª©**: {title[:200]}")
            remaining_length -= len(title) + 20
        
        # 2. ê´€ë ¨ ë¬¸ì¥ (ìµœìš°ì„ )
        if relevant_sentences:
            relevant_list = []
            for sent in relevant_sentences[:5]:
                sent_text = sent.get("sentence", "")[:400]
                if sent_text and len(sent_text) <= remaining_length - 50:
                    relevant_list.append(f"- {sent_text}")
                    remaining_length -= len(sent_text) + 10
            
            if relevant_list:
                parts.append(f"**í•µì‹¬ ë‚´ìš©**:\n" + "\n".join(relevant_list))
        
        # 3. í‚¤ì›Œë“œ ì£¼ë³€ ë¬¸ë§¥
        if remaining_length > 200 and keywords:
            keyword_contexts = self._extract_keyword_contexts(content, keywords, remaining_length)
            if keyword_contexts:
                parts.append(f"**ê´€ë ¨ ë¬¸ë§¥**:\n{keyword_contexts}")
        
        result = "\n\n".join(parts)
        return result[:max_length] if len(result) > max_length else result
    
    def _extract_general_key_parts(
        self,
        content: str,
        query: str,
        keywords: List[str],
        max_length: int,
        relevant_sentences: List[Dict[str, Any]] = None
    ) -> str:
        """ì¼ë°˜ ë¬¸ì„œ í•µì‹¬ ë¶€ë¶„ ì¶”ì¶œ"""
        parts = []
        remaining_length = max_length
        
        # 1. ê´€ë ¨ ë¬¸ì¥ (ìµœìš°ì„ )
        if relevant_sentences:
            relevant_list = []
            for sent in relevant_sentences[:5]:
                sent_text = sent.get("sentence", "")[:400]
                if sent_text and len(sent_text) <= remaining_length - 50:
                    relevant_list.append(f"- {sent_text}")
                    remaining_length -= len(sent_text) + 10
            
            if relevant_list:
                parts.append(f"**í•µì‹¬ ë‚´ìš©**:\n" + "\n".join(relevant_list))
        
        # 2. í‚¤ì›Œë“œ ì£¼ë³€ ë¬¸ë§¥
        if remaining_length > 200 and keywords:
            keyword_contexts = self._extract_keyword_contexts(content, keywords, remaining_length)
            if keyword_contexts:
                parts.append(f"**ê´€ë ¨ ë¬¸ë§¥**:\n{keyword_contexts}")
        
        # 3. í´ë°±: ì•ë¶€ë¶„ + ë’·ë¶€ë¶„
        if not parts and len(content) > max_length:
            front = content[:max_length // 2]
            back = content[-max_length // 2:] if len(content) > max_length else ""
            return f"{front}\n\n[... ì¤‘ê°„ ìƒëµ ...]\n\n{back}"
        
        result = "\n\n".join(parts) if parts else content[:max_length]
        return result[:max_length] if len(result) > max_length else result
    
    def _extract_keyword_contexts(
        self,
        content: str,
        keywords: List[str],
        max_length: int
    ) -> str:
        """í‚¤ì›Œë“œ ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ"""
        if not keywords or not content:
            return ""
        
        contexts = []
        content_lower = content.lower()
        used_positions = set()
        
        for keyword in keywords[:5]:
            if not keyword or len(keyword) < 2:
                continue
            
            keyword_lower = keyword.lower()
            if keyword_lower not in content_lower:
                continue
            
            # í‚¤ì›Œë“œ ìœ„ì¹˜ ì°¾ê¸° (ì¤‘ë³µ ë°©ì§€)
            start_pos = 0
            while True:
                idx = content_lower.find(keyword_lower, start_pos)
                if idx == -1:
                    break
                
                # ì´ë¯¸ ì‚¬ìš©ëœ ìœ„ì¹˜ ê·¼ì²˜ì¸ì§€ í™•ì¸
                is_duplicate = any(abs(idx - pos) < 100 for pos in used_positions)
                if not is_duplicate:
                    # ì•ë’¤ 200ìì”© ì¶”ì¶œ
                    context_start = max(0, idx - 200)
                    context_end = min(len(content), idx + len(keyword) + 200)
                    context = content[context_start:context_end]
                    
                    if context and context not in contexts:
                        contexts.append(context)
                        used_positions.add(idx)
                    
                    if len("\n\n[...]\n\n".join(contexts)) >= max_length:
                        break
                
                start_pos = idx + 1
            
            if len("\n\n[...]\n\n".join(contexts)) >= max_length:
                break
        
        if contexts:
            result = "\n\n[...]\n\n".join(contexts[:3])
            return result[:max_length]
        
        return ""
    
    def select_balanced_documents(
        self,
        sorted_docs: List[Dict[str, Any]],
        max_docs: int = 10
    ) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì˜ ê· í˜•ì„ ë§ì¶°ì„œ ë¬¸ì„œ ì„ íƒ (ë¬¸ì„œ ì†ì‹¤ ë°©ì§€ ê°•í™”)"""
        if not sorted_docs:
            return []
        
        # ê°œì„ : ë¬¸ì„œ ìˆ˜ê°€ max_docsë³´ë‹¤ ì ìœ¼ë©´ ëª¨ë“  ë¬¸ì„œ ë°˜í™˜ (ì†ì‹¤ ë°©ì§€)
        if len(sorted_docs) <= max_docs:
            self.logger.debug(
                f"âœ… [DOCUMENT SELECTION] ëª¨ë“  ë¬¸ì„œ ì„ íƒ (ë¬¸ì„œ ìˆ˜={len(sorted_docs)} <= max_docs={max_docs})"
            )
            return sorted_docs
        
        semantic_docs = [doc for doc in sorted_docs if doc.get("search_type") == "semantic"]
        keyword_docs = [doc for doc in sorted_docs if doc.get("search_type") == "keyword"]
        hybrid_docs = [doc for doc in sorted_docs if doc.get("search_type") not in ["semantic", "keyword"]]
        
        selected_docs = []
        seen_ids = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ID ì¶”ì 
        
        # ë¬¸ì„œ ID ì¶”ì¶œ í•¨ìˆ˜
        def get_doc_id(doc):
            return (
                doc.get("id") or 
                doc.get("doc_id") or 
                doc.get("document_id") or 
                id(doc)  # ìµœí›„ì˜ í´ë°±
            )
        
        top_count = max(1, max_docs // 2)
        for doc in sorted_docs[:top_count]:
            doc_id = get_doc_id(doc)
            if doc_id not in seen_ids:
                selected_docs.append(doc)
                seen_ids.add(doc_id)
        
        remaining_slots = max_docs - len(selected_docs)
        
        if remaining_slots > 0:
            semantic_to_add = []
            for doc in semantic_docs:
                doc_id = get_doc_id(doc)
                if doc_id not in seen_ids:
                    semantic_to_add.append(doc)
            
            keyword_to_add = []
            for doc in keyword_docs:
                doc_id = get_doc_id(doc)
                if doc_id not in seen_ids:
                    keyword_to_add.append(doc)
            
            max_alternate = remaining_slots // 2
            for i in range(min(max_alternate, max(len(semantic_to_add), len(keyword_to_add)))):
                if i < len(semantic_to_add) and len(selected_docs) < max_docs:
                    doc = semantic_to_add[i]
                    doc_id = get_doc_id(doc)
                    if doc_id not in seen_ids:
                        selected_docs.append(doc)
                        seen_ids.add(doc_id)
                if i < len(keyword_to_add) and len(selected_docs) < max_docs:
                    doc = keyword_to_add[i]
                    doc_id = get_doc_id(doc)
                    if doc_id not in seen_ids:
                        selected_docs.append(doc)
                        seen_ids.add(doc_id)
            
            if len(selected_docs) < max_docs:
                for doc in hybrid_docs:
                    doc_id = get_doc_id(doc)
                    if doc_id not in seen_ids and len(selected_docs) < max_docs:
                        selected_docs.append(doc)
                        seen_ids.add(doc_id)
            
            if len(selected_docs) < max_docs:
                for doc in sorted_docs:
                    doc_id = get_doc_id(doc)
                    if doc_id not in seen_ids and len(selected_docs) < max_docs:
                        selected_docs.append(doc)
                        seen_ids.add(doc_id)
        
        selected_docs = sorted(
            selected_docs,
            key=lambda x: (
                x.get("final_weighted_score", x.get("relevance_score", 0.0)),
                x.get("keyword_match_score", 0.0)
            ),
            reverse=True
        )
        
        result = selected_docs[:max_docs]
        
        # ë¬¸ì„œ ì†ì‹¤ í™•ì¸ ë° ë¡œê¹…
        if len(result) < len(sorted_docs):
            lost_count = len(sorted_docs) - len(result)
            loss_ratio = lost_count / len(sorted_docs) if sorted_docs else 0.0
            self.logger.warning(
                f"âš ï¸ [DOCUMENT LOSS] select_balanced_documents: {lost_count} documents lost "
                f"({len(sorted_docs)} â†’ {len(result)}, max_docs={max_docs}, loss_ratio={loss_ratio:.1%})"
            )
        else:
            self.logger.debug(
                f"âœ… [DOCUMENT SELECTION] ë¬¸ì„œ ì„ íƒ ì™„ë£Œ: {len(result)}ê°œ ì„ íƒë¨"
            )
        
        return result
    
    def select_balanced_documents_relevance_first(
        self,
        sorted_docs: List[Dict[str, Any]],
        query: str,
        extracted_keywords: List[str],
        query_type: str,
        max_docs: int = 7
    ) -> List[Dict[str, Any]]:
        """
        ê°œì„  12: ê´€ë ¨ì„± ìš°ì„  ë¬¸ì„œ ì„ íƒ (ë‹¤ì–‘ì„±ë³´ë‹¤ ê´€ë ¨ì„± ìš°ì„ ) - ë¬¸ì„œ ì†ì‹¤ ë°©ì§€ ê°•í™”
        
        Args:
            sorted_docs: ì ìˆ˜ë¡œ ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            query: ì‚¬ìš©ì ì§ˆë¬¸
            extracted_keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            query_type: ì§ˆë¬¸ ìœ í˜•
            max_docs: ì„ íƒí•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜
        
        Returns:
            ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not sorted_docs:
            return []
        
        # ê°œì„ : ë¬¸ì„œ ìˆ˜ê°€ max_docsë³´ë‹¤ ì ìœ¼ë©´ ëª¨ë“  ë¬¸ì„œ ë°˜í™˜ (ì†ì‹¤ ë°©ì§€)
        if len(sorted_docs) <= max_docs:
            self.logger.debug(
                f"âœ… [DOCUMENT SELECTION] ëª¨ë“  ë¬¸ì„œ ì„ íƒ (ë¬¸ì„œ ìˆ˜={len(sorted_docs)} <= max_docs={max_docs})"
            )
            return sorted_docs
        
        # ê°œì„  9: ì§ˆë¬¸ ìœ í˜•ë³„ ë¬¸ì„œ í•„í„°ë§ ê¸°ì¤€ ì ìš©
        query_lower = query.lower()
        query_keywords_lower = [kw.lower() for kw in extracted_keywords if kw and len(kw) > 1]
        
        selected_docs = []
        seen_sources = set()
        
        # ê°œì„ : Citation ê°€ëŠ¥ì„±ì´ ë†’ì€ ë¬¸ì„œ ì‹ë³„ (ë²•ë ¹ ì¡°ë¬¸, íŒë¡€ ë“±)
        import re
        citation_pattern = re.compile(r'[ê°€-í£]+ë²•\s*ì œ?\s*\d+\s*ì¡°')
        precedent_pattern = re.compile(r'[ê°€-í£]+(?:ì§€ë°©)?ë²•ì›|ëŒ€ë²•ì›|íŒê²°|ì‚¬ê±´')
        
        # ë¬¸ì„œì— citation ì ìˆ˜ ë¶€ì—¬
        for doc in sorted_docs:
            content = (doc.get("content") or doc.get("text") or "").lower()
            doc_type = doc.get("type") or doc.get("source_type") or ""
            
            citation_score = 0.0
            # ë²•ë ¹ ì¡°ë¬¸ íƒ€ì…ì´ë©´ ë†’ì€ ì ìˆ˜
            if doc_type in ["statute_article", "statute"]:
                citation_score += 0.5
            # íŒë¡€ íƒ€ì…ì´ë©´ ë†’ì€ ì ìˆ˜
            elif doc_type in ["case_paragraph", "precedent", "decision_paragraph"]:
                citation_score += 0.4
            # ë‚´ìš©ì—ì„œ ë²•ë ¹ ì¡°ë¬¸ ë°œê²¬
            if citation_pattern.search(content):
                citation_score += 0.3
            # ë‚´ìš©ì—ì„œ íŒë¡€ ë°œê²¬
            if precedent_pattern.search(content):
                citation_score += 0.2
            
            doc["citation_potential_score"] = min(1.0, citation_score)
        
        # ê°œì„ : ë¬¸ì„œ ìˆ˜ê°€ ì ì„ ë•Œ í•„í„°ë§ ì™„í™” (ë¬¸ì„œ ì†ì‹¤ ë°©ì§€)
        # ë¬¸ì„œ ìˆ˜ê°€ max_docsì˜ 1.5ë°° ì´í•˜ë©´ ëª¨ë“  ë¬¸ì„œ ë°˜í™˜
        if len(sorted_docs) <= int(max_docs * 1.5):
            self.logger.debug(
                f"âœ… [DOCUMENT SELECTION] ë¬¸ì„œ ìˆ˜ê°€ ì ì–´ ëª¨ë“  ë¬¸ì„œ ì„ íƒ "
                f"(ë¬¸ì„œ ìˆ˜={len(sorted_docs)} <= {int(max_docs * 1.5)})"
            )
            return sorted_docs[:max_docs]
        
        # ìš°ì„ ìˆœìœ„ 1 ê°œì„ : ê´€ë ¨ë„ ì„ê³„ê°’ ëŒ€í­ ì™„í™” (ë¬¸ì„œ ì†ì‹¤ ë°©ì§€)
        # 1ë‹¨ê³„: ê´€ë ¨ë„ê°€ ë†’ì€ ë¬¸ì„œ ìš°ì„  ì„ íƒ (ì„ê³„ê°’ ì™„í™”: 0.40 â†’ 0.20)
        high_relevance_docs = [
            doc for doc in sorted_docs 
            if (doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)) >= 0.20
        ]
        
        # citation ê°€ëŠ¥ì„± ìˆœìœ¼ë¡œ ì •ë ¬
        high_relevance_docs.sort(
            key=lambda x: (
                x.get("citation_potential_score", 0.0),
                x.get("relevance_score", 0.0) or x.get("final_weighted_score", 0.0)
            ),
            reverse=True
        )
        
        for doc in high_relevance_docs:
            if len(selected_docs) >= max_docs:
                break
            
            source = doc.get("source", "") or doc.get("id", "") or str(doc.get("doc_id", ""))
            if not source or source not in seen_sources:
                selected_docs.append(doc)
                if source:
                    seen_sources.add(source)
        
        # 2ë‹¨ê³„: ê´€ë ¨ë„ê°€ ë‚®ì•„ë„ citation ê°€ëŠ¥ì„±ì´ ë†’ì€ ë¬¸ì„œ ì„ íƒ (ì„ê³„ê°’ ì™„í™”: 0.30 â†’ 0.10)
        if len(selected_docs) < max_docs:
            low_relevance_docs = [
                doc for doc in sorted_docs 
                if (doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)) >= 0.10
                and doc not in selected_docs
            ]
            
            low_relevance_docs.sort(
                key=lambda x: (
                    x.get("citation_potential_score", 0.0),
                    x.get("relevance_score", 0.0) or x.get("final_weighted_score", 0.0)
                ),
                reverse=True
            )
            
            for doc in low_relevance_docs:
                if len(selected_docs) >= max_docs:
                    break
                
                content = (doc.get("content") or doc.get("text") or "").lower()
                has_relevant_keyword = False
                
                for qkw in query_keywords_lower:
                    if qkw in content or qkw in query_lower:
                        has_relevant_keyword = True
                        break
                
                citation_potential = doc.get("citation_potential_score", 0.0)
                keyword_match = doc.get("keyword_match_score", 0.0)
                
                if citation_potential >= 0.2 or has_relevant_keyword or keyword_match > 0.0:
                    source = doc.get("source", "")
                    if not source or source not in seen_sources:
                        selected_docs.append(doc)
                        if source:
                            seen_sources.add(source)
        
        # 3ë‹¨ê³„: ë¶€ì¡±í•˜ë©´ ìƒìœ„ ë¬¸ì„œë¡œ ì±„ìš°ê¸° (í•„í„°ë§ ì—†ì´)
        if len(selected_docs) < max_docs:
            for doc in sorted_docs:
                if len(selected_docs) >= max_docs:
                    break
                if doc not in selected_docs:
                    selected_docs.append(doc)
        
        # ìµœì†Œ ë¬¸ì„œ ìˆ˜ ë³´ì¥ (ë¬¸ì„œ ì†ì‹¤ ë°©ì§€)
        min_docs = min(len(sorted_docs), max_docs)
        if len(selected_docs) < min_docs:
            for doc in sorted_docs:
                if len(selected_docs) >= min_docs:
                    break
                if doc not in selected_docs:
                    selected_docs.append(doc)
            self.logger.info(
                f"ğŸ“Š [MIN DOCS] ìµœì†Œ ë¬¸ì„œ ìˆ˜ ë³´ì¥: {len(selected_docs)}ê°œ (ëª©í‘œ: {min_docs}ê°œ)"
            )
        
        # ë¬¸ì„œ ì†ì‹¤ í™•ì¸ ë° ë¡œê¹…
        if len(selected_docs) < len(sorted_docs):
            lost_count = len(sorted_docs) - len(selected_docs)
            loss_ratio = lost_count / len(sorted_docs) if sorted_docs else 0.0
            self.logger.warning(
                f"âš ï¸ [DOCUMENT LOSS] select_balanced_documents_relevance_first: {lost_count} documents lost "
                f"({len(sorted_docs)} â†’ {len(selected_docs)}, max_docs={max_docs}, loss_ratio={loss_ratio:.1%})"
            )
        else:
            self.logger.debug(
                f"âœ… [DOCUMENT SELECTION] ë¬¸ì„œ ì„ íƒ ì™„ë£Œ: {len(selected_docs)}ê°œ ì„ íƒë¨"
            )
        
        self.logger.info(
            f"select_balanced_documents_relevance_first: Selected {len(selected_docs)}/{len(sorted_docs)} documents "
            f"(high_relevance: {len([d for d in selected_docs if (d.get('relevance_score', 0.0) or d.get('final_weighted_score', 0.0)) >= 0.40])}, "
            f"medium_relevance: {len([d for d in selected_docs if 0.30 <= (d.get('relevance_score', 0.0) or d.get('final_weighted_score', 0.0)) < 0.40])})"
        )
        
        return selected_docs[:max_docs]
    
    def select_diverse_documents(
        self,
        documents: List[Dict[str, Any]],
        query: str,
        max_docs: int = 7,
        diversity_weight: float = 0.3
    ) -> List[Dict[str, Any]]:
        """
        MMR (Maximal Marginal Relevance) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„±ì˜ ê· í˜•ì„ ë§ì¶˜ ë¬¸ì„œ ì„ íƒ
        
        Args:
            documents: ì„ íƒí•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ì´ë¯¸ ì ìˆ˜ë¡œ ì •ë ¬ëœ ìƒíƒœ)
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_docs: ì„ íƒí•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜
            diversity_weight: ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜ (0.0 ~ 1.0, ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¤‘ì‹œ)
        
        Returns:
            ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„±ì´ ê· í˜•ì¡íŒ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not documents:
            return []
        
        selected = []
        remaining = documents.copy()
        
        # ì²« ë²ˆì§¸ ë¬¸ì„œ: ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ
        if remaining:
            selected.append(remaining.pop(0))
        
        # ë‚˜ë¨¸ì§€ ë¬¸ì„œ: MMR ì ìˆ˜ë¡œ ì„ íƒ
        while len(selected) < max_docs and remaining:
            best_doc = None
            best_score = -1
            
            for doc in remaining:
                # ê´€ë ¨ì„± ì ìˆ˜
                relevance = doc.get("final_weighted_score", doc.get("relevance_score", 0.0))
                
                # ë‹¤ì–‘ì„± ì ìˆ˜ (ì´ë¯¸ ì„ íƒëœ ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ìµœì†Œí™”)
                min_similarity = 1.0
                doc_content = (doc.get("content") or doc.get("text") or "").lower()
                doc_words = set(doc_content.split())
                
                for selected_doc in selected:
                    selected_content = (selected_doc.get("content") or selected_doc.get("text") or "").lower()
                    selected_words = set(selected_content.split())
                    
                    # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
                    if doc_words or selected_words:
                        intersection = len(doc_words & selected_words)
                        union = len(doc_words | selected_words)
                        similarity = intersection / union if union > 0 else 0.0
                        min_similarity = min(min_similarity, similarity)
                
                # MMR ì ìˆ˜: (1 - diversity_weight) * relevance + diversity_weight * (1 - similarity)
                mmr_score = (
                    (1 - diversity_weight) * relevance +
                    diversity_weight * (1 - min_similarity)
                )
                
                if mmr_score > best_score:
                    best_score = mmr_score
                    best_doc = doc
            
            if best_doc:
                selected.append(best_doc)
                remaining.remove(best_doc)
            else:
                break
        
        self.logger.info(
            f"MMR diversity selection: {len(selected)}/{len(documents)} documents selected "
            f"(diversity_weight={diversity_weight:.2f})"
        )
        
        return selected
    
    def _validate_final_documents(
        self,
        sorted_docs: List[Dict[str, Any]],
        query: str,
        extracted_keywords: List[str],
        query_type: str
    ) -> Dict[str, Any]:
        """
        ê°œì„  10: í”„ë¡¬í”„íŠ¸ ìƒì„± í›„ ìµœì¢… ê²€ì¦
        
        Args:
            sorted_docs: ìµœì¢… ì„ íƒëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            query: ì‚¬ìš©ì ì§ˆë¬¸
            extracted_keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            query_type: ì§ˆë¬¸ ìœ í˜•
        
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        validation_result = {
            "total_docs": len(sorted_docs),
            "high_relevance_count": 0,
            "medium_relevance_count": 0,
            "low_relevance_count": 0,
            "low_relevance_warning": None,
            "avg_relevance_score": 0.0,
            "min_relevance_score": 0.0,
            "max_relevance_score": 0.0
        }
        
        if not sorted_docs:
            return validation_result
        
        relevance_scores = []
        query_lower = query.lower()
        query_keywords_lower = [kw.lower() for kw in extracted_keywords if kw and len(kw) > 1]
        
        for doc in sorted_docs:
            relevance_score = doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)
            relevance_scores.append(relevance_score)
            
            if relevance_score >= 0.65:
                validation_result["high_relevance_count"] += 1
            elif relevance_score >= 0.55:
                validation_result["medium_relevance_count"] += 1
            else:
                validation_result["low_relevance_count"] += 1
                
                # ê´€ë ¨ë„ê°€ ë‚®ì€ ë¬¸ì„œì˜ ê²½ìš° í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                content = (doc.get("content") or doc.get("text") or "").lower()
                has_keyword = False
                for qkw in query_keywords_lower:
                    if qkw in content:
                        has_keyword = True
                        break
                
                if not has_keyword and doc.get("keyword_match_score", 0.0) == 0.0:
                    source = doc.get("source", "Unknown")
                    if not validation_result["low_relevance_warning"]:
                        validation_result["low_relevance_warning"] = f"Low relevance documents detected: {source}"
                    else:
                        validation_result["low_relevance_warning"] += f", {source}"
        
        if relevance_scores:
            validation_result["avg_relevance_score"] = sum(relevance_scores) / len(relevance_scores)
            validation_result["min_relevance_score"] = min(relevance_scores)
            validation_result["max_relevance_score"] = max(relevance_scores)
        
        # ê²½ê³  ì¡°ê±´: ê´€ë ¨ë„ê°€ ë‚®ì€ ë¬¸ì„œê°€ ì „ì²´ì˜ 30% ì´ìƒì´ê±°ë‚˜ í‰ê·  ê´€ë ¨ë„ê°€ 0.60 ë¯¸ë§Œ
        if validation_result["low_relevance_count"] > 0:
            low_relevance_ratio = validation_result["low_relevance_count"] / validation_result["total_docs"]
            if low_relevance_ratio >= 0.3 or validation_result["avg_relevance_score"] < 0.60:
                if not validation_result["low_relevance_warning"]:
                    validation_result["low_relevance_warning"] = (
                        f"Low relevance ratio: {low_relevance_ratio:.1%}, "
                        f"avg_score: {validation_result['avg_relevance_score']:.3f}"
                    )
        
        self.logger.info(
            f"_validate_final_documents: Validation complete - "
            f"total: {validation_result['total_docs']}, "
            f"high: {validation_result['high_relevance_count']}, "
            f"medium: {validation_result['medium_relevance_count']}, "
            f"low: {validation_result['low_relevance_count']}, "
            f"avg_score: {validation_result['avg_relevance_score']:.3f}"
        )
        
        return validation_result
    
    def select_high_value_documents(
        self,
        documents: List[Dict],
        query: str,
        min_relevance: float = 0.7,
        max_docs: int = 5
    ) -> List[Dict]:
        """ì •ë³´ ë°€ë„ ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ"""
        if not documents:
            return documents
        
        try:
            high_value_docs = []
            
            for doc in documents:
                doc_content = doc.get("content", "")
                if not doc_content or len(doc_content) < 20:
                    continue
                
                citation_pattern = r'[ê°€-í£]+ë²•\s*ì œ?\s*\d+\s*ì¡°'
                citations = re.findall(citation_pattern, doc_content)
                citation_count = len(citations)
                citation_score = min(1.0, citation_count / 5.0)
                
                query_words = set(query.lower().split())
                content_words = set(doc_content.lower().split())
                explanation_completeness = 0.0
                if query_words and content_words:
                    overlap = len(query_words.intersection(content_words))
                    explanation_completeness = min(1.0, overlap / max(1, len(query_words)))
                
                sentences = doc_content.split('ã€‚') or doc_content.split('.')
                avg_sentence_length = sum(len(s.strip()) for s in sentences if s.strip()) / max(1, len(sentences))
                
                descriptive_score_bonus = 0.0
                if 20 <= avg_sentence_length <= 100:
                    descriptive_score_bonus = 0.2
                elif avg_sentence_length > 100:
                    descriptive_score_bonus = 0.1
                
                explanation_completeness = min(1.0, explanation_completeness + descriptive_score_bonus)
                
                keyword_coverage = 0.0
                if query_words and content_words:
                    keyword_coverage = len(query_words.intersection(content_words)) / max(1, len(query_words))
                
                relevance_score = doc.get("final_relevance_score") or doc.get("combined_score", 0.0) or doc.get("relevance_score", 0.0)
                
                information_density = (
                    0.3 * citation_score +
                    0.3 * explanation_completeness +
                    0.2 * keyword_coverage +
                    0.2 * min(1.0, relevance_score)
                )
                
                doc["information_density_score"] = information_density
                doc["citation_count"] = citation_count
                doc["explanation_completeness"] = explanation_completeness
                
                combined_value_score = 0.6 * relevance_score + 0.4 * information_density
                doc["combined_value_score"] = combined_value_score
                
                if combined_value_score >= min_relevance:
                    high_value_docs.append(doc)
            
            high_value_docs.sort(key=lambda x: x.get("combined_value_score", 0.0), reverse=True)
            
            selected_docs = high_value_docs[:max_docs]
            
            self.logger.info(
                f"ğŸ“š [HIGH VALUE SELECTION] Selected {len(selected_docs)}/{len(documents)} documents. "
                f"Avg density: {sum(d.get('information_density_score', 0.0) for d in selected_docs) / max(1, len(selected_docs)):.3f}"
            )
            
            return selected_docs
        
        except Exception as e:
            self.logger.warning(f"High value document selection failed: {e}, using first {max_docs} documents")
            return documents[:max_docs]
    
    def filter_by_keyword_coverage(
        self,
        documents: List[Dict[str, Any]],
        extracted_keywords: List[str],
        min_coverage: float = 0.3
    ) -> List[Dict[str, Any]]:
        """Keyword Coverage ê¸°ë°˜ í•„í„°ë§ (ê°œì„ : Phase 1)"""
        if not documents or not extracted_keywords:
            return documents
        
        filtered = []
        excluded_count = 0
        
        for doc in documents:
            keyword_coverage = doc.get("keyword_coverage", 0.0)
            
            # Keyword Coverageê°€ ì„ê³„ê°’ ì´ìƒì¸ ë¬¸ì„œë§Œ í¬í•¨
            if keyword_coverage >= min_coverage:
                filtered.append(doc)
            else:
                # ê°œì„ : í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸ ê°•í™” (ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì§ì ‘ í™•ì¸)
                has_core_keyword = False
                content = (doc.get("content", "") or doc.get("text", "")).lower()
                core_keywords = extracted_keywords[:3] if len(extracted_keywords) >= 3 else extracted_keywords
                
                # matched_keywordsì—ì„œ í™•ì¸
                matched_keywords = doc.get("matched_keywords", [])
                if matched_keywords:
                    has_core_keyword = any(
                        str(kw).lower() in [str(mk).lower() for mk in matched_keywords] 
                        for kw in core_keywords if isinstance(kw, str)
                    )
                
                # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì§ì ‘ í™•ì¸ (matched_keywordsê°€ ì—†ëŠ” ê²½ìš°)
                if not has_core_keyword and content:
                    has_core_keyword = any(
                        str(kw).lower() in content 
                        for kw in core_keywords if isinstance(kw, str) and len(kw) >= 2
                    )
                
                if has_core_keyword:
                    filtered.append(doc)
                    self.logger.debug(
                        f"Document included due to core keyword match: "
                        f"coverage={keyword_coverage:.3f}, core_keywords={core_keywords[:2]}"
                    )
                else:
                    excluded_count += 1
                    self.logger.debug(
                        f"Document filtered: coverage={keyword_coverage:.3f} < {min_coverage}, "
                        f"no core keyword match"
                    )
        
        if excluded_count > 0:
            self.logger.info(
                f"ğŸ” [KEYWORD FILTERING] Filtered {excluded_count}/{len(documents)} documents "
                f"by keyword coverage (min_coverage={min_coverage})"
            )
        
        return filtered
    
    def generate_document_based_instructions(
        self,
        documents: List[Dict[str, Any]],
        query: str,
        query_type: str
    ) -> str:
        """ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±í•˜ë¼ëŠ” ëª…ì‹œì  ì§€ì‹œì‚¬í•­ ìƒì„±"""
        instructions = f"""ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ë¬¸ì„œë“¤ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

**ì§ˆë¬¸**: {query}
**ì§ˆë¬¸ ìœ í˜•**: {query_type}

**ë‹µë³€ ìƒì„± ê·œì¹™**:
1. **ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€**: ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
2. **ë¬¸ì„œ ì¸ìš© í•„ìˆ˜**: ë‹µë³€ì—ì„œ ë¬¸ì„œë¥¼ ì¸ìš©í•  ë•ŒëŠ” "ë¬¸ì„œ [ë²ˆí˜¸]ì— ë”°ë¥´ë©´..." í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
3. **ì •í™•ì„±**: ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ë¡ í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
4. **êµ¬ì¡°í™”**: ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”:
   - í•µì‹¬ ë‹µë³€
   - ê´€ë ¨ ë²•ë ¹ ë° ì¡°í•­
   - ì‹¤ë¬´ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­
   - ì°¸ê³ í•  ë§Œí•œ íŒë¡€ (ìˆëŠ” ê²½ìš°)
5. **ì¶œì²˜ ëª…ì‹œ**: ê° ì¸ìš©ë¬¸ì— ëŒ€í•´ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
"""
        
        return instructions
    
    def _validate_document_metadata(self, doc: Dict[str, Any]) -> bool:
        """ìš°ì„ ìˆœìœ„ 4 ê°œì„ : ë©”íƒ€ë°ì´í„° ê²€ì¦ (ì™„í™”ëœ ê¸°ì¤€)"""
        # í•„ìˆ˜ í•„ë“œë§Œ ê²€ì¦ (contentëŠ” í•„ìˆ˜, sourceì™€ typeì€ ì„ íƒì )
        has_content = bool(doc.get("content") or doc.get("text"))
        
        if not has_content:
            return False
        
        # sourceì™€ typeì´ ì—†ì–´ë„ ê²½ê³ ë§Œ ì¶œë ¥í•˜ê³  í†µê³¼
        has_source = bool(doc.get("source"))
        has_type = bool(doc.get("type") or doc.get("source_type"))
        
        if not has_source:
            self.logger.debug(f"âš ï¸ [METADATA] source í•„ë“œ ì—†ìŒ: {doc.get('id', 'unknown')}")
        if not has_type:
            self.logger.debug(f"âš ï¸ [METADATA] type í•„ë“œ ì—†ìŒ: {doc.get('id', 'unknown')}")
        
        # ë©”íƒ€ë°ì´í„° ì™„ì „ì„± ê²€ì¦
        metadata = doc.get("metadata", {})
        if isinstance(metadata, dict):
            # metadataê°€ ìˆìœ¼ë©´ ìµœì†Œí•œì˜ êµ¬ì¡°ëŠ” ìˆì–´ì•¼ í•¨
            pass
        
        return True
    
    def _validate_document_content_quality(self, doc: Dict[str, Any], content: str) -> bool:
        """ìš°ì„ ìˆœìœ„ 4 ê°œì„ : ë¬¸ì„œ ë‚´ìš© í’ˆì§ˆ ê²€ì¦ (ì™„í™”ëœ ê¸°ì¤€)"""
        # ìµœì†Œ ê¸¸ì´ ì™„í™”: 10ì â†’ 5ì
        if not content or len(content.strip()) < 5:
            return False
        
        content_stripped = content.strip()
        
        # íŠ¹ìˆ˜ ë¬¸ìë§Œ ìˆëŠ” ë¬¸ì„œ ì œì™¸
        # ì˜ë¯¸ ìˆëŠ” ë¬¸ì(í•œê¸€, ì˜ë¬¸, ìˆ«ì) ë¹„ìœ¨ ê³„ì‚°
        meaningful_chars = re.findall(r'[ê°€-í£a-zA-Z0-9]', content_stripped)
        total_chars = len(content_stripped)
        if total_chars == 0:
            return False
        
        meaningful_ratio = len(meaningful_chars) / total_chars
        # ì˜ë¯¸ ìˆëŠ” ë¬¸ì ë¹„ìœ¨ ì™„í™”: 50% â†’ 40%
        if meaningful_ratio < 0.4:
            return False
        
        # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œì™¸ (ë¬¸ì¥ ëì´ ì—†ëŠ” ê²½ìš°ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì œì™¸)
        # 100ì ì´ìƒì¸ ê²½ìš°ì—ë§Œ ë¬¸ì¥ ë í™•ì¸ (ë” ê¸´ í…ìŠ¤íŠ¸ì—ì„œë§Œ ì ìš©)
        sentence_endings = content_stripped.count('.') + content_stripped.count('ã€‚') + content_stripped.count('!') + content_stripped.count('?')
        if len(content_stripped) > 200 and sentence_endings == 0:
            # 200ì ì´ìƒì¸ë° ë¬¸ì¥ ëì´ ì—†ìœ¼ë©´ ì œì™¸ (100ì â†’ 200ìë¡œ ì™„í™”)
            return False
        
        return True
    
    def _validate_document_source_reliability(self, doc: Dict[str, Any]) -> bool:
        """ìš°ì„ ìˆœìœ„ 4 ê°œì„ : ì¶œì²˜ ì‹ ë¢°ë„ ê²€ì¦ (ì™„í™”ëœ ê¸°ì¤€)"""
        import re
        source = doc.get("source", "")
        
        # ì¶œì²˜ê°€ ì—†ì–´ë„ ë‚´ìš©ì´ ìœ ìš©í•˜ë©´ í¬í•¨ (ì™„í™”)
        if not source or len(source.strip()) < 1:
            # ì¶œì²˜ê°€ ì—†ì–´ë„ í†µê³¼ (ê²½ê³ ë§Œ ì¶œë ¥)
            self.logger.debug(f"âš ï¸ [SOURCE] source í•„ë“œ ì—†ìŒ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ: {doc.get('id', 'unknown')}")
            return True  # ì¶œì²˜ê°€ ì—†ì–´ë„ í†µê³¼
        
        source_stripped = source.strip()
        
        # ì¶œì²˜ í˜•ì‹ ê²€ì¦ (ì™„í™”ëœ ê¸°ì¤€)
        # ê¸°ë³¸ì ì¸ ì¶œì²˜ í˜•ì‹ ê²€ì¦ - ë” ê´€ëŒ€í•œ ê¸°ì¤€
        has_valid_format = (
            any(keyword in source_stripped for keyword in ["ë²•", "ë²•ì›", "ìœ„ì›íšŒ", "ë¶€", "ì²­", "ì›"]) or
            bool(re.match(r'[ê°€-í£]+ë²•', source_stripped)) or
            bool(re.match(r'.*ë²•ì›.*', source_stripped)) or
            len(source_stripped) >= 2  # ìµœì†Œ ê¸¸ì´ ì™„í™”: 3 â†’ 2
        )
        
        # í˜•ì‹ì´ ë§ì§€ ì•Šì•„ë„ í†µê³¼ (ê²½ê³ ë§Œ ì¶œë ¥)
        if not has_valid_format:
            self.logger.debug(f"âš ï¸ [SOURCE] ì¶œì²˜ í˜•ì‹ì´ í‘œì¤€ê³¼ ë‹¤ë¦„: {source_stripped}")
            return True  # í˜•ì‹ì´ ë§ì§€ ì•Šì•„ë„ í†µê³¼
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶œì²˜ ì •ë³´ í™•ì¸
        metadata = doc.get("metadata", {})
        if isinstance(metadata, dict):
            # statute_name, case_name ë“±ì´ ìˆìœ¼ë©´ ë” ì‹ ë¢°í•  ìˆ˜ ìˆìŒ
            pass
        
        return True

