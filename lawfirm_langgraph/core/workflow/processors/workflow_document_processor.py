# -*- coding: utf-8 -*-
"""
ì›Œí¬í”Œë¡œìš° ë¬¸ì„œ ì²˜ë¦¬ í”„ë¡œì„¸ì„œ
ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ì„ íƒ, ì»¨í…ìŠ¤íŠ¸ ë¹Œë”©, í”„ë¡¬í”„íŠ¸ ìµœì í™” ë“±ì„ ë‹´ë‹¹
"""

import logging
import re
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class WorkflowDocumentProcessor:
    """ì›Œí¬í”Œë¡œìš° ë¬¸ì„œ ì²˜ë¦¬ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, logger: Optional[logging.Logger] = None, query_enhancer=None):
        self.logger = logger or logging.getLogger(__name__)
        self.query_enhancer = query_enhancer
    
    def build_prompt_optimized_context(
        self,
        retrieved_docs: List[Dict[str, Any]],
        query: str,
        extracted_keywords: List[str],
        query_type: str,
        legal_field: str,
        select_balanced_documents_func=None,
        extract_query_relevant_sentences_func=None,
        generate_document_based_instructions_func=None
    ) -> Dict[str, Any]:
        """í”„ë¡¬í”„íŠ¸ì— ìµœëŒ€í•œ ë°˜ì˜ë˜ë„ë¡ ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•"""
        try:
            if not retrieved_docs:
                self.logger.warning("build_prompt_optimized_context: retrieved_docs is empty")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }
            
            valid_docs = []
            invalid_docs_count = 0
            
            # ê°œì„  1, 4: ë¬¸ì„œ íƒ€ì…ë³„ í•„í„°ë§ ê¸°ì¤€ ì°¨ë“±í™” (ì‹¤ì œ ì ìˆ˜ ë²”ìœ„ì— ë§ê²Œ ì™„í™”)
            # ê²€ìƒ‰ ê²°ê³¼ í‰ê·  ì ìˆ˜: 0.458, ë²”ìœ„: 0.373~0.732
            min_relevance_score_semantic = 0.35
            min_relevance_score_keyword = 0.35
            min_relevance_score_statute_article = 0.30
            min_relevance_score_precedent = 0.35
            min_relevance_score_general = 0.40
            
            # ê°œì„  6: í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ìµœì†Œ ê¸°ì¤€
            min_keyword_match_score = 0.01
            
            # ê°œì„  7: ì§ˆë¬¸ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
            query_lower = query.lower()
            query_keywords = []
            for keyword in extracted_keywords:
                if keyword and len(keyword) > 1:
                    query_keywords.append(keyword.lower())
            
            for doc in retrieved_docs:
                if not isinstance(doc, dict):
                    invalid_docs_count += 1
                    continue
                
                content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                
                # contentê°€ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš° ë³µì› ì‹œë„ (ìµœì†Œ ê¸¸ì´ ì™„í™”: 10ì â†’ 5ì)
                if not content or len(content.strip()) < 5:
                    # metadataì—ì„œ ë³µì› ì‹œë„
                    metadata = doc.get("metadata", {})
                    if isinstance(metadata, dict):
                        content = metadata.get("content") or metadata.get("text") or content
                    
                    # ì—¬ì „íˆ ì—†ìœ¼ë©´ ìµœì†Œí•œì˜ ì •ë³´ë¼ë„ ìœ ì§€ (í•„í„°ë§í•˜ì§€ ì•ŠìŒ, ìµœì†Œ ê¸¸ì´ ì™„í™”: 10ì â†’ 5ì)
                    if not content or len(content.strip()) < 5:
                        # docì˜ ë‹¤ë¥¸ í•„ë“œì—ì„œ ì •ë³´ ì¶”ì¶œ
                        title = doc.get("title") or doc.get("name") or ""
                        if title:
                            content = title
                        else:
                            # ìµœí›„ì˜ ìˆ˜ë‹¨: doc_idë‚˜ ë‹¤ë¥¸ ì‹ë³„ì ì‚¬ìš©
                            doc_id = doc.get("doc_id") or doc.get("id") or ""
                            if doc_id:
                                content = f"Document {doc_id}"
                    
                    # contentë¥¼ docì— ë‹¤ì‹œ ì„¤ì •
                    if content:
                        doc["content"] = content
                        doc["text"] = content
                
                # ìµœì†Œ ê¸¸ì´ ê²€ì¦ (ë”ìš± ì™„í™”ëœ ê¸°ì¤€)
                doc_type = doc.get("type", "").lower() if doc.get("type") else ""
                source_type = doc.get("source_type", "").lower() if doc.get("source_type") else ""
                is_legal_doc = "statute" in doc_type or "statute" in source_type or "case" in doc_type or "case" in source_type
                # ë²•ë ¹/íŒë¡€ ë¬¸ì„œëŠ” 3ì, ê¸°íƒ€ëŠ” 5ìë¡œ ë” ì™„í™”
                min_content_length = 3 if is_legal_doc else 5
                
                if not content or len(content.strip()) < min_content_length:
                    invalid_docs_count += 1
                    self.logger.debug(f"Document filtered: content too short or empty (length: {len(content) if content else 0}, min_required: {min_content_length}, source: {doc.get('source', 'Unknown')})")
                    continue
                
                search_type = doc.get("search_type", "semantic")
                relevance_score = doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)
                keyword_match_score = doc.get("keyword_match_score", 0.0)
                matched_keywords = doc.get("matched_keywords", [])
                has_keyword_match = keyword_match_score > 0.0 or len(matched_keywords) > 0
                
                # ê°œì„  6: í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§ (ë”ìš± ì™„í™”)
                if keyword_match_score == 0.0 and not matched_keywords:
                    content_lower = content.lower()
                    has_query_keyword = False
                    for qkw in query_keywords:
                        if qkw in content_lower:
                            has_query_keyword = True
                            break
                    
                    # ê´€ë ¨ì„± ì„ê³„ê°’ ë”ìš± ì™„í™” (ë²•ë ¹/íŒë¡€ ë¬¸ì„œëŠ” 0.30, ê¸°íƒ€ëŠ” 0.40)
                    relevance_threshold = 0.30 if is_legal_doc else 0.40
                    if not has_query_keyword and relevance_score < relevance_threshold:
                        invalid_docs_count += 1
                        self.logger.debug(
                            f"Document filtered: no keyword match and low relevance "
                            f"(relevance: {relevance_score:.3f}, threshold: {relevance_threshold}, source: {doc.get('source', 'Unknown')})"
                        )
                        continue
                
                # ë¬¸ì„œ íƒ€ì… í™•ì¸ (ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨, ì¶”ê°€ í™•ì¸ë§Œ ìˆ˜í–‰)
                is_statute_article = (
                    doc_type == "statute_article" or 
                    source_type == "statute_article" or
                    "statute_article" in doc_type or
                    "statute_article" in source_type or
                    doc.get("direct_match", False) or
                    search_type == "direct_statute"
                )
                is_precedent = (
                    doc_type == "precedent" or
                    source_type == "precedent" or
                    "precedent" in doc_type or
                    "precedent" in source_type or
                    "case_paragraph" in doc_type or
                    "case_paragraph" in source_type or
                    "íŒë¡€" in content[:200] or
                    "ëŒ€ë²•ì›" in content[:200]
                )
                # is_legal_docëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨
                
                # ê°œì„  4: ë¬¸ì„œ íƒ€ì…ë³„ í•„í„°ë§ ê¸°ì¤€ ì°¨ë“±í™” (í‚¤ì›Œë“œ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ì™„í™”)
                if is_statute_article:
                    min_score = min_relevance_score_statute_article
                elif is_precedent:
                    min_score = min_relevance_score_precedent
                elif search_type == "keyword" and has_keyword_match:
                    min_score = min_relevance_score_keyword
                elif search_type == "semantic":
                    min_score = min_relevance_score_semantic
                else:
                    min_score = min_relevance_score_general
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ê¸°ì¤€ì„ ë” ì™„í™” (0.10 ê°ì†Œ)
                if has_keyword_match or has_query_keyword:
                    min_score = max(0.20, min_score - 0.10)
                
                # ì²« ë²ˆì§¸ í•„í„°ë§(í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ì„ ë•Œ)ì„ í†µê³¼í•œ ê²½ìš°, ë‘ ë²ˆì§¸ í•„í„°ë§ì€ ë” ì™„í™”
                if not has_keyword_match and not has_query_keyword and relevance_score >= 0.30:
                    # ì´ë¯¸ ì²« ë²ˆì§¸ í•„í„°ë§ì„ í†µê³¼í–ˆìœ¼ë¯€ë¡œ ë‘ ë²ˆì§¸ í•„í„°ë§ì€ ë” ì™„í™”
                    min_score = max(0.25, min_score - 0.15)
                
                if relevance_score < min_score:
                    invalid_docs_count += 1
                    self.logger.debug(
                        f"Document filtered: relevance score too low ({relevance_score:.3f} < {min_score:.3f}) "
                        f"(source: {doc.get('source', 'Unknown')}, type: {search_type}, doc_type: {doc_type}, "
                        f"has_keyword: {has_keyword_match or has_query_keyword})"
                    )
                    continue
                
                valid_docs.append(doc)
            
            if invalid_docs_count > 0:
                self.logger.warning(
                    f"build_prompt_optimized_context: Filtered {invalid_docs_count} invalid documents "
                    f"(no content, content too short, or relevance < threshold). Valid docs: {len(valid_docs)}"
                )
            
            if not valid_docs:
                self.logger.error("build_prompt_optimized_context: No valid documents with content found")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }
            
            sorted_docs = sorted(
                valid_docs,
                key=lambda x: (
                    x.get("final_weighted_score", x.get("relevance_score", 0.0)),
                    x.get("keyword_match_score", 0.0)
                ),
                reverse=True
            )
            
            # ê°œì„  8: í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  ë¬¸ì„œ ìˆ˜ ì œí•œ (5-7ê°œ)
            max_docs_for_prompt = 7
            
            # ê°œì„  12: ë¬¸ì„œ ì„ íƒ ë¡œì§ ê°œì„  (ê´€ë ¨ì„± ìš°ì„ )
            if select_balanced_documents_func:
                balanced_docs = select_balanced_documents_func(sorted_docs, max_docs=max_docs_for_prompt)
            else:
                balanced_docs = self.select_balanced_documents_relevance_first(
                    sorted_docs, 
                    query=query,
                    extracted_keywords=extracted_keywords,
                    query_type=query_type,
                    max_docs=max_docs_for_prompt
                )
            
            if not balanced_docs and sorted_docs:
                balanced_docs = sorted_docs[:min(max_docs_for_prompt, len(sorted_docs))]
            
            sorted_docs = balanced_docs
            
            if not sorted_docs:
                self.logger.error("build_prompt_optimized_context: sorted_docs is empty after filtering")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }
            
            if generate_document_based_instructions_func:
                document_instructions = generate_document_based_instructions_func(
                    documents=sorted_docs,
                    query=query,
                    query_type=query_type
                )
            else:
                document_instructions = self.generate_document_based_instructions(
                    documents=sorted_docs,
                    query=query,
                    query_type=query_type
                )
            
            semantic_count = sum(1 for doc in sorted_docs if doc.get("search_type") == "semantic")
            keyword_count = sum(1 for doc in sorted_docs if doc.get("search_type") == "keyword")
            hybrid_count = len(sorted_docs) - semantic_count - keyword_count
            
            prompt_section = f"""## ë‹µë³€ ìƒì„± ì§€ì‹œì‚¬í•­

{document_instructions}

## ì°¸ê³  ë¬¸ì„œ ëª©ë¡

ë‹¤ìŒ {len(sorted_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
ê° ë¬¸ì„œëŠ” ê´€ë ¨ì„± ì ìˆ˜ì™€ í•µì‹¬ ë‚´ìš©ì´ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ê²€ìƒ‰ ê²°ê³¼ í†µê³„:**
- ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼: {semantic_count}ê°œ
- í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {keyword_count}ê°œ
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼: {hybrid_count}ê°œ
- ì´ ë¬¸ì„œ ìˆ˜: {len(sorted_docs)}ê°œ

**ì°¸ê³ :** ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ëŠ” ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼, í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ëŠ” í‚¤ì›Œë“œ ë§¤ì¹­ ì •ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
ë‘ ê²€ìƒ‰ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

"""
            
            for idx, doc in enumerate(sorted_docs, 1):
                relevance_score = doc.get("final_weighted_score") or doc.get("relevance_score", 0.0)
                source = doc.get("source", "Unknown")
                content = doc.get("content", "")
                
                if extract_query_relevant_sentences_func:
                    relevant_sentences = extract_query_relevant_sentences_func(
                        doc_content=content,
                        query=query,
                        extracted_keywords=extracted_keywords
                    )
                elif self.query_enhancer:
                    relevant_sentences = self.query_enhancer.extract_query_relevant_sentences(
                        content, query, extracted_keywords
                    )
                else:
                    relevant_sentences = []
                
                search_type = doc.get("search_type", "hybrid")
                search_method = doc.get("search_method", "hybrid_search")
                keyword_match_score = doc.get("keyword_match_score", 0.0)
                matched_keywords = doc.get("matched_keywords", [])
                
                doc_section = f"""
### ë¬¸ì„œ {idx}: {source} (ê´€ë ¨ì„± ì ìˆ˜: {relevance_score:.2f})

**ê²€ìƒ‰ ì •ë³´:**
- ê²€ìƒ‰ ë°©ì‹: {search_type} ({search_method})
- í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜: {keyword_match_score:.2f}
- ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(matched_keywords[:5]) if matched_keywords else 'ì—†ìŒ'}

**í•µì‹¬ ë‚´ìš©:**
"""
                
                if relevant_sentences:
                    doc_section += "\n".join([
                        f"- [ì¤‘ìš”] {sent['sentence']}"
                        for sent in relevant_sentences[:3]
                    ])
                    doc_section += "\n\n"
                
                # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™”: í† í° ìˆ˜ ê¸°ë°˜ ë™ì  ì¡°ì •
                # í•œê¸€ ê¸°ì¤€ ëŒ€ëµ 1í† í° = 2-3ì, ì˜ì–´ ê¸°ì¤€ 1í† í° = 4ì
                # ì•ˆì „í•˜ê²Œ 1í† í° = 2.5ìë¡œ ê³„ì‚°
                max_tokens_per_doc = 600  # ë¬¸ì„œë‹¹ ìµœëŒ€ í† í° ìˆ˜
                max_content_length = int(max_tokens_per_doc * 2.5)  # ì•½ 1500ì
                
                # ì§ˆë¬¸ íƒ€ì…ë³„ ë™ì  ì¡°ì •
                if query_type == "law_inquiry":
                    max_tokens_per_doc = 800  # ë²•ë ¹ ì¡°íšŒ: ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ í—ˆìš©
                    max_content_length = int(max_tokens_per_doc * 2.5)  # ì•½ 2000ì
                elif query_type == "complex_question":
                    max_tokens_per_doc = 1000  # ë³µì¡í•œ ì§ˆë¬¸: ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ í—ˆìš©
                    max_content_length = int(max_tokens_per_doc * 2.5)  # ì•½ 2500ì
                
                if len(content) > max_content_length:
                    content = content[:max_content_length] + "..."
                
                doc_section += f"""**ì „ì²´ ë‚´ìš©:**
{content}

---
"""
                
                prompt_section += doc_section
            
            prompt_section += """
## ë¬¸ì„œ ì¸ìš© ê·œì¹™

ë‹µë³€ì—ì„œ ìœ„ ë¬¸ì„œë¥¼ ì¸ìš©í•  ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ ëª…ì‹œí•˜ì„¸ìš”:
- "ë¬¸ì„œ {0}ì— ë”°ë¥´ë©´..." ë˜ëŠ” "[{0}] ì¸ìš© ë‚´ìš©"
- ê° ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œ

## ì¤‘ìš” ì‚¬í•­

- ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”
- ë¬¸ì„œì—ì„œ ì¶”ë¡ í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì¼ê´€ëœ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”
""".format("n")
            
            content_validation = {
                "has_document_content": False,
                "total_content_length": 0,
                "documents_with_content": 0
            }
            
            for doc in sorted_docs:
                content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                if content and len(content.strip()) >= 10:
                    content_preview = content[:100]
                    if content_preview in prompt_section:
                        content_validation["has_document_content"] = True
                        content_validation["total_content_length"] += len(content)
                        content_validation["documents_with_content"] += 1
            
            if not content_validation["has_document_content"]:
                self.logger.error(
                    f"build_prompt_optimized_context: WARNING - prompt_section does not contain actual document content! "
                    f"Documents processed: {len(sorted_docs)}, "
                    f"Prompt length: {len(prompt_section)}"
                )
            else:
                self.logger.info(
                    f"build_prompt_optimized_context: Successfully included content from {content_validation['documents_with_content']} documents "
                    f"(total content length: {content_validation['total_content_length']} chars, "
                    f"prompt length: {len(prompt_section)} chars)"
                )
            
            if not content_validation["has_document_content"] and len(sorted_docs) > 0:
                self.logger.warning(
                    f"build_prompt_optimized_context: Content validation failed, but returning prompt anyway "
                    f"(may contain instructions only without actual document content)"
                )
            
            # ê°œì„  10: í”„ë¡¬í”„íŠ¸ ìƒì„± í›„ ìµœì¢… ê²€ì¦
            final_validation = self._validate_final_documents(
                sorted_docs=sorted_docs,
                query=query,
                extracted_keywords=extracted_keywords,
                query_type=query_type
            )
            
            if final_validation.get("low_relevance_warning"):
                self.logger.warning(
                    f"build_prompt_optimized_context: {final_validation['low_relevance_warning']} "
                    f"(low_relevance_count: {final_validation.get('low_relevance_count', 0)})"
                )
            
            return {
                "prompt_optimized_text": prompt_section,
                "structured_documents": {
                    "total_count": len(sorted_docs),
                    "documents": [{
                        "document_id": idx,
                        "source": doc.get("source", "Unknown"),
                        "relevance_score": doc.get("final_weighted_score") or doc.get("relevance_score", 0.0),
                        "content": (doc.get("content") or doc.get("text") or doc.get("content_text", ""))[:2000]
                    } for idx, doc in enumerate(sorted_docs, 1)]
                },
                "document_count": len(sorted_docs),
                "total_context_length": len(prompt_section),
                "content_validation": content_validation,
                "final_validation": final_validation
            }
        
        except Exception as e:
            self.logger.error(f"Prompt optimized context building failed: {e}")
            return {
                "prompt_optimized_text": "",
                "structured_documents": {},
                "document_count": 0,
                "total_context_length": 0
            }
    
    def select_balanced_documents(
        self,
        sorted_docs: List[Dict[str, Any]],
        max_docs: int = 10
    ) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì˜ ê· í˜•ì„ ë§ì¶°ì„œ ë¬¸ì„œ ì„ íƒ"""
        if not sorted_docs:
            return []
        
        semantic_docs = [doc for doc in sorted_docs if doc.get("search_type") == "semantic"]
        keyword_docs = [doc for doc in sorted_docs if doc.get("search_type") == "keyword"]
        hybrid_docs = [doc for doc in sorted_docs if doc.get("search_type") not in ["semantic", "keyword"]]
        
        selected_docs = []
        
        top_count = max(1, max_docs // 2)
        selected_docs.extend(sorted_docs[:top_count])
        
        remaining_slots = max_docs - len(selected_docs)
        
        if remaining_slots > 0:
            semantic_to_add = []
            for doc in semantic_docs:
                if doc not in selected_docs:
                    semantic_to_add.append(doc)
            
            keyword_to_add = []
            for doc in keyword_docs:
                if doc not in selected_docs:
                    keyword_to_add.append(doc)
            
            max_alternate = remaining_slots // 2
            for i in range(min(max_alternate, max(len(semantic_to_add), len(keyword_to_add)))):
                if i < len(semantic_to_add) and len(selected_docs) < max_docs:
                    if semantic_to_add[i] not in selected_docs:
                        selected_docs.append(semantic_to_add[i])
                if i < len(keyword_to_add) and len(selected_docs) < max_docs:
                    if keyword_to_add[i] not in selected_docs:
                        selected_docs.append(keyword_to_add[i])
            
            if len(selected_docs) < max_docs:
                for doc in hybrid_docs:
                    if doc not in selected_docs and len(selected_docs) < max_docs:
                        selected_docs.append(doc)
            
            if len(selected_docs) < max_docs:
                for doc in sorted_docs:
                    if doc not in selected_docs and len(selected_docs) < max_docs:
                        selected_docs.append(doc)
        
        selected_docs = sorted(
            selected_docs,
            key=lambda x: (
                x.get("final_weighted_score", x.get("relevance_score", 0.0)),
                x.get("keyword_match_score", 0.0)
            ),
            reverse=True
        )
        
        return selected_docs[:max_docs]
    
    def select_balanced_documents_relevance_first(
        self,
        sorted_docs: List[Dict[str, Any]],
        query: str,
        extracted_keywords: List[str],
        query_type: str,
        max_docs: int = 7
    ) -> List[Dict[str, Any]]:
        """
        ê°œì„  12: ê´€ë ¨ì„± ìš°ì„  ë¬¸ì„œ ì„ íƒ (ë‹¤ì–‘ì„±ë³´ë‹¤ ê´€ë ¨ì„± ìš°ì„ )
        
        Args:
            sorted_docs: ì ìˆ˜ë¡œ ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            query: ì‚¬ìš©ì ì§ˆë¬¸
            extracted_keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            query_type: ì§ˆë¬¸ ìœ í˜•
            max_docs: ì„ íƒí•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜
        
        Returns:
            ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not sorted_docs:
            return []
        
        # ê°œì„  9: ì§ˆë¬¸ ìœ í˜•ë³„ ë¬¸ì„œ í•„í„°ë§ ê¸°ì¤€ ì ìš©
        query_lower = query.lower()
        query_keywords_lower = [kw.lower() for kw in extracted_keywords if kw and len(kw) > 1]
        
        selected_docs = []
        seen_sources = set()
        
        # ê°œì„ : Citation ê°€ëŠ¥ì„±ì´ ë†’ì€ ë¬¸ì„œ ì‹ë³„ (ë²•ë ¹ ì¡°ë¬¸, íŒë¡€ ë“±)
        import re
        citation_pattern = re.compile(r'[ê°€-í£]+ë²•\s*ì œ?\s*\d+\s*ì¡°')
        precedent_pattern = re.compile(r'[ê°€-í£]+(?:ì§€ë°©)?ë²•ì›|ëŒ€ë²•ì›|íŒê²°|ì‚¬ê±´')
        
        # ë¬¸ì„œì— citation ì ìˆ˜ ë¶€ì—¬
        for doc in sorted_docs:
            content = (doc.get("content") or doc.get("text") or "").lower()
            doc_type = doc.get("type") or doc.get("source_type") or ""
            
            citation_score = 0.0
            # ë²•ë ¹ ì¡°ë¬¸ íƒ€ì…ì´ë©´ ë†’ì€ ì ìˆ˜
            if doc_type in ["statute_article", "statute"]:
                citation_score += 0.5
            # íŒë¡€ íƒ€ì…ì´ë©´ ë†’ì€ ì ìˆ˜
            elif doc_type in ["case_paragraph", "precedent", "decision_paragraph"]:
                citation_score += 0.4
            # ë‚´ìš©ì—ì„œ ë²•ë ¹ ì¡°ë¬¸ ë°œê²¬
            if citation_pattern.search(content):
                citation_score += 0.3
            # ë‚´ìš©ì—ì„œ íŒë¡€ ë°œê²¬
            if precedent_pattern.search(content):
                citation_score += 0.2
            
            doc["citation_potential_score"] = min(1.0, citation_score)
        
        # 1ë‹¨ê³„: ê´€ë ¨ë„ê°€ ë†’ê³  citation ê°€ëŠ¥ì„±ì´ ë†’ì€ ë¬¸ì„œ ìš°ì„  ì„ íƒ
        high_relevance_docs = [
            doc for doc in sorted_docs 
            if (doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)) >= 0.65
        ]
        
        # citation ê°€ëŠ¥ì„± ìˆœìœ¼ë¡œ ì •ë ¬
        high_relevance_docs.sort(
            key=lambda x: (
                x.get("citation_potential_score", 0.0),
                x.get("relevance_score", 0.0) or x.get("final_weighted_score", 0.0)
            ),
            reverse=True
        )
        
        for doc in high_relevance_docs:
            if len(selected_docs) >= max_docs:
                break
            
            source = doc.get("source", "")
            if source and source not in seen_sources:
                selected_docs.append(doc)
                seen_sources.add(source)
        
        # 2ë‹¨ê³„: ê´€ë ¨ë„ê°€ ì¤‘ê°„ì´ì§€ë§Œ citation ê°€ëŠ¥ì„±ì´ ë†’ì€ ë¬¸ì„œ ìš°ì„  ì„ íƒ
        if len(selected_docs) < max_docs:
            medium_relevance_docs = [
                doc for doc in sorted_docs 
                if 0.55 <= (doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)) < 0.65
                and doc not in selected_docs
            ]
            
            # citation ê°€ëŠ¥ì„± ìˆœìœ¼ë¡œ ì •ë ¬
            medium_relevance_docs.sort(
                key=lambda x: (
                    x.get("citation_potential_score", 0.0),
                    x.get("relevance_score", 0.0) or x.get("final_weighted_score", 0.0)
                ),
                reverse=True
            )
            
            for doc in medium_relevance_docs:
                if len(selected_docs) >= max_docs:
                    break
                
                # ê°œì„ : citation ê°€ëŠ¥ì„±ì´ ë†’ê±°ë‚˜ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ìˆëŠ” ë¬¸ì„œ ìš°ì„ 
                content = (doc.get("content") or doc.get("text") or "").lower()
                has_relevant_keyword = False
                
                for qkw in query_keywords_lower:
                    if qkw in content or qkw in query_lower:
                        has_relevant_keyword = True
                        break
                
                citation_potential = doc.get("citation_potential_score", 0.0)
                keyword_match = doc.get("keyword_match_score", 0.0)
                
                # citation ê°€ëŠ¥ì„±ì´ ë†’ê±°ë‚˜ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ì„ íƒ
                if citation_potential >= 0.3 or has_relevant_keyword or keyword_match > 0.0:
                    source = doc.get("source", "")
                    if not source or source not in seen_sources:
                        selected_docs.append(doc)
                        if source:
                            seen_sources.add(source)
        
        # 3ë‹¨ê³„: ë¶€ì¡±í•˜ë©´ ìƒìœ„ ë¬¸ì„œë¡œ ì±„ìš°ê¸°
        if len(selected_docs) < max_docs:
            for doc in sorted_docs:
                if len(selected_docs) >= max_docs:
                    break
                if doc not in selected_docs:
                    selected_docs.append(doc)
        
        self.logger.info(
            f"select_balanced_documents_relevance_first: Selected {len(selected_docs)}/{len(sorted_docs)} documents "
            f"(high_relevance: {len([d for d in selected_docs if (d.get('relevance_score', 0.0) or d.get('final_weighted_score', 0.0)) >= 0.65])})"
        )
        
        return selected_docs[:max_docs]
    
    def _validate_final_documents(
        self,
        sorted_docs: List[Dict[str, Any]],
        query: str,
        extracted_keywords: List[str],
        query_type: str
    ) -> Dict[str, Any]:
        """
        ê°œì„  10: í”„ë¡¬í”„íŠ¸ ìƒì„± í›„ ìµœì¢… ê²€ì¦
        
        Args:
            sorted_docs: ìµœì¢… ì„ íƒëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            query: ì‚¬ìš©ì ì§ˆë¬¸
            extracted_keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            query_type: ì§ˆë¬¸ ìœ í˜•
        
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        validation_result = {
            "total_docs": len(sorted_docs),
            "high_relevance_count": 0,
            "medium_relevance_count": 0,
            "low_relevance_count": 0,
            "low_relevance_warning": None,
            "avg_relevance_score": 0.0,
            "min_relevance_score": 0.0,
            "max_relevance_score": 0.0
        }
        
        if not sorted_docs:
            return validation_result
        
        relevance_scores = []
        query_lower = query.lower()
        query_keywords_lower = [kw.lower() for kw in extracted_keywords if kw and len(kw) > 1]
        
        for doc in sorted_docs:
            relevance_score = doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)
            relevance_scores.append(relevance_score)
            
            if relevance_score >= 0.65:
                validation_result["high_relevance_count"] += 1
            elif relevance_score >= 0.55:
                validation_result["medium_relevance_count"] += 1
            else:
                validation_result["low_relevance_count"] += 1
                
                # ê´€ë ¨ë„ê°€ ë‚®ì€ ë¬¸ì„œì˜ ê²½ìš° í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                content = (doc.get("content") or doc.get("text") or "").lower()
                has_keyword = False
                for qkw in query_keywords_lower:
                    if qkw in content:
                        has_keyword = True
                        break
                
                if not has_keyword and doc.get("keyword_match_score", 0.0) == 0.0:
                    source = doc.get("source", "Unknown")
                    if not validation_result["low_relevance_warning"]:
                        validation_result["low_relevance_warning"] = f"Low relevance documents detected: {source}"
                    else:
                        validation_result["low_relevance_warning"] += f", {source}"
        
        if relevance_scores:
            validation_result["avg_relevance_score"] = sum(relevance_scores) / len(relevance_scores)
            validation_result["min_relevance_score"] = min(relevance_scores)
            validation_result["max_relevance_score"] = max(relevance_scores)
        
        # ê²½ê³  ì¡°ê±´: ê´€ë ¨ë„ê°€ ë‚®ì€ ë¬¸ì„œê°€ ì „ì²´ì˜ 30% ì´ìƒì´ê±°ë‚˜ í‰ê·  ê´€ë ¨ë„ê°€ 0.60 ë¯¸ë§Œ
        if validation_result["low_relevance_count"] > 0:
            low_relevance_ratio = validation_result["low_relevance_count"] / validation_result["total_docs"]
            if low_relevance_ratio >= 0.3 or validation_result["avg_relevance_score"] < 0.60:
                if not validation_result["low_relevance_warning"]:
                    validation_result["low_relevance_warning"] = (
                        f"Low relevance ratio: {low_relevance_ratio:.1%}, "
                        f"avg_score: {validation_result['avg_relevance_score']:.3f}"
                    )
        
        self.logger.info(
            f"_validate_final_documents: Validation complete - "
            f"total: {validation_result['total_docs']}, "
            f"high: {validation_result['high_relevance_count']}, "
            f"medium: {validation_result['medium_relevance_count']}, "
            f"low: {validation_result['low_relevance_count']}, "
            f"avg_score: {validation_result['avg_relevance_score']:.3f}"
        )
        
        return validation_result
    
    def select_high_value_documents(
        self,
        documents: List[Dict],
        query: str,
        min_relevance: float = 0.7,
        max_docs: int = 5
    ) -> List[Dict]:
        """ì •ë³´ ë°€ë„ ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ"""
        if not documents:
            return documents
        
        try:
            high_value_docs = []
            
            for doc in documents:
                doc_content = doc.get("content", "")
                if not doc_content or len(doc_content) < 20:
                    continue
                
                citation_pattern = r'[ê°€-í£]+ë²•\s*ì œ?\s*\d+\s*ì¡°'
                citations = re.findall(citation_pattern, doc_content)
                citation_count = len(citations)
                citation_score = min(1.0, citation_count / 5.0)
                
                query_words = set(query.lower().split())
                content_words = set(doc_content.lower().split())
                explanation_completeness = 0.0
                if query_words and content_words:
                    overlap = len(query_words.intersection(content_words))
                    explanation_completeness = min(1.0, overlap / max(1, len(query_words)))
                
                sentences = doc_content.split('ã€‚') or doc_content.split('.')
                avg_sentence_length = sum(len(s.strip()) for s in sentences if s.strip()) / max(1, len(sentences))
                
                descriptive_score_bonus = 0.0
                if 20 <= avg_sentence_length <= 100:
                    descriptive_score_bonus = 0.2
                elif avg_sentence_length > 100:
                    descriptive_score_bonus = 0.1
                
                explanation_completeness = min(1.0, explanation_completeness + descriptive_score_bonus)
                
                keyword_coverage = 0.0
                if query_words and content_words:
                    keyword_coverage = len(query_words.intersection(content_words)) / max(1, len(query_words))
                
                relevance_score = doc.get("final_relevance_score") or doc.get("combined_score", 0.0) or doc.get("relevance_score", 0.0)
                
                information_density = (
                    0.3 * citation_score +
                    0.3 * explanation_completeness +
                    0.2 * keyword_coverage +
                    0.2 * min(1.0, relevance_score)
                )
                
                doc["information_density_score"] = information_density
                doc["citation_count"] = citation_count
                doc["explanation_completeness"] = explanation_completeness
                
                combined_value_score = 0.6 * relevance_score + 0.4 * information_density
                doc["combined_value_score"] = combined_value_score
                
                if combined_value_score >= min_relevance:
                    high_value_docs.append(doc)
            
            high_value_docs.sort(key=lambda x: x.get("combined_value_score", 0.0), reverse=True)
            
            selected_docs = high_value_docs[:max_docs]
            
            self.logger.info(
                f"ğŸ“š [HIGH VALUE SELECTION] Selected {len(selected_docs)}/{len(documents)} documents. "
                f"Avg density: {sum(d.get('information_density_score', 0.0) for d in selected_docs) / max(1, len(selected_docs)):.3f}"
            )
            
            return selected_docs
        
        except Exception as e:
            self.logger.warning(f"High value document selection failed: {e}, using first {max_docs} documents")
            return documents[:max_docs]
    
    def generate_document_based_instructions(
        self,
        documents: List[Dict[str, Any]],
        query: str,
        query_type: str
    ) -> str:
        """ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±í•˜ë¼ëŠ” ëª…ì‹œì  ì§€ì‹œì‚¬í•­ ìƒì„±"""
        instructions = f"""ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ë¬¸ì„œë“¤ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

**ì§ˆë¬¸**: {query}
**ì§ˆë¬¸ ìœ í˜•**: {query_type}

**ë‹µë³€ ìƒì„± ê·œì¹™**:
1. **ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€**: ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
2. **ë¬¸ì„œ ì¸ìš© í•„ìˆ˜**: ë‹µë³€ì—ì„œ ë¬¸ì„œë¥¼ ì¸ìš©í•  ë•ŒëŠ” "ë¬¸ì„œ [ë²ˆí˜¸]ì— ë”°ë¥´ë©´..." í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
3. **ì •í™•ì„±**: ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ë¡ í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
4. **êµ¬ì¡°í™”**: ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”:
   - í•µì‹¬ ë‹µë³€
   - ê´€ë ¨ ë²•ë ¹ ë° ì¡°í•­
   - ì‹¤ë¬´ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­
   - ì°¸ê³ í•  ë§Œí•œ íŒë¡€ (ìˆëŠ” ê²½ìš°)
5. **ì¶œì²˜ ëª…ì‹œ**: ê° ì¸ìš©ë¬¸ì— ëŒ€í•´ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
"""
        
        return instructions

