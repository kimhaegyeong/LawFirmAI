# -*- coding: utf-8 -*-
"""
ì›Œí¬í”Œë¡œìš° ë¬¸ì„œ ì²˜ë¦¬ í”„ë¡œì„¸ì„œ
ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ì„ íƒ, ì»¨í…ìŠ¤íŠ¸ ë¹Œë”©, í”„ë¡¬í”„íŠ¸ ìµœì í™” ë“±ì„ ë‹´ë‹¹
"""

import logging
import re
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class WorkflowDocumentProcessor:
    """ì›Œí¬í”Œë¡œìš° ë¬¸ì„œ ì²˜ë¦¬ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, logger: Optional[logging.Logger] = None, query_enhancer=None):
        self.logger = logger or logging.getLogger(__name__)
        self.query_enhancer = query_enhancer
    
    def build_prompt_optimized_context(
        self,
        retrieved_docs: List[Dict[str, Any]],
        query: str,
        extracted_keywords: List[str],
        query_type: str,
        legal_field: str,
        select_balanced_documents_func=None,
        extract_query_relevant_sentences_func=None,
        generate_document_based_instructions_func=None
    ) -> Dict[str, Any]:
        """í”„ë¡¬í”„íŠ¸ì— ìµœëŒ€í•œ ë°˜ì˜ë˜ë„ë¡ ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•"""
        try:
            if not retrieved_docs:
                self.logger.warning("build_prompt_optimized_context: retrieved_docs is empty")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }
            
            valid_docs = []
            invalid_docs_count = 0
            min_relevance_score_semantic = 0.2
            min_relevance_score_keyword = 0.15
            
            for doc in retrieved_docs:
                if not isinstance(doc, dict):
                    invalid_docs_count += 1
                    continue
                
                content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                if not content or len(content.strip()) < 10:
                    invalid_docs_count += 1
                    self.logger.debug(f"Document filtered: content too short or empty (source: {doc.get('source', 'Unknown')})")
                    continue
                
                search_type = doc.get("search_type", "semantic")
                relevance_score = doc.get("relevance_score", 0.0) or doc.get("final_weighted_score", 0.0)
                keyword_match_score = doc.get("keyword_match_score", 0.0)
                has_keyword_match = keyword_match_score > 0.0 or len(doc.get("matched_keywords", [])) > 0
                
                min_score = min_relevance_score_keyword if search_type == "keyword" else min_relevance_score_semantic
                
                if search_type == "keyword" and has_keyword_match:
                    min_score = min_relevance_score_keyword
                elif search_type == "semantic":
                    min_score = min_relevance_score_semantic
                
                if relevance_score < min_score:
                    invalid_docs_count += 1
                    self.logger.debug(
                        f"Document filtered: relevance score too low ({relevance_score:.3f} < {min_score:.3f}) "
                        f"(source: {doc.get('source', 'Unknown')}, type: {search_type})"
                    )
                    continue
                
                valid_docs.append(doc)
            
            if invalid_docs_count > 0:
                self.logger.warning(
                    f"build_prompt_optimized_context: Filtered {invalid_docs_count} invalid documents "
                    f"(no content, content too short, or relevance < threshold). Valid docs: {len(valid_docs)}"
                )
            
            if not valid_docs:
                self.logger.error("build_prompt_optimized_context: No valid documents with content found")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }
            
            sorted_docs = sorted(
                valid_docs,
                key=lambda x: (
                    x.get("final_weighted_score", x.get("relevance_score", 0.0)),
                    x.get("keyword_match_score", 0.0)
                ),
                reverse=True
            )
            
            if select_balanced_documents_func:
                balanced_docs = select_balanced_documents_func(sorted_docs, max_docs=10)
            else:
                balanced_docs = self.select_balanced_documents(sorted_docs, max_docs=10)
            
            if not balanced_docs and sorted_docs:
                balanced_docs = sorted_docs[:min(8, len(sorted_docs))]
            
            sorted_docs = balanced_docs
            
            if not sorted_docs:
                self.logger.error("build_prompt_optimized_context: sorted_docs is empty after filtering")
                return {
                    "prompt_optimized_text": "",
                    "structured_documents": {},
                    "document_count": 0,
                    "total_context_length": 0
                }
            
            if generate_document_based_instructions_func:
                document_instructions = generate_document_based_instructions_func(
                    documents=sorted_docs,
                    query=query,
                    query_type=query_type
                )
            else:
                document_instructions = self.generate_document_based_instructions(
                    documents=sorted_docs,
                    query=query,
                    query_type=query_type
                )
            
            semantic_count = sum(1 for doc in sorted_docs if doc.get("search_type") == "semantic")
            keyword_count = sum(1 for doc in sorted_docs if doc.get("search_type") == "keyword")
            hybrid_count = len(sorted_docs) - semantic_count - keyword_count
            
            prompt_section = f"""## ë‹µë³€ ìƒì„± ì§€ì‹œì‚¬í•­

{document_instructions}

## ì°¸ê³  ë¬¸ì„œ ëª©ë¡

ë‹¤ìŒ {len(sorted_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
ê° ë¬¸ì„œëŠ” ê´€ë ¨ì„± ì ìˆ˜ì™€ í•µì‹¬ ë‚´ìš©ì´ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ê²€ìƒ‰ ê²°ê³¼ í†µê³„:**
- ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼: {semantic_count}ê°œ
- í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {keyword_count}ê°œ
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼: {hybrid_count}ê°œ
- ì´ ë¬¸ì„œ ìˆ˜: {len(sorted_docs)}ê°œ

**ì°¸ê³ :** ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ëŠ” ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼, í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ëŠ” í‚¤ì›Œë“œ ë§¤ì¹­ ì •ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
ë‘ ê²€ìƒ‰ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

"""
            
            for idx, doc in enumerate(sorted_docs, 1):
                relevance_score = doc.get("final_weighted_score") or doc.get("relevance_score", 0.0)
                source = doc.get("source", "Unknown")
                content = doc.get("content", "")
                
                if extract_query_relevant_sentences_func:
                    relevant_sentences = extract_query_relevant_sentences_func(
                        doc_content=content,
                        query=query,
                        extracted_keywords=extracted_keywords
                    )
                elif self.query_enhancer:
                    relevant_sentences = self.query_enhancer.extract_query_relevant_sentences(
                        content, query, extracted_keywords
                    )
                else:
                    relevant_sentences = []
                
                search_type = doc.get("search_type", "hybrid")
                search_method = doc.get("search_method", "hybrid_search")
                keyword_match_score = doc.get("keyword_match_score", 0.0)
                matched_keywords = doc.get("matched_keywords", [])
                
                doc_section = f"""
### ë¬¸ì„œ {idx}: {source} (ê´€ë ¨ì„± ì ìˆ˜: {relevance_score:.2f})

**ê²€ìƒ‰ ì •ë³´:**
- ê²€ìƒ‰ ë°©ì‹: {search_type} ({search_method})
- í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜: {keyword_match_score:.2f}
- ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(matched_keywords[:5]) if matched_keywords else 'ì—†ìŒ'}

**í•µì‹¬ ë‚´ìš©:**
"""
                
                if relevant_sentences:
                    doc_section += "\n".join([
                        f"- [ì¤‘ìš”] {sent['sentence']}"
                        for sent in relevant_sentences[:3]
                    ])
                    doc_section += "\n\n"
                
                max_content_length = 1500
                if len(content) > max_content_length:
                    content = content[:max_content_length] + "..."
                
                doc_section += f"""**ì „ì²´ ë‚´ìš©:**
{content}

---
"""
                
                prompt_section += doc_section
            
            prompt_section += """
## ë¬¸ì„œ ì¸ìš© ê·œì¹™

ë‹µë³€ì—ì„œ ìœ„ ë¬¸ì„œë¥¼ ì¸ìš©í•  ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ ëª…ì‹œí•˜ì„¸ìš”:
- "ë¬¸ì„œ {0}ì— ë”°ë¥´ë©´..." ë˜ëŠ” "[{0}] ì¸ìš© ë‚´ìš©"
- ê° ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œ

## ì¤‘ìš” ì‚¬í•­

- ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”
- ë¬¸ì„œì—ì„œ ì¶”ë¡ í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì¼ê´€ëœ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”
""".format("n")
            
            content_validation = {
                "has_document_content": False,
                "total_content_length": 0,
                "documents_with_content": 0
            }
            
            for doc in sorted_docs:
                content = doc.get("content") or doc.get("text") or doc.get("content_text", "")
                if content and len(content.strip()) >= 10:
                    content_preview = content[:100]
                    if content_preview in prompt_section:
                        content_validation["has_document_content"] = True
                        content_validation["total_content_length"] += len(content)
                        content_validation["documents_with_content"] += 1
            
            if not content_validation["has_document_content"]:
                self.logger.error(
                    f"build_prompt_optimized_context: WARNING - prompt_section does not contain actual document content! "
                    f"Documents processed: {len(sorted_docs)}, "
                    f"Prompt length: {len(prompt_section)}"
                )
            else:
                self.logger.info(
                    f"build_prompt_optimized_context: Successfully included content from {content_validation['documents_with_content']} documents "
                    f"(total content length: {content_validation['total_content_length']} chars, "
                    f"prompt length: {len(prompt_section)} chars)"
                )
            
            if not content_validation["has_document_content"] and len(sorted_docs) > 0:
                self.logger.warning(
                    f"build_prompt_optimized_context: Content validation failed, but returning prompt anyway "
                    f"(may contain instructions only without actual document content)"
                )
            
            return {
                "prompt_optimized_text": prompt_section,
                "structured_documents": {
                    "total_count": len(sorted_docs),
                    "documents": [{
                        "document_id": idx,
                        "source": doc.get("source", "Unknown"),
                        "relevance_score": doc.get("final_weighted_score") or doc.get("relevance_score", 0.0),
                        "content": (doc.get("content") or doc.get("text") or doc.get("content_text", ""))[:2000]
                    } for idx, doc in enumerate(sorted_docs, 1)]
                },
                "document_count": len(sorted_docs),
                "total_context_length": len(prompt_section),
                "content_validation": content_validation
            }
        
        except Exception as e:
            self.logger.error(f"Prompt optimized context building failed: {e}")
            return {
                "prompt_optimized_text": "",
                "structured_documents": {},
                "document_count": 0,
                "total_context_length": 0
            }
    
    def select_balanced_documents(
        self,
        sorted_docs: List[Dict[str, Any]],
        max_docs: int = 10
    ) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì˜ ê· í˜•ì„ ë§ì¶°ì„œ ë¬¸ì„œ ì„ íƒ"""
        if not sorted_docs:
            return []
        
        semantic_docs = [doc for doc in sorted_docs if doc.get("search_type") == "semantic"]
        keyword_docs = [doc for doc in sorted_docs if doc.get("search_type") == "keyword"]
        hybrid_docs = [doc for doc in sorted_docs if doc.get("search_type") not in ["semantic", "keyword"]]
        
        selected_docs = []
        
        top_count = max(1, max_docs // 2)
        selected_docs.extend(sorted_docs[:top_count])
        
        remaining_slots = max_docs - len(selected_docs)
        
        if remaining_slots > 0:
            semantic_to_add = []
            for doc in semantic_docs:
                if doc not in selected_docs:
                    semantic_to_add.append(doc)
            
            keyword_to_add = []
            for doc in keyword_docs:
                if doc not in selected_docs:
                    keyword_to_add.append(doc)
            
            max_alternate = remaining_slots // 2
            for i in range(min(max_alternate, max(len(semantic_to_add), len(keyword_to_add)))):
                if i < len(semantic_to_add) and len(selected_docs) < max_docs:
                    if semantic_to_add[i] not in selected_docs:
                        selected_docs.append(semantic_to_add[i])
                if i < len(keyword_to_add) and len(selected_docs) < max_docs:
                    if keyword_to_add[i] not in selected_docs:
                        selected_docs.append(keyword_to_add[i])
            
            if len(selected_docs) < max_docs:
                for doc in hybrid_docs:
                    if doc not in selected_docs and len(selected_docs) < max_docs:
                        selected_docs.append(doc)
            
            if len(selected_docs) < max_docs:
                for doc in sorted_docs:
                    if doc not in selected_docs and len(selected_docs) < max_docs:
                        selected_docs.append(doc)
        
        selected_docs = sorted(
            selected_docs,
            key=lambda x: (
                x.get("final_weighted_score", x.get("relevance_score", 0.0)),
                x.get("keyword_match_score", 0.0)
            ),
            reverse=True
        )
        
        return selected_docs[:max_docs]
    
    def select_high_value_documents(
        self,
        documents: List[Dict],
        query: str,
        min_relevance: float = 0.7,
        max_docs: int = 5
    ) -> List[Dict]:
        """ì •ë³´ ë°€ë„ ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ"""
        if not documents:
            return documents
        
        try:
            high_value_docs = []
            
            for doc in documents:
                doc_content = doc.get("content", "")
                if not doc_content or len(doc_content) < 20:
                    continue
                
                citation_pattern = r'[ê°€-í£]+ë²•\s*ì œ?\s*\d+\s*ì¡°'
                citations = re.findall(citation_pattern, doc_content)
                citation_count = len(citations)
                citation_score = min(1.0, citation_count / 5.0)
                
                query_words = set(query.lower().split())
                content_words = set(doc_content.lower().split())
                explanation_completeness = 0.0
                if query_words and content_words:
                    overlap = len(query_words.intersection(content_words))
                    explanation_completeness = min(1.0, overlap / max(1, len(query_words)))
                
                sentences = doc_content.split('ã€‚') or doc_content.split('.')
                avg_sentence_length = sum(len(s.strip()) for s in sentences if s.strip()) / max(1, len(sentences))
                
                descriptive_score_bonus = 0.0
                if 20 <= avg_sentence_length <= 100:
                    descriptive_score_bonus = 0.2
                elif avg_sentence_length > 100:
                    descriptive_score_bonus = 0.1
                
                explanation_completeness = min(1.0, explanation_completeness + descriptive_score_bonus)
                
                keyword_coverage = 0.0
                if query_words and content_words:
                    keyword_coverage = len(query_words.intersection(content_words)) / max(1, len(query_words))
                
                relevance_score = doc.get("final_relevance_score") or doc.get("combined_score", 0.0) or doc.get("relevance_score", 0.0)
                
                information_density = (
                    0.3 * citation_score +
                    0.3 * explanation_completeness +
                    0.2 * keyword_coverage +
                    0.2 * min(1.0, relevance_score)
                )
                
                doc["information_density_score"] = information_density
                doc["citation_count"] = citation_count
                doc["explanation_completeness"] = explanation_completeness
                
                combined_value_score = 0.6 * relevance_score + 0.4 * information_density
                doc["combined_value_score"] = combined_value_score
                
                if combined_value_score >= min_relevance:
                    high_value_docs.append(doc)
            
            high_value_docs.sort(key=lambda x: x.get("combined_value_score", 0.0), reverse=True)
            
            selected_docs = high_value_docs[:max_docs]
            
            self.logger.info(
                f"ğŸ“š [HIGH VALUE SELECTION] Selected {len(selected_docs)}/{len(documents)} documents. "
                f"Avg density: {sum(d.get('information_density_score', 0.0) for d in selected_docs) / max(1, len(selected_docs)):.3f}"
            )
            
            return selected_docs
        
        except Exception as e:
            self.logger.warning(f"High value document selection failed: {e}, using first {max_docs} documents")
            return documents[:max_docs]
    
    def generate_document_based_instructions(
        self,
        documents: List[Dict[str, Any]],
        query: str,
        query_type: str
    ) -> str:
        """ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±í•˜ë¼ëŠ” ëª…ì‹œì  ì§€ì‹œì‚¬í•­ ìƒì„±"""
        instructions = f"""ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ë¬¸ì„œë“¤ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

**ì§ˆë¬¸**: {query}
**ì§ˆë¬¸ ìœ í˜•**: {query_type}

**ë‹µë³€ ìƒì„± ê·œì¹™**:
1. **ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€**: ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
2. **ë¬¸ì„œ ì¸ìš© í•„ìˆ˜**: ë‹µë³€ì—ì„œ ë¬¸ì„œë¥¼ ì¸ìš©í•  ë•ŒëŠ” "ë¬¸ì„œ [ë²ˆí˜¸]ì— ë”°ë¥´ë©´..." í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
3. **ì •í™•ì„±**: ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ë¡ í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
4. **êµ¬ì¡°í™”**: ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”:
   - í•µì‹¬ ë‹µë³€
   - ê´€ë ¨ ë²•ë ¹ ë° ì¡°í•­
   - ì‹¤ë¬´ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­
   - ì°¸ê³ í•  ë§Œí•œ íŒë¡€ (ìˆëŠ” ê²½ìš°)
5. **ì¶œì²˜ ëª…ì‹œ**: ê° ì¸ìš©ë¬¸ì— ëŒ€í•´ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
"""
        
        return instructions

