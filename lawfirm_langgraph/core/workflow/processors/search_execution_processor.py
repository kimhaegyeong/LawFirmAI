# -*- coding: utf-8 -*-
"""
Search Execution Processor
ê²€ìƒ‰ ì‹¤í–‰ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” í”„ë¡œì„¸ì„œ
"""

import logging
import os
import sys
import time
from concurrent.futures import ThreadPoolExecutor, CancelledError as FutureCancelledError
from typing import Any, Dict, List, Optional, Tuple

try:
    from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
except ImportError:
    from core.workflow.state.state_definitions import LegalWorkflowState
try:
    from lawfirm_langgraph.core.workflow.state.state_helpers import ensure_state_group, set_retrieved_docs
except ImportError:
    from core.workflow.state.state_helpers import ensure_state_group, set_retrieved_docs
try:
    from lawfirm_langgraph.core.workflow.utils.workflow_constants import WorkflowConstants
except ImportError:
    from core.workflow.utils.workflow_constants import WorkflowConstants
try:
    from lawfirm_langgraph.core.workflow.utils.query_diversifier import QueryDiversifier
except ImportError:
    from core.workflow.utils.query_diversifier import QueryDiversifier
try:
    from lawfirm_langgraph.core.workflow.utils.search_result_balancer import SearchResultBalancer
except ImportError:
    from core.workflow.utils.search_result_balancer import SearchResultBalancer


class SearchExecutionProcessor:
    """ê²€ìƒ‰ ì‹¤í–‰ í”„ë¡œì„¸ì„œ"""

    def __init__(
        self,
        search_handler,
        logger,
        config,
        keyword_search_func=None,
        get_state_value_func=None,
        set_state_value_func=None,
        get_query_type_str_func=None,
        determine_search_parameters_func=None,
        save_metadata_safely_func=None,
        update_processing_time_func=None,
        handle_error_func=None,
        semantic_search_engine=None
    ):
        self.search_handler = search_handler
        self.logger = logger
        self.config = config
        self.keyword_search_func = keyword_search_func
        self._get_state_value_func = get_state_value_func
        self._set_state_value_func = set_state_value_func
        self._get_query_type_str_func = get_query_type_str_func
        self._determine_search_parameters_func = determine_search_parameters_func
        self._save_metadata_safely_func = save_metadata_safely_func
        self._update_processing_time_func = update_processing_time_func
        self._handle_error_func = handle_error_func
        
        # semantic_search_engine ì €ì¥ (íƒ€ì…ë³„ ê²€ìƒ‰ìš©)
        self.semantic_search_engine = semantic_search_engine
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ë‹¤ë³€í™” ë° ê²°ê³¼ ê· í˜• ì¡°ì • ìœ í‹¸ë¦¬í‹°
        self.query_diversifier = QueryDiversifier()
        self.result_balancer = SearchResultBalancer(min_per_type=1, max_per_type=5)
        
        # State ì ‘ê·¼ ìºì‹± (ì„±ëŠ¥ ìµœì í™”)
        self._state_cache = {}
        self._state_cache_key = None
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ì¤‘ë³µ ë°©ì§€ ìºì‹œ
        self._executed_queries = set()  # ì‹¤í–‰ëœ ì¿¼ë¦¬ ì¶”ì 

    def get_search_params(self, state: LegalWorkflowState) -> Dict[str, Any]:
        """ê²€ìƒ‰ì— í•„ìš”í•œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸° (State ì ‘ê·¼ ìµœì í™”)"""
        from core.workflow.state.state_helpers import get_field
        import hashlib

        # State ìºì‹±: state í•´ì‹œë¡œ ìºì‹œ í‚¤ ìƒì„±
        state_str = str(sorted(state.items())) if isinstance(state, dict) else str(state)
        state_hash = hashlib.md5(state_str.encode()).hexdigest()
        
        # ìºì‹œ íˆíŠ¸ í™•ì¸
        if self._state_cache_key == state_hash and self._state_cache:
            self.logger.debug("âœ… [PERFORMANCE] State cache hit in get_search_params")
            return self._state_cache.copy()
        
        # ìºì‹œ ë¯¸ìŠ¤: Stateì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
        # Multi-Query ê°•í™”: stateì˜ ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ optimized_queries ì°¾ê¸° (ìˆœì„œ ì¤‘ìš”)
        # _get_state_valueê°€ Noneì„ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¨¼ì € ì§ì ‘ í™•ì¸
        optimized_queries = None
        
        # ë””ë²„ê¹…: state êµ¬ì¡° í™•ì¸ (ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ)
        if os.getenv("DEBUG_STATE_ACCESS", "false").lower() == "true":
            state_keys = list(state.keys()) if isinstance(state, dict) else []
            if self.logger.isEnabledFor(logging.DEBUG):
                self.logger.debug(f"ğŸ” [MULTI-QUERY] get_search_params: state keys={state_keys}")
        
        # searchì™€ common ê·¸ë£¹ì˜ êµ¬ì¡°ë„ í™•ì¸
        if "search" in state and isinstance(state["search"], dict):
            search_keys = list(state["search"].keys())
            self.logger.debug(f"[MULTI-QUERY] search group keys={search_keys}")
        if "common" in state and isinstance(state.get("common"), dict):
            common_keys = list(state["common"].keys())
            self.logger.debug(f"[MULTI-QUERY] common group keys={common_keys}")
            if "search" in state["common"] and isinstance(state["common"]["search"], dict):
                common_search_keys = list(state["common"]["search"].keys())
                self.logger.debug(f"[MULTI-QUERY] common.search keys={common_search_keys}")
        
        # 1. top-level stateì—ì„œ ì§ì ‘ í™•ì¸ (ê°€ì¥ ìš°ì„ )
        if "optimized_queries" in state and isinstance(state["optimized_queries"], dict) and len(state["optimized_queries"]) > 0:
            optimized_queries = state["optimized_queries"]
            self.logger.debug(f"[MULTI-QUERY] Found optimized_queries in top-level state (keys: {list(optimized_queries.keys())})")
            self.logger.info(f"ğŸ” [MULTI-QUERY] Found optimized_queries in top-level state (keys: {list(optimized_queries.keys())})")
        
        # 2. search groupì—ì„œ í™•ì¸ (top-levelì— ì—†ìœ¼ë©´)
        if (not optimized_queries or (isinstance(optimized_queries, dict) and len(optimized_queries) == 0)) and "search" in state and isinstance(state["search"], dict):
            search_group = state["search"]
            search_optimized = search_group.get("optimized_queries")
            self.logger.debug(f"[MULTI-QUERY] Checking search group: optimized_queries type={type(search_optimized)}, value={search_optimized}")
            if search_optimized and isinstance(search_optimized, dict):
                if len(search_optimized) > 0:
                    optimized_queries = search_optimized
                    self.logger.debug(f"[MULTI-QUERY] Found optimized_queries in search group (keys: {list(optimized_queries.keys())})")
                    self.logger.info(f"ğŸ” [MULTI-QUERY] Found optimized_queries in search group (keys: {list(optimized_queries.keys())})")
                else:
                    self.logger.debug("[MULTI-QUERY] search group optimized_queries is empty dict")
            else:
                self.logger.debug(f"[MULTI-QUERY] search group optimized_queries is not a dict or None: {search_optimized}")
        
        # 3. common.searchì—ì„œ í™•ì¸ (ìœ„ì—ì„œ ì°¾ì§€ ëª»í–ˆìœ¼ë©´)
        if (not optimized_queries or len(optimized_queries) == 0) and "common" in state and isinstance(state.get("common"), dict):
            common_search = state["common"].get("search", {})
            if isinstance(common_search, dict) and common_search.get("optimized_queries"):
                optimized_queries = common_search["optimized_queries"]
                self.logger.debug(f"[MULTI-QUERY] Found optimized_queries in common.search (keys: {list(optimized_queries.keys())})")
                self.logger.info(f"ğŸ” [MULTI-QUERY] Found optimized_queries in common.search (keys: {list(optimized_queries.keys())})")
        # 4. _get_state_valueë¡œ í™•ì¸ (fallback)
        if not optimized_queries or len(optimized_queries) == 0:
            optimized_queries = self._get_state_value(state, "optimized_queries", {})
            if optimized_queries and len(optimized_queries) > 0:
                self.logger.debug(f"[MULTI-QUERY] Found optimized_queries via _get_state_value (keys: {list(optimized_queries.keys())})")
                self.logger.info(f"ğŸ” [MULTI-QUERY] Found optimized_queries via _get_state_value (keys: {list(optimized_queries.keys())})")
            else:
                self.logger.debug(f"[MULTI-QUERY] _get_state_value returned: {optimized_queries}")
        
        # optimized_queriesê°€ Noneì´ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
        if optimized_queries is None:
            optimized_queries = {}
            self.logger.debug("[MULTI-QUERY] optimized_queries was None, initialized to empty dict")
        
        # 5. Global cacheì—ì„œ í™•ì¸ (state reduction ëŒ€ì‘)
        if (not optimized_queries or len(optimized_queries) == 0):
            try:
                from core.shared.wrappers.node_wrappers import _global_search_results_cache
                if _global_search_results_cache and isinstance(_global_search_results_cache, dict):
                    if "search" in _global_search_results_cache and isinstance(_global_search_results_cache["search"], dict):
                        cached_optimized = _global_search_results_cache["search"].get("optimized_queries")
                        if cached_optimized and isinstance(cached_optimized, dict) and len(cached_optimized) > 0:
                            optimized_queries = cached_optimized.copy()
                            self.logger.debug(f"[MULTI-QUERY] Found optimized_queries in global cache (keys: {list(optimized_queries.keys())})")
                            self.logger.info(f"ğŸ” [MULTI-QUERY] Found optimized_queries in global cache (keys: {list(optimized_queries.keys())})")
            except Exception as e:
                self.logger.debug(f"Failed to get optimized_queries from global cache: {e}")
        
        # 6. get_fieldë¡œ í™•ì¸ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        if not optimized_queries or len(optimized_queries) == 0:
            optimized_queries_raw = get_field(state, "optimized_queries")
            if optimized_queries_raw and isinstance(optimized_queries_raw, dict) and len(optimized_queries_raw) > 0:
                optimized_queries = optimized_queries_raw
                self.logger.info("ğŸ” [MULTI-QUERY] Found optimized_queries via get_field")
        
        search_params = self._get_state_value(state, "search_params", {})
        query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
        legal_field = self._get_state_value(state, "legal_field", "")
        extracted_keywords = self._get_state_value(state, "extracted_keywords", [])
        original_query = self._get_state_value(state, "query", "")

        if "search" in state and isinstance(state["search"], dict):
            search_group = state["search"]
            if "extracted_keywords" in search_group and search_group["extracted_keywords"]:
                extracted_keywords = search_group["extracted_keywords"]

            if search_group.get("optimized_queries") and isinstance(search_group["optimized_queries"], dict) and len(search_group["optimized_queries"]) > 0:
                # search groupì˜ optimized_queriesê°€ ë” ì™„ì „í•˜ë©´ ì‚¬ìš©
                if "multi_queries" in search_group["optimized_queries"] or len(search_group["optimized_queries"]) > len(optimized_queries):
                    optimized_queries = search_group["optimized_queries"]
                    self.logger.debug("ğŸ” [MULTI-QUERY] Using optimized_queries from search group (more complete)")
                if not extracted_keywords and "expanded_keywords" in optimized_queries:
                    extracted_keywords = optimized_queries.get("expanded_keywords", [])

            if search_group.get("search_params") and isinstance(search_group["search_params"], dict) and len(search_group["search_params"]) > 0:
                search_params = search_group["search_params"]

        if not extracted_keywords:
            extracted_keywords_raw = get_field(state, "extracted_keywords")
            if extracted_keywords_raw and len(extracted_keywords_raw) > 0:
                extracted_keywords = extracted_keywords_raw
        
        # Multi-Query ë³µì›: optimized_queriesê°€ ìˆì§€ë§Œ multi_queriesê°€ ì—†ëŠ” ê²½ìš° stateì—ì„œ ì§ì ‘ í™•ì¸
        if optimized_queries and "multi_queries" not in optimized_queries:
            # stateì˜ ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ multi_queries í™•ì¸ (ìˆœì„œ ì¤‘ìš”)
            state_multi_queries = None
            # 1. top-level stateì—ì„œ ì§ì ‘ í™•ì¸ (ê°€ì¥ ìš°ì„ )
            if "optimized_queries" in state and isinstance(state["optimized_queries"], dict):
                state_multi_queries = state["optimized_queries"].get("multi_queries")
                if state_multi_queries:
                    self.logger.info(f"ğŸ” [MULTI-QUERY] Found multi_queries in top-level state (count: {len(state_multi_queries)})")
            # 2. search groupì—ì„œ í™•ì¸
            if not state_multi_queries and "search" in state and isinstance(state.get("search"), dict):
                search_optimized = state["search"].get("optimized_queries", {})
                if isinstance(search_optimized, dict):
                    state_multi_queries = search_optimized.get("multi_queries")
                    if state_multi_queries:
                        self.logger.info(f"ğŸ” [MULTI-QUERY] Found multi_queries in search group (count: {len(state_multi_queries)})")
            # 3. common.searchì—ì„œ í™•ì¸
            if not state_multi_queries and "common" in state and isinstance(state.get("common"), dict):
                common_search = state["common"].get("search", {})
                if isinstance(common_search, dict) and common_search.get("optimized_queries"):
                    common_optimized = common_search["optimized_queries"]
                    if isinstance(common_optimized, dict):
                        state_multi_queries = common_optimized.get("multi_queries")
                        if state_multi_queries:
                            self.logger.info(f"ğŸ” [MULTI-QUERY] Found multi_queries in common.search (count: {len(state_multi_queries)})")
            # 4. common groupì—ì„œ ì§ì ‘ í™•ì¸
            if not state_multi_queries and "common" in state and isinstance(state.get("common"), dict):
                common_optimized = state["common"].get("optimized_queries", {})
                if isinstance(common_optimized, dict):
                    state_multi_queries = common_optimized.get("multi_queries")
                    if state_multi_queries:
                        self.logger.info(f"ğŸ” [MULTI-QUERY] Found multi_queries in common group (count: {len(state_multi_queries)})")
            
            if state_multi_queries:
                if not optimized_queries:
                    optimized_queries = {}
                optimized_queries["multi_queries"] = state_multi_queries
                self.logger.info(f"âœ… [MULTI-QUERY] Restored multi_queries from state (count: {len(state_multi_queries)})")
            else:
                # ë””ë²„ê¹…: state êµ¬ì¡° í™•ì¸
                self.logger.warning(f"âš ï¸ [MULTI-QUERY] Could not find multi_queries in state. State keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")
                if isinstance(state, dict) and "search" in state:
                    self.logger.warning(f"âš ï¸ [MULTI-QUERY] search group keys: {list(state['search'].keys()) if isinstance(state['search'], dict) else 'N/A'}")
        
        # Multi-Query í™•ì¸ ë¡œê·¸ (í•­ìƒ ì¶œë ¥)
        has_multi = optimized_queries and "multi_queries" in optimized_queries
        keys_str = list(optimized_queries.keys()) if optimized_queries else "None"
        self.logger.debug(f"[MULTI-QUERY] get_search_params: optimized_queries keys={keys_str}, has_multi_queries={has_multi}")
        if has_multi:
            self.logger.info(f"ğŸ” [MULTI-QUERY] get_search_params: Found multi_queries with {len(optimized_queries.get('multi_queries', []))} queries")
        elif optimized_queries:
            self.logger.warning(f"âš ï¸ [MULTI-QUERY] get_search_params: optimized_queries exists but no multi_queries (keys: {keys_str})")

        if not search_params or len(search_params) == 0:
            search_params_raw = get_field(state, "search_params")
            if search_params_raw and len(search_params_raw) > 0:
                search_params = search_params_raw

        if not original_query and "input" in state and isinstance(state.get("input"), dict):
            original_query = state["input"].get("query", "")

        result = {
            "optimized_queries": optimized_queries,
            "search_params": search_params,
            "query_type_str": query_type_str,
            "legal_field": legal_field,
            "extracted_keywords": extracted_keywords,
            "original_query": original_query
        }
        
        # ìºì‹œ ì €ì¥
        self._state_cache = result.copy()
        self._state_cache_key = state_hash
        
        return result

    def _generate_search_cache_key(
        self,
        optimized_queries: Dict[str, Any],
        search_params: Dict[str, Any],
        original_query: str
    ) -> Optional[str]:
        """ê²€ìƒ‰ ê²°ê³¼ ìºì‹±ì„ ìœ„í•œ í‚¤ ìƒì„±"""
        import hashlib
        
        try:
            # ìºì‹œ í‚¤ ìƒì„± ìš”ì†Œ
            cache_key_parts = [
                str(optimized_queries.get("semantic_query", "")),
                str(original_query),
                str(search_params.get("semantic_k", "")),
                str(search_params.get("keyword_k", "")),
                ",".join(sorted(optimized_queries.get("keyword_queries", []))) if optimized_queries.get("keyword_queries") else ""
            ]
            cache_key_str = ":".join(cache_key_parts)
            cache_key = hashlib.md5(cache_key_str.encode('utf-8')).hexdigest()
            return f"search_results:{cache_key}"
        except Exception as e:
            if self.logger.isEnabledFor(logging.DEBUG):
                self.logger.debug(f"Failed to generate cache key: {e}")
            return None
    
    def _get_cached_search_results(
        self,
        cache_key: str
    ) -> Optional[Dict[str, Any]]:
        """ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # PerformanceCacheê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if hasattr(self, 'performance_optimizer') and hasattr(self.performance_optimizer, 'cache'):
                cached_result = self.performance_optimizer.cache.get_cached_answer(cache_key, "search")
                if cached_result and isinstance(cached_result, dict):
                    if self.logger.isEnabledFor(logging.DEBUG):
                        self.logger.debug(f"âœ… [SEARCH CACHE HIT] Found cached search results: {cache_key[:16]}...")
                    return cached_result
        except Exception as e:
            if self.logger.isEnabledFor(logging.DEBUG):
                self.logger.debug(f"Failed to get cached search results: {e}")
        return None
    
    def _cache_search_results(
        self,
        cache_key: str,
        search_results: Dict[str, Any]
    ) -> None:
        """ê²€ìƒ‰ ê²°ê³¼ ìºì‹±"""
        try:
            # PerformanceCacheê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if hasattr(self, 'performance_optimizer') and hasattr(self.performance_optimizer, 'cache'):
                self.performance_optimizer.cache.cache_answer(
                    cache_key,
                    search_results,
                    query_type="search",
                    ttl=3600  # 1ì‹œê°„ TTL
                )
                if self.logger.isEnabledFor(logging.DEBUG):
                    self.logger.debug(f"âœ… [SEARCH CACHE] Cached search results: {cache_key[:16]}...")
        except Exception as e:
            if self.logger.isEnabledFor(logging.DEBUG):
                self.logger.debug(f"Failed to cache search results: {e}")
    
    def _apply_cached_results(
        self,
        state: LegalWorkflowState,
        cached_results: Dict[str, Any]
    ) -> LegalWorkflowState:
        """ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ stateì— ì ìš©"""
        semantic_results = cached_results.get("semantic_results", [])
        keyword_results = cached_results.get("keyword_results", [])
        semantic_count = len(semantic_results)
        keyword_count = len(keyword_results)
        
        self._set_state_value(state, "semantic_results", semantic_results)
        self._set_state_value(state, "keyword_results", keyword_results)
        self._set_state_value(state, "semantic_count", semantic_count)
        self._set_state_value(state, "keyword_count", keyword_count)
        
        merged_docs = semantic_results + keyword_results
        set_retrieved_docs(state, merged_docs)
        
        return state

    def _calculate_result_quality(
        self,
        results: List[Dict[str, Any]]
    ) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not results:
            return 0.0
        
        # í‰ê·  ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°
        scores = []
        for doc in results:
            score = doc.get("relevance_score") or doc.get("score") or doc.get("final_weighted_score", 0.0)
            if isinstance(score, (int, float)):
                scores.append(float(score))
        
        if not scores:
            return 0.0
        
        avg_score = sum(scores) / len(scores)
        return min(1.0, max(0.0, avg_score))
    
    def _adjust_search_priority(
        self,
        semantic_results: List[Dict[str, Any]],
        keyword_results: List[Dict[str, Any]],
        query_type: str
    ) -> Dict[str, int]:
        """ê²€ìƒ‰ í’ˆì§ˆì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„ ì¡°ì •"""
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        semantic_quality = self._calculate_result_quality(semantic_results)
        keyword_quality = self._calculate_result_quality(keyword_results)
        
        # ê¸°ë³¸ ìš°ì„ ìˆœìœ„ (ë‚®ì€ ìˆ«ìê°€ ë†’ì€ ìš°ì„ ìˆœìœ„)
        priorities = {
            "semantic": 1,  # ê¸°ë³¸ ìš°ì„ ìˆœìœ„
            "keyword": 1,
            "multi_query": 2,  # ë‚®ì€ ìš°ì„ ìˆœìœ„
            "direct_statute": 2
        }
        
        # í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ë³´ì¡° ê²€ìƒ‰ ìš°ì„ ìˆœìœ„ ìƒí–¥
        if semantic_quality < 0.5 and keyword_quality < 0.5:
            priorities["multi_query"] = 1  # ìš°ì„ ìˆœìœ„ ìƒí–¥
            priorities["direct_statute"] = 1
        
        # ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„ ì¡°ì •
        if query_type == "law_inquiry":
            priorities["direct_statute"] = 0  # ìµœìš°ì„ 
        elif query_type == "precedent_search":
            priorities["semantic"] = 0  # semantic ìµœìš°ì„ 
        
        return priorities

    def _calculate_dynamic_k_values(
        self,
        query_type_str: str,
        query_complexity: int,
        keyword_count: int,
        is_retry: bool,
        original_query: str,
        search_params: Dict[str, Any]
    ) -> Tuple[int, int]:
        """ë‹¤ì°¨ì› ë™ì  k ê°’ ê³„ì‚° (ê°œì„ : ì¿¼ë¦¬ íƒ€ì…, ë³µì¡ë„, í‚¤ì›Œë“œ ìˆ˜ ë“± ì¢…í•© ê³ ë ¤)"""
        
        # ê¸°ë³¸ê°’
        base_semantic_k = search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K)
        base_keyword_k = search_params.get("keyword_k", WorkflowConstants.KEYWORD_SEARCH_K)
        
        # 1. ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ì¡°ì •
        type_multiplier = {
            "precedent_search": 1.3,  # íŒë¡€ ê²€ìƒ‰: ë” ë§ì€ ê²°ê³¼ (1.5 â†’ 1.3ìœ¼ë¡œ ì™„í™”)
            "law_inquiry": 1.2,        # ë²•ë ¹ ì¡°íšŒ: ë” ë§ì€ ê²°ê³¼ (1.3 â†’ 1.2ë¡œ ì™„í™”)
            "legal_advice": 1.1,       # ë²•ë¥  ìƒë‹´: ì•½ê°„ ì¦ê°€ (1.2 â†’ 1.1ë¡œ ì™„í™”)
            "general_question": 1.0,   # ì¼ë°˜ ì§ˆë¬¸: ê¸°ë³¸ê°’
            "term_explanation": 0.9,   # ìš©ì–´ ì„¤ëª…: ê°ì†Œ (ë¹ ë¥¸ ì‘ë‹µ)
            "procedure_guide": 1.0     # ì ˆì°¨ ì•ˆë‚´: ê¸°ë³¸ê°’
        }
        multiplier = type_multiplier.get(query_type_str, 1.0)
        
        # 2. ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¥¸ ì¡°ì •
        query_length = len(original_query) if original_query else 0
        if query_complexity > 100 or query_length > 100:
            multiplier += 0.2  # ë³µì¡í•œ ì¿¼ë¦¬: ë” ë§ì€ ê²°ê³¼
        elif query_complexity > 50 or query_length > 50:
            multiplier += 0.1  # ì¤‘ê°„ ë³µì¡ë„: ì•½ê°„ ì¦ê°€
        elif query_length < 15:  # ë§¤ìš° ì§§ì€ ì¿¼ë¦¬
            multiplier = max(0.7, multiplier - 0.2)  # ê°ì†Œ (ë¹ ë¥¸ ì‘ë‹µ)
        
        # 3. í‚¤ì›Œë“œ ìˆ˜ì— ë”°ë¥¸ ì¡°ì •
        if keyword_count > 10:
            multiplier += 0.15  # ë§ì€ í‚¤ì›Œë“œ: ë” ë§ì€ ê²°ê³¼
        elif keyword_count > 5:
            multiplier += 0.1
        elif keyword_count == 0:
            multiplier = max(0.8, multiplier - 0.1)  # í‚¤ì›Œë“œ ì—†ìŒ: ê°ì†Œ
        
        # 4. ì¬ì‹œë„ ì—¬ë¶€ì— ë”°ë¥¸ ì¡°ì •
        if is_retry:
            multiplier += 0.3  # ì¬ì‹œë„: ë” ë§ì€ ê²°ê³¼ (0.5 â†’ 0.3ìœ¼ë¡œ ì™„í™”)
        
        # 5. ì„±ëŠ¥ ìµœì í™” ëª¨ë“œ ì ìš©
        performance_mode = os.getenv("SEARCH_PERFORMANCE_MODE", "balanced").lower()
        if performance_mode == "fast":
            # ë¹ ë¥¸ ëª¨ë“œ: k ê°’ì„ 20-30% ê°ì†Œ
            multiplier *= 0.75
        elif performance_mode == "balanced":
            # ê· í˜• ëª¨ë“œ: ê¸°ë³¸ê°’ ìœ ì§€
            pass
        elif performance_mode == "quality":
            # í’ˆì§ˆ ìš°ì„  ëª¨ë“œ: k ê°’ì„ 10-20% ì¦ê°€
            multiplier *= 1.15
        
        # 6. ìµœì¢… k ê°’ ê³„ì‚° (ìƒí•œ/í•˜í•œ ì ìš©)
        semantic_k = int(base_semantic_k * multiplier)
        keyword_k = int(base_keyword_k * multiplier)
        
        # ìƒí•œ/í•˜í•œ ì ìš©
        semantic_k = max(5, min(semantic_k, 20))   # ìµœì†Œ 5ê°œ, ìµœëŒ€ 20ê°œ
        keyword_k = max(3, min(keyword_k, 15))     # ìµœì†Œ 3ê°œ, ìµœëŒ€ 15ê°œ
        
        return semantic_k, keyword_k

    def execute_searches_parallel(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
        try:
            start_time = time.time()

            debug_mode = os.getenv("DEBUG_SEARCH", "false").lower() == "true"

            params = self.get_search_params(state)
            optimized_queries = params["optimized_queries"]
            search_params = params["search_params"]
            query_type_str = params["query_type_str"]
            legal_field = params["legal_field"]
            extracted_keywords = params["extracted_keywords"]
            original_query = params["original_query"]

            # ì„±ëŠ¥ ìµœì í™”: extracted_keywordsë¥¼ í•œ ë²ˆë§Œ í™•ì¸ (ì¤‘ë³µ ì ‘ê·¼ ì œê±°)
            if not extracted_keywords or len(extracted_keywords) == 0:
                # í•œ ë²ˆì— ëª¨ë“  ê°€ëŠ¥í•œ ìœ„ì¹˜ í™•ì¸
                extracted_keywords = (
                    self._get_state_value(state, "extracted_keywords", []) or
                    (state.get("search", {}).get("extracted_keywords", []) if isinstance(state.get("search"), dict) else []) or
                    state.get("extracted_keywords", []) or
                    []
                )
                if debug_mode:
                    self.logger.debug(f"extracted_keywords from batch was empty, got {len(extracted_keywords)} from state directly")
            elif debug_mode:
                self.logger.debug(f"extracted_keywords from batch: {len(extracted_keywords)} keywords")

            # ë¡œê¹… ìµœì í™”: ë¡œê¹… ë ˆë²¨ ì²´í¬ ë° ë°°ì¹˜ ë¡œê¹…
            if self.logger.isEnabledFor(logging.DEBUG):
                debug_info = {
                    "optimized_queries": {
                        "type": type(optimized_queries).__name__,
                        "exists": bool(optimized_queries),
                        "keys": list(optimized_queries.keys()) if isinstance(optimized_queries, dict) else None
                    },
                    "search_params": {
                        "type": type(search_params).__name__,
                        "exists": bool(search_params),
                        "keys": list(search_params.keys()) if isinstance(search_params, dict) else None
                    }
                }
                self.logger.debug(f"execute_searches_parallel: START - {debug_info}")

            semantic_query_value = optimized_queries.get("semantic_query", "") if optimized_queries else ""

            if not semantic_query_value or not str(semantic_query_value).strip():
                if original_query:
                    if debug_mode:
                        self.logger.warning(f"semantic_query is empty in execute_searches_parallel, using base query: '{original_query[:50]}...'")
                    optimized_queries["semantic_query"] = original_query
                    semantic_query_value = original_query

            has_semantic_query = optimized_queries and semantic_query_value and len(str(semantic_query_value).strip()) > 0
            keyword_queries_value = optimized_queries.get("keyword_queries", []) if optimized_queries else []

            if not keyword_queries_value or len(keyword_queries_value) == 0:
                if original_query:
                    if debug_mode:
                        self.logger.warning("keyword_queries is empty in execute_searches_parallel, using base query")
                    optimized_queries["keyword_queries"] = [original_query]
                    keyword_queries_value = [original_query]

            has_keyword_queries = optimized_queries and keyword_queries_value and len(keyword_queries_value) > 0

            # ë¡œê¹… ìµœì í™”: ê²€ì¦ ì •ë³´ ë°°ì¹˜ ë¡œê¹…
            if self.logger.isEnabledFor(logging.DEBUG):
                validation_info = {
                    "semantic_query": semantic_query_value[:50] if semantic_query_value else 'EMPTY',
                    "has_semantic_query": has_semantic_query,
                    "keyword_queries_count": len(keyword_queries_value) if keyword_queries_value else 0,
                    "has_keyword_queries": has_keyword_queries,
                    "search_params": {
                        "is_none": search_params is None,
                        "is_empty": search_params == {},
                        "keys": list(search_params.keys()) if search_params else []
                    }
                }
                self.logger.debug(f"Validation: {validation_info}")

            if not search_params or not isinstance(search_params, dict) or len(search_params) == 0:
                self.logger.warning("ğŸ” [SEARCH] search_params is empty, setting default values")
                search_params = self._determine_search_parameters(
                    query_type=query_type_str,
                    query_complexity=len(original_query) if original_query else 0,
                    keyword_count=len(extracted_keywords) if extracted_keywords else 0,
                    is_retry=False
                )
                self.logger.info(f"ğŸ” [SEARCH] Default search_params set: {search_params}")

            optimized_queries_valid = optimized_queries and isinstance(optimized_queries, dict) and len(optimized_queries) > 0
            search_params_valid = search_params and isinstance(search_params, dict) and len(search_params) > 0
            # ë¡œê¹… ìµœì í™”: ê²€ì¦ ì²´í¬ ë°°ì¹˜ ë¡œê¹…
            if self.logger.isEnabledFor(logging.DEBUG):
                validation_check_info = {
                    "optimized_queries_valid": optimized_queries_valid,
                    "optimized_queries": {
                        "type": type(optimized_queries).__name__,
                        "len": len(optimized_queries) if isinstance(optimized_queries, dict) else 'N/A'
                    },
                    "search_params_valid": search_params_valid,
                    "search_params": {
                        "type": type(search_params).__name__,
                        "len": len(search_params) if isinstance(search_params, dict) else 'N/A'
                    },
                    "has_semantic_query": has_semantic_query
                }
                self.logger.debug(f"ğŸ” [SEARCH] Validation check: {validation_check_info}")

            if not optimized_queries_valid or not search_params_valid or not has_semantic_query:
                self.logger.warning(f"ğŸ” [SEARCH] PARALLEL SEARCH SKIP: optimized_queries_valid={optimized_queries_valid}, search_params_valid={search_params_valid}, has_semantic_query={has_semantic_query}")
                if debug_mode:
                    self.logger.warning("Optimized queries or search params not found")
                    self.logger.debug(f"PARALLEL SEARCH SKIP: optimized_queries={optimized_queries is not None}, search_params={search_params is not None}")
                self._set_state_value(state, "semantic_results", [])
                self._set_state_value(state, "keyword_results", [])
                self._set_state_value(state, "semantic_count", 0)
                self._set_state_value(state, "keyword_count", 0)
                return state

            semantic_results = []
            semantic_count = 0
            keyword_results = []
            keyword_count = 0

            # Multi-Query í™•ì¸ ë¡œê·¸ (ë¡œê¹… ìµœì í™”)
            multi_queries = optimized_queries.get("multi_queries", [])
            if multi_queries and debug_mode:
                self.logger.debug(f"[MULTI-QUERY] execute_searches_parallel: Found {len(multi_queries)} multi-queries in optimized_queries")
                self.logger.debug(f"ğŸ” [MULTI-QUERY] execute_searches_parallel: Found {len(multi_queries)} multi-queries")
            elif not multi_queries and debug_mode:
                self.logger.debug("âš ï¸ [MULTI-QUERY] execute_searches_parallel: No multi_queries in optimized_queries")
            
            if debug_mode:
                self.logger.debug(f"PARALLEL SEARCH START: semantic_query={optimized_queries.get('semantic_query', 'N/A')[:50]}, keyword_queries={len(optimized_queries.get('keyword_queries', []))}, multi_queries={len(multi_queries) if multi_queries else 0}, original_query={original_query[:50] if original_query else 'N/A'}...")

            # ì„±ëŠ¥ ìµœì í™”: extracted_keywords ì¬í™•ì¸ ì œê±° (ì´ë¯¸ ìœ„ì—ì„œ í™•ì¸í•¨)
            final_keywords = extracted_keywords if extracted_keywords else []
            keywords_copy = list(final_keywords) if final_keywords else []
            
            if debug_mode:
                self.logger.debug(f"Final extracted_keywords: {len(final_keywords)} keywords, keywords_copy: {len(keywords_copy)} keywords")

            # ì„±ëŠ¥ ìµœì í™”: ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ë˜ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰
            # ë²•ë ¹ ì¡°ë¬¸ ì§ì ‘ ê²€ìƒ‰ë„ ë³‘ë ¬í™”í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            semantic_results, semantic_count = [], 0
            keyword_results, keyword_count = [], 0
            
            # ì¡°ê¸° ì¢…ë£Œ ìµœì í™”: ë™ì  ì„ê³„ê°’ ê³„ì‚° (ê°œì„ : ë‹¤ì°¨ì› ë™ì  k ê°’ ì¡°ì •)
            # query_complexityì™€ is_retry ê°’ ê°€ì ¸ì˜¤ê¸°
            query_complexity = self._get_state_value(state, "query_complexity", len(original_query) if original_query else 0)
            if isinstance(query_complexity, str):
                # complexity_levelì´ ë¬¸ìì—´ì¸ ê²½ìš° ìˆ«ìë¡œ ë³€í™˜
                complexity_map = {"simple": 20, "moderate": 50, "complex": 100}
                query_complexity = complexity_map.get(query_complexity.lower(), 50)
            elif not isinstance(query_complexity, int):
                query_complexity = len(original_query) if original_query else 0
            
            is_retry = self._get_state_value(state, "needs_retry", False)
            if not isinstance(is_retry, bool):
                is_retry = False
            
            # ë‹¤ì°¨ì› ë™ì  k ê°’ ê³„ì‚°
            semantic_k, keyword_k = self._calculate_dynamic_k_values(
                query_type_str=query_type_str,
                query_complexity=query_complexity,
                keyword_count=len(extracted_keywords) if extracted_keywords else 0,
                is_retry=is_retry,
                original_query=original_query,
                search_params=search_params
            )
            
            # search_paramsì— ë™ì ìœ¼ë¡œ ê³„ì‚°ëœ k ê°’ ì—…ë°ì´íŠ¸
            search_params["semantic_k"] = semantic_k
            search_params["keyword_k"] = keyword_k
            
            # ê²€ìƒ‰ ê²°ê³¼ ìºì‹± í™•ì¸ (ì¬ì‹œë„ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
            if not is_retry:
                cache_key = self._generate_search_cache_key(
                    optimized_queries, search_params, original_query
                )
                if cache_key:
                    cached_results = self._get_cached_search_results(cache_key)
                    if cached_results:
                        self.logger.info("âœ… [SEARCH CACHE HIT] Using cached search results")
                        return self._apply_cached_results(state, cached_results)
            
            min_required_results = semantic_k + keyword_k
            early_exit_threshold = int(min_required_results * 0.9)  # 10% ì—¬ìœ  (1.0 â†’ 0.9)
            max_results_threshold = min_required_results * 2  # ìµœëŒ€ 2ë°°ê¹Œì§€ë§Œ
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ê°•í™”: ìµœì†Œ ê²°ê³¼ ìˆ˜ ì„¤ì • (ë™ì  k ê°’ì— ë§ì¶° ì¡°ì •)
            min_semantic_for_early_exit = max(3, int(semantic_k * 0.4))  # 40% (ê¸°ì¡´ //2 ëŒ€ì‹ )
            min_keyword_for_early_exit = max(2, int(keyword_k * 0.4))
            
            # ë²•ë ¹ ì¡°ë¬¸ ì§ì ‘ ê²€ìƒ‰ë„ ë³‘ë ¬ ì‹¤í–‰ (max_workers=3)
            needs_direct_statute = original_query and query_type_str == "law_inquiry"
            
            # Multi-Query ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”: Multi-Query ì¤€ë¹„ (ì ì‘í˜• í™œìš©)
            multi_queries = optimized_queries.get("multi_queries", [])
            multi_queries_to_process = []
            if multi_queries and len(multi_queries) > 1:
                multi_queries_to_process = multi_queries[1:]  # ì²« ë²ˆì§¸ëŠ” ì´ë¯¸ ì²˜ë¦¬ë¨
                # ìµœëŒ€ ê°œìˆ˜ëŠ” Phase 2ì—ì„œ ê²°ê³¼ ë¶€ì¡± ì •ë„ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê²°ì •
            
            # ë™ì  worker ìˆ˜ ê³„ì‚° (Multi-Query í¬í•¨)
            base_workers = 2  # semantic + keyword
            if needs_direct_statute:
                base_workers += 1
            if multi_queries_to_process:
                base_workers += len(multi_queries_to_process)
            max_workers = min(base_workers, 6)  # ìµœëŒ€ 6ê°œë¡œ ì œí•œ
            
            # ë™ì  íƒ€ì„ì•„ì›ƒ ê³„ì‚°: ì‘ì—… ìˆ˜ì™€ ì˜ˆìƒ ê²°ê³¼ ìˆ˜ì— ë”°ë¼ ì¡°ì • (ìµœì í™”: íƒ€ì„ì•„ì›ƒ ê°ì†Œ)
            base_timeout = 6  # ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ 6ì´ˆ (8 â†’ 6ì´ˆë¡œ ê°ì†Œ)
            worker_count = base_workers
            timeout_per_worker = 1.5  # ì‘ì—…ë‹¹ 1.5ì´ˆ ì¶”ê°€ (2 â†’ 1.5ì´ˆë¡œ ê°ì†Œ)
            dynamic_timeout = base_timeout + (worker_count * timeout_per_worker)
            dynamic_timeout = min(dynamic_timeout, 15)  # ìµœëŒ€ 15ì´ˆë¡œ ì œí•œ (20 â†’ 15ì´ˆë¡œ ê°ì†Œ)
            
            # ì¡°ê¸° ì¢…ë£Œ í”Œë˜ê·¸
            early_exit_triggered = False
            early_exit_reason = None
            
            # 2ë‹¨ê³„ ìš°ì„ ìˆœìœ„ ê²€ìƒ‰: Phase 1 (í•µì‹¬ ê²€ìƒ‰) ë¨¼ì € ì‹¤í–‰
            phase1_sufficient = False
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ ì¤‘ë³µ ë°©ì§€: ë™ì¼í•œ ì¿¼ë¦¬ë¡œ ì´ë¯¸ ê²€ìƒ‰í–ˆëŠ”ì§€ í™•ì¸
            import hashlib
            query_hash = hashlib.md5(
                f"{original_query}:{str(optimized_queries.get('semantic_query', ''))}".encode('utf-8')
            ).hexdigest()
            
            if query_hash in self._executed_queries:
                self.logger.info(f"âš ï¸ [DUPLICATE SEARCH] ë™ì¼í•œ ì¿¼ë¦¬ë¡œ ì´ë¯¸ ê²€ìƒ‰ë¨: {query_hash[:16]}... (ìŠ¤í‚µ)")
            else:
                self._executed_queries.add(query_hash)
            
            # Phase 1: í•µì‹¬ ê²€ìƒ‰ ì‘ì—…ë§Œ ë¨¼ì € ì‹¤í–‰ (semantic + keyword)
            with ThreadPoolExecutor(max_workers=2) as executor:
                semantic_future = executor.submit(
                    self.execute_semantic_search,
                    optimized_queries,
                    search_params,
                    original_query,
                    keywords_copy
                )

                keyword_future = executor.submit(
                    self.execute_keyword_search,
                    optimized_queries,
                    search_params,
                    query_type_str,
                    legal_field,
                    extracted_keywords,
                    original_query
                )
                
                # Phase 1 ì™„ë£Œ ëŒ€ê¸° (ë™ì  íƒ€ì„ì•„ì›ƒ ì¡°ì • - ìµœì í™”: 10-15ì´ˆë¡œ ë‹¨ì¶•)
                # ë™ì  k ê°’ì— ë”°ë¼ íƒ€ì„ì•„ì›ƒ ì¡°ì •: ìµœì†Œ 10ì´ˆ, ìµœëŒ€ 15ì´ˆ, k ê°’ì— ë”°ë¼ ì¡°ì •
                # ì²˜ë¦¬ ì‹œê°„ ìµœì í™”ë¥¼ ìœ„í•´ íƒ€ì„ì•„ì›ƒ ë‹¨ì¶• (20-35ì´ˆ -> 10-15ì´ˆ)
                phase1_timeout = max(10, min(15, 8 + (semantic_k + keyword_k) // 5))
                
                try:
                    for future in as_completed([semantic_future, keyword_future], timeout=phase1_timeout):
                        try:
                            if future == semantic_future:
                                semantic_results, semantic_count = future.result()
                                # ğŸ”¥ ìµœì í™”: Semantic ê²€ìƒ‰ì´ 0ê°œ ê²°ê³¼ë©´ ì¦‰ì‹œ ì¡°ê¸° ì¢…ë£Œ
                                if len(semantic_results) == 0 and semantic_count == 0:
                                    self.logger.warning(
                                        "âš ï¸ [EARLY EXIT] Semantic search returned 0 results, "
                                        "skipping Phase 2 and retries"
                                    )
                                    # Keyword ê²€ìƒ‰ ê²°ê³¼ë§Œ ê¸°ë‹¤ë¦¬ê³  ì¢…ë£Œ
                                    if not keyword_future.done():
                                        try:
                                            keyword_results, keyword_count = keyword_future.result(timeout=5.0)
                                        except (TimeoutError, FutureCancelledError, Exception):
                                            keyword_results, keyword_count = [], 0
                                    early_exit_triggered = True
                                    early_exit_reason = "Semantic search returned 0 results"
                                    break
                            elif future == keyword_future:
                                keyword_results, keyword_count = future.result()
                            
                            # Phase 1 ì¡°ê¸° ì¢…ë£Œ ê°•í™”: ê° future ì™„ë£Œ ì‹œë§ˆë‹¤ ì¦‰ì‹œ ì²´í¬
                            phase1_total = len(semantic_results) + len(keyword_results)
                            phase1_sufficient = (
                                phase1_total >= early_exit_threshold and
                                len(semantic_results) >= min_semantic_for_early_exit and
                                len(keyword_results) >= min_keyword_for_early_exit
                            )
                            
                            if phase1_sufficient:
                                # ë‚˜ë¨¸ì§€ future ì·¨ì†Œ ì‹œë„
                                remaining_futures = [f for f in [semantic_future, keyword_future] if not f.done()]
                                for remaining_future in remaining_futures:
                                    if not remaining_future.running():
                                        try:
                                            remaining_future.cancel()
                                            if self.logger.isEnabledFor(logging.DEBUG):
                                                self.logger.debug("Cancelled remaining search (early exit)")
                                        except (FutureCancelledError, Exception) as cancel_error:
                                            # ì·¨ì†Œ ì¤‘ ë°œìƒí•˜ëŠ” ì˜ˆì™¸ëŠ” ë¬´ì‹œ
                                            if self.logger.isEnabledFor(logging.DEBUG):
                                                self.logger.debug(f"Future cancellation error (ignored): {cancel_error}")
                                
                                self.logger.info(
                                    f"âš¡ [PRIORITY SEARCH] Phase 1 sufficient "
                                    f"(total: {phase1_total}, semantic: {len(semantic_results)}, keyword: {len(keyword_results)}), "
                                    f"skipping Phase 2"
                                )
                                early_exit_triggered = True
                                early_exit_reason = f"Phase 1 sufficient: {phase1_total} results"
                                break  # ì¡°ê¸° ì¢…ë£Œ
                            
                        except (FutureCancelledError, Exception) as e:
                            # CancelledErrorëŠ” ì •ìƒì ì¸ ì·¨ì†Œì´ë¯€ë¡œ ê²½ê³ ë§Œ ë¡œê¹…
                            if isinstance(e, FutureCancelledError):
                                if self.logger.isEnabledFor(logging.DEBUG):
                                    self.logger.debug(f"Search future cancelled: {future}")
                                if future == semantic_future:
                                    semantic_results, semantic_count = [], 0
                                elif future == keyword_future:
                                    keyword_results, keyword_count = [], 0
                            else:
                                if future == semantic_future:
                                    self.logger.error(f"Semantic search failed: {e}")
                                    semantic_results, semantic_count = [], 0
                                elif future == keyword_future:
                                    self.logger.error(f"Keyword search failed: {e}")
                                    keyword_results, keyword_count = [], 0
                    
                    # Phase 1 ê²°ê³¼ í‰ê°€ (ì¡°ê¸° ì¢…ë£Œë˜ì§€ ì•Šì€ ê²½ìš°)
                    if not early_exit_triggered:
                        # ğŸ”¥ ìµœì í™”: Semantic ê²€ìƒ‰ì´ 0ê°œ ê²°ê³¼ë©´ ì¦‰ì‹œ ì¡°ê¸° ì¢…ë£Œ
                        if len(semantic_results) == 0 and semantic_count == 0:
                            self.logger.warning(
                                "âš ï¸ [EARLY EXIT] Semantic search returned 0 results after Phase 1, "
                                "skipping Phase 2 and retries"
                            )
                            early_exit_triggered = True
                            early_exit_reason = "Semantic search returned 0 results"
                        else:
                            phase1_total = len(semantic_results) + len(keyword_results)
                            phase1_sufficient = (
                                phase1_total >= early_exit_threshold and
                                len(semantic_results) >= min_semantic_for_early_exit and
                                len(keyword_results) >= min_keyword_for_early_exit
                            )
                            
                            if phase1_sufficient:
                                self.logger.info(
                                    f"âš¡ [PRIORITY SEARCH] Phase 1 sufficient "
                                    f"(total: {phase1_total}, semantic: {len(semantic_results)}, keyword: {len(keyword_results)}), "
                                    f"skipping Phase 2"
                                )
                                early_exit_triggered = True
                                early_exit_reason = f"Phase 1 sufficient: {phase1_total} results"
                
                except (TimeoutError, FutureCancelledError) as e:
                    # Phase 1 íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì·¨ì†Œ: ë¶€ë¶„ ê²°ê³¼ë¼ë„ ì‚¬ìš©
                    if isinstance(e, TimeoutError):
                        self.logger.warning("âš ï¸ Phase 1 timeout, using partial results")
                    else:
                        self.logger.warning("âš ï¸ Phase 1 cancelled, using partial results")
                    try:
                        if not semantic_results and semantic_future.done():
                            try:
                                semantic_results, semantic_count = semantic_future.result()
                            except (FutureCancelledError, Exception):
                                semantic_results, semantic_count = [], 0
                    except Exception:
                        semantic_results, semantic_count = [], 0
                    try:
                        if not keyword_results and keyword_future.done():
                            try:
                                keyword_results, keyword_count = keyword_future.result()
                            except (FutureCancelledError, Exception):
                                keyword_results, keyword_count = [], 0
                    except Exception:
                        keyword_results, keyword_count = [], 0
                    
                    # ğŸ”¥ ìµœì í™”: íƒ€ì„ì•„ì›ƒ ë°œìƒ í›„ì—ë„ Semantic ê²€ìƒ‰ì´ 0ê°œ ê²°ê³¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                    if len(semantic_results) == 0 and semantic_count == 0:
                        self.logger.warning(
                            "âš ï¸ [EARLY EXIT] Phase 1 timeout with 0 semantic results, "
                            "skipping Phase 2 and retries"
                        )
                        early_exit_triggered = True
                        early_exit_reason = "Phase 1 timeout with 0 semantic results"
                        phase1_sufficient = False  # Phase 2 ìŠ¤í‚µì„ ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
            
            # Phase 2: ë³´ì¡° ê²€ìƒ‰ ì‘ì—… ì‹¤í–‰ (ê²°ê³¼ê°€ ë¶€ì¡±í•œ ê²½ìš°ë§Œ)
            # ğŸ”¥ ìµœì í™”: Semantic ê²€ìƒ‰ì´ 0ê°œ ê²°ê³¼ë©´ Phase 2 ìŠ¤í‚µ
            if not phase1_sufficient and not (len(semantic_results) == 0 and semantic_count == 0):
                # ğŸ”¥ ê°œì„  1: Phase 1 í’ˆì§ˆ í‰ê°€ (ë°©ì•ˆ 5)
                phase1_quality = self._evaluate_search_quality(
                    semantic_results,
                    keyword_results,
                    original_query
                )
                
                if phase1_quality >= 0.7:
                    # í’ˆì§ˆì´ ì¶©ë¶„í•˜ë©´ multi-query ìŠ¤í‚µ
                    self.logger.info(
                        f"âœ… [MULTI-QUERY] Phase 1 quality sufficient "
                        f"(score: {phase1_quality:.2f}), skipping multi-query"
                    )
                    multi_queries_to_process = []
                else:
                    self.logger.info(
                        f"ğŸ”„ [MULTI-QUERY] Phase 1 quality insufficient "
                        f"(score: {phase1_quality:.2f}), will execute multi-query"
                    )
                
                self.logger.info(
                    f"ğŸ”„ [PRIORITY SEARCH] Phase 1 insufficient "
                    f"(total: {len(semantic_results) + len(keyword_results)}, "
                    f"semantic: {len(semantic_results)}, keyword: {len(keyword_results)}, "
                    f"quality: {phase1_quality:.2f}), starting Phase 2"
                )
                
                # ì ì‘í˜• ìš°ì„ ìˆœìœ„ ì¡°ì •
                priorities = self._adjust_search_priority(
                    semantic_results=semantic_results,
                    keyword_results=keyword_results,
                    query_type=query_type_str
                )
                
                # ğŸ”¥ ê°œì„  2: ì ì‘í˜• Multi-Query ê°œìˆ˜ ê²°ì • (ë°©ì•ˆ 1)
                current_total = len(semantic_results) + len(keyword_results)
                missing_results = max(0, early_exit_threshold - current_total)
                
                if multi_queries_to_process:
                    if missing_results > 10:
                        # ë§ì´ ë¶€ì¡±í•˜ë©´ ìµœëŒ€ 3ê°œ ì‚¬ìš©
                        multi_queries_to_use = multi_queries_to_process[:3]
                        self.logger.info(
                            f"ğŸ”„ [MULTI-QUERY] High missing results ({missing_results}), "
                            f"using {len(multi_queries_to_use)} multi-queries"
                        )
                    elif missing_results > 5:
                        # ì¤‘ê°„ ì •ë„ ë¶€ì¡±í•˜ë©´ 2ê°œ ì‚¬ìš©
                        multi_queries_to_use = multi_queries_to_process[:2]
                        self.logger.info(
                            f"ğŸ”„ [MULTI-QUERY] Moderate missing results ({missing_results}), "
                            f"using {len(multi_queries_to_use)} multi-queries"
                        )
                    else:
                        # ì¡°ê¸ˆë§Œ ë¶€ì¡±í•˜ë©´ 1ê°œë§Œ ì‚¬ìš©
                        multi_queries_to_use = multi_queries_to_process[:1]
                        self.logger.info(
                            f"ğŸ”„ [MULTI-QUERY] Low missing results ({missing_results}), "
                            f"using {len(multi_queries_to_use)} multi-query"
                        )
                else:
                    multi_queries_to_use = []
                
                # Phase 2ë¥¼ ìœ„í•œ ThreadPoolExecutor (ìš°ì„ ìˆœìœ„ì— ë”°ë¼)
                phase2_workers = len(multi_queries_to_use)  # Multi-Query ê°œìˆ˜ì— ë”°ë¼ ì¡°ì •
                if needs_direct_statute and priorities.get("direct_statute", 2) <= 1:
                    phase2_workers += 1  # direct_statute ì¶”ê°€
                phase2_workers = min(phase2_workers, 6)  # ìµœëŒ€ 6ê°œë¡œ ì œí•œ
                
                with ThreadPoolExecutor(max_workers=phase2_workers) as executor:
                    # ë²•ë ¹ ì¡°ë¬¸ ì§ì ‘ ê²€ìƒ‰ (ë²•ë ¹ ì¡°íšŒì¸ ê²½ìš°ë§Œ)
                    direct_statute_future = None
                    direct_statute_results = []
                    if needs_direct_statute:
                        def _search_direct_statute():
                            try:
                                from core.search.connectors.legal_data_connector_v2 import LegalDataConnectorV2
                                data_connector = LegalDataConnectorV2()
                                return data_connector.search_statute_article_direct(original_query, limit=5)
                            except Exception as e:
                                if debug_mode:
                                    self.logger.debug(f"Direct statute search error: {e}")
                                return []
                        
                        direct_statute_future = executor.submit(_search_direct_statute)

                    # ğŸ”¥ ê°œì„  3: Multi-Query ë³‘ë ¬ ì‹¤í–‰ (ë°©ì•ˆ 1)
                    multi_query_futures = {}
                    if multi_queries_to_use:
                        for idx, mq in enumerate(multi_queries_to_use):
                            # ë¶€ì¡±í•œ ê²°ê³¼ ìˆ˜ë¥¼ multi-query ê°œìˆ˜ë¡œ ë‚˜ëˆ ì„œ ê°ê° í• ë‹¹
                            k_per_query = max(3, missing_results // len(multi_queries_to_use)) if missing_results > 0 else max(3, semantic_k // 4)
                            mq_future = executor.submit(
                                self._execute_semantic_search_single,
                                mq,
                                k_per_query,
                                keywords_copy,
                                original_query
                            )
                            multi_query_futures[mq_future] = ('multi_query', mq[:50], idx)
                    
                    # Phase 2 futures map
                    futures_map = {}
                    if direct_statute_future:
                        futures_map[direct_statute_future] = ('direct_statute', None)
                    # Multi-query futures ì¶”ê°€ (íŠœí”Œ í˜•ì‹ìœ¼ë¡œ ì €ì¥)
                    for mq_future, mq_info in multi_query_futures.items():
                        futures_map[mq_future] = mq_info
                
                completed_count = 0
                direct_statute_results = []
                unfinished_futures = []
                
                # ë™ì  íƒ€ì„ì•„ì›ƒ ì‚¬ìš© (ì‘ì—… ìˆ˜ì— ë”°ë¼ ì¡°ì •)
                # ë¡œê¹… ìµœì í™”: ì™„ë£Œëœ ì‘ì—…ì„ ëª¨ì•„ì„œ í•œ ë²ˆì— ë¡œê¹…
                completed_tasks = []
                # ì„±ëŠ¥ ìµœì í™”: ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ seen_idsì™€ seen_hashesë¥¼ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©
                seen_ids = set()
                seen_hashes = set()
                
                try:
                    for future in as_completed(futures_map.keys(), timeout=dynamic_timeout):
                        # futures_map ê°’ì´ íŠœí”Œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                        future_info = futures_map[future]
                        if isinstance(future_info, tuple):
                            if len(future_info) >= 2:
                                search_type, query_type = future_info[0], future_info[1:]
                            else:
                                search_type, query_type = future_info[0], None
                        else:
                            search_type, query_type = future_info, None
                        try:
                            if search_type == 'semantic':
                                if query_type == 'main':
                                    semantic_results, semantic_count = future.result()
                                    # seen_idsì™€ seen_hashes ì—…ë°ì´íŠ¸
                                    for doc in semantic_results:
                                        doc_id = doc.get("id") or doc.get("doc_id")
                                        if doc_id:
                                            seen_ids.add(doc_id)
                                        # content hash ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ì²˜ìŒ 100ì í•´ì‹œ)
                                        content = doc.get("content", "") or doc.get("text", "")
                                        if content:
                                            content_hash = hash(content[:100])
                                            seen_hashes.add(content_hash)
                                    completed_tasks.append(('semantic', semantic_count))
                            elif search_type == 'multi_query':
                                # ğŸ”¥ ê°œì„  4: Multi-Query ê²°ê³¼ ì²˜ë¦¬ (ë‹¤ì–‘ì„± ë³´ì¥ ë³‘í•©)
                                mq_results, mq_count = future.result()
                                # query_typeì€ íŠœí”Œ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸: (mq_query, idx) ë˜ëŠ” [mq_query, idx]
                                if isinstance(query_type, (tuple, list)) and len(query_type) >= 2:
                                    mq_query = query_type[0] if query_type[0] else "unknown"
                                    mq_idx = query_type[1] if len(query_type) > 1 else 0
                                else:
                                    mq_query = "unknown"
                                    mq_idx = 0
                                    
                                    if mq_results:
                                        # ë‹¤ì–‘ì„± ë³´ì¥ ë³‘í•©
                                        new_results = self._merge_multi_query_results_single(
                                            main_results=semantic_results,
                                            mq_results=mq_results,
                                            mq_query=mq_query or "unknown",
                                            original_query=original_query,
                                            seen_ids=seen_ids,
                                            seen_hashes=seen_hashes
                                        )
                                        
                                        # seen_idsì™€ seen_hashes ì—…ë°ì´íŠ¸
                                        for doc in new_results:
                                            doc_id = doc.get("id") or doc.get("doc_id")
                                            if doc_id:
                                                seen_ids.add(doc_id)
                                            content = doc.get("content", "") or doc.get("text", "")
                                            if content:
                                                content_hash = hash(content[:100])
                                                seen_hashes.add(content_hash)
                                        
                                        semantic_results.extend(new_results)
                                        completed_tasks.append(('multi_query', len(new_results), mq_idx))
                            elif search_type == 'keyword':
                                keyword_results, keyword_count = future.result()
                                # seen_idsì™€ seen_hashes ì—…ë°ì´íŠ¸ (semantic_resultsì™€ì˜ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´)
                                for doc in keyword_results:
                                    doc_id = doc.get("id") or doc.get("doc_id")
                                    if doc_id:
                                        seen_ids.add(doc_id)
                                    # content hash ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ì²˜ìŒ 100ì í•´ì‹œ)
                                    content = doc.get("content", "") or doc.get("text", "")
                                    if content:
                                        content_hash = hash(content[:100])
                                        seen_hashes.add(content_hash)
                                completed_tasks.append(('keyword', keyword_count))
                            elif search_type == 'direct_statute':
                                direct_statute_results = future.result()
                                completed_tasks.append(('direct_statute', len(direct_statute_results) if direct_statute_results else 0))
                            
                            completed_count += 1
                            
                            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬ ê°•í™”: ê° ì‘ì—… ì™„ë£Œ ì‹œë§ˆë‹¤ ì²´í¬ (ê°œì„ : ì¡°ê±´ ê°•í™”)
                            current_total = len(semantic_results) + len(keyword_results)
                            semantic_count_current = len(semantic_results)
                            keyword_count_current = len(keyword_results)
                            
                            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ê°•í™”: ì—¬ëŸ¬ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë§Œ ë§Œì¡±í•´ë„ ì¢…ë£Œ
                            has_both_types_sufficient = (
                                semantic_count_current >= min_semantic_for_early_exit and
                                keyword_count_current >= min_keyword_for_early_exit
                            )
                            has_semantic_sufficient = semantic_count_current >= semantic_k and semantic_count_current >= 5
                            has_keyword_sufficient = keyword_count_current >= keyword_k and keyword_count_current >= 5
                            
                            if (current_total >= early_exit_threshold and has_both_types_sufficient) or \
                               has_semantic_sufficient or \
                               has_keyword_sufficient:
                                early_exit_triggered = True
                                if has_both_types_sufficient:
                                    early_exit_reason = f"Sufficient results (both types): {current_total} >= {early_exit_threshold} (semantic: {semantic_count_current}, keyword: {keyword_count_current})"
                                elif has_semantic_sufficient:
                                    early_exit_reason = f"Sufficient semantic results: {semantic_count_current} >= {semantic_k}"
                                else:
                                    early_exit_reason = f"Sufficient keyword results: {keyword_count_current} >= {keyword_k}"
                                
                                # ë‚˜ë¨¸ì§€ ë¯¸ì™„ë£Œ future ì·¨ì†Œ (ê°œì„ : Multi-Query ìš°ì„  ì·¨ì†Œ)
                                remaining_futures = [f for f in futures_map.keys() if not f.done()]
                                
                                # Multi-Query futures ìš°ì„  ì·¨ì†Œ (ëœ ì¤‘ìš”í•œ ì‘ì—…)
                                for remaining_future in remaining_futures:
                                    remaining_type, _ = futures_map[remaining_future]
                                    if remaining_type == 'multi_query' and not remaining_future.running():
                                        try:
                                            remaining_future.cancel()
                                            if self.logger.isEnabledFor(logging.DEBUG):
                                                self.logger.debug(f"Cancelled {remaining_type} search (early exit)")
                                        except (FutureCancelledError, Exception) as cancel_error:
                                            # ì·¨ì†Œ ì¤‘ ë°œìƒí•˜ëŠ” ì˜ˆì™¸ëŠ” ë¬´ì‹œ
                                            if self.logger.isEnabledFor(logging.DEBUG):
                                                self.logger.debug(f"Future cancellation error (ignored): {cancel_error}")
                                
                                # ë‹¤ë¥¸ futures ì·¨ì†Œ
                                for remaining_future in remaining_futures:
                                    remaining_type, _ = futures_map[remaining_future]
                                    if remaining_type != 'multi_query' and not remaining_future.running():
                                        try:
                                            remaining_future.cancel()
                                            if self.logger.isEnabledFor(logging.DEBUG):
                                                self.logger.debug(f"Cancelled {remaining_type} search (early exit)")
                                        except (FutureCancelledError, Exception) as cancel_error:
                                            # ì·¨ì†Œ ì¤‘ ë°œìƒí•˜ëŠ” ì˜ˆì™¸ëŠ” ë¬´ì‹œ
                                            if self.logger.isEnabledFor(logging.DEBUG):
                                                self.logger.debug(f"Future cancellation error (ignored): {cancel_error}")
                                
                                if self.logger.isEnabledFor(logging.DEBUG):
                                    self.logger.debug(f"âš¡ [EARLY EXIT] {early_exit_reason}")
                                break
                                
                        except (FutureCancelledError, Exception) as e:
                            # CancelledErrorëŠ” ì •ìƒì ì¸ ì·¨ì†Œì´ë¯€ë¡œ ê²½ê³ ë§Œ ë¡œê¹…
                            if isinstance(e, FutureCancelledError):
                                if self.logger.isEnabledFor(logging.DEBUG):
                                    self.logger.debug(f"{search_type} search cancelled: {e}")
                                if search_type == 'direct_statute':
                                    direct_statute_results = []
                                elif search_type == 'semantic':
                                    semantic_results, semantic_count = [], 0
                                else:
                                    keyword_results, keyword_count = [], 0
                            else:
                                if search_type == 'direct_statute':
                                    if self.logger.isEnabledFor(logging.DEBUG):
                                        self.logger.debug(f"Direct statute search failed: {e}")
                                    direct_statute_results = []
                                    completed_tasks.append(('direct_statute', 'error', str(e)))
                                else:
                                    self.logger.error(f"{search_type} search failed: {e}")
                                    if self.logger.isEnabledFor(logging.DEBUG):
                                        self.logger.debug(f"{search_type} search exception: {e}")
                                    completed_tasks.append((search_type, 'error', str(e)))
                                    if search_type == 'semantic':
                                        semantic_results, semantic_count = [], 0
                                    else:
                                        keyword_results, keyword_count = [], 0
                            completed_count += 1
                    
                    # ë¡œê¹… ìµœì í™”: ì™„ë£Œëœ ì‘ì—… í•œ ë²ˆì— ë¡œê¹…
                    if self.logger.isEnabledFor(logging.DEBUG) and completed_tasks:
                        self.logger.debug(f"Completed tasks: {completed_tasks}")
                except (TimeoutError, FutureCancelledError) as e:
                    # íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì·¨ì†Œ ë°œìƒ ì‹œ ì™„ë£Œë˜ì§€ ì•Šì€ future ìˆ˜ì§‘
                    unfinished_futures = [f for f in futures_map.keys() if not f.done()]
                    if isinstance(e, TimeoutError):
                        self.logger.warning(
                            f"âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ ë°œìƒ: {len(unfinished_futures)} (of {len(futures_map)}) futures unfinished"
                        )
                    else:
                        self.logger.warning(
                            f"âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ ì·¨ì†Œ ë°œìƒ: {len(unfinished_futures)} (of {len(futures_map)}) futures unfinished"
                        )
                
                    # Phase 2 íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ (ìµœì í™”: íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
                    phase2_timeout = 5  # 5ì´ˆ (8 â†’ 5ì´ˆë¡œ ê°ì†Œ)
                    
                    try:
                        for future in as_completed(futures_map.keys(), timeout=phase2_timeout):
                            search_type, query_type = futures_map[future]
                            try:
                                if search_type == 'multi_query':
                                    # Multi-Query ê²°ê³¼ ì²˜ë¦¬ (ì¤‘ë³µ ì œê±°)
                                    mq_results, mq_count = future.result()
                                    if mq_results:
                                        new_results = []
                                        for doc in mq_results:
                                            doc_id = doc.get("id") or doc.get("doc_id")
                                            content = doc.get("content", "") or doc.get("text", "")
                                            content_hash = hash(content[:100]) if content else None
                                            
                                            # IDì™€ content hash ëª¨ë‘ í™•ì¸í•˜ì—¬ ì¤‘ë³µ ì œê±°
                                            if (not doc_id or doc_id not in seen_ids) and \
                                               (not content_hash or content_hash not in seen_hashes):
                                                if doc_id:
                                                    seen_ids.add(doc_id)
                                                if content_hash:
                                                    seen_hashes.add(content_hash)
                                                new_results.append(doc)
                                        
                                        semantic_results.extend(new_results)
                                        semantic_count += len(new_results)
                                        self.logger.info(f"âœ… [PHASE 2] Multi-Query added {len(new_results)} new results")
                                
                                elif search_type == 'direct_statute':
                                    direct_statute_results = future.result()
                                    if direct_statute_results:
                                        # ì¤‘ë³µ ì œê±° í›„ keyword_resultsì— ì¶”ê°€
                                        new_statute_results = []
                                        for doc in direct_statute_results:
                                            doc_id = doc.get("id") or doc.get("doc_id")
                                            if doc_id and doc_id not in seen_ids:
                                                seen_ids.add(doc_id)
                                                new_statute_results.append(doc)
                                        keyword_results.extend(new_statute_results)
                                        keyword_count += len(new_statute_results)
                                        self.logger.info(f"âœ… [PHASE 2] Direct statute added {len(new_statute_results)} new results")
                            
                            except (FutureCancelledError, Exception) as e:
                                if isinstance(e, FutureCancelledError):
                                    if self.logger.isEnabledFor(logging.DEBUG):
                                        self.logger.debug(f"{search_type} search cancelled in Phase 2")
                                else:
                                    if search_type == 'direct_statute':
                                        if self.logger.isEnabledFor(logging.DEBUG):
                                            self.logger.debug(f"Direct statute search failed: {e}")
                                        direct_statute_results = []
                                    else:
                                        self.logger.warning(f"Multi-query search failed: {e}")
                    
                    except (TimeoutError, FutureCancelledError) as e:
                        if isinstance(e, TimeoutError):
                            self.logger.warning("âš ï¸ Phase 2 timeout, using partial results")
                        else:
                            self.logger.warning("âš ï¸ Phase 2 cancelled, using partial results")
                
                # ì¡°ê¸° ì¢…ë£Œ ë¡œê¹…
                if early_exit_triggered:
                    self.logger.info(
                        f"âš¡ [EARLY EXIT] {early_exit_reason} - "
                        f"Semantic: {len(semantic_results)}, Keyword: {len(keyword_results)}"
                    )

            # ê²€ìƒ‰ ê²°ê³¼ íƒ€ì… ê· í˜• ì¡°ì • (ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ê°€ ë§ì„ ë•Œë§Œ ìˆ˜í–‰)
            total_results = len(semantic_results) + len(keyword_results)
            should_balance = total_results > 20  # ê²°ê³¼ê°€ 20ê°œ ì´ìƒì¼ ë•Œë§Œ ê· í˜• ì¡°ì •
            
            if should_balance:
                try:
                    # numpy íƒ€ì… ë³€í™˜ í•¨ìˆ˜ (ìµœì í™”: í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë³€í™˜)
                    def convert_numpy_types(obj, _depth=0):
                        # ì¬ê·€ ê¹Šì´ ì œí•œìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
                        if _depth > 5:
                            return obj
                        import numpy as np
                        if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
                            return int(obj)
                        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
                            return float(obj)
                        elif isinstance(obj, np.ndarray):
                            return obj.tolist()
                        elif isinstance(obj, dict):
                            return {k: convert_numpy_types(v, _depth + 1) for k, v in obj.items()}
                        elif isinstance(obj, (list, tuple)):
                            return [convert_numpy_types(item, _depth + 1) for item in obj]
                        return obj
                    
                    # ê²€ìƒ‰ ê²°ê³¼ì— numpy íƒ€ì… ë³€í™˜ ì ìš© (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
                    has_numpy = False
                    for doc in semantic_results[:5] + keyword_results[:5]:
                        import numpy as np
                        if any(isinstance(v, (np.integer, np.floating, np.ndarray)) for v in (doc.values() if isinstance(doc, dict) else [])):
                            has_numpy = True
                            break
                    
                    if has_numpy:
                        semantic_results = [convert_numpy_types(doc) for doc in semantic_results]
                        keyword_results = [convert_numpy_types(doc) for doc in keyword_results]
                    
                    # semantic_resultsì™€ keyword_resultsë¥¼ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
                    all_results = semantic_results + keyword_results
                    grouped_results = self.result_balancer.group_results_by_type(all_results)
                    
                    # íƒ€ì…ë³„ ë¶„í¬ í™•ì¸ (ë¡œê¹… ìµœì í™”)
                    type_distribution = {doc_type: len(docs) for doc_type, docs in grouped_results.items()}
                    # ë¡œê¹… ìµœì í™”: íƒ€ì…ë³„ ë¶„í¬ ë°°ì¹˜ ë¡œê¹…
                    if self.logger.isEnabledFor(logging.DEBUG):
                        non_zero_types = {k: v for k, v in type_distribution.items() if v > 0}
                        if non_zero_types:
                            self.logger.debug(f"ğŸ“Š [SEARCH BALANCE] Type distribution: {non_zero_types}")
                    
                    # ë‹¨ì¼ íƒ€ì…ë§Œ ê²€ìƒ‰ëœ ê²½ìš° ê²½ê³  (ë¡œê¹… ìµœì í™”)
                    non_zero_types = [t for t, c in type_distribution.items() if c > 0]
                    if len(non_zero_types) == 1:
                        single_type = non_zero_types[0]
                        self.logger.warning(
                            f"âš ï¸ [TYPE DIVERSITY] ë‹¨ì¼ íƒ€ì…ë§Œ ê²€ìƒ‰ë¨: {single_type} ({type_distribution[single_type]}ê°œ)"
                        )
                    elif len(non_zero_types) == 0:
                        self.logger.warning("âš ï¸ [TYPE DIVERSITY] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                    elif debug_mode:
                        # íƒ€ì… ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚° (ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ)
                        total_docs = sum(type_distribution.values())
                        if total_docs > 0:
                            import math
                            entropy = 0.0
                            for count in type_distribution.values():
                                if count > 0:
                                    p = count / total_docs
                                    entropy -= p * math.log2(p)
                            max_entropy = math.log2(len(non_zero_types)) if len(non_zero_types) > 1 else 1.0
                            diversity_score = entropy / max_entropy if max_entropy > 0 else 0.0
                            self.logger.debug(
                                f"âœ… [TYPE DIVERSITY] íƒ€ì… ë‹¤ì–‘ì„± ì ìˆ˜: {diversity_score:.2f} "
                                f"(ê²€ìƒ‰ëœ íƒ€ì…: {len(non_zero_types)}ê°œ, ì´ ë¬¸ì„œ: {total_docs}ê°œ)"
                            )
                    
                    # ê· í˜• ì¡°ì •ëœ ê²°ê³¼ ìƒì„±
                    semantic_k = search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K)
                    keyword_k = search_params.get("keyword_k", WorkflowConstants.KEYWORD_SEARCH_K)
                    balanced_results = self.result_balancer.balance_search_results(
                        grouped_results,
                        total_limit=semantic_k + keyword_k
                    )
                    
                    # ê· í˜• ì¡°ì •ëœ ê²°ê³¼ë¥¼ semantic_resultsì™€ keyword_resultsë¡œ ì¬ë¶„ë°°
                    if balanced_results:
                        semantic_results_balanced = [
                            doc for doc in balanced_results 
                            if doc.get("relevance_score", 0.0) >= 0.5
                        ]
                        keyword_results_balanced = [
                            doc for doc in balanced_results 
                            if doc.get("relevance_score", 0.0) < 0.5 or doc not in semantic_results_balanced
                        ]
                        
                        # ê¸°ì¡´ ê²°ê³¼ì™€ ë³‘í•© (ì¤‘ë³µ ì œê±° - ì„±ëŠ¥ ìµœì í™”)
                        existing_ids = {id(doc) for doc in semantic_results + keyword_results}
                        
                        semantic_results = semantic_results + [
                            doc for doc in semantic_results_balanced 
                            if id(doc) not in existing_ids
                        ]
                        keyword_results = keyword_results + [
                            doc for doc in keyword_results_balanced 
                            if id(doc) not in existing_ids
                        ]
                        
                        semantic_count = len(semantic_results)
                        keyword_count = len(keyword_results)
                        
                        if debug_mode:
                            self.logger.debug(
                                f"âœ… [SEARCH BALANCE] ê· í˜• ì¡°ì • ì™„ë£Œ: "
                                f"semantic={semantic_count}, keyword={keyword_count}"
                            )
                except Exception as e:
                    if debug_mode:
                        self.logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ê· í˜• ì¡°ì • ì‹¤íŒ¨ (ê¸°ì¡´ ê²°ê³¼ ì‚¬ìš©): {e}")

            ensure_state_group(state, "search")

            if debug_mode:
                self.logger.debug(f"PARALLEL SEARCH: Before save - semantic_results={len(semantic_results)}, keyword_results={len(keyword_results)}")

            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "semantic_count", semantic_count)
            self._set_state_value(state, "keyword_count", keyword_count)
            
            # State êµ¬ì¡° ì¼ê´€ì„± í™•ë³´: retrieved_docsë¥¼ í—¬í¼ í•¨ìˆ˜ë¡œ ì €ì¥
            merged_docs = semantic_results + keyword_results
            set_retrieved_docs(state, merged_docs)
            
            # ê²€ìƒ‰ ê²°ê³¼ ìºì‹± ì €ì¥ (ì¬ì‹œë„ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
            if not is_retry:
                cache_key = self._generate_search_cache_key(
                    optimized_queries, search_params, original_query
                )
                if cache_key and (semantic_results or keyword_results):
                    self._cache_search_results(cache_key, {
                        'semantic_results': semantic_results,
                        'keyword_results': keyword_results,
                        'semantic_count': semantic_count,
                        'keyword_count': keyword_count
                    })

            if debug_mode:
                stored_semantic = self._get_state_value(state, "semantic_results", [])
                stored_keyword = self._get_state_value(state, "keyword_results", [])
                self.logger.debug(f"PARALLEL SEARCH: After save - semantic_results={len(stored_semantic)}, keyword_results={len(stored_keyword)}")

                if "search" in state and isinstance(state.get("search"), dict):
                    direct_semantic = state["search"].get("semantic_results", [])
                    direct_keyword = state["search"].get("keyword_results", [])
                    self.logger.debug(f"PARALLEL SEARCH: Direct state['search'] check - semantic={len(direct_semantic)}, keyword={len(direct_keyword)}")
                else:
                    self.logger.debug(f"PARALLEL SEARCH: state['search'] not found or not dict, state keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")

            self._save_metadata_safely(state, "_last_executed_node", "execute_searches_parallel")
            self._update_processing_time(state, start_time)

            elapsed_time = time.time() - start_time

            self.logger.info(
                f"âœ… [PARALLEL SEARCH] Completed in {elapsed_time:.3f}s - "
                f"Semantic: {semantic_count} results, Keyword: {keyword_count} results"
            )

            if debug_mode:
                self.logger.debug(f"PARALLEL SEARCH: Semantic={semantic_count}, Keyword={keyword_count}")

                if semantic_results:
                    semantic_scores = [doc.get("relevance_score", 0.0) for doc in semantic_results[:5]]
                    self.logger.info(
                        f"ğŸ” [DEBUG] Semantic search details: "
                        f"Top scores: {semantic_scores}, "
                        f"Sample sources: {[doc.get('source', 'Unknown')[:30] for doc in semantic_results[:3]]}"
                    )
                else:
                    self.logger.warning("âš ï¸ [DEBUG] Semantic search returned 0 results")

                if keyword_results:
                    keyword_scores = [doc.get("relevance_score", doc.get("score", 0.0)) for doc in keyword_results[:5]]
                    self.logger.info(
                        f"ğŸ” [DEBUG] Keyword search details: "
                        f"Top scores: {keyword_scores}, "
                        f"Sample sources: {[doc.get('source', 'Unknown')[:30] for doc in keyword_results[:3]]}"
                    )
                else:
                    self.logger.warning("âš ï¸ [DEBUG] Keyword search returned 0 results")

        except (TimeoutError, FutureCancelledError) as timeout_err:
            # CancelledErrorëŠ” ì •ìƒì ì¸ ì·¨ì†Œì´ë¯€ë¡œ ê²½ê³ ë§Œ ë¡œê¹…
            if isinstance(timeout_err, FutureCancelledError):
                self.logger.warning(f"âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ ì·¨ì†Œ ë°œìƒ: {timeout_err}")
            else:
                self.logger.warning(f"âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ ë°œìƒ: {timeout_err}")
            
            # ğŸ”¥ ê°œì„  3: ê²€ìƒ‰ ê²°ê³¼ê°€ 0ê°œì¼ ë•Œ ì¦‰ì‹œ ë°˜í™˜ (timeout ë°©ì§€)
            if semantic_count == 0 and keyword_count == 0:
                self.logger.warning(
                    "âš ï¸ [SEARCH TIMEOUT PREVENTION] ê²€ìƒ‰ ê²°ê³¼ê°€ 0ê°œì…ë‹ˆë‹¤. "
                    "íƒ€ì„ì•„ì›ƒ ë°©ì§€ë¥¼ ìœ„í•´ ì¦‰ì‹œ ë°˜í™˜í•©ë‹ˆë‹¤."
                )
                ensure_state_group(state, "search")
                state["search"]["semantic_results"] = []
                state["search"]["keyword_results"] = []
                state["search"]["semantic_count"] = 0
                state["search"]["keyword_count"] = 0
                return state
            
            self.logger.info("ğŸ”„ ìˆœì°¨ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± ì‹œë„ ì¤‘...")
            try:
                return self.fallback_sequential_search(state)
            except Exception as fallback_err:
                self.logger.error(f"âŒ ìˆœì°¨ ê²€ìƒ‰ í´ë°±ë„ ì‹¤íŒ¨: {fallback_err}", exc_info=True)
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ ê²°ê³¼ë¡œë¼ë„ ê³„ì† ì§„í–‰
                self.logger.warning("âš ï¸ ìµœì†Œí•œì˜ ê²°ê³¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                semantic_results, semantic_count = [], 0
                keyword_results, keyword_count = [], 0
                # ë¯¸ì™„ë£Œëœ future ì·¨ì†Œ ì‹œë„
                try:
                    if 'semantic_future' in locals() and not semantic_future.done():
                        semantic_future.cancel()
                    if 'keyword_future' in locals() and not keyword_future.done():
                        keyword_future.cancel()
                except Exception:
                    pass
                # ë¹ˆ ê²°ê³¼ë¼ë„ stateì— ì €ì¥í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ê°€ ê³„ì† ì§„í–‰ë˜ë„ë¡ í•¨
                ensure_state_group(state, "search")
                state["search"]["semantic_results"] = semantic_results
                state["search"]["keyword_results"] = keyword_results
                state["search"]["semantic_count"] = semantic_count
                state["search"]["keyword_count"] = keyword_count
                return state
        except (FutureCancelledError, Exception) as e:
            # CancelledErrorëŠ” ì •ìƒì ì¸ ì·¨ì†Œì´ë¯€ë¡œ ê²½ê³ ë§Œ ë¡œê¹…í•˜ê³  ë¹ˆ ê²°ê³¼ ë°˜í™˜
            if isinstance(e, FutureCancelledError):
                self.logger.warning(f"âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ ì·¨ì†Œ ë°œìƒ: {e}")
                # ì·¨ì†Œëœ ê²½ìš° ë¹ˆ ê²°ê³¼ë¡œë¼ë„ ê³„ì† ì§„í–‰
                semantic_results, semantic_count = [], 0
                keyword_results, keyword_count = [], 0
                ensure_state_group(state, "search")
                state["search"]["semantic_results"] = semantic_results
                state["search"]["keyword_results"] = keyword_results
                state["search"]["semantic_count"] = semantic_count
                state["search"]["keyword_count"] = keyword_count
                return state
            
            self.logger.error(f"âŒ ë³‘ë ¬ ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            self.logger.info("ğŸ”„ ìˆœì°¨ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± ì‹œë„ ì¤‘...")
            try:
                return self.fallback_sequential_search(state)
            except Exception as fallback_err:
                self.logger.error(f"âŒ ìˆœì°¨ ê²€ìƒ‰ í´ë°±ë„ ì‹¤íŒ¨: {fallback_err}", exc_info=True)
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ ê²°ê³¼ë¡œë¼ë„ ê³„ì† ì§„í–‰
                self.logger.warning("âš ï¸ ìµœì†Œí•œì˜ ê²°ê³¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                semantic_results, semantic_count = [], 0
                keyword_results, keyword_count = [], 0
                ensure_state_group(state, "search")
                state["search"]["semantic_results"] = semantic_results
                state["search"]["keyword_results"] = keyword_results
                state["search"]["semantic_count"] = semantic_count
                state["search"]["keyword_count"] = keyword_count
                return state

        debug_mode = os.getenv("DEBUG_SEARCH", "false").lower() == "true"

        if debug_mode:
            if "search" in state and isinstance(state.get("search"), dict):
                final_search = state["search"]
                final_semantic = len(final_search.get("semantic_results", []))
                final_keyword = len(final_search.get("keyword_results", []))
                self.logger.debug(f"[DEBUG] execute_searches_parallel: Returning state with search group - semantic_results={final_semantic}, keyword_results={final_keyword}")
                self.logger.debug(f"[DEBUG] execute_searches_parallel: Returning state keys={list(state.keys()) if isinstance(state, dict) else 'N/A'}")
            else:
                self.logger.debug("[DEBUG] execute_searches_parallel: WARNING - Returning state WITHOUT search group!")
                self.logger.debug(f"[DEBUG] execute_searches_parallel: Returning state keys={list(state.keys()) if isinstance(state, dict) else 'N/A'}")

        return state

    def _execute_semantic_search_single(
        self,
        query: str,
        k: int,
        extracted_keywords: Optional[List[str]] = None,
        original_query: Optional[str] = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """ë‹¨ì¼ semantic ê²€ìƒ‰ ì‹¤í–‰ (Multi-Queryìš©)"""
        if not query or not query.strip():
            return [], 0
        
        try:
            results, count = self.search_handler.semantic_search(
                query,
                k=k,
                extracted_keywords=extracted_keywords
            )
            return results, count
        except Exception as e:
            self.logger.warning(f"Single semantic search failed for '{query[:30]}...': {e}")
            return [], 0

    def execute_semantic_search(
        self,
        optimized_queries: Dict[str, Any],
        search_params: Dict[str, Any],
        original_query: str = "",
        extracted_keywords: Optional[List[str]] = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤í–‰"""
        self.logger.info("ğŸ” [EXECUTE_SEMANTIC_SEARCH] ë©”ì„œë“œ í˜¸ì¶œë¨")
        self.logger.info(f"ğŸ” [EXECUTE_SEMANTIC_SEARCH] original_query: {original_query[:50] if original_query else 'N/A'}...")
        semantic_results = []
        semantic_count = 0

        semantic_query = optimized_queries.get("semantic_query", "") if optimized_queries else ""
        semantic_k = search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K) if search_params else WorkflowConstants.SEMANTIC_SEARCH_K
        expanded_keywords = optimized_queries.get("expanded_keywords", []) if optimized_queries else []
        
        # ë¹ˆ ì¿¼ë¦¬ ê²€ì¦: semantic_queryê°€ ë¹„ì–´ìˆìœ¼ë©´ original_query ì‚¬ìš©, ë‘˜ ë‹¤ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if not semantic_query or not str(semantic_query).strip():
            if original_query and original_query.strip():
                semantic_query = original_query
                if optimized_queries:
                    optimized_queries["semantic_query"] = original_query
                self.logger.info(f"ğŸ” [EXECUTE_SEMANTIC_SEARCH] semantic_queryê°€ ë¹„ì–´ìˆì–´ original_query ì‚¬ìš©: '{original_query[:50]}...'")
            else:
                self.logger.warning("âš ï¸ [EXECUTE_SEMANTIC_SEARCH] semantic_queryì™€ original_queryê°€ ëª¨ë‘ ë¹„ì–´ìˆì–´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return [], 0
        
        # ê°œì„ : textToSQL ë¼ìš°íŒ… í™•ì¸ ë° ì ìš© (ìš°ì„ ìˆœìœ„ 1)
        if original_query and original_query.strip():
            from core.search.connectors.legal_data_connector_v2 import route_query, LegalDataConnectorV2
            route = route_query(original_query)
            self.logger.info(f"ğŸ” [TEXT2SQL SEMANTIC] route_query result: '{route}' for query: '{original_query[:50]}...'")
            if route == "text2sql":
                self.logger.info(f"ğŸ” [TEXT2SQL SEMANTIC] Detected text2sql route for semantic search: '{original_query[:50]}...'")
                try:
                    data_connector = LegalDataConnectorV2()
                    text2sql_results = data_connector.search_documents(original_query, limit=semantic_k)
                    if text2sql_results:
                        semantic_results.extend(text2sql_results)
                        semantic_count += len(text2sql_results)
                        self.logger.info(f"âœ… [TEXT2SQL SEMANTIC] {len(text2sql_results)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì„±ê³µ (semantic_resultsì— ì¶”ê°€)")
                    else:
                        self.logger.warning("âš ï¸ [TEXT2SQL SEMANTIC] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ [TEXT2SQL SEMANTIC] ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        if extracted_keywords is None:
            extracted_keywords = []

        self.logger.info(
            f"ğŸ” [QUERY USAGE] semantic_query from optimized_queries: '{semantic_query[:100]}...' "
            f"(length={len(semantic_query)}, expanded_keywords_count={len(expanded_keywords) if expanded_keywords else 0})"
        )
        if expanded_keywords:
            self.logger.info(
                f"ğŸ” [QUERY USAGE] expanded_keywords: {expanded_keywords[:10]} "
                f"(total={len(expanded_keywords)}, included_in_query={len([t for t in expanded_keywords if t in semantic_query])})"
            )

        self.logger.info(
            f"ğŸ” [DEBUG] _execute_semantic_search_internal received: extracted_keywords={len(extracted_keywords)} (type: {type(extracted_keywords).__name__}), query='{semantic_query[:50]}...', k={semantic_k}"
        )

        self.logger.info(
            f"ğŸ” [DEBUG] Executing semantic search: query='{semantic_query[:50]}...', k={semantic_k}, original_query='{original_query[:50] if original_query else 'N/A'}...', extracted_keywords={len(extracted_keywords)}"
        )

        enhanced_semantic_query = semantic_query
        if extracted_keywords and len(extracted_keywords) > 0:
            core_keywords = []
            for kw in extracted_keywords[:5]:
                if isinstance(kw, str):
                    if any(term in kw for term in ["ë²•", "ì¡°", "ì œ", "ë¯¼ë²•", "í˜•ë²•", "ìƒë²•", "ì„ëŒ€ì°¨", "ê³„ì•½"]):
                        core_keywords.insert(0, kw)
                    else:
                        core_keywords.append(kw)

            if core_keywords:
                query_keywords = set(semantic_query.split())
                new_keywords = [kw for kw in core_keywords if kw not in query_keywords]
                if new_keywords:
                    enhanced_semantic_query = f"{semantic_query} {' '.join(new_keywords[:3])}"
                    self.logger.info(
                        f"ğŸ” [QUERY ENHANCEMENT] Enhanced semantic query: "
                        f"original='{semantic_query[:80]}...', "
                        f"enhanced='{enhanced_semantic_query[:100]}...', "
                        f"added_keywords={new_keywords[:3]}"
                    )
                else:
                    self.logger.info(
                        f"ğŸ” [QUERY ENHANCEMENT] No new keywords to add (all keywords already in query): "
                        f"query='{semantic_query[:80]}...'"
                    )
            else:
                self.logger.debug("ğŸ” [QUERY ENHANCEMENT] No core keywords extracted from extracted_keywords")
        else:
            self.logger.info(
                f"ğŸ” [QUERY ENHANCEMENT] Using original semantic_query (no extracted_keywords): "
                f"'{semantic_query[:100]}...'"
            )

        main_semantic, main_count = self.search_handler.semantic_search(
            enhanced_semantic_query,
            k=semantic_k,
            extracted_keywords=extracted_keywords
        )
        semantic_results.extend(main_semantic)
        semantic_count += main_count

        self.logger.info(
            f"ğŸ” [DEBUG] Main semantic search: {main_count} results (query: '{enhanced_semantic_query[:50]}...')"
        )

        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬: ë©”ì¸ ê²€ìƒ‰ë§Œìœ¼ë¡œ ì¶©ë¶„í•œ ê²½ìš°
        max_results_threshold = semantic_k * 3
        if len(semantic_results) >= max_results_threshold:
            self.logger.info(
                f"â­ï¸ [EARLY EXIT] Main semantic search sufficient: "
                f"{len(semantic_results)} >= {max_results_threshold}, skipping additional searches"
            )
            return semantic_results, semantic_count

        if original_query and original_query.strip():
            enhanced_original_query = original_query
            if extracted_keywords and len(extracted_keywords) > 0:
                core_keywords = [str(kw) for kw in extracted_keywords[:3] if isinstance(kw, str)]
                if core_keywords:
                    query_keywords = set(original_query.split())
                    new_keywords = [kw for kw in core_keywords if kw not in query_keywords]
                    if new_keywords:
                        enhanced_original_query = f"{original_query} {' '.join(new_keywords[:2])}"

            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬: original_query ê²€ìƒ‰ ì „ í™•ì¸
            if len(semantic_results) >= max_results_threshold:
                self.logger.info(
                    f"â­ï¸ [EARLY EXIT] Skipping original query search: "
                    f"{len(semantic_results)} >= {max_results_threshold}"
                )
            else:
                # ì¤‘ë³µ ê²€ìƒ‰ ì œê±°: semantic_queryì™€ original_queryê°€ ê°™ìœ¼ë©´ ìŠ¤í‚µ
                semantic_query_normalized = str(semantic_query).strip().lower()
                original_query_normalized = str(original_query).strip().lower()
                
                if semantic_query_normalized == original_query_normalized:
                    if self.logger.isEnabledFor(logging.DEBUG):
                        self.logger.debug(
                            "â­ï¸ [SKIP DUPLICATE] semantic_queryì™€ original_queryê°€ ë™ì¼í•˜ì—¬ "
                            "original_query ê²€ìƒ‰ ìŠ¤í‚µ"
                        )
                else:
                    original_semantic, original_count = self.search_handler.semantic_search(
                        enhanced_original_query,
                        k=semantic_k // 2,
                        extracted_keywords=extracted_keywords
                    )
                    semantic_results.extend(original_semantic)
                    semantic_count += original_count
                    if self.logger.isEnabledFor(logging.DEBUG):
                        self.logger.debug(
                            f"ğŸ” [DEBUG] Original query semantic search: {original_count} results "
                            f"(query: '{enhanced_original_query[:50]}...')"
                        )
                
                # ë‹¤ì‹œ ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                if len(semantic_results) >= max_results_threshold:
                    self.logger.info(
                        f"â­ï¸ [EARLY EXIT] After original query search: "
                        f"{len(semantic_results)} >= {max_results_threshold}, skipping multi-query"
                    )
                    return semantic_results, semantic_count

        # Multi-Query Retrieval ì ìš© (LLM ê¸°ë°˜ ì§ˆë¬¸ ì¬ì‘ì„±)
        # ê°œì„ : ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ë° ë³‘ë ¬ ì‹¤í–‰
        multi_queries = optimized_queries.get("multi_queries", [])
        min_results_threshold = semantic_k  # ìµœì†Œ ê²°ê³¼ ìˆ˜
        max_results_threshold = semantic_k * 3  # ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ
        
        # ì¡°ê¸° ì¢…ë£Œ: ì´ë¯¸ ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë©€í‹° ì¿¼ë¦¬ ìŠ¤í‚µ
        if len(semantic_results) >= max_results_threshold:
            self.logger.info(f"â­ï¸ [MULTI-QUERY] Skipping multi-query: already have {len(semantic_results)} results (threshold: {max_results_threshold})")
        elif multi_queries and len(multi_queries) > 1:
            # ê°œì„ : Multi-Query ë³‘ë ¬ ì‹¤í–‰
            # ê°œì„ : Multi-Query ë³‘ë ¬ ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ìŠ¤í‚µ)
            max_semantic_results_before_multi = semantic_k * 2  # Multi-Query ì „ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            if len(semantic_results) >= max_semantic_results_before_multi:
                self.logger.info(
                    f"âš¡ [PERFORMANCE] Skipping multi-query search "
                    f"(already have {len(semantic_results)} results, threshold={max_semantic_results_before_multi})"
                )
                multi_queries_to_process = []
            else:
                multi_queries_to_process = multi_queries[1:]  # ì²« ë²ˆì§¸ëŠ” ì´ë¯¸ ì²˜ë¦¬ë¨
                # ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜ ì œí•œ (ì„±ëŠ¥ ìµœì í™”)
                max_multi_queries = min(len(multi_queries_to_process), 2)  # ìµœëŒ€ 2ê°œë¡œ ê°ì†Œ (3 â†’ 2)
                multi_queries_to_process = multi_queries_to_process[:max_multi_queries]
            
            if multi_queries_to_process:
                self.logger.debug(f"[MULTI-QUERY] Found {len(multi_queries)} queries, processing {len(multi_queries_to_process)} in parallel...")
                self.logger.info(f"ğŸ” [MULTI-QUERY] Found {len(multi_queries)} queries, processing {len(multi_queries_to_process)} in parallel...")
                
                # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ seen_ids ë° ë‚´ìš© ìœ ì‚¬ë„ ì¶”ì 
                seen_ids = set()
                seen_contents = {}  # content_hash -> doc
                
                # ì›ë³¸ ì¿¼ë¦¬ ê²°ê³¼ì˜ IDì™€ ë‚´ìš© í•´ì‹œ ìˆ˜ì§‘
                for doc in semantic_results:
                    doc_id = doc.get("id") or doc.get("doc_id") or doc.get("document_id")
                    if doc_id:
                        seen_ids.add(doc_id)
                    # ë‚´ìš© í•´ì‹œë¡œë„ ì¤‘ë³µ í™•ì¸
                    content = doc.get("content") or doc.get("text", "")
                    if content:
                        import hashlib
                        content_hash = hashlib.md5(content[:200].encode('utf-8')).hexdigest()
                        if content_hash not in seen_contents:
                            seen_contents[content_hash] = doc
                from concurrent.futures import as_completed
                import threading
                
                # ìŠ¤ë ˆë“œ ì•ˆì „ì„ ìœ„í•œ ë½
                results_lock = threading.Lock()
                
                def process_multi_query(mq: str) -> List[Dict[str, Any]]:
                    """ë‹¨ì¼ Multi-Query ì²˜ë¦¬ í•¨ìˆ˜"""
                    if not mq or not mq.strip() or mq == semantic_query:
                        return []
                    
                    try:
                        # ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ ìˆ˜ ì œí•œ
                        mq_semantic, mq_count = self.search_handler.semantic_search(
                            mq,
                            k=max(5, semantic_k // 3),  # ìµœì†Œ 5ê°œ, ìµœëŒ€ semantic_k // 3
                            extracted_keywords=extracted_keywords
                        )
                        return mq_semantic
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ [MULTI-QUERY] Query '{mq[:30]}...' failed: {e}")
                        return []
                
                # ë°°ì¹˜ ê²€ìƒ‰ ìµœì í™”: ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ í•œ ë²ˆì— ë°°ì¹˜ë¡œ ê²€ìƒ‰
                if hasattr(self.search_handler, 'semantic_search_batch') and len(multi_queries_to_process) > 1:
                    try:
                        self.logger.info(f"âœ… [BATCH SEARCH] Processing {len(multi_queries_to_process)} queries in batch")
                        batch_k = max(5, semantic_k // 3)
                        batch_results = self.search_handler.semantic_search_batch(
                            queries=multi_queries_to_process,
                            k=batch_k,
                            extracted_keywords=extracted_keywords
                        )
                        
                        # ë°°ì¹˜ ê²°ê³¼ ì²˜ë¦¬
                        for query, (mq_semantic, mq_count) in zip(multi_queries_to_process, batch_results):
                            if mq_semantic:
                                # ì¡°ê¸° ì¢…ë£Œ í™•ì¸
                                if len(semantic_results) >= max_results_threshold:
                                    self.logger.info(
                                        f"â­ï¸ [MULTI-QUERY] Early exit: {len(semantic_results)} results "
                                        f"(threshold: {max_results_threshold})"
                                    )
                                    break
                                
                                # ì¤‘ë³µ ì œê±° ë° ê²°ê³¼ ì¶”ê°€
                                added_count = 0
                                for doc in mq_semantic:
                                    doc_id = doc.get("id") or doc.get("doc_id") or doc.get("document_id")
                                    content = doc.get("content") or doc.get("text", "")
                                    
                                    # ID ê¸°ë°˜ ì¤‘ë³µ í™•ì¸
                                    if doc_id and doc_id in seen_ids:
                                        continue
                                    
                                    # ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ í™•ì¸
                                    is_duplicate = False
                                    if content:
                                        import hashlib
                                        content_hash = hashlib.md5(content[:200].encode('utf-8')).hexdigest()
                                        if content_hash in seen_contents:
                                            existing_doc = seen_contents[content_hash]
                                            existing_content = existing_doc.get("content") or existing_doc.get("text", "")
                                            if len(content) > 0 and len(existing_content) > 0:
                                                common_chars = len(set(content[:100]) & set(existing_content[:100]))
                                                similarity = common_chars / max(len(set(content[:100])), len(set(existing_content[:100])), 1)
                                                if similarity > 0.8:
                                                    is_duplicate = True
                                        else:
                                            seen_contents[content_hash] = doc
                                    
                                    if not is_duplicate:
                                        semantic_results.append(doc)
                                        if doc_id:
                                            seen_ids.add(doc_id)
                                        added_count += 1
                                
                                if added_count > 0:
                                    self.logger.info(
                                        f"ğŸ” [MULTI-QUERY] Query '{query[:30]}...' added {added_count} unique results"
                                    )
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ [BATCH SEARCH] Batch search failed: {e}, falling back to parallel search")
                        # í´ë°±: ê¸°ì¡´ ë³‘ë ¬ ì²˜ë¦¬ ë°©ì‹ ì‚¬ìš©
                        with ThreadPoolExecutor(max_workers=min(len(multi_queries_to_process), 4)) as executor:
                            future_to_query = {
                                executor.submit(process_multi_query, mq): mq 
                                for mq in multi_queries_to_process
                            }
                            
                            for future in as_completed(future_to_query, timeout=20):
                                query = future_to_query[future]
                                try:
                                    mq_semantic = future.result()
                                    if mq_semantic:
                                        with results_lock:
                                            if len(semantic_results) >= max_results_threshold:
                                                break
                                            
                                            added_count = 0
                                            for doc in mq_semantic:
                                                doc_id = doc.get("id") or doc.get("doc_id") or doc.get("document_id")
                                                content = doc.get("content") or doc.get("text", "")
                                                
                                                if doc_id and doc_id in seen_ids:
                                                    continue
                                                
                                                is_duplicate = False
                                                if content:
                                                    import hashlib
                                                    content_hash = hashlib.md5(content[:200].encode('utf-8')).hexdigest()
                                                    if content_hash in seen_contents:
                                                        existing_doc = seen_contents[content_hash]
                                                        existing_content = existing_doc.get("content") or existing_doc.get("text", "")
                                                        if len(content) > 0 and len(existing_content) > 0:
                                                            common_chars = len(set(content[:100]) & set(existing_content[:100]))
                                                            similarity = common_chars / max(len(set(content[:100])), len(set(existing_content[:100])), 1)
                                                            if similarity > 0.8:
                                                                is_duplicate = True
                                                    else:
                                                        seen_contents[content_hash] = doc
                                                
                                                if not is_duplicate:
                                                    semantic_results.append(doc)
                                                    if doc_id:
                                                        seen_ids.add(doc_id)
                                                    added_count += 1
                                            
                                            if added_count > 0:
                                                self.logger.info(
                                                    f"ğŸ” [MULTI-QUERY] Query '{query[:30]}...' added {added_count} unique results"
                                                )
                                except Exception as e:
                                    self.logger.warning(f"âš ï¸ [MULTI-QUERY] Query '{query[:30]}...' failed: {e}")
                else:
                    # ë‹¨ì¼ ì¿¼ë¦¬ì´ê±°ë‚˜ ë°°ì¹˜ ê²€ìƒ‰ ë¯¸ì§€ì›: ê¸°ì¡´ ë³‘ë ¬ ì²˜ë¦¬ ë°©ì‹ ì‚¬ìš©
                    with ThreadPoolExecutor(max_workers=min(len(multi_queries_to_process), 4)) as executor:
                        future_to_query = {
                            executor.submit(process_multi_query, mq): mq 
                            for mq in multi_queries_to_process
                        }
                        
                        for future in as_completed(future_to_query, timeout=20):
                            query = future_to_query[future]
                            try:
                                mq_semantic = future.result()
                                if mq_semantic:
                                    with results_lock:
                                        if len(semantic_results) >= max_results_threshold:
                                            break
                                        
                                        added_count = 0
                                        for doc in mq_semantic:
                                            doc_id = doc.get("id") or doc.get("doc_id") or doc.get("document_id")
                                            content = doc.get("content") or doc.get("text", "")
                                            
                                            if doc_id and doc_id in seen_ids:
                                                continue
                                            
                                            is_duplicate = False
                                            if content:
                                                import hashlib
                                                content_hash = hashlib.md5(content[:200].encode('utf-8')).hexdigest()
                                                if content_hash in seen_contents:
                                                    existing_doc = seen_contents[content_hash]
                                                    existing_content = existing_doc.get("content") or existing_doc.get("text", "")
                                                    if len(content) > 0 and len(existing_content) > 0:
                                                        common_chars = len(set(content[:100]) & set(existing_content[:100]))
                                                        similarity = common_chars / max(len(set(content[:100])), len(set(existing_content[:100])), 1)
                                                        if similarity > 0.8:
                                                            is_duplicate = True
                                                else:
                                                    seen_contents[content_hash] = doc
                                            
                                            if not is_duplicate:
                                                semantic_results.append(doc)
                                                if doc_id:
                                                    seen_ids.add(doc_id)
                                                added_count += 1
                                        
                                        if added_count > 0:
                                            self.logger.info(
                                                f"ğŸ” [MULTI-QUERY] Query '{query[:30]}...' added {added_count} unique results"
                                            )
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ [MULTI-QUERY] Query '{query[:30]}...' failed: {e}")
            
            # ìµœì¢… ê²°ê³¼ ìˆ˜ ì—…ë°ì´íŠ¸
            semantic_count = len(semantic_results)
            self.logger.info(
                f"âœ… [MULTI-QUERY] Processing completed: {semantic_count} total results "
                f"(from {len(multi_queries_to_process)} queries)"
            )
        
        # í‚¤ì›Œë“œ ì¿¼ë¦¬ë¡œ ì¶”ê°€ ì˜ë¯¸ì  ê²€ìƒ‰ (Multi-Queryê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•œ ê²½ìš°)
        # ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ ìˆ˜ê°€ ì´ë¯¸ ì¶©ë¶„í•˜ë©´ ìŠ¤í‚µ
        max_semantic_results = semantic_k * 3  # ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ (ì˜ˆ: 12 * 3 = 36)
        if len(semantic_results) < max_semantic_results and (not multi_queries or len(multi_queries) <= 1):
            keyword_queries = optimized_queries.get("keyword_queries", [])[:1]  # 2ê°œ â†’ 1ê°œë¡œ ê°ì†Œ
            for i, kw_query in enumerate(keyword_queries, 1):
                if kw_query and kw_query.strip() and kw_query != semantic_query:
                    # ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ ìˆ˜ ì œí•œ
                    kw_semantic, kw_count = self.search_handler.semantic_search(
                        kw_query,
                        k=max(5, semantic_k // 3),  # ìµœì†Œ 5ê°œ, ìµœëŒ€ semantic_k // 3
                        extracted_keywords=extracted_keywords
                    )
                    semantic_results.extend(kw_semantic)
                    semantic_count += kw_count
                    self.logger.info(
                        f"ğŸ” [DEBUG] Keyword-based semantic search #{i}: {kw_count} results (query: '{kw_query[:50]}...')"
                    )
                    if self.logger.isEnabledFor(logging.DEBUG):
                        self.logger.debug(f"[DEBUG] _execute_semantic_search_internal: Added {kw_count} results from keyword query #{i}")
                    
                    # ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ ìˆ˜ê°€ ì´ë¯¸ ì¶©ë¶„í•˜ë©´ ì¤‘ë‹¨
                    if len(semantic_results) >= max_semantic_results:
                        self.logger.info(f"âš¡ [PERFORMANCE] Stopping keyword-based search (already have {len(semantic_results)} results)")
                        break

        # Phase 1 + Phase 2: íƒ€ì…ë³„ ë³„ë„ ê²€ìƒ‰ ìˆ˜í–‰ ë° ì¿¼ë¦¬ ë‹¤ë³€í™” ì ìš© (íƒ€ì… ë‹¤ì–‘ì„± ê°œì„ )
        # ê°œì„ : íƒ€ì…ë³„ ê²€ìƒ‰ ì „ ì¡°ê±´ ì²´í¬ (ì¡°ê¸° ìŠ¤í‚µ)
        max_semantic_results = semantic_k * 3  # ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ
        
        # ì¡°ê±´ 1: ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ìŠ¤í‚µ (60% ì´ìƒìœ¼ë¡œ ì™„í™”í•˜ì—¬ ë” ë¹¨ë¦¬ ìŠ¤í‚µ)
        should_skip_type_diversity = False
        if len(semantic_results) >= max_semantic_results * 0.6:
            self.logger.info(
                f"âš¡ [PERFORMANCE] Skipping type diversity search "
                f"(already have {len(semantic_results)} results, threshold={max_semantic_results * 0.6:.0f})"
            )
            should_skip_type_diversity = True
        else:
            # ì¡°ê±´ 2: íƒ€ì… ë¶„í¬ í™•ì¸
            def _calculate_type_distribution(docs):
                """íƒ€ì… ë¶„í¬ ê³„ì‚°"""
                type_counts = {}
                for doc in docs:
                    doc_type = (
                        doc.get("type") or
                        doc.get("source_type") or
                        (doc.get("metadata", {}).get("source_type") if isinstance(doc.get("metadata"), dict) else "") or
                        "unknown"
                    )
                    # íƒ€ì… ë§¤í•‘
                    type_mapping = {
                        "statute_article": "statute",
                        "case_paragraph": "case",
                        "decision_paragraph": "decision",
                        "interpretation_paragraph": "interpretation"
                    }
                    mapped_type = type_mapping.get(doc_type, doc_type)
                    type_counts[mapped_type] = type_counts.get(mapped_type, 0) + 1
                return type_counts
            
            type_distribution = _calculate_type_distribution(semantic_results)
            
            # ì´ë¯¸ 2ê°œ ì´ìƒ íƒ€ì…ì´ë©´ ìŠ¤í‚µ (3ê°œ â†’ 2ê°œë¡œ ì™„í™”)
            if len(type_distribution) >= 2:
                self.logger.info(
                    f"âš¡ [PERFORMANCE] Skipping type diversity search "
                    f"(sufficient type diversity: {len(type_distribution)} types)"
                )
                should_skip_type_diversity = True
        
        type_specific_results = {}
        type_specific_count = 0
        
        if not should_skip_type_diversity:
            document_types = {
                "statute_article": "statute",
                "case_paragraph": "case",
                "decision_paragraph": "decision",
                "interpretation_paragraph": "interpretation"
            }
            
            # semantic_search_engine í™•ì¸ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
            semantic_engine = None
        if self.logger.isEnabledFor(logging.DEBUG):
            self.logger.debug("ğŸ” [TYPE DIVERSITY] semantic_search_engine í™•ì¸ ì‹œì‘")
        self.logger.info(f"ğŸ” [TYPE DIVERSITY] self.semantic_search_engine: {self.semantic_search_engine is not None}")
        
        # SemanticSearchEngineV2 ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        def is_semantic_search_engine(obj):
            """SemanticSearchEngineV2 ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸"""
            if obj is None:
                return False
            # íƒ€ì… ì´ë¦„ìœ¼ë¡œ í™•ì¸ (import ì—†ì´)
            type_name = type(obj).__name__
            if type_name == 'SemanticSearchEngineV2':
                return True
            # hasattrë¡œ search ë©”ì„œë“œ í™•ì¸
            if hasattr(obj, 'search') and callable(getattr(obj, 'search', None)):
                # í•¨ìˆ˜ê°€ ì•„ë‹Œ ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸
                if not callable(obj) or hasattr(obj, '__class__'):
                    return True
            return False
        
        if self.semantic_search_engine and is_semantic_search_engine(self.semantic_search_engine):
            semantic_engine = self.semantic_search_engine
            self.logger.info(f"âœ… [TYPE DIVERSITY] semantic_search_engine from self: {type(semantic_engine).__name__}")
        elif hasattr(self.search_handler, 'semantic_search_engine') and self.search_handler.semantic_search_engine:
            candidate = self.search_handler.semantic_search_engine
            if is_semantic_search_engine(candidate):
                semantic_engine = candidate
                self.logger.info(f"âœ… [TYPE DIVERSITY] semantic_search_engine from search_handler: {type(semantic_engine).__name__}")
            else:
                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler.semantic_search_engine is not a valid engine: {type(candidate).__name__}")
        elif hasattr(self.search_handler, 'semantic_search') and self.search_handler.semantic_search:
            candidate = self.search_handler.semantic_search
            # semantic_searchê°€ í•¨ìˆ˜ì¸ì§€ í™•ì¸
            if callable(candidate) and not is_semantic_search_engine(candidate):
                self.logger.warning("âš ï¸ [TYPE DIVERSITY] search_handler.semantic_search is a function, not an engine instance")
            elif is_semantic_search_engine(candidate):
                semantic_engine = candidate
                self.logger.info(f"âœ… [TYPE DIVERSITY] semantic_search_engine from search_handler.semantic_search: {type(semantic_engine).__name__}")
            else:
                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler.semantic_search is not a valid engine: {type(candidate).__name__}")
        else:
            self.logger.warning("âš ï¸ [TYPE DIVERSITY] semantic_search_engine not found")
            self.logger.warning(f"   - self.semantic_search_engine: {self.semantic_search_engine} ({type(self.semantic_search_engine).__name__ if self.semantic_search_engine else 'None'})")
            self.logger.warning(f"   - search_handler.semantic_search_engine: {getattr(self.search_handler, 'semantic_search_engine', 'N/A')}")
            self.logger.warning(f"   - search_handler.semantic_search: {getattr(self.search_handler, 'semantic_search', 'N/A')}")
        
        if self.logger.isEnabledFor(logging.DEBUG):
            self.logger.debug(f"ğŸ” [TYPE DIVERSITY] semantic_engine í™•ì¸ ê²°ê³¼: {semantic_engine is not None}")
        
        if semantic_engine and not should_skip_type_diversity:
            if self.logger.isEnabledFor(logging.DEBUG):
                self.logger.debug("âœ… [TYPE DIVERSITY] semantic_engine ë°œê²¬, íƒ€ì…ë³„ ê²€ìƒ‰ ì§„í–‰")
            # Phase 2: QueryDiversifierë¡œ íƒ€ì…ë³„ ì¿¼ë¦¬ ìƒì„±
            try:
                diversified_queries = self.query_diversifier.diversify_search_queries(original_query or enhanced_semantic_query)
                self.logger.info(
                    f"ğŸ” [TYPE DIVERSITY] ë‹¤ë³€í™”ëœ ì¿¼ë¦¬ ìƒì„±: "
                    f"statute={len(diversified_queries.get('statute', []))}, "
                    f"case={len(diversified_queries.get('case', []))}, "
                    f"decision={len(diversified_queries.get('decision', []))}, "
                    f"interpretation={len(diversified_queries.get('interpretation', []))}"
                )
            except Exception as e:
                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] ì¿¼ë¦¬ ë‹¤ë³€í™” ì‹¤íŒ¨: {e}")
                diversified_queries = {}
            
            if self.logger.isEnabledFor(logging.DEBUG):
                self.logger.debug("ğŸ” [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì‹œì‘ (ë³‘ë ¬ ì‹¤í–‰)")
                self.logger.debug(f"ğŸ” [TYPE DIVERSITY] ê²€ìƒ‰í•  íƒ€ì…: {list(document_types.keys())}")
            
            # íƒ€ì…ë³„ ê²€ìƒ‰ ë³‘ë ¬í™”
            def search_by_type(doc_type, query_type):
                """íƒ€ì…ë³„ ê²€ìƒ‰ í•¨ìˆ˜ (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
                if self.logger.isEnabledFor(logging.DEBUG):
                    self.logger.debug(f"ğŸ” [TYPE DIVERSITY] {doc_type} ê²€ìƒ‰ ì‹œì‘ (query_type={query_type})")
                try:
                    # Phase 2: íƒ€ì…ë³„ ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©
                    type_queries = diversified_queries.get(query_type, [])
                    search_query = enhanced_semantic_query  # ê¸°ë³¸ ì¿¼ë¦¬
                    
                    # íƒ€ì…ë³„ ìµœì í™”ëœ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                    if type_queries:
                        search_query = type_queries[0]  # ì²« ë²ˆì§¸ ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©: '{search_query[:50]}...'")
                    else:
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: '{search_query[:50]}...'")
                    
                    # ìš°ì„ ìˆœìœ„ 5: íƒ€ì…ë³„ ê²€ìƒ‰ ê°•í™” (ì„±ëŠ¥ ìµœì í™”: k ê°’ ê°ì†Œ)
                    k_per_type = 15  # 20 â†’ 15ë¡œ ê°ì†Œ (ì„±ëŠ¥ ê°œì„ )
                    min_score_by_type = {
                        "statute_article": 0.4,  # ë²•ë ¹ ì¡°ë¬¸: ë‚®ì€ ì„ê³„ê°’
                        "case_paragraph": 0.5,
                        "decision_paragraph": 0.5,
                        "interpretation_paragraph": 0.5
                    }
                    min_score = min_score_by_type.get(doc_type, 0.5)
                    
                    self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type} ê²€ìƒ‰ ì‹œì‘ (k={k_per_type}, threshold={min_score}, source_types=[{doc_type}])")
                    type_results = semantic_engine.search(
                        search_query,
                        k=k_per_type,  # k * 2 â†’ kë¡œ ê°ì†Œ (ì„±ëŠ¥ ê°œì„ )
                        source_types=[doc_type],  # íƒ€ì…ë³„ í•„í„° ì ìš©
                        similarity_threshold=min_score,  # íƒ€ì…ë³„ ìµœì†Œ ì ìˆ˜
                        min_results=1,  # ìµœì†Œ 1ê°œëŠ” ë³´ì¥
                        disable_retry=False  # ì¬ì‹œë„ ë¡œì§ í™œì„±í™” (ì„ê³„ê°’ ìë™ ì¡°ì •)
                    )
                    
                    # í’ˆì§ˆ í•„í„°ë§ (íƒ€ì…ë³„ ìµœì†Œ ì ìˆ˜)
                    if type_results:
                        filtered_results = [
                            doc for doc in type_results
                            if doc.get("similarity", doc.get("relevance_score", 0.0)) >= min_score
                        ]
                        # ìƒìœ„ k_per_typeê°œ ì„ íƒ
                        type_results = filtered_results[:k_per_type]
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: {len(type_results)}ê°œ ê²°ê³¼ (í•„í„°ë§ í›„)")
                    
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë” ì¼ë°˜ì ì¸ ì¿¼ë¦¬ë¡œ ì¬ì‹œë„
                    if not type_results:
                        # ì›ë³¸ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì—¬ ì¬ì‹œë„
                        core_keywords = original_query.split()[:3] if original_query else search_query.split()[:3]
                        fallback_query = " ".join(core_keywords)
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: í´ë°± ì¿¼ë¦¬ë¡œ ì¬ì‹œë„: '{fallback_query}'")
                        try:
                            type_results = semantic_engine.search(
                                fallback_query,
                                k=20,
                                source_types=[doc_type],
                                similarity_threshold=0.0,  # ìµœì†Œ ì„ê³„ê°’
                                min_results=1,
                                disable_retry=False  # ì¬ì‹œë„ ë¡œì§ í™œì„±í™”
                            )
                            if type_results:
                                self.logger.info(f"âœ… [TYPE DIVERSITY] {doc_type}: í´ë°± ì¿¼ë¦¬ë¡œ {len(type_results)}ê°œ ê²€ìƒ‰ ì„±ê³µ")
                        except Exception as e:
                            self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type} í´ë°± ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    
                    # ê°œì„ : ì¼ë°˜ í‚¤ì›Œë“œ ì¬ì‹œë„ ì œê±° (3ë‹¨ê³„ â†’ 2ë‹¨ê³„ë¡œ ë‹¨ìˆœí™”)
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´
                    self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ìµœì¢… {len(type_results)}ê°œ ê²€ìƒ‰ë¨")
                    
                    # ìµœì¢… ë°©ì•ˆ: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ íƒ€ì…ë³„ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´
                    if not type_results:
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´ ì‹œë„")
                        try:
                            type_results = self._get_type_sample(semantic_engine, doc_type, k=2)
                            if type_results:
                                if self.logger.isEnabledFor(logging.DEBUG):
                                    self.logger.debug(f"âœ… [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ìœ¼ë¡œ {len(type_results)}ê°œ ê°€ì ¸ì˜´")
                                # ìƒ˜í”Œë§ëœ ë¬¸ì„œ ìƒì„¸ ë¡œê·¸
                                for idx, sample_doc in enumerate(type_results, 1):
                                    self.logger.debug(
                                        f"   ìƒ˜í”Œ {idx}: id={sample_doc.get('id')}, "
                                        f"source_type={sample_doc.get('source_type')}, "
                                        f"type={sample_doc.get('type')}, "
                                        f"relevance_score={sample_doc.get('relevance_score')}"
                                    )
                            else:
                                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ ê²°ê³¼ë„ ì—†ìŒ")
                        except Exception as e:
                            self.logger.error(f"âŒ [TYPE DIVERSITY] {doc_type} ìƒ˜í”Œë§ ì‹¤íŒ¨: {e}")
                            import traceback
                            self.logger.debug(f"ìƒ˜í”Œë§ ì˜ˆì™¸ ìƒì„¸: {traceback.format_exc()}")
                    
                    return doc_type, type_results
                except Exception as e:
                    self.logger.error(f"âŒ [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì‹¤íŒ¨ ({doc_type}): {e}")
                    import traceback
                    self.logger.debug(f"íƒ€ì…ë³„ ê²€ìƒ‰ ì˜ˆì™¸ ìƒì„¸: {traceback.format_exc()}")
                    return doc_type, []
            
            # ë³‘ë ¬ ì‹¤í–‰
            with ThreadPoolExecutor(max_workers=len(document_types)) as executor:
                futures = {
                    executor.submit(search_by_type, doc_type, query_type): doc_type
                    for doc_type, query_type in document_types.items()
                }
                
                for future in futures:
                    doc_type = futures[future]
                    try:
                        result_doc_type, type_results = future.result(timeout=15)  # 30ì´ˆ â†’ 15ì´ˆë¡œ ìµœì í™”
                        if type_results:
                            type_specific_results[result_doc_type] = type_results
                            semantic_results.extend(type_results)
                            type_specific_count += len(type_results)
                            if self.logger.isEnabledFor(logging.DEBUG):
                                self.logger.debug(
                                    f"âœ… [TYPE DIVERSITY] {result_doc_type}: {len(type_results)}ê°œ ê²€ìƒ‰ ì„±ê³µ "
                                    f"(ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€ë¨, ì´ semantic_results: {len(semantic_results)}ê°œ)"
                                )
                        else:
                            if self.logger.isEnabledFor(logging.DEBUG):
                                self.logger.debug(
                                    f"âš ï¸ [TYPE DIVERSITY] {result_doc_type}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë°ì´í„° ì—†ìŒ ë˜ëŠ” ì¿¼ë¦¬ ê´€ë ¨ì„± ë‚®ìŒ)"
                                )
                    except Exception as e:
                        self.logger.error(f"âŒ [TYPE DIVERSITY] {doc_type} ë³‘ë ¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                
                # íƒ€ì…ë³„ ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
                if 'type_specific_count' in locals() and type_specific_count > 0:
                    self.logger.info(
                        f"âœ… [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì™„ë£Œ: ì´ {type_specific_count}ê°œ ì¶”ê°€ "
                        f"(ì´ semantic_results: {len(semantic_results)}ê°œ)"
                    )
        elif not semantic_engine:
            self.logger.warning("âš ï¸ [TYPE DIVERSITY] semantic_search_engineì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íƒ€ì…ë³„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] semantic_search_engine í™•ì¸: self.semantic_search_engine={self.semantic_search_engine is not None}")
            if hasattr(self, 'search_handler'):
                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler í™•ì¸: {self.search_handler is not None}")
                if self.search_handler:
                    self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler.semantic_search í™•ì¸: {hasattr(self.search_handler, 'semantic_search')}")
                    if hasattr(self.search_handler, 'semantic_search_engine'):
                        self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler.semantic_search_engine í™•ì¸: {self.search_handler.semantic_search_engine is not None}")
        
        semantic_count += type_specific_count
        
        if type_specific_count > 0:
            type_distribution = dict((k, len(v)) for k, v in type_specific_results.items())
            self.logger.info(
                f"âœ… [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì™„ë£Œ: ì´ {type_specific_count}ê°œ ì¶”ê°€ "
                f"(íƒ€ì…ë³„ ë¶„í¬: {type_distribution})"
            )
            # interpretation_paragraph í™•ì¸
            if "interpretation_paragraph" in type_specific_results:
                self.logger.info(
                    f"âœ… [TYPE DIVERSITY] interpretation_paragraph: {len(type_specific_results['interpretation_paragraph'])}ê°œ "
                    f"ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë¨"
                )
            else:
                self.logger.warning(
                    f"âš ï¸ [TYPE DIVERSITY] interpretation_paragraph: ê²€ìƒ‰ ê²°ê³¼ì— ì—†ìŒ "
                    f"(type_specific_results keys: {list(type_specific_results.keys())})"
                )
        else:
            self.logger.info("âš ï¸ [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë°ì´í„° ë¶ˆê· í˜• ë˜ëŠ” ê²€ìƒ‰ ì‹¤íŒ¨)")

        self.logger.info(
            f"ğŸ” [DEBUG] Total semantic search results: {semantic_count} (unique: {len(semantic_results)})"
        )
        if self.logger.isEnabledFor(logging.DEBUG):
            self.logger.debug(f"[DEBUG] SEMANTIC SEARCH INTERNAL: Total={semantic_count}, Unique={len(semantic_results)}")

        search_queries_used = []
        if semantic_query:
            search_queries_used.append(f"semantic_query({len(semantic_query)} chars)")
        if original_query:
            search_queries_used.append(f"original_query({len(original_query)} chars)")
        keyword_queries_used = optimized_queries.get("keyword_queries", [])[:2]
        if keyword_queries_used:
            search_queries_used.append(f"keyword_queries({len(keyword_queries_used)} queries)")
        if self.logger.isEnabledFor(logging.DEBUG):
            self.logger.debug(f"[DEBUG] SEMANTIC SEARCH INTERNAL: Queries used: {', '.join(search_queries_used)}")

        return semantic_results, semantic_count

    def execute_keyword_search(
        self,
        optimized_queries: Dict[str, Any],
        search_params: Dict[str, Any],
        query_type_str: str,
        legal_field: str,
        extracted_keywords: List[str],
        original_query: str = ""
    ) -> Tuple[List[Dict[str, Any]], int]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰"""
        keyword_results = []
        keyword_count = 0

        keyword_queries = optimized_queries.get("keyword_queries", [])
        keyword_limit = search_params.get("keyword_limit", WorkflowConstants.CATEGORY_SEARCH_LIMIT)

        self.logger.info(
            f"ğŸ” [DEBUG] Executing keyword search: {len(keyword_queries)} queries, "
            f"limit={keyword_limit}, field={legal_field}, "
            f"keywords={extracted_keywords[:5] if extracted_keywords else []}, "
            f"original_query='{original_query[:50] if original_query else 'N/A'}...'"
        )

        # ê°œì„ : textToSQL ë¼ìš°íŒ… í™•ì¸ ë° ì ìš©
        from core.search.connectors.legal_data_connector_v2 import route_query, LegalDataConnectorV2
        
        # original_queryì— ëŒ€í•´ ë¼ìš°íŒ… í™•ì¸
        self.logger.debug(f"[TEXT2SQL DEBUG] original_query='{original_query[:50] if original_query else 'EMPTY'}...', has_query={bool(original_query and original_query.strip())}")
        self.logger.info(f"ğŸ” [TEXT2SQL DEBUG] original_query='{original_query[:50] if original_query else 'EMPTY'}...', has_query={bool(original_query and original_query.strip())}")
        if original_query and original_query.strip():
            route = route_query(original_query)
            if self.logger.isEnabledFor(logging.DEBUG):
                self.logger.debug(f"ğŸ” [TEXT2SQL DEBUG] route_query result: '{route}' for query: '{original_query[:50]}...'")
            if route == "text2sql":
                # textToSQL ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰
                if self.logger.isEnabledFor(logging.DEBUG):
                    self.logger.debug(f"ğŸ” [TEXT2SQL] Detected text2sql route for query: '{original_query[:50]}...'")
                try:
                    data_connector = LegalDataConnectorV2()
                    text2sql_results = data_connector.search_documents(original_query, limit=keyword_limit)
                    if text2sql_results:
                        keyword_results.extend(text2sql_results)
                        keyword_count += len(text2sql_results)
                        if self.logger.isEnabledFor(logging.DEBUG):
                            self.logger.debug(f"âœ… [TEXT2SQL] {len(text2sql_results)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì„±ê³µ")
                    else:
                        if self.logger.isEnabledFor(logging.DEBUG):
                            self.logger.debug("âš ï¸ [TEXT2SQL] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                except Exception as e:
                    if self.logger.isEnabledFor(logging.DEBUG):
                        self.logger.debug(f"âš ï¸ [TEXT2SQL] ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ê¸°ì¡´ keyword_search_func ë¡œì§ë„ ìœ ì§€ (í•˜ì´ë¸Œë¦¬ë“œ)
            if self.keyword_search_func:
                original_kw_results, original_kw_count = self.keyword_search_func(
                    query=original_query,
                    query_type_str=query_type_str,
                    limit=keyword_limit,
                    legal_field=legal_field,
                    extracted_keywords=extracted_keywords
                )
                keyword_results.extend(original_kw_results)
                keyword_count += original_kw_count
                self.logger.info(
                    f"ğŸ” [DEBUG] Original query keyword search: {original_kw_count} results (query: '{original_query[:50]}...')"
                )

        # keyword_queriesì— ëŒ€í•´ì„œë„ ë¼ìš°íŒ… í™•ì¸
        for i, kw_query in enumerate(keyword_queries, 1):
            if kw_query and kw_query.strip() and kw_query != original_query:
                route = route_query(kw_query)
                if route == "text2sql":
                    self.logger.info(f"ğŸ” [TEXT2SQL] Detected text2sql route for keyword query #{i}: '{kw_query[:50]}...'")
                    try:
                        data_connector = LegalDataConnectorV2()
                        text2sql_results = data_connector.search_documents(kw_query, limit=keyword_limit)
                        if text2sql_results:
                            keyword_results.extend(text2sql_results)
                            keyword_count += len(text2sql_results)
                            self.logger.info(f"âœ… [TEXT2SQL] Query #{i}: {len(text2sql_results)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì„±ê³µ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ [TEXT2SQL] Query #{i} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                
                # ê¸°ì¡´ keyword_search_func ë¡œì§ë„ ìœ ì§€ (í•˜ì´ë¸Œë¦¬ë“œ)
                if self.keyword_search_func:
                    kw_results, kw_count = self.keyword_search_func(
                        query=kw_query,
                        query_type_str=query_type_str,
                        limit=keyword_limit,
                        legal_field=legal_field,
                        extracted_keywords=extracted_keywords
                    )
                    keyword_results.extend(kw_results)
                    keyword_count += kw_count
                    self.logger.info(
                        f"ğŸ” [DEBUG] Keyword search #{i}: {kw_count} results (query: '{kw_query[:50]}...')"
                    )

        self.logger.info(
            f"ğŸ” [DEBUG] Total keyword search results: {keyword_count} (unique: {len(keyword_results)})"
        )
        if self.logger.isEnabledFor(logging.DEBUG):
            self.logger.debug(f"[DEBUG] KEYWORD SEARCH INTERNAL: Total={keyword_count}, Unique={len(keyword_results)}")

        return keyword_results, keyword_count

    def fallback_sequential_search(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ìˆœì°¨ ê²€ìƒ‰ ì‹¤í–‰ (í´ë°±) - ì•ˆì „í•œ ì˜¤ë¥˜ ì²˜ë¦¬"""
        semantic_results, semantic_count = [], 0
        keyword_results, keyword_count = [], 0
        
        try:
            self.logger.info("ğŸ”„ ìˆœì°¨ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")

            optimized_queries = self._get_state_value(state, "optimized_queries", {})
            search_params = self._get_state_value(state, "search_params", {})
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            legal_field = self._get_state_value(state, "legal_field", "")
            extracted_keywords = optimized_queries.get("expanded_keywords", [])

            original_query = self._get_state_value(state, "query", "")
            if not original_query and "input" in state and isinstance(state.get("input"), dict):
                original_query = state["input"].get("query", "")
            
            # ë¹ˆ ì¿¼ë¦¬ ê²€ì¦ ì¶”ê°€
            if not original_query or not original_query.strip():
                self.logger.error("âŒ ìˆœì°¨ ê²€ìƒ‰: queryê°€ ë¹„ì–´ìˆì–´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.logger.warning("âš ï¸ ìˆœì°¨ ê²€ìƒ‰: queryë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. state êµ¬ì¡° í™•ì¸:")
                self.logger.debug(f"   state keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")
                if "input" in state:
                    self.logger.debug(f"   input type: {type(state.get('input'))}")
                    if isinstance(state.get("input"), dict):
                        self.logger.debug(f"   input keys: {list(state.get('input', {}).keys())}")
                # ë¹ˆ ê²°ê³¼ ë°˜í™˜
                self._set_state_value(state, "semantic_results", [])
                self._set_state_value(state, "keyword_results", [])
                self._set_state_value(state, "semantic_count", 0)
                self._set_state_value(state, "keyword_count", 0)
                return state

            extracted_keywords_for_search = self._get_state_value(state, "extracted_keywords", [])
            
            # ì˜ë¯¸ì  ê²€ìƒ‰ ì‹œë„ (ì˜¤ë¥˜ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰)
            try:
                semantic_results, semantic_count = self.execute_semantic_search(
                    optimized_queries, search_params, original_query, extracted_keywords_for_search
                )
                self.logger.info(f"âœ… ìˆœì°¨ ê²€ìƒ‰: ì˜ë¯¸ì  ê²€ìƒ‰ ì™„ë£Œ ({semantic_count}ê°œ ê²°ê³¼)")
            except Exception as semantic_err:
                self.logger.warning(f"âš ï¸ ìˆœì°¨ ê²€ìƒ‰: ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤íŒ¨: {semantic_err}")
                self.logger.debug(f"   ì˜ë¯¸ì  ê²€ìƒ‰ ì˜¤ë¥˜ ìƒì„¸: {semantic_err}", exc_info=True)
                semantic_results, semantic_count = [], 0

            # í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œë„ (ì˜¤ë¥˜ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰)
            try:
                keyword_results, keyword_count = self.execute_keyword_search(
                    optimized_queries, search_params, query_type_str, legal_field, extracted_keywords, original_query
                )
                self.logger.info(f"âœ… ìˆœì°¨ ê²€ìƒ‰: í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ ({keyword_count}ê°œ ê²°ê³¼)")
            except Exception as keyword_err:
                self.logger.warning(f"âš ï¸ ìˆœì°¨ ê²€ìƒ‰: í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {keyword_err}")
                self.logger.debug(f"   í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜ ìƒì„¸: {keyword_err}", exc_info=True)
                keyword_results, keyword_count = [], 0

            # ê²°ê³¼ ì €ì¥ (ì¼ë¶€ë¼ë„ ì„±ê³µí•˜ë©´ ì €ì¥)
            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "semantic_count", semantic_count)
            self._set_state_value(state, "keyword_count", keyword_count)
            
            # State êµ¬ì¡° ì¼ê´€ì„± í™•ë³´: retrieved_docsë¥¼ í—¬í¼ í•¨ìˆ˜ë¡œ ì €ì¥
            merged_docs = semantic_results + keyword_results
            set_retrieved_docs(state, merged_docs)

            total_results = semantic_count + keyword_count
            if total_results > 0:
                self.logger.info(f"âœ… ìˆœì°¨ ê²€ìƒ‰ ì™„ë£Œ: ì˜ë¯¸ì  {semantic_count}ê°œ, í‚¤ì›Œë“œ {keyword_count}ê°œ (ì´ {total_results}ê°œ)")
            else:
                self.logger.warning(f"âš ï¸ ìˆœì°¨ ê²€ìƒ‰ ì™„ë£Œ: ê²°ê³¼ ì—†ìŒ (ì˜ë¯¸ì  {semantic_count}ê°œ, í‚¤ì›Œë“œ {keyword_count}ê°œ)")

        except Exception as e:
            self.logger.error(f"âŒ ìˆœì°¨ ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            self._handle_error(state, str(e), "ìˆœì°¨ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # ìµœì†Œí•œì˜ ê²°ê³¼ë¼ë„ ì €ì¥
            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "semantic_count", semantic_count)
            self._set_state_value(state, "keyword_count", keyword_count)
            
            # State êµ¬ì¡° ì¼ê´€ì„± í™•ë³´: retrieved_docsë¥¼ í—¬í¼ í•¨ìˆ˜ë¡œ ì €ì¥
            merged_docs = semantic_results + keyword_results
            set_retrieved_docs(state, merged_docs)

        return state

    def _get_state_value(self, state: LegalWorkflowState, key: str, default: Any = None) -> Any:
        """Stateì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
        if self._get_state_value_func:
            return self._get_state_value_func(state, key, default)
        if isinstance(state, dict):
            if key in state:
                return state[key]
            if "search" in state and isinstance(state.get("search"), dict) and key in state["search"]:
                return state["search"][key]
        return default

    def _set_state_value(self, state: LegalWorkflowState, key: str, value: Any) -> None:
        """Stateì— ê°’ ì„¤ì •"""
        if self._set_state_value_func:
            self._set_state_value_func(state, key, value)
        elif isinstance(state, dict):
            if "search" not in state or not isinstance(state.get("search"), dict):
                state["search"] = {}
            state["search"][key] = value

    def _get_query_type_str(self, query_type) -> str:
        """QueryTypeì„ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if self._get_query_type_str_func:
            return self._get_query_type_str_func(query_type)
        if isinstance(query_type, str):
            return query_type
        if hasattr(query_type, 'value'):
            return query_type.value
        return str(query_type) if query_type else ""

    def _determine_search_parameters(
        self,
        query_type: str,
        query_complexity: int,
        keyword_count: int,
        is_retry: bool
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ê²°ì •"""
        if self._determine_search_parameters_func:
            return self._determine_search_parameters_func(query_type, query_complexity, keyword_count, is_retry)
        return {
            "semantic_k": WorkflowConstants.SEMANTIC_SEARCH_K,
            "keyword_limit": WorkflowConstants.CATEGORY_SEARCH_LIMIT,
            "min_relevance": self.config.similarity_threshold if hasattr(self.config, 'similarity_threshold') else 0.5,
            "max_results": WorkflowConstants.MAX_DOCUMENTS
        }

    def _save_metadata_safely(self, state: LegalWorkflowState, key: str, value: Any) -> None:
        """ë©”íƒ€ë°ì´í„° ì•ˆì „í•˜ê²Œ ì €ì¥"""
        if self._save_metadata_safely_func:
            self._save_metadata_safely_func(state, key, value)
        elif isinstance(state, dict):
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            state["metadata"][key] = value

    def _update_processing_time(self, state: LegalWorkflowState, start_time: float) -> None:
        """ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        if self._update_processing_time_func:
            self._update_processing_time_func(state, start_time)
        elif isinstance(state, dict):
            elapsed = time.time() - start_time
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            if "processing_time" not in state["metadata"]:
                state["metadata"]["processing_time"] = 0.0
            state["metadata"]["processing_time"] += elapsed

    def _get_type_sample(self, semantic_engine, doc_type: str, k: int = 2) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • íƒ€ì…ì˜ ëœë¤ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸° (ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        
        Args:
            semantic_engine: SemanticSearchEngineV2 ì¸ìŠ¤í„´ìŠ¤
            doc_type: ë¬¸ì„œ íƒ€ì…
            k: ê°€ì ¸ì˜¬ ìƒ˜í”Œ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ìƒ˜í”Œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # DBì—ì„œ í•´ë‹¹ íƒ€ì…ì˜ ëœë¤ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ì„±ëŠ¥ ìµœì í™”: ì¸ë±ìŠ¤ í™œìš©)
            conn = semantic_engine._get_connection()
            
            # ë¨¼ì € í•´ë‹¹ íƒ€ì…ì˜ ë¬¸ì„œ ìˆ˜ í™•ì¸
            count_cursor = conn.execute(
                "SELECT COUNT(*) as count FROM text_chunks WHERE source_type = ? AND text IS NOT NULL AND LENGTH(text) > 50",
                (doc_type,)
            )
            count_row = count_cursor.fetchone()
            total_count = count_row['count'] if count_row else 0
            
            if total_count == 0:
                self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§í•  ë¬¸ì„œ ì—†ìŒ (ì´ 0ê°œ)")
                return []
            
            # ëœë¤ ìƒ˜í”Œë§ (ì„±ëŠ¥ ìµœì í™”: LIMIT ì‚¬ìš©)
            cursor = conn.execute(
                """
                SELECT id, text, source_id, source_type
                FROM text_chunks
                WHERE source_type = ? AND text IS NOT NULL AND LENGTH(text) > 50
                ORDER BY RANDOM()
                LIMIT ?
                """,
                (doc_type, k)
            )
            rows = cursor.fetchall()
            
            if not rows:
                self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ ê²°ê³¼ ì—†ìŒ (ì´ {total_count}ê°œ ì¤‘)")
                return []
            
            self.logger.info(f"âœ… [TYPE DIVERSITY] {doc_type}: {len(rows)}ê°œ ìƒ˜í”Œë§ ì„±ê³µ (ì´ {total_count}ê°œ ì¤‘)")
            
            # ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            samples = []
            for row in rows:
                chunk_id = row['id']
                text = row['text'] or ""
                source_id = row['source_id']
                
                # ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)
                source_meta = {}
                try:
                    if hasattr(semantic_engine, '_get_source_metadata'):
                        source_meta = semantic_engine._get_source_metadata(conn, doc_type, source_id)
                        if not source_meta:
                            # ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ ì‹œ text_chunksì—ì„œ ê¸°ë³¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                            cursor_meta = conn.execute(
                                "SELECT source_type, source_id, text FROM text_chunks WHERE id = ?",
                                (chunk_id,)
                            )
                            row_meta = cursor_meta.fetchone()
                            if row_meta:
                                source_meta = {
                                    "source_type": row_meta['source_type'],
                                    "source_id": row_meta['source_id'],
                                    "text": row_meta['text']
                                }
                except Exception as e:
                    self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ ({doc_type}, source_id={source_id}): {e}")
                    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
                    source_meta = {
                        "source_type": doc_type,
                        "source_id": source_id
                    }
                
                # UnifiedSourceFormatterë¡œ ì¶œì²˜ ì •ë³´ ìƒì„± (ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê°œì„ )
                try:
                    from core.generation.formatters.unified_source_formatter import UnifiedSourceFormatter
                    formatter = UnifiedSourceFormatter()
                    source_info = formatter.format_source(doc_type, source_meta)
                    source_name = source_info.name
                    source_url = source_info.url
                    
                    # source_nameì´ ë¹„ì–´ìˆê±°ë‚˜ ê¸°ë³¸ê°’ì´ë©´ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ ì‹œë„
                    if not source_name or source_name == doc_type:
                        if doc_type == "statute_article":
                            source_name = source_meta.get("statute_name") or source_meta.get("name") or "ë²•ë ¹ ì¡°ë¬¸"
                        elif doc_type == "case_paragraph":
                            source_name = source_meta.get("casenames") or source_meta.get("doc_id") or "íŒë¡€"
                        elif doc_type == "decision_paragraph":
                            source_name = f"{source_meta.get('org', '')} {source_meta.get('doc_id', '')}".strip() or "ê²°ì •ë¡€"
                        elif doc_type == "interpretation_paragraph":
                            source_name = f"{source_meta.get('org', '')} {source_meta.get('title', '')}".strip() or "í•´ì„ë¡€"
                except Exception as e:
                    self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] ì¶œì²˜ ì •ë³´ ìƒì„± ì‹¤íŒ¨ ({doc_type}): {e}")
                    source_name = doc_type
                    source_url = ""
                
                # ê³ ìœ í•œ ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)
                unique_id = f"sample_{doc_type}_{chunk_id}_{source_id}"
                samples.append({
                    "id": unique_id,  # ê³ ìœ  IDë¡œ ë³€ê²½
                    "content_id": unique_id,  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ëŒ€ì²´ ID
                    "text": text,
                    "content": text,
                    "score": 0.3,  # ë‚®ì€ ì ìˆ˜ (ê°•ì œ ìƒ˜í”Œë§)
                    "similarity": 0.3,
                    "type": doc_type,
                    "source_type": doc_type,
                    "source": source_name,
                    "source_url": source_url,
                    "source_id": source_id,
                    "metadata": {
                        "chunk_id": chunk_id,
                        "source_type": doc_type,
                        "source_id": source_id,
                        "text": text,
                        "is_sample": True,  # ìƒ˜í”Œë§ëœ ë¬¸ì„œ í‘œì‹œ
                        "search_type": "type_sample",  # ë©”íƒ€ë°ì´í„°ì—ë„ ì¶”ê°€
                        **source_meta
                    },
                    "relevance_score": 0.3,
                    "search_type": "type_sample"
                })
                self.logger.debug(f"ğŸ” [TYPE DIVERSITY] ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±: id={unique_id}, doc_type={doc_type}, chunk_id={chunk_id}")
            
            return samples
        except Exception as e:
            self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] íƒ€ì… ìƒ˜í”Œë§ ì‹¤íŒ¨ ({doc_type}): {e}")
            import traceback
            self.logger.debug(f"íƒ€ì… ìƒ˜í”Œë§ ì˜ˆì™¸ ìƒì„¸: {traceback.format_exc()}")
            return []

    def _handle_error(self, state: LegalWorkflowState, error_msg: str, context: str) -> None:
        """ì—ëŸ¬ ì²˜ë¦¬"""
        if self._handle_error_func:
            self._handle_error_func(state, error_msg, context)
        else:
            self.logger.error(f"{context}: {error_msg}")
            if isinstance(state, dict):
                if "errors" not in state:
                    state["errors"] = []
                state["errors"].append(f"{context}: {error_msg}")

    def _evaluate_search_quality(
        self,
        semantic_results: List[Dict[str, Any]],
        keyword_results: List[Dict[str, Any]],
        original_query: str
    ) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€ (0.0 ~ 1.0)
        
        Args:
            semantic_results: ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼
            keyword_results: í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
            original_query: ì›ë³¸ ì¿¼ë¦¬
            
        Returns:
            í’ˆì§ˆ ì ìˆ˜ (0.0 ~ 1.0)
        """
        try:
            # 1. ê²°ê³¼ ìˆ˜ ê¸°ë°˜ ì ìˆ˜ (0.0 ~ 0.4)
            total_results = len(semantic_results) + len(keyword_results)
            semantic_count = len(semantic_results)
            keyword_count = len(keyword_results)
            
            # ê²°ê³¼ ìˆ˜ ì ìˆ˜ ê³„ì‚°
            count_score = min(0.4, (total_results / 20.0) * 0.4)  # 20ê°œ ì´ìƒì´ë©´ 0.4ì 
            
            # 2. ê²°ê³¼ ë‹¤ì–‘ì„± ì ìˆ˜ (0.0 ~ 0.3)
            diversity_score = 0.0
            if semantic_count > 0 and keyword_count > 0:
                # ë‘ íƒ€ì… ëª¨ë‘ ìˆìœ¼ë©´ ë‹¤ì–‘ì„± ì ìˆ˜
                diversity_score = 0.3
            elif total_results > 0:
                # í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ë¶€ë¶„ ì ìˆ˜
                diversity_score = 0.15
            
            # 3. ê´€ë ¨ì„± ì ìˆ˜ (0.0 ~ 0.3) - í‰ê·  relevance_score ê¸°ë°˜
            relevance_score = 0.0
            all_scores = []
            
            for doc in semantic_results + keyword_results:
                score = doc.get("relevance_score") or doc.get("score") or 0.0
                if score > 0:
                    all_scores.append(score)
            
            if all_scores:
                avg_score = sum(all_scores) / len(all_scores)
                # í‰ê·  ì ìˆ˜ê°€ 0.7 ì´ìƒì´ë©´ ë†’ì€ ê´€ë ¨ì„±
                relevance_score = min(0.3, (avg_score / 0.7) * 0.3)
            
            # ìµœì¢… í’ˆì§ˆ ì ìˆ˜
            quality_score = count_score + diversity_score + relevance_score
            
            return min(1.0, quality_score)
            
        except Exception as e:
            self.logger.debug(f"Error evaluating search quality: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜
            total_results = len(semantic_results) + len(keyword_results)
            return min(1.0, total_results / 20.0)

    def _merge_multi_query_results_single(
        self,
        main_results: List[Dict[str, Any]],
        mq_results: List[Dict[str, Any]],
        mq_query: str,
        original_query: str,
        seen_ids: set,
        seen_hashes: set
    ) -> List[Dict[str, Any]]:
        """Multi-Query ê²°ê³¼ë¥¼ ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„±ì„ ê³ ë ¤í•˜ì—¬ ë³‘í•© (ë‹¨ì¼)
        
        Args:
            main_results: ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼
            mq_results: Multi-Query ê²€ìƒ‰ ê²°ê³¼
            mq_query: Multi-Query í…ìŠ¤íŠ¸
            original_query: ì›ë³¸ ì¿¼ë¦¬
            seen_ids: ì´ë¯¸ ë³¸ ë¬¸ì„œ ID ì§‘í•©
            seen_hashes: ì´ë¯¸ ë³¸ ì½˜í…ì¸  í•´ì‹œ ì§‘í•©
            
        Returns:
            ë³‘í•©ëœ ìƒˆë¡œìš´ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        new_results = []
        
        for doc in mq_results:
            doc_id = doc.get("id") or doc.get("doc_id")
            content = doc.get("content", "") or doc.get("text", "")
            content_hash = hash(content[:100]) if content else None
            
            # ì¤‘ë³µ ì²´í¬
            if (not doc_id or doc_id not in seen_ids) and \
               (not content_hash or content_hash not in seen_hashes):
                # Multi-query ì¶œì²˜ ì •ë³´ ì¶”ê°€
                doc["multi_query_source"] = mq_query
                doc["multi_query_boost"] = 0.9  # Multi-query ê²°ê³¼ëŠ” ì•½ê°„ ë‚®ì€ ê°€ì¤‘ì¹˜
                
                # ì›ë³¸ ì¿¼ë¦¬ì™€ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
                if content:
                    similarity = self._calculate_query_similarity(original_query, content)
                    doc["original_query_similarity"] = similarity
                    
                    # ê´€ë ¨ì„± ì ìˆ˜ ë°˜ì˜
                    base_score = doc.get("relevance_score", 0.0) or doc.get("score", 0.0)
                    if base_score > 0:
                        # ì›ë³¸ ì¿¼ë¦¬ì™€ì˜ ê´€ë ¨ì„±ì„ ê³ ë ¤í•˜ì—¬ ì ìˆ˜ ì¡°ì •
                        adjusted_score = base_score * (0.7 + 0.3 * similarity)
                        doc["relevance_score"] = adjusted_score
                        doc["score"] = adjusted_score
                
                new_results.append(doc)
        
        return new_results

    def _calculate_query_similarity(self, query1: str, text: str) -> float:
        """ì¿¼ë¦¬ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
        
        Args:
            query1: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            text: ë¹„êµí•  í…ìŠ¤íŠ¸
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        try:
            if not query1 or not text:
                return 0.0
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
            query_words = set(query1.lower().split())
            text_words = set(text.lower().split())
            
            if not query_words or not text_words:
                return 0.0
            
            # Jaccard ìœ ì‚¬ë„
            intersection = len(query_words & text_words)
            union = len(query_words | text_words)
            
            if union == 0:
                return 0.0
            
            similarity = intersection / union
            
            # ì •ê·œí™” (0.0 ~ 1.0)
            return min(1.0, similarity)
            
        except Exception as e:
            self.logger.debug(f"Error calculating query similarity: {e}")
            return 0.5  # ê¸°ë³¸ê°’

