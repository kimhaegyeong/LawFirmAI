# -*- coding: utf-8 -*-
"""
Search Execution Processor
ê²€ìƒ‰ ì‹¤í–‰ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” í”„ë¡œì„¸ì„œ
"""

import logging
import os
import time
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List, Optional, Tuple

from core.workflow.state.state_definitions import LegalWorkflowState
from core.workflow.state.state_helpers import ensure_state_group
from core.workflow.utils.workflow_constants import WorkflowConstants
from core.workflow.utils.query_diversifier import QueryDiversifier
from core.workflow.utils.search_result_balancer import SearchResultBalancer


class SearchExecutionProcessor:
    """ê²€ìƒ‰ ì‹¤í–‰ í”„ë¡œì„¸ì„œ"""

    def __init__(
        self,
        search_handler,
        logger,
        config,
        keyword_search_func=None,
        get_state_value_func=None,
        set_state_value_func=None,
        get_query_type_str_func=None,
        determine_search_parameters_func=None,
        save_metadata_safely_func=None,
        update_processing_time_func=None,
        handle_error_func=None,
        semantic_search_engine=None
    ):
        self.search_handler = search_handler
        self.logger = logger
        self.config = config
        self.keyword_search_func = keyword_search_func
        self._get_state_value_func = get_state_value_func
        self._set_state_value_func = set_state_value_func
        self._get_query_type_str_func = get_query_type_str_func
        self._determine_search_parameters_func = determine_search_parameters_func
        self._save_metadata_safely_func = save_metadata_safely_func
        self._update_processing_time_func = update_processing_time_func
        self._handle_error_func = handle_error_func
        
        # semantic_search_engine ì €ì¥ (íƒ€ì…ë³„ ê²€ìƒ‰ìš©)
        self.semantic_search_engine = semantic_search_engine
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ë‹¤ë³€í™” ë° ê²°ê³¼ ê· í˜• ì¡°ì • ìœ í‹¸ë¦¬í‹°
        self.query_diversifier = QueryDiversifier()
        self.result_balancer = SearchResultBalancer(min_per_type=1, max_per_type=5)

    def get_search_params(self, state: LegalWorkflowState) -> Dict[str, Any]:
        """ê²€ìƒ‰ì— í•„ìš”í•œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸° (State ì ‘ê·¼ ìµœì í™”)"""
        from core.workflow.state.state_helpers import get_field

        optimized_queries = self._get_state_value(state, "optimized_queries", {})
        search_params = self._get_state_value(state, "search_params", {})
        query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
        legal_field = self._get_state_value(state, "legal_field", "")
        extracted_keywords = self._get_state_value(state, "extracted_keywords", [])
        original_query = self._get_state_value(state, "query", "")

        if "search" in state and isinstance(state["search"], dict):
            search_group = state["search"]
            if "extracted_keywords" in search_group and search_group["extracted_keywords"]:
                extracted_keywords = search_group["extracted_keywords"]

            if search_group.get("optimized_queries") and isinstance(search_group["optimized_queries"], dict) and len(search_group["optimized_queries"]) > 0:
                optimized_queries = search_group["optimized_queries"]
                if not extracted_keywords and "expanded_keywords" in optimized_queries:
                    extracted_keywords = optimized_queries.get("expanded_keywords", [])

            if search_group.get("search_params") and isinstance(search_group["search_params"], dict) and len(search_group["search_params"]) > 0:
                search_params = search_group["search_params"]

        if not extracted_keywords:
            extracted_keywords_raw = get_field(state, "extracted_keywords")
            if extracted_keywords_raw and len(extracted_keywords_raw) > 0:
                extracted_keywords = extracted_keywords_raw

        if not optimized_queries or len(optimized_queries) == 0:
            optimized_queries_raw = get_field(state, "optimized_queries")
            if optimized_queries_raw and len(optimized_queries_raw) > 0:
                optimized_queries = optimized_queries_raw
                if not extracted_keywords and "expanded_keywords" in optimized_queries:
                    extracted_keywords = optimized_queries.get("expanded_keywords", [])

        if not search_params or len(search_params) == 0:
            search_params_raw = get_field(state, "search_params")
            if search_params_raw and len(search_params_raw) > 0:
                search_params = search_params_raw

        if not original_query and "input" in state and isinstance(state.get("input"), dict):
            original_query = state["input"].get("query", "")

        return {
            "optimized_queries": optimized_queries,
            "search_params": search_params,
            "query_type_str": query_type_str,
            "legal_field": legal_field,
            "extracted_keywords": extracted_keywords,
            "original_query": original_query
        }

    def execute_searches_parallel(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
        try:
            start_time = time.time()

            debug_mode = os.getenv("DEBUG_SEARCH", "false").lower() == "true"

            params = self.get_search_params(state)
            optimized_queries = params["optimized_queries"]
            search_params = params["search_params"]
            query_type_str = params["query_type_str"]
            legal_field = params["legal_field"]
            extracted_keywords = params["extracted_keywords"]
            original_query = params["original_query"]

            if not extracted_keywords or len(extracted_keywords) == 0:
                extracted_keywords = self._get_state_value(state, "extracted_keywords", [])
                if not extracted_keywords and "search" in state and isinstance(state.get("search"), dict):
                    extracted_keywords = state["search"].get("extracted_keywords", [])
                if not extracted_keywords:
                    extracted_keywords = state.get("extracted_keywords", [])
                self.logger.info(f"ğŸ” [SEARCH] extracted_keywords from batch was empty, got {len(extracted_keywords)} from state directly")
            else:
                self.logger.info(f"ğŸ” [SEARCH] extracted_keywords from batch: {len(extracted_keywords)} keywords")

            if debug_mode:
                self.logger.debug(f"execute_searches_parallel: START")
                self.logger.debug(f"  - optimized_queries: {type(optimized_queries).__name__}, exists={bool(optimized_queries)}")
                self.logger.debug(f"  - search_params: {type(search_params).__name__}, exists={bool(search_params)}")

            semantic_query_value = optimized_queries.get("semantic_query", "") if optimized_queries else ""

            if not semantic_query_value or not str(semantic_query_value).strip():
                if original_query:
                    if debug_mode:
                        self.logger.warning(f"semantic_query is empty in execute_searches_parallel, using base query: '{original_query[:50]}...'")
                    optimized_queries["semantic_query"] = original_query
                    semantic_query_value = original_query

            has_semantic_query = optimized_queries and semantic_query_value and len(str(semantic_query_value).strip()) > 0
            keyword_queries_value = optimized_queries.get("keyword_queries", []) if optimized_queries else []

            if not keyword_queries_value or len(keyword_queries_value) == 0:
                if original_query:
                    if debug_mode:
                        self.logger.warning(f"keyword_queries is empty in execute_searches_parallel, using base query")
                    optimized_queries["keyword_queries"] = [original_query]
                    keyword_queries_value = [original_query]

            has_keyword_queries = optimized_queries and keyword_queries_value and len(keyword_queries_value) > 0

            if debug_mode:
                self.logger.debug(f"  - Validation: semantic_query='{semantic_query_value[:50] if semantic_query_value else 'EMPTY'}...', has_semantic_query={has_semantic_query}")
                self.logger.debug(f"  - Validation: keyword_queries={len(keyword_queries_value) if keyword_queries_value else 0}, has_keyword_queries={has_keyword_queries}")
                self.logger.debug(f"  - Validation: search_params is None={search_params is None}, is empty={search_params == {}}, keys={list(search_params.keys()) if search_params else []}")

            if not search_params or not isinstance(search_params, dict) or len(search_params) == 0:
                self.logger.warning(f"ğŸ” [SEARCH] search_params is empty, setting default values")
                search_params = self._determine_search_parameters(
                    query_type=query_type_str,
                    query_complexity=len(original_query) if original_query else 0,
                    keyword_count=len(extracted_keywords) if extracted_keywords else 0,
                    is_retry=False
                )
                self.logger.info(f"ğŸ” [SEARCH] Default search_params set: {search_params}")

            optimized_queries_valid = optimized_queries and isinstance(optimized_queries, dict) and len(optimized_queries) > 0
            search_params_valid = search_params and isinstance(search_params, dict) and len(search_params) > 0
            self.logger.info(f"ğŸ” [SEARCH] Validation check: optimized_queries_valid={optimized_queries_valid} (type: {type(optimized_queries).__name__}, len: {len(optimized_queries) if isinstance(optimized_queries, dict) else 'N/A'}), search_params_valid={search_params_valid} (type: {type(search_params).__name__}, len: {len(search_params) if isinstance(search_params, dict) else 'N/A'}), has_semantic_query={has_semantic_query}")

            if not optimized_queries_valid or not search_params_valid or not has_semantic_query:
                self.logger.warning(f"ğŸ” [SEARCH] PARALLEL SEARCH SKIP: optimized_queries_valid={optimized_queries_valid}, search_params_valid={search_params_valid}, has_semantic_query={has_semantic_query}")
                if debug_mode:
                    self.logger.warning("Optimized queries or search params not found")
                    self.logger.debug(f"PARALLEL SEARCH SKIP: optimized_queries={optimized_queries is not None}, search_params={search_params is not None}")
                self._set_state_value(state, "semantic_results", [])
                self._set_state_value(state, "keyword_results", [])
                self._set_state_value(state, "semantic_count", 0)
                self._set_state_value(state, "keyword_count", 0)
                return state

            semantic_results = []
            semantic_count = 0
            keyword_results = []
            keyword_count = 0

            if debug_mode:
                self.logger.debug(f"PARALLEL SEARCH START: semantic_query={optimized_queries.get('semantic_query', 'N/A')[:50]}, keyword_queries={len(optimized_queries.get('keyword_queries', []))}, original_query={original_query[:50] if original_query else 'N/A'}...")

            self.logger.info(f"ğŸ” [SEARCH] Before check: extracted_keywords={len(extracted_keywords) if extracted_keywords else 0} (type: {type(extracted_keywords).__name__})")
            if not extracted_keywords or len(extracted_keywords) == 0:
                extracted_keywords = self._get_state_value(state, "extracted_keywords", [])
                if not extracted_keywords and "search" in state and isinstance(state.get("search"), dict):
                    extracted_keywords = state["search"].get("extracted_keywords", [])
                if not extracted_keywords:
                    extracted_keywords = state.get("extracted_keywords", [])
                self.logger.info(f"ğŸ” [SEARCH] Re-fetched extracted_keywords for semantic search: {len(extracted_keywords)} keywords")
            else:
                self.logger.info(f"ğŸ” [SEARCH] extracted_keywords already has {len(extracted_keywords)} keywords, skipping re-fetch")

            final_keywords = extracted_keywords if extracted_keywords else []
            self.logger.info(f"ğŸ” [SEARCH] Final extracted_keywords before ThreadPoolExecutor: {len(final_keywords)} keywords (type: {type(final_keywords).__name__}, is_empty: {not final_keywords})")

            keywords_copy = list(final_keywords) if final_keywords else []
            self.logger.info(f"ğŸ” [SEARCH] keywords_copy created: {len(keywords_copy)} keywords (type: {type(keywords_copy).__name__})")

            with ThreadPoolExecutor(max_workers=2) as executor:
                semantic_future = executor.submit(
                    self.execute_semantic_search,
                    optimized_queries,
                    search_params,
                    original_query,
                    keywords_copy
                )

                keyword_future = executor.submit(
                    self.execute_keyword_search,
                    optimized_queries,
                    search_params,
                    query_type_str,
                    legal_field,
                    extracted_keywords,
                    original_query
                )

                try:
                    semantic_results, semantic_count = semantic_future.result(timeout=20)
                    if debug_mode:
                        self.logger.debug(f"Semantic future completed: {semantic_count} results")
                except Exception as e:
                    self.logger.error(f"Semantic search failed: {e}")
                    if debug_mode:
                        self.logger.debug(f"Semantic search exception: {e}")
                    semantic_results, semantic_count = [], 0

                try:
                    keyword_results, keyword_count = keyword_future.result(timeout=20)
                    if debug_mode:
                        self.logger.debug(f"Keyword future completed: {keyword_count} results")
                except Exception as e:
                    self.logger.error(f"Keyword search failed: {e}")
                    if debug_mode:
                        self.logger.debug(f"Keyword search exception: {e}")
                    keyword_results, keyword_count = [], 0

            # ë²•ë ¹ ì¡°ë¬¸ ì§ì ‘ ê²€ìƒ‰ ì¶”ê°€ (ê°œì„  #10) - ThreadPoolExecutor ì™„ë£Œ í›„ ë³‘í•©
            direct_statute_results = []
            try:
                if original_query and query_type_str == "law_inquiry":
                    from core.agents.legal_data_connector_v2 import LegalDataConnectorV2
                    data_connector = LegalDataConnectorV2()
                    direct_statute_results = data_connector.search_statute_article_direct(original_query, limit=5)
                    if direct_statute_results:
                        self.logger.info(f"âš–ï¸ [DIRECT STATUTE] {len(direct_statute_results)}ê°œ ì¡°ë¬¸ ì§ì ‘ ê²€ìƒ‰ ì„±ê³µ")
                        # ì§ì ‘ ê²€ìƒ‰ëœ ì¡°ë¬¸ì„ keyword_results ìµœìƒìœ„ì— ì¶”ê°€ (relevance_score=1.0ì´ë¯€ë¡œ ìµœìƒìœ„ë¡œ)
                        keyword_results = direct_statute_results + keyword_results
                        keyword_count += len(direct_statute_results)
                        self.logger.info(f"âš–ï¸ [DIRECT STATUTE] keyword_resultsì— {len(direct_statute_results)}ê°œ ì¡°ë¬¸ ì¶”ê°€ ì™„ë£Œ (ì´ {keyword_count}ê°œ)")
            except Exception as e:
                self.logger.warning(f"ë²•ë ¹ ì¡°ë¬¸ ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

            # ê²€ìƒ‰ ê²°ê³¼ íƒ€ì… ê· í˜• ì¡°ì • (ê°œì„ )
            try:
                # numpy íƒ€ì… ë³€í™˜ í•¨ìˆ˜ (msgpack ì§ë ¬í™” ì˜¤ë¥˜ ë°©ì§€)
                def convert_numpy_types(obj):
                    import numpy as np
                    if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
                        return int(obj)
                    elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
                        return float(obj)
                    elif isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, dict):
                        return {k: convert_numpy_types(v) for k, v in obj.items()}
                    elif isinstance(obj, (list, tuple)):
                        return [convert_numpy_types(item) for item in obj]
                    return obj
                
                # ê²€ìƒ‰ ê²°ê³¼ì— numpy íƒ€ì… ë³€í™˜ ì ìš©
                semantic_results = [convert_numpy_types(doc) for doc in semantic_results]
                keyword_results = [convert_numpy_types(doc) for doc in keyword_results]
                
                # semantic_resultsì™€ keyword_resultsë¥¼ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
                all_results = semantic_results + keyword_results
                grouped_results = self.result_balancer.group_results_by_type(all_results)
                
                # Phase 3: íƒ€ì…ë³„ ë¶„í¬ í™•ì¸ ë° ê²½ê³ 
                type_distribution = {}
                for doc_type, docs in grouped_results.items():
                    count = len(docs)
                    type_distribution[doc_type] = count
                    self.logger.info(f"ğŸ“Š [SEARCH BALANCE] {doc_type}: {count}ê°œ")
                
                # ë‹¨ì¼ íƒ€ì…ë§Œ ê²€ìƒ‰ëœ ê²½ìš° ê²½ê³ 
                non_zero_types = [t for t, c in type_distribution.items() if c > 0]
                if len(non_zero_types) == 1:
                    single_type = non_zero_types[0]
                    self.logger.warning(
                        f"âš ï¸ [TYPE DIVERSITY] ë‹¨ì¼ íƒ€ì…ë§Œ ê²€ìƒ‰ë¨: {single_type} ({type_distribution[single_type]}ê°œ). "
                        f"ë‹¤ë¥¸ íƒ€ì…ì˜ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°ì´í„° ë¶ˆê· í˜• ë˜ëŠ” ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )
                elif len(non_zero_types) == 0:
                    self.logger.warning(
                        f"âš ï¸ [TYPE DIVERSITY] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¿¼ë¦¬ë‚˜ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”."
                    )
                else:
                    # íƒ€ì… ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
                    total_docs = sum(type_distribution.values())
                    if total_docs > 0:
                        # ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë‹¤ì–‘ì„± ì ìˆ˜
                        import math
                        entropy = 0.0
                        for count in type_distribution.values():
                            if count > 0:
                                p = count / total_docs
                                entropy -= p * math.log2(p)
                        max_entropy = math.log2(len(non_zero_types)) if len(non_zero_types) > 1 else 1.0
                        diversity_score = entropy / max_entropy if max_entropy > 0 else 0.0
                        
                        self.logger.info(
                            f"âœ… [TYPE DIVERSITY] íƒ€ì… ë‹¤ì–‘ì„± ì ìˆ˜: {diversity_score:.2f} "
                            f"(ê²€ìƒ‰ëœ íƒ€ì…: {len(non_zero_types)}ê°œ, ì´ ë¬¸ì„œ: {total_docs}ê°œ)"
                        )
                        
                        if diversity_score < 0.5:
                            self.logger.warning(
                                f"âš ï¸ [TYPE DIVERSITY] íƒ€ì… ë‹¤ì–‘ì„±ì´ ë‚®ìŠµë‹ˆë‹¤ (ì ìˆ˜: {diversity_score:.2f}). "
                                f"ê²€ìƒ‰ ì¿¼ë¦¬ ë‹¤ë³€í™” ë˜ëŠ” ë°ì´í„° ê· í˜• ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”."
                            )
                
                # ê· í˜• ì¡°ì •ëœ ê²°ê³¼ ìƒì„±
                semantic_k = search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K)
                keyword_k = search_params.get("keyword_k", WorkflowConstants.KEYWORD_SEARCH_K)
                balanced_results = self.result_balancer.balance_search_results(
                    grouped_results,
                    total_limit=semantic_k + keyword_k
                )
                
                # ê· í˜• ì¡°ì •ëœ ê²°ê³¼ë¥¼ semantic_resultsì™€ keyword_resultsë¡œ ì¬ë¶„ë°°
                # (ê¸°ì¡´ ë¡œì§ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ë˜, ê· í˜• ì¡°ì •ëœ ê²°ê³¼ë¥¼ ìš°ì„  ì‚¬ìš©)
                if balanced_results:
                    # semantic_resultsì™€ keyword_resultsë¥¼ ê· í˜• ì¡°ì •ëœ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
                    # ê´€ë ¨ë„ê°€ ë†’ì€ ê²°ê³¼ë¥¼ semantic_resultsì—, ë‚˜ë¨¸ì§€ë¥¼ keyword_resultsì— ë°°ì¹˜
                    semantic_results_balanced = [
                        doc for doc in balanced_results 
                        if doc.get("relevance_score", 0.0) >= 0.5
                    ]
                    keyword_results_balanced = [
                        doc for doc in balanced_results 
                        if doc.get("relevance_score", 0.0) < 0.5 or doc not in semantic_results_balanced
                    ]
                    
                    # ê¸°ì¡´ ê²°ê³¼ì™€ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                    existing_semantic_ids = {id(doc) for doc in semantic_results}
                    existing_keyword_ids = {id(doc) for doc in keyword_results}
                    
                    semantic_results = semantic_results + [
                        doc for doc in semantic_results_balanced 
                        if id(doc) not in existing_semantic_ids
                    ]
                    keyword_results = keyword_results + [
                        doc for doc in keyword_results_balanced 
                        if id(doc) not in existing_keyword_ids and id(doc) not in existing_semantic_ids
                    ]
                    
                    semantic_count = len(semantic_results)
                    keyword_count = len(keyword_results)
                    
                    self.logger.info(
                        f"âœ… [SEARCH BALANCE] ê· í˜• ì¡°ì • ì™„ë£Œ: "
                        f"semantic={semantic_count}, keyword={keyword_count}, "
                        f"íƒ€ì…ë³„ ë¶„í¬={dict((k, len(v)) for k, v in grouped_results.items())}"
                    )
            except Exception as e:
                self.logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ê· í˜• ì¡°ì • ì‹¤íŒ¨ (ê¸°ì¡´ ê²°ê³¼ ì‚¬ìš©): {e}")

            ensure_state_group(state, "search")

            if debug_mode:
                self.logger.debug(f"PARALLEL SEARCH: Before save - semantic_results={len(semantic_results)}, keyword_results={len(keyword_results)}")

            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "semantic_count", semantic_count)
            self._set_state_value(state, "keyword_count", keyword_count)

            if debug_mode:
                stored_semantic = self._get_state_value(state, "semantic_results", [])
                stored_keyword = self._get_state_value(state, "keyword_results", [])
                self.logger.debug(f"PARALLEL SEARCH: After save - semantic_results={len(stored_semantic)}, keyword_results={len(stored_keyword)}")

                if "search" in state and isinstance(state.get("search"), dict):
                    direct_semantic = state["search"].get("semantic_results", [])
                    direct_keyword = state["search"].get("keyword_results", [])
                    self.logger.debug(f"PARALLEL SEARCH: Direct state['search'] check - semantic={len(direct_semantic)}, keyword={len(direct_keyword)}")
                else:
                    self.logger.debug(f"PARALLEL SEARCH: state['search'] not found or not dict, state keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")

            self._save_metadata_safely(state, "_last_executed_node", "execute_searches_parallel")
            self._update_processing_time(state, start_time)

            elapsed_time = time.time() - start_time

            self.logger.info(
                f"âœ… [PARALLEL SEARCH] Completed in {elapsed_time:.3f}s - "
                f"Semantic: {semantic_count} results, Keyword: {keyword_count} results"
            )

            if debug_mode:
                self.logger.debug(f"PARALLEL SEARCH: Semantic={semantic_count}, Keyword={keyword_count}")

                if semantic_results:
                    semantic_scores = [doc.get("relevance_score", 0.0) for doc in semantic_results[:5]]
                    self.logger.info(
                        f"ğŸ” [DEBUG] Semantic search details: "
                        f"Top scores: {semantic_scores}, "
                        f"Sample sources: {[doc.get('source', 'Unknown')[:30] for doc in semantic_results[:3]]}"
                    )
                else:
                    self.logger.warning("âš ï¸ [DEBUG] Semantic search returned 0 results")

                if keyword_results:
                    keyword_scores = [doc.get("relevance_score", doc.get("score", 0.0)) for doc in keyword_results[:5]]
                    self.logger.info(
                        f"ğŸ” [DEBUG] Keyword search details: "
                        f"Top scores: {keyword_scores}, "
                        f"Sample sources: {[doc.get('source', 'Unknown')[:30] for doc in keyword_results[:3]]}"
                    )
                else:
                    self.logger.warning("âš ï¸ [DEBUG] Keyword search returned 0 results")

        except Exception as e:
            self.logger.error(f"Error in parallel search: {e}", exc_info=True)
            return self.fallback_sequential_search(state)

        debug_mode = os.getenv("DEBUG_SEARCH", "false").lower() == "true"

        if debug_mode:
            if "search" in state and isinstance(state.get("search"), dict):
                final_search = state["search"]
                final_semantic = len(final_search.get("semantic_results", []))
                final_keyword = len(final_search.get("keyword_results", []))
                self.logger.debug(f"[DEBUG] execute_searches_parallel: Returning state with search group - semantic_results={final_semantic}, keyword_results={final_keyword}")
                self.logger.debug(f"[DEBUG] execute_searches_parallel: Returning state keys={list(state.keys()) if isinstance(state, dict) else 'N/A'}")
            else:
                self.logger.debug(f"[DEBUG] execute_searches_parallel: WARNING - Returning state WITHOUT search group!")
                self.logger.debug(f"[DEBUG] execute_searches_parallel: Returning state keys={list(state.keys()) if isinstance(state, dict) else 'N/A'}")

        return state

    def execute_semantic_search(
        self,
        optimized_queries: Dict[str, Any],
        search_params: Dict[str, Any],
        original_query: str = "",
        extracted_keywords: Optional[List[str]] = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤í–‰"""
        semantic_results = []
        semantic_count = 0

        semantic_query = optimized_queries.get("semantic_query", "")
        semantic_k = search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K)

        if extracted_keywords is None:
            extracted_keywords = []

        self.logger.info(
            f"ğŸ” [DEBUG] _execute_semantic_search_internal received: extracted_keywords={len(extracted_keywords)} (type: {type(extracted_keywords).__name__}), query='{semantic_query[:50]}...', k={semantic_k}"
        )

        self.logger.info(
            f"ğŸ” [DEBUG] Executing semantic search: query='{semantic_query[:50]}...', k={semantic_k}, original_query='{original_query[:50] if original_query else 'N/A'}...', extracted_keywords={len(extracted_keywords)}"
        )

        enhanced_semantic_query = semantic_query
        if extracted_keywords and len(extracted_keywords) > 0:
            core_keywords = []
            for kw in extracted_keywords[:5]:
                if isinstance(kw, str):
                    if any(term in kw for term in ["ë²•", "ì¡°", "ì œ", "ë¯¼ë²•", "í˜•ë²•", "ìƒë²•", "ì„ëŒ€ì°¨", "ê³„ì•½"]):
                        core_keywords.insert(0, kw)
                    else:
                        core_keywords.append(kw)

            if core_keywords:
                query_keywords = set(semantic_query.split())
                new_keywords = [kw for kw in core_keywords if kw not in query_keywords]
                if new_keywords:
                    enhanced_semantic_query = f"{semantic_query} {' '.join(new_keywords[:3])}"
                    self.logger.info(f"ğŸ” [SEMANTIC SEARCH] Enhanced semantic query with keywords: '{enhanced_semantic_query[:100]}...'")

        main_semantic, main_count = self.search_handler.semantic_search(
            enhanced_semantic_query,
            k=semantic_k,
            extracted_keywords=extracted_keywords
        )
        semantic_results.extend(main_semantic)
        semantic_count += main_count

        self.logger.info(
            f"ğŸ” [DEBUG] Main semantic search: {main_count} results (query: '{enhanced_semantic_query[:50]}...')"
        )

        if original_query and original_query.strip():
            enhanced_original_query = original_query
            if extracted_keywords and len(extracted_keywords) > 0:
                core_keywords = [str(kw) for kw in extracted_keywords[:3] if isinstance(kw, str)]
                if core_keywords:
                    query_keywords = set(original_query.split())
                    new_keywords = [kw for kw in core_keywords if kw not in query_keywords]
                    if new_keywords:
                        enhanced_original_query = f"{original_query} {' '.join(new_keywords[:2])}"

            original_semantic, original_count = self.search_handler.semantic_search(
                enhanced_original_query,
                k=semantic_k // 2,
                extracted_keywords=extracted_keywords
            )
            semantic_results.extend(original_semantic)
            semantic_count += original_count
            self.logger.info(
                f"ğŸ” [DEBUG] Original query semantic search: {original_count} results (query: '{enhanced_original_query[:50]}...')"
            )
            print(f"[DEBUG] _execute_semantic_search_internal: Added {original_count} results from original query search")

        keyword_queries = optimized_queries.get("keyword_queries", [])[:2]
        for i, kw_query in enumerate(keyword_queries, 1):
            if kw_query and kw_query.strip() and kw_query != semantic_query:
                kw_semantic, kw_count = self.search_handler.semantic_search(
                    kw_query,
                    k=semantic_k // 2,
                    extracted_keywords=extracted_keywords
                )
                semantic_results.extend(kw_semantic)
                semantic_count += kw_count
                self.logger.info(
                    f"ğŸ” [DEBUG] Keyword-based semantic search #{i}: {kw_count} results (query: '{kw_query[:50]}...')"
                )
                print(f"[DEBUG] _execute_semantic_search_internal: Added {kw_count} results from keyword query #{i}")

        # Phase 1 + Phase 2: íƒ€ì…ë³„ ë³„ë„ ê²€ìƒ‰ ìˆ˜í–‰ ë° ì¿¼ë¦¬ ë‹¤ë³€í™” ì ìš© (íƒ€ì… ë‹¤ì–‘ì„± ê°œì„ )
        document_types = {
            "statute_article": "statute",
            "case_paragraph": "case",
            "decision_paragraph": "decision",
            "interpretation_paragraph": "interpretation"
        }
        type_specific_results = {}
        type_specific_count = 0
        
        # semantic_search_engine í™•ì¸ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        semantic_engine = None
        if self.semantic_search_engine:
            semantic_engine = self.semantic_search_engine
            self.logger.info(f"ğŸ” [TYPE DIVERSITY] semantic_search_engine from self: {type(semantic_engine).__name__}")
        elif hasattr(self.search_handler, 'semantic_search_engine') and self.search_handler.semantic_search_engine:
            semantic_engine = self.search_handler.semantic_search_engine
            self.logger.info(f"ğŸ” [TYPE DIVERSITY] semantic_search_engine from search_handler: {type(semantic_engine).__name__}")
        elif hasattr(self.search_handler, 'semantic_search') and self.search_handler.semantic_search:
            semantic_engine = self.search_handler.semantic_search
            self.logger.info(f"ğŸ” [TYPE DIVERSITY] semantic_search_engine from search_handler.semantic_search: {type(semantic_engine).__name__}")
        else:
            self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] semantic_search_engine not found: self.semantic_search_engine={self.semantic_search_engine}, search_handler.semantic_search_engine={getattr(self.search_handler, 'semantic_search_engine', 'N/A')}, search_handler.semantic_search={getattr(self.search_handler, 'semantic_search', 'N/A')}")
        
        if semantic_engine:
            # Phase 2: QueryDiversifierë¡œ íƒ€ì…ë³„ ì¿¼ë¦¬ ìƒì„±
            try:
                diversified_queries = self.query_diversifier.diversify_search_queries(original_query or enhanced_semantic_query)
                self.logger.info(
                    f"ğŸ” [TYPE DIVERSITY] ë‹¤ë³€í™”ëœ ì¿¼ë¦¬ ìƒì„±: "
                    f"statute={len(diversified_queries.get('statute', []))}, "
                    f"case={len(diversified_queries.get('case', []))}, "
                    f"decision={len(diversified_queries.get('decision', []))}, "
                    f"interpretation={len(diversified_queries.get('interpretation', []))}"
                )
            except Exception as e:
                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] ì¿¼ë¦¬ ë‹¤ë³€í™” ì‹¤íŒ¨: {e}")
                diversified_queries = {}
            
            self.logger.info("ğŸ” [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì‹œì‘")
            for doc_type, query_type in document_types.items():
                try:
                    # Phase 2: íƒ€ì…ë³„ ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©
                    type_queries = diversified_queries.get(query_type, [])
                    search_query = enhanced_semantic_query  # ê¸°ë³¸ ì¿¼ë¦¬
                    
                    # íƒ€ì…ë³„ ìµœì í™”ëœ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                    if type_queries:
                        search_query = type_queries[0]  # ì²« ë²ˆì§¸ ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©: '{search_query[:50]}...'")
                    else:
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: '{search_query[:50]}...'")
                    
                    # ê° íƒ€ì…ë³„ë¡œ ë³„ë„ ì˜ë¯¸ì  ê²€ìƒ‰ ìˆ˜í–‰ (ì¬ì‹œë„ ë¡œì§ í™œìš©)
                    self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type} ê²€ìƒ‰ ì‹œì‘ (k=20, threshold=0.05, source_types=[{doc_type}])")
                    type_results = semantic_engine.search(
                        search_query,
                        k=20,  # ë” ë§ì€ ê²°ê³¼ ê²€ìƒ‰
                        source_types=[doc_type],  # íƒ€ì…ë³„ í•„í„° ì ìš©
                        similarity_threshold=0.05,  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì‹œì‘ (ì¬ì‹œë„ ë¡œì§ì´ ë” ë‚®ì¶¤)
                        min_results=1,  # ìµœì†Œ 1ê°œëŠ” ë³´ì¥
                        disable_retry=False  # ì¬ì‹œë„ ë¡œì§ í™œì„±í™” (ì„ê³„ê°’ ìë™ ì¡°ì •)
                    )
                    
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë” ì¼ë°˜ì ì¸ ì¿¼ë¦¬ë¡œ ì¬ì‹œë„
                    if not type_results:
                        # ì›ë³¸ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì—¬ ì¬ì‹œë„
                        core_keywords = original_query.split()[:3] if original_query else search_query.split()[:3]
                        fallback_query = " ".join(core_keywords)
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: í´ë°± ì¿¼ë¦¬ë¡œ ì¬ì‹œë„: '{fallback_query}'")
                        try:
                            type_results = semantic_engine.search(
                                fallback_query,
                                k=20,
                                source_types=[doc_type],
                                similarity_threshold=0.0,  # ìµœì†Œ ì„ê³„ê°’
                                min_results=1,
                                disable_retry=False  # ì¬ì‹œë„ ë¡œì§ í™œì„±í™”
                            )
                            if type_results:
                                self.logger.info(f"âœ… [TYPE DIVERSITY] {doc_type}: í´ë°± ì¿¼ë¦¬ë¡œ {len(type_results)}ê°œ ê²€ìƒ‰ ì„±ê³µ")
                        except Exception as e:
                            self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type} í´ë°± ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    
                    # ê²°ê³¼ê°€ ì—¬ì „íˆ ì—†ìœ¼ë©´ ë§¤ìš° ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„
                    if not type_results:
                        # íƒ€ì…ë³„ ì¼ë°˜ í‚¤ì›Œë“œ ì‚¬ìš©
                        type_keywords = {
                            "statute_article": "ë²•ë ¹ ì¡°ë¬¸",
                            "case_paragraph": "íŒë¡€",
                            "decision_paragraph": "ê²°ì •ë¡€",
                            "interpretation_paragraph": "í•´ì„ë¡€"
                        }
                        general_query = type_keywords.get(doc_type, "ë²•ë¥ ")
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ì¼ë°˜ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„: '{general_query}'")
                        try:
                            type_results = semantic_engine.search(
                                general_query,
                                k=5,  # ìµœì†Œí•œ 5ê°œë§Œ
                                source_types=[doc_type],
                                similarity_threshold=0.0,
                                min_results=1,
                                disable_retry=False
                            )
                            if type_results:
                                self.logger.info(f"âœ… [TYPE DIVERSITY] {doc_type}: ì¼ë°˜ í‚¤ì›Œë“œë¡œ {len(type_results)}ê°œ ê²€ìƒ‰ ì„±ê³µ")
                        except Exception as e:
                            self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type} ì¼ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    
                    self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ìµœì¢… {len(type_results)}ê°œ ê²€ìƒ‰ë¨")
                    
                    # ìµœì¢… ë°©ì•ˆ: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ íƒ€ì…ë³„ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´
                    if not type_results:
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´ ì‹œë„")
                        try:
                            type_results = self._get_type_sample(semantic_engine, doc_type, k=2)
                            if type_results:
                                self.logger.info(f"âœ… [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ìœ¼ë¡œ {len(type_results)}ê°œ ê°€ì ¸ì˜´")
                        except Exception as e:
                            self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type} ìƒ˜í”Œë§ ì‹¤íŒ¨: {e}")
                    
                    if type_results:
                        type_specific_results[doc_type] = type_results
                        semantic_results.extend(type_results)
                        type_specific_count += len(type_results)
                        self.logger.info(
                            f"âœ… [TYPE DIVERSITY] {doc_type}: {len(type_results)}ê°œ ê²€ìƒ‰ ì„±ê³µ (ì¿¼ë¦¬: '{search_query[:30]}...')"
                        )
                    else:
                        self.logger.warning(
                            f"âš ï¸ [TYPE DIVERSITY] {doc_type}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë°ì´í„° ì—†ìŒ ë˜ëŠ” ì¿¼ë¦¬ ê´€ë ¨ì„± ë‚®ìŒ, ì¿¼ë¦¬: '{search_query[:30]}...')"
                        )
                except Exception as e:
                    self.logger.error(f"âŒ [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì‹¤íŒ¨ ({doc_type}): {e}")
                    import traceback
                    self.logger.debug(f"íƒ€ì…ë³„ ê²€ìƒ‰ ì˜ˆì™¸ ìƒì„¸: {traceback.format_exc()}")
        else:
            self.logger.warning("âš ï¸ [TYPE DIVERSITY] semantic_search_engineì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íƒ€ì…ë³„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        semantic_count += type_specific_count
        
        if type_specific_count > 0:
            self.logger.info(
                f"âœ… [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì™„ë£Œ: ì´ {type_specific_count}ê°œ ì¶”ê°€ "
                f"(íƒ€ì…ë³„ ë¶„í¬: {dict((k, len(v)) for k, v in type_specific_results.items())})"
            )
        else:
            self.logger.info("âš ï¸ [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë°ì´í„° ë¶ˆê· í˜• ë˜ëŠ” ê²€ìƒ‰ ì‹¤íŒ¨)")

        self.logger.info(
            f"ğŸ” [DEBUG] Total semantic search results: {semantic_count} (unique: {len(semantic_results)})"
        )
        print(f"[DEBUG] SEMANTIC SEARCH INTERNAL: Total={semantic_count}, Unique={len(semantic_results)}")

        search_queries_used = []
        if semantic_query:
            search_queries_used.append(f"semantic_query({len(semantic_query)} chars)")
        if original_query:
            search_queries_used.append(f"original_query({len(original_query)} chars)")
        keyword_queries_used = optimized_queries.get("keyword_queries", [])[:2]
        if keyword_queries_used:
            search_queries_used.append(f"keyword_queries({len(keyword_queries_used)} queries)")
        print(f"[DEBUG] SEMANTIC SEARCH INTERNAL: Queries used: {', '.join(search_queries_used)}")

        return semantic_results, semantic_count

    def execute_keyword_search(
        self,
        optimized_queries: Dict[str, Any],
        search_params: Dict[str, Any],
        query_type_str: str,
        legal_field: str,
        extracted_keywords: List[str],
        original_query: str = ""
    ) -> Tuple[List[Dict[str, Any]], int]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰"""
        keyword_results = []
        keyword_count = 0

        keyword_queries = optimized_queries.get("keyword_queries", [])
        keyword_limit = search_params.get("keyword_limit", WorkflowConstants.CATEGORY_SEARCH_LIMIT)

        self.logger.info(
            f"ğŸ” [DEBUG] Executing keyword search: {len(keyword_queries)} queries, "
            f"limit={keyword_limit}, field={legal_field}, "
            f"keywords={extracted_keywords[:5] if extracted_keywords else []}, "
            f"original_query='{original_query[:50] if original_query else 'N/A'}...'"
        )

        if original_query and original_query.strip():
            if self.keyword_search_func:
                original_kw_results, original_kw_count = self.keyword_search_func(
                    query=original_query,
                    query_type_str=query_type_str,
                    limit=keyword_limit,
                    legal_field=legal_field,
                    extracted_keywords=extracted_keywords
                )
            else:
                original_kw_results, original_kw_count = [], 0
            keyword_results.extend(original_kw_results)
            keyword_count += original_kw_count
            self.logger.info(
                f"ğŸ” [DEBUG] Original query keyword search: {original_kw_count} results (query: '{original_query[:50]}...')"
            )

        for i, kw_query in enumerate(keyword_queries, 1):
            if kw_query and kw_query.strip() and kw_query != original_query:
                if self.keyword_search_func:
                    kw_results, kw_count = self.keyword_search_func(
                        query=kw_query,
                        query_type_str=query_type_str,
                        limit=keyword_limit,
                        legal_field=legal_field,
                        extracted_keywords=extracted_keywords
                    )
                else:
                    kw_results, kw_count = [], 0
                keyword_results.extend(kw_results)
                keyword_count += kw_count
                self.logger.info(
                    f"ğŸ” [DEBUG] Keyword search #{i}: {kw_count} results (query: '{kw_query[:50]}...')"
                )

        self.logger.info(
            f"ğŸ” [DEBUG] Total keyword search results: {keyword_count} (unique: {len(keyword_results)})"
        )
        print(f"[DEBUG] KEYWORD SEARCH INTERNAL: Total={keyword_count}, Unique={len(keyword_results)}")

        return keyword_results, keyword_count

    def fallback_sequential_search(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ìˆœì°¨ ê²€ìƒ‰ ì‹¤í–‰ (í´ë°±)"""
        try:
            self.logger.warning("Falling back to sequential search")

            optimized_queries = self._get_state_value(state, "optimized_queries", {})
            search_params = self._get_state_value(state, "search_params", {})
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            legal_field = self._get_state_value(state, "legal_field", "")
            extracted_keywords = optimized_queries.get("expanded_keywords", [])

            original_query = self._get_state_value(state, "query", "")
            if not original_query and "input" in state and isinstance(state.get("input"), dict):
                original_query = state["input"].get("query", "")

            extracted_keywords_for_search = self._get_state_value(state, "extracted_keywords", [])
            semantic_results, semantic_count = self.execute_semantic_search(
                optimized_queries, search_params, original_query, extracted_keywords_for_search
            )

            keyword_results, keyword_count = self.execute_keyword_search(
                optimized_queries, search_params, query_type_str, legal_field, extracted_keywords, original_query
            )

            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "semantic_count", semantic_count)
            self._set_state_value(state, "keyword_count", keyword_count)

            self.logger.info(f"Sequential search completed: {semantic_count} semantic, {keyword_count} keyword")

        except Exception as e:
            self.logger.error(f"Error in sequential search: {e}", exc_info=True)
            self._handle_error(state, str(e), "ìˆœì°¨ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

        return state

    def _get_state_value(self, state: LegalWorkflowState, key: str, default: Any = None) -> Any:
        """Stateì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
        if self._get_state_value_func:
            return self._get_state_value_func(state, key, default)
        if isinstance(state, dict):
            if key in state:
                return state[key]
            if "search" in state and isinstance(state.get("search"), dict) and key in state["search"]:
                return state["search"][key]
        return default

    def _set_state_value(self, state: LegalWorkflowState, key: str, value: Any) -> None:
        """Stateì— ê°’ ì„¤ì •"""
        if self._set_state_value_func:
            self._set_state_value_func(state, key, value)
        elif isinstance(state, dict):
            if "search" not in state or not isinstance(state.get("search"), dict):
                state["search"] = {}
            state["search"][key] = value

    def _get_query_type_str(self, query_type) -> str:
        """QueryTypeì„ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if self._get_query_type_str_func:
            return self._get_query_type_str_func(query_type)
        if isinstance(query_type, str):
            return query_type
        if hasattr(query_type, 'value'):
            return query_type.value
        return str(query_type) if query_type else ""

    def _determine_search_parameters(
        self,
        query_type: str,
        query_complexity: int,
        keyword_count: int,
        is_retry: bool
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ê²°ì •"""
        if self._determine_search_parameters_func:
            return self._determine_search_parameters_func(query_type, query_complexity, keyword_count, is_retry)
        return {
            "semantic_k": WorkflowConstants.SEMANTIC_SEARCH_K,
            "keyword_limit": WorkflowConstants.CATEGORY_SEARCH_LIMIT,
            "min_relevance": self.config.similarity_threshold if hasattr(self.config, 'similarity_threshold') else 0.5,
            "max_results": WorkflowConstants.MAX_DOCUMENTS
        }

    def _save_metadata_safely(self, state: LegalWorkflowState, key: str, value: Any) -> None:
        """ë©”íƒ€ë°ì´í„° ì•ˆì „í•˜ê²Œ ì €ì¥"""
        if self._save_metadata_safely_func:
            self._save_metadata_safely_func(state, key, value)
        elif isinstance(state, dict):
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            state["metadata"][key] = value

    def _update_processing_time(self, state: LegalWorkflowState, start_time: float) -> None:
        """ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        if self._update_processing_time_func:
            self._update_processing_time_func(state, start_time)
        elif isinstance(state, dict):
            elapsed = time.time() - start_time
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            if "processing_time" not in state["metadata"]:
                state["metadata"]["processing_time"] = 0.0
            state["metadata"]["processing_time"] += elapsed

    def _get_type_sample(self, semantic_engine, doc_type: str, k: int = 2) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • íƒ€ì…ì˜ ëœë¤ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸° (ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        
        Args:
            semantic_engine: SemanticSearchEngineV2 ì¸ìŠ¤í„´ìŠ¤
            doc_type: ë¬¸ì„œ íƒ€ì…
            k: ê°€ì ¸ì˜¬ ìƒ˜í”Œ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ìƒ˜í”Œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # DBì—ì„œ í•´ë‹¹ íƒ€ì…ì˜ ëœë¤ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ì„±ëŠ¥ ìµœì í™”: ì¸ë±ìŠ¤ í™œìš©)
            conn = semantic_engine._get_connection()
            
            # ë¨¼ì € í•´ë‹¹ íƒ€ì…ì˜ ë¬¸ì„œ ìˆ˜ í™•ì¸
            count_cursor = conn.execute(
                "SELECT COUNT(*) as count FROM text_chunks WHERE source_type = ? AND text IS NOT NULL AND LENGTH(text) > 50",
                (doc_type,)
            )
            count_row = count_cursor.fetchone()
            total_count = count_row['count'] if count_row else 0
            
            if total_count == 0:
                self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§í•  ë¬¸ì„œ ì—†ìŒ (ì´ 0ê°œ)")
                return []
            
            # ëœë¤ ìƒ˜í”Œë§ (ì„±ëŠ¥ ìµœì í™”: LIMIT ì‚¬ìš©)
            cursor = conn.execute(
                """
                SELECT id, text, source_id, source_type
                FROM text_chunks
                WHERE source_type = ? AND text IS NOT NULL AND LENGTH(text) > 50
                ORDER BY RANDOM()
                LIMIT ?
                """,
                (doc_type, k)
            )
            rows = cursor.fetchall()
            
            if not rows:
                self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ ê²°ê³¼ ì—†ìŒ (ì´ {total_count}ê°œ ì¤‘)")
                return []
            
            self.logger.info(f"âœ… [TYPE DIVERSITY] {doc_type}: {len(rows)}ê°œ ìƒ˜í”Œë§ ì„±ê³µ (ì´ {total_count}ê°œ ì¤‘)")
            
            # ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            samples = []
            for row in rows:
                chunk_id = row['id']
                text = row['text'] or ""
                source_id = row['source_id']
                
                # ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)
                source_meta = {}
                try:
                    if hasattr(semantic_engine, '_get_source_metadata'):
                        source_meta = semantic_engine._get_source_metadata(conn, doc_type, source_id)
                        if not source_meta:
                            # ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ ì‹œ text_chunksì—ì„œ ê¸°ë³¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                            cursor_meta = conn.execute(
                                "SELECT source_type, source_id, text FROM text_chunks WHERE id = ?",
                                (chunk_id,)
                            )
                            row_meta = cursor_meta.fetchone()
                            if row_meta:
                                source_meta = {
                                    "source_type": row_meta['source_type'],
                                    "source_id": row_meta['source_id'],
                                    "text": row_meta['text']
                                }
                except Exception as e:
                    self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ ({doc_type}, source_id={source_id}): {e}")
                    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
                    source_meta = {
                        "source_type": doc_type,
                        "source_id": source_id
                    }
                
                # UnifiedSourceFormatterë¡œ ì¶œì²˜ ì •ë³´ ìƒì„± (ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê°œì„ )
                try:
                    from core.generation.formatters.unified_source_formatter import UnifiedSourceFormatter
                    formatter = UnifiedSourceFormatter()
                    source_info = formatter.format_source(doc_type, source_meta)
                    source_name = source_info.name
                    source_url = source_info.url
                    
                    # source_nameì´ ë¹„ì–´ìˆê±°ë‚˜ ê¸°ë³¸ê°’ì´ë©´ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ ì‹œë„
                    if not source_name or source_name == doc_type:
                        if doc_type == "statute_article":
                            source_name = source_meta.get("statute_name") or source_meta.get("name") or "ë²•ë ¹ ì¡°ë¬¸"
                        elif doc_type == "case_paragraph":
                            source_name = source_meta.get("casenames") or source_meta.get("doc_id") or "íŒë¡€"
                        elif doc_type == "decision_paragraph":
                            source_name = f"{source_meta.get('org', '')} {source_meta.get('doc_id', '')}".strip() or "ê²°ì •ë¡€"
                        elif doc_type == "interpretation_paragraph":
                            source_name = f"{source_meta.get('org', '')} {source_meta.get('title', '')}".strip() or "í•´ì„ë¡€"
                except Exception as e:
                    self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] ì¶œì²˜ ì •ë³´ ìƒì„± ì‹¤íŒ¨ ({doc_type}): {e}")
                    source_name = doc_type
                    source_url = ""
                
                samples.append({
                    "id": f"chunk_{chunk_id}",
                    "text": text,
                    "content": text,
                    "score": 0.3,  # ë‚®ì€ ì ìˆ˜ (ê°•ì œ ìƒ˜í”Œë§)
                    "similarity": 0.3,
                    "type": doc_type,
                    "source_type": doc_type,
                    "source": source_name,
                    "source_url": source_url,
                    "source_id": source_id,
                    "metadata": {
                        "chunk_id": chunk_id,
                        "source_type": doc_type,
                        "source_id": source_id,
                        "text": text,
                        "is_sample": True,  # ìƒ˜í”Œë§ëœ ë¬¸ì„œ í‘œì‹œ
                        **source_meta
                    },
                    "relevance_score": 0.3,
                    "search_type": "type_sample"
                })
            
            return samples
        except Exception as e:
            self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] íƒ€ì… ìƒ˜í”Œë§ ì‹¤íŒ¨ ({doc_type}): {e}")
            import traceback
            self.logger.debug(f"íƒ€ì… ìƒ˜í”Œë§ ì˜ˆì™¸ ìƒì„¸: {traceback.format_exc()}")
            return []

    def _handle_error(self, state: LegalWorkflowState, error_msg: str, context: str) -> None:
        """ì—ëŸ¬ ì²˜ë¦¬"""
        if self._handle_error_func:
            self._handle_error_func(state, error_msg, context)
        else:
            self.logger.error(f"{context}: {error_msg}")
            if isinstance(state, dict):
                if "errors" not in state:
                    state["errors"] = []
                state["errors"].append(f"{context}: {error_msg}")

