# -*- coding: utf-8 -*-
"""
Search Execution Processor
ê²€ìƒ‰ ì‹¤í–‰ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” í”„ë¡œì„¸ì„œ
"""

import logging
import os
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List, Optional, Tuple

from core.workflow.state.state_definitions import LegalWorkflowState
from core.workflow.state.state_helpers import ensure_state_group, get_retrieved_docs, set_retrieved_docs
from core.workflow.utils.workflow_constants import WorkflowConstants
from core.workflow.utils.query_diversifier import QueryDiversifier
from core.workflow.utils.search_result_balancer import SearchResultBalancer


class SearchExecutionProcessor:
    """ê²€ìƒ‰ ì‹¤í–‰ í”„ë¡œì„¸ì„œ"""

    def __init__(
        self,
        search_handler,
        logger,
        config,
        keyword_search_func=None,
        get_state_value_func=None,
        set_state_value_func=None,
        get_query_type_str_func=None,
        determine_search_parameters_func=None,
        save_metadata_safely_func=None,
        update_processing_time_func=None,
        handle_error_func=None,
        semantic_search_engine=None
    ):
        self.search_handler = search_handler
        self.logger = logger
        self.config = config
        self.keyword_search_func = keyword_search_func
        self._get_state_value_func = get_state_value_func
        self._set_state_value_func = set_state_value_func
        self._get_query_type_str_func = get_query_type_str_func
        self._determine_search_parameters_func = determine_search_parameters_func
        self._save_metadata_safely_func = save_metadata_safely_func
        self._update_processing_time_func = update_processing_time_func
        self._handle_error_func = handle_error_func
        
        # semantic_search_engine ì €ì¥ (íƒ€ì…ë³„ ê²€ìƒ‰ìš©)
        self.semantic_search_engine = semantic_search_engine
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ë‹¤ë³€í™” ë° ê²°ê³¼ ê· í˜• ì¡°ì • ìœ í‹¸ë¦¬í‹°
        self.query_diversifier = QueryDiversifier()
        self.result_balancer = SearchResultBalancer(min_per_type=1, max_per_type=5)
        
        # State ì ‘ê·¼ ìºì‹± (ì„±ëŠ¥ ìµœì í™”)
        self._state_cache = {}
        self._state_cache_key = None

    def get_search_params(self, state: LegalWorkflowState) -> Dict[str, Any]:
        """ê²€ìƒ‰ì— í•„ìš”í•œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸° (State ì ‘ê·¼ ìµœì í™”)"""
        from core.workflow.state.state_helpers import get_field
        import hashlib

        # State ìºì‹±: state í•´ì‹œë¡œ ìºì‹œ í‚¤ ìƒì„±
        state_str = str(sorted(state.items())) if isinstance(state, dict) else str(state)
        state_hash = hashlib.md5(state_str.encode()).hexdigest()
        
        # ìºì‹œ íˆíŠ¸ í™•ì¸
        if self._state_cache_key == state_hash and self._state_cache:
            self.logger.debug("âœ… [PERFORMANCE] State cache hit in get_search_params")
            return self._state_cache.copy()
        
        # ìºì‹œ ë¯¸ìŠ¤: Stateì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
        # Multi-Query ê°•í™”: stateì˜ ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ optimized_queries ì°¾ê¸° (ìˆœì„œ ì¤‘ìš”)
        # _get_state_valueê°€ Noneì„ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¨¼ì € ì§ì ‘ í™•ì¸
        optimized_queries = None
        
        # ë””ë²„ê¹…: state êµ¬ì¡° í™•ì¸ (ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ)
        if os.getenv("DEBUG_STATE_ACCESS", "false").lower() == "true":
            state_keys = list(state.keys()) if isinstance(state, dict) else []
            print(f"[MULTI-QUERY] get_search_params: state keys={state_keys}", flush=True, file=sys.stdout)
            self.logger.debug(f"ğŸ” [MULTI-QUERY] get_search_params: state keys={state_keys}")
        
        # searchì™€ common ê·¸ë£¹ì˜ êµ¬ì¡°ë„ í™•ì¸
        if "search" in state and isinstance(state["search"], dict):
            search_keys = list(state["search"].keys())
            print(f"[MULTI-QUERY] search group keys={search_keys}", flush=True, file=sys.stdout)
        if "common" in state and isinstance(state.get("common"), dict):
            common_keys = list(state["common"].keys())
            print(f"[MULTI-QUERY] common group keys={common_keys}", flush=True, file=sys.stdout)
            if "search" in state["common"] and isinstance(state["common"]["search"], dict):
                common_search_keys = list(state["common"]["search"].keys())
                print(f"[MULTI-QUERY] common.search keys={common_search_keys}", flush=True, file=sys.stdout)
        
        # 1. top-level stateì—ì„œ ì§ì ‘ í™•ì¸ (ê°€ì¥ ìš°ì„ )
        if "optimized_queries" in state and isinstance(state["optimized_queries"], dict) and len(state["optimized_queries"]) > 0:
            optimized_queries = state["optimized_queries"]
            print(f"[MULTI-QUERY] Found optimized_queries in top-level state (keys: {list(optimized_queries.keys())})", flush=True, file=sys.stdout)
            self.logger.info(f"ğŸ” [MULTI-QUERY] Found optimized_queries in top-level state (keys: {list(optimized_queries.keys())})")
        
        # 2. search groupì—ì„œ í™•ì¸ (top-levelì— ì—†ìœ¼ë©´)
        if (not optimized_queries or (isinstance(optimized_queries, dict) and len(optimized_queries) == 0)) and "search" in state and isinstance(state["search"], dict):
            search_group = state["search"]
            search_optimized = search_group.get("optimized_queries")
            print(f"[MULTI-QUERY] Checking search group: optimized_queries type={type(search_optimized)}, value={search_optimized}", flush=True, file=sys.stdout)
            if search_optimized and isinstance(search_optimized, dict):
                if len(search_optimized) > 0:
                    optimized_queries = search_optimized
                    print(f"[MULTI-QUERY] Found optimized_queries in search group (keys: {list(optimized_queries.keys())})", flush=True, file=sys.stdout)
                    self.logger.info(f"ğŸ” [MULTI-QUERY] Found optimized_queries in search group (keys: {list(optimized_queries.keys())})")
                else:
                    print(f"[MULTI-QUERY] search group optimized_queries is empty dict", flush=True, file=sys.stdout)
            else:
                print(f"[MULTI-QUERY] search group optimized_queries is not a dict or None: {search_optimized}", flush=True, file=sys.stdout)
        
        # 3. common.searchì—ì„œ í™•ì¸ (ìœ„ì—ì„œ ì°¾ì§€ ëª»í–ˆìœ¼ë©´)
        if (not optimized_queries or len(optimized_queries) == 0) and "common" in state and isinstance(state.get("common"), dict):
            common_search = state["common"].get("search", {})
            if isinstance(common_search, dict) and common_search.get("optimized_queries"):
                optimized_queries = common_search["optimized_queries"]
                print(f"[MULTI-QUERY] Found optimized_queries in common.search (keys: {list(optimized_queries.keys())})", flush=True, file=sys.stdout)
                self.logger.info(f"ğŸ” [MULTI-QUERY] Found optimized_queries in common.search (keys: {list(optimized_queries.keys())})")
        # 4. _get_state_valueë¡œ í™•ì¸ (fallback)
        if not optimized_queries or len(optimized_queries) == 0:
            optimized_queries = self._get_state_value(state, "optimized_queries", {})
            if optimized_queries and len(optimized_queries) > 0:
                print(f"[MULTI-QUERY] Found optimized_queries via _get_state_value (keys: {list(optimized_queries.keys())})", flush=True, file=sys.stdout)
                self.logger.info(f"ğŸ” [MULTI-QUERY] Found optimized_queries via _get_state_value (keys: {list(optimized_queries.keys())})")
            else:
                print(f"[MULTI-QUERY] _get_state_value returned: {optimized_queries}", flush=True, file=sys.stdout)
        
        # optimized_queriesê°€ Noneì´ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
        if optimized_queries is None:
            optimized_queries = {}
            print(f"[MULTI-QUERY] optimized_queries was None, initialized to empty dict", flush=True, file=sys.stdout)
        
        # 5. Global cacheì—ì„œ í™•ì¸ (state reduction ëŒ€ì‘)
        if (not optimized_queries or len(optimized_queries) == 0):
            try:
                from core.shared.wrappers.node_wrappers import _global_search_results_cache
                if _global_search_results_cache and isinstance(_global_search_results_cache, dict):
                    if "search" in _global_search_results_cache and isinstance(_global_search_results_cache["search"], dict):
                        cached_optimized = _global_search_results_cache["search"].get("optimized_queries")
                        if cached_optimized and isinstance(cached_optimized, dict) and len(cached_optimized) > 0:
                            optimized_queries = cached_optimized.copy()
                            print(f"[MULTI-QUERY] Found optimized_queries in global cache (keys: {list(optimized_queries.keys())})", flush=True, file=sys.stdout)
                            self.logger.info(f"ğŸ” [MULTI-QUERY] Found optimized_queries in global cache (keys: {list(optimized_queries.keys())})")
            except Exception as e:
                self.logger.debug(f"Failed to get optimized_queries from global cache: {e}")
        
        # 6. get_fieldë¡œ í™•ì¸ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        if not optimized_queries or len(optimized_queries) == 0:
            optimized_queries_raw = get_field(state, "optimized_queries")
            if optimized_queries_raw and isinstance(optimized_queries_raw, dict) and len(optimized_queries_raw) > 0:
                optimized_queries = optimized_queries_raw
                self.logger.info("ğŸ” [MULTI-QUERY] Found optimized_queries via get_field")
        
        search_params = self._get_state_value(state, "search_params", {})
        query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
        legal_field = self._get_state_value(state, "legal_field", "")
        extracted_keywords = self._get_state_value(state, "extracted_keywords", [])
        original_query = self._get_state_value(state, "query", "")

        if "search" in state and isinstance(state["search"], dict):
            search_group = state["search"]
            if "extracted_keywords" in search_group and search_group["extracted_keywords"]:
                extracted_keywords = search_group["extracted_keywords"]

            if search_group.get("optimized_queries") and isinstance(search_group["optimized_queries"], dict) and len(search_group["optimized_queries"]) > 0:
                # search groupì˜ optimized_queriesê°€ ë” ì™„ì „í•˜ë©´ ì‚¬ìš©
                if "multi_queries" in search_group["optimized_queries"] or len(search_group["optimized_queries"]) > len(optimized_queries):
                    optimized_queries = search_group["optimized_queries"]
                    self.logger.debug("ğŸ” [MULTI-QUERY] Using optimized_queries from search group (more complete)")
                if not extracted_keywords and "expanded_keywords" in optimized_queries:
                    extracted_keywords = optimized_queries.get("expanded_keywords", [])

            if search_group.get("search_params") and isinstance(search_group["search_params"], dict) and len(search_group["search_params"]) > 0:
                search_params = search_group["search_params"]

        if not extracted_keywords:
            extracted_keywords_raw = get_field(state, "extracted_keywords")
            if extracted_keywords_raw and len(extracted_keywords_raw) > 0:
                extracted_keywords = extracted_keywords_raw
        
        # Multi-Query ë³µì›: optimized_queriesê°€ ìˆì§€ë§Œ multi_queriesê°€ ì—†ëŠ” ê²½ìš° stateì—ì„œ ì§ì ‘ í™•ì¸
        if optimized_queries and "multi_queries" not in optimized_queries:
            # stateì˜ ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ multi_queries í™•ì¸ (ìˆœì„œ ì¤‘ìš”)
            state_multi_queries = None
            # 1. top-level stateì—ì„œ ì§ì ‘ í™•ì¸ (ê°€ì¥ ìš°ì„ )
            if "optimized_queries" in state and isinstance(state["optimized_queries"], dict):
                state_multi_queries = state["optimized_queries"].get("multi_queries")
                if state_multi_queries:
                    self.logger.info(f"ğŸ” [MULTI-QUERY] Found multi_queries in top-level state (count: {len(state_multi_queries)})")
            # 2. search groupì—ì„œ í™•ì¸
            if not state_multi_queries and "search" in state and isinstance(state.get("search"), dict):
                search_optimized = state["search"].get("optimized_queries", {})
                if isinstance(search_optimized, dict):
                    state_multi_queries = search_optimized.get("multi_queries")
                    if state_multi_queries:
                        self.logger.info(f"ğŸ” [MULTI-QUERY] Found multi_queries in search group (count: {len(state_multi_queries)})")
            # 3. common.searchì—ì„œ í™•ì¸
            if not state_multi_queries and "common" in state and isinstance(state.get("common"), dict):
                common_search = state["common"].get("search", {})
                if isinstance(common_search, dict) and common_search.get("optimized_queries"):
                    common_optimized = common_search["optimized_queries"]
                    if isinstance(common_optimized, dict):
                        state_multi_queries = common_optimized.get("multi_queries")
                        if state_multi_queries:
                            self.logger.info(f"ğŸ” [MULTI-QUERY] Found multi_queries in common.search (count: {len(state_multi_queries)})")
            # 4. common groupì—ì„œ ì§ì ‘ í™•ì¸
            if not state_multi_queries and "common" in state and isinstance(state.get("common"), dict):
                common_optimized = state["common"].get("optimized_queries", {})
                if isinstance(common_optimized, dict):
                    state_multi_queries = common_optimized.get("multi_queries")
                    if state_multi_queries:
                        self.logger.info(f"ğŸ” [MULTI-QUERY] Found multi_queries in common group (count: {len(state_multi_queries)})")
            
            if state_multi_queries:
                if not optimized_queries:
                    optimized_queries = {}
                optimized_queries["multi_queries"] = state_multi_queries
                self.logger.info(f"âœ… [MULTI-QUERY] Restored multi_queries from state (count: {len(state_multi_queries)})")
            else:
                # ë””ë²„ê¹…: state êµ¬ì¡° í™•ì¸
                self.logger.warning(f"âš ï¸ [MULTI-QUERY] Could not find multi_queries in state. State keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")
                if isinstance(state, dict) and "search" in state:
                    self.logger.warning(f"âš ï¸ [MULTI-QUERY] search group keys: {list(state['search'].keys()) if isinstance(state['search'], dict) else 'N/A'}")
        
        # Multi-Query í™•ì¸ ë¡œê·¸ (í•­ìƒ ì¶œë ¥)
        has_multi = optimized_queries and "multi_queries" in optimized_queries
        keys_str = list(optimized_queries.keys()) if optimized_queries else "None"
        print(f"[MULTI-QUERY] get_search_params: optimized_queries keys={keys_str}, has_multi_queries={has_multi}", flush=True, file=sys.stdout)
        if has_multi:
            self.logger.info(f"ğŸ” [MULTI-QUERY] get_search_params: Found multi_queries with {len(optimized_queries.get('multi_queries', []))} queries")
        elif optimized_queries:
            self.logger.warning(f"âš ï¸ [MULTI-QUERY] get_search_params: optimized_queries exists but no multi_queries (keys: {keys_str})")

        if not search_params or len(search_params) == 0:
            search_params_raw = get_field(state, "search_params")
            if search_params_raw and len(search_params_raw) > 0:
                search_params = search_params_raw

        if not original_query and "input" in state and isinstance(state.get("input"), dict):
            original_query = state["input"].get("query", "")

        result = {
            "optimized_queries": optimized_queries,
            "search_params": search_params,
            "query_type_str": query_type_str,
            "legal_field": legal_field,
            "extracted_keywords": extracted_keywords,
            "original_query": original_query
        }
        
        # ìºì‹œ ì €ì¥
        self._state_cache = result.copy()
        self._state_cache_key = state_hash
        
        return result

    def execute_searches_parallel(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
        try:
            start_time = time.time()

            debug_mode = os.getenv("DEBUG_SEARCH", "false").lower() == "true"

            params = self.get_search_params(state)
            optimized_queries = params["optimized_queries"]
            search_params = params["search_params"]
            query_type_str = params["query_type_str"]
            legal_field = params["legal_field"]
            extracted_keywords = params["extracted_keywords"]
            original_query = params["original_query"]

            # ì„±ëŠ¥ ìµœì í™”: extracted_keywordsë¥¼ í•œ ë²ˆë§Œ í™•ì¸ (ì¤‘ë³µ ì ‘ê·¼ ì œê±°)
            if not extracted_keywords or len(extracted_keywords) == 0:
                # í•œ ë²ˆì— ëª¨ë“  ê°€ëŠ¥í•œ ìœ„ì¹˜ í™•ì¸
                extracted_keywords = (
                    self._get_state_value(state, "extracted_keywords", []) or
                    (state.get("search", {}).get("extracted_keywords", []) if isinstance(state.get("search"), dict) else []) or
                    state.get("extracted_keywords", []) or
                    []
                )
                if debug_mode:
                    self.logger.debug(f"extracted_keywords from batch was empty, got {len(extracted_keywords)} from state directly")
            elif debug_mode:
                self.logger.debug(f"extracted_keywords from batch: {len(extracted_keywords)} keywords")

            # ë¡œê¹… ìµœì í™”: ë¡œê¹… ë ˆë²¨ ì²´í¬ ë° ë°°ì¹˜ ë¡œê¹…
            if self.logger.isEnabledFor(logging.DEBUG):
                debug_info = {
                    "optimized_queries": {
                        "type": type(optimized_queries).__name__,
                        "exists": bool(optimized_queries),
                        "keys": list(optimized_queries.keys()) if isinstance(optimized_queries, dict) else None
                    },
                    "search_params": {
                        "type": type(search_params).__name__,
                        "exists": bool(search_params),
                        "keys": list(search_params.keys()) if isinstance(search_params, dict) else None
                    }
                }
                self.logger.debug(f"execute_searches_parallel: START - {debug_info}")

            semantic_query_value = optimized_queries.get("semantic_query", "") if optimized_queries else ""

            if not semantic_query_value or not str(semantic_query_value).strip():
                if original_query:
                    if debug_mode:
                        self.logger.warning(f"semantic_query is empty in execute_searches_parallel, using base query: '{original_query[:50]}...'")
                    optimized_queries["semantic_query"] = original_query
                    semantic_query_value = original_query

            has_semantic_query = optimized_queries and semantic_query_value and len(str(semantic_query_value).strip()) > 0
            keyword_queries_value = optimized_queries.get("keyword_queries", []) if optimized_queries else []

            if not keyword_queries_value or len(keyword_queries_value) == 0:
                if original_query:
                    if debug_mode:
                        self.logger.warning(f"keyword_queries is empty in execute_searches_parallel, using base query")
                    optimized_queries["keyword_queries"] = [original_query]
                    keyword_queries_value = [original_query]

            has_keyword_queries = optimized_queries and keyword_queries_value and len(keyword_queries_value) > 0

            # ë¡œê¹… ìµœì í™”: ê²€ì¦ ì •ë³´ ë°°ì¹˜ ë¡œê¹…
            if self.logger.isEnabledFor(logging.DEBUG):
                validation_info = {
                    "semantic_query": semantic_query_value[:50] if semantic_query_value else 'EMPTY',
                    "has_semantic_query": has_semantic_query,
                    "keyword_queries_count": len(keyword_queries_value) if keyword_queries_value else 0,
                    "has_keyword_queries": has_keyword_queries,
                    "search_params": {
                        "is_none": search_params is None,
                        "is_empty": search_params == {},
                        "keys": list(search_params.keys()) if search_params else []
                    }
                }
                self.logger.debug(f"Validation: {validation_info}")

            if not search_params or not isinstance(search_params, dict) or len(search_params) == 0:
                self.logger.warning(f"ğŸ” [SEARCH] search_params is empty, setting default values")
                search_params = self._determine_search_parameters(
                    query_type=query_type_str,
                    query_complexity=len(original_query) if original_query else 0,
                    keyword_count=len(extracted_keywords) if extracted_keywords else 0,
                    is_retry=False
                )
                self.logger.info(f"ğŸ” [SEARCH] Default search_params set: {search_params}")

            optimized_queries_valid = optimized_queries and isinstance(optimized_queries, dict) and len(optimized_queries) > 0
            search_params_valid = search_params and isinstance(search_params, dict) and len(search_params) > 0
            # ë¡œê¹… ìµœì í™”: ê²€ì¦ ì²´í¬ ë°°ì¹˜ ë¡œê¹…
            if self.logger.isEnabledFor(logging.DEBUG):
                validation_check_info = {
                    "optimized_queries_valid": optimized_queries_valid,
                    "optimized_queries": {
                        "type": type(optimized_queries).__name__,
                        "len": len(optimized_queries) if isinstance(optimized_queries, dict) else 'N/A'
                    },
                    "search_params_valid": search_params_valid,
                    "search_params": {
                        "type": type(search_params).__name__,
                        "len": len(search_params) if isinstance(search_params, dict) else 'N/A'
                    },
                    "has_semantic_query": has_semantic_query
                }
                self.logger.debug(f"ğŸ” [SEARCH] Validation check: {validation_check_info}")

            if not optimized_queries_valid or not search_params_valid or not has_semantic_query:
                self.logger.warning(f"ğŸ” [SEARCH] PARALLEL SEARCH SKIP: optimized_queries_valid={optimized_queries_valid}, search_params_valid={search_params_valid}, has_semantic_query={has_semantic_query}")
                if debug_mode:
                    self.logger.warning("Optimized queries or search params not found")
                    self.logger.debug(f"PARALLEL SEARCH SKIP: optimized_queries={optimized_queries is not None}, search_params={search_params is not None}")
                self._set_state_value(state, "semantic_results", [])
                self._set_state_value(state, "keyword_results", [])
                self._set_state_value(state, "semantic_count", 0)
                self._set_state_value(state, "keyword_count", 0)
                return state

            semantic_results = []
            semantic_count = 0
            keyword_results = []
            keyword_count = 0

            # Multi-Query í™•ì¸ ë¡œê·¸ (ë¡œê¹… ìµœì í™”)
            multi_queries = optimized_queries.get("multi_queries", [])
            if multi_queries and debug_mode:
                print(f"[MULTI-QUERY] execute_searches_parallel: Found {len(multi_queries)} multi-queries in optimized_queries", flush=True, file=sys.stdout)
                self.logger.debug(f"ğŸ” [MULTI-QUERY] execute_searches_parallel: Found {len(multi_queries)} multi-queries")
            elif not multi_queries and debug_mode:
                self.logger.debug(f"âš ï¸ [MULTI-QUERY] execute_searches_parallel: No multi_queries in optimized_queries")
            
            if debug_mode:
                self.logger.debug(f"PARALLEL SEARCH START: semantic_query={optimized_queries.get('semantic_query', 'N/A')[:50]}, keyword_queries={len(optimized_queries.get('keyword_queries', []))}, multi_queries={len(multi_queries) if multi_queries else 0}, original_query={original_query[:50] if original_query else 'N/A'}...")

            # ì„±ëŠ¥ ìµœì í™”: extracted_keywords ì¬í™•ì¸ ì œê±° (ì´ë¯¸ ìœ„ì—ì„œ í™•ì¸í•¨)
            final_keywords = extracted_keywords if extracted_keywords else []
            keywords_copy = list(final_keywords) if final_keywords else []
            
            if debug_mode:
                self.logger.debug(f"Final extracted_keywords: {len(final_keywords)} keywords, keywords_copy: {len(keywords_copy)} keywords")

            # ì„±ëŠ¥ ìµœì í™”: ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ë˜ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰
            # ë²•ë ¹ ì¡°ë¬¸ ì§ì ‘ ê²€ìƒ‰ë„ ë³‘ë ¬í™”í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            semantic_results, semantic_count = [], 0
            keyword_results, keyword_count = [], 0
            
            # ì¡°ê¸° ì¢…ë£Œ ìµœì í™”: ë™ì  ì„ê³„ê°’ ê³„ì‚°
            semantic_k = search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K)
            keyword_k = search_params.get("keyword_k", WorkflowConstants.KEYWORD_SEARCH_K)
            min_required_results = semantic_k + keyword_k
            early_exit_threshold = int(min_required_results * 1.2)  # 20% ì—¬ìœ 
            max_results_threshold = min_required_results * 2  # ìµœëŒ€ 2ë°°ê¹Œì§€ë§Œ
            
            # ì¡°ê¸° ì¢…ë£Œ í”Œë˜ê·¸
            early_exit_triggered = False
            early_exit_reason = None
            
            # ë²•ë ¹ ì¡°ë¬¸ ì§ì ‘ ê²€ìƒ‰ë„ ë³‘ë ¬ ì‹¤í–‰ (max_workers=3)
            needs_direct_statute = original_query and query_type_str == "law_inquiry"
            
            # Multi-Query ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”: Multi-Query ì¤€ë¹„
            multi_queries = optimized_queries.get("multi_queries", [])
            multi_queries_to_process = []
            if multi_queries and len(multi_queries) > 1:
                max_semantic_results_before_multi = semantic_k * 2
                multi_queries_to_process = multi_queries[1:]  # ì²« ë²ˆì§¸ëŠ” ì´ë¯¸ ì²˜ë¦¬ë¨
                max_multi_queries = min(len(multi_queries_to_process), 2)
                multi_queries_to_process = multi_queries_to_process[:max_multi_queries]
            
            # ë™ì  worker ìˆ˜ ê³„ì‚° (Multi-Query í¬í•¨)
            base_workers = 2  # semantic + keyword
            if needs_direct_statute:
                base_workers += 1
            if multi_queries_to_process:
                base_workers += len(multi_queries_to_process)
            max_workers = min(base_workers, 6)  # ìµœëŒ€ 6ê°œë¡œ ì œí•œ
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ëª¨ë“  ì‘ì—…ì„ í•œ ë²ˆì— ì œì¶œ
                semantic_future = executor.submit(
                    self.execute_semantic_search,
                    optimized_queries,
                    search_params,
                    original_query,
                    keywords_copy
                )

                keyword_future = executor.submit(
                    self.execute_keyword_search,
                    optimized_queries,
                    search_params,
                    query_type_str,
                    legal_field,
                    extracted_keywords,
                    original_query
                )
                
                # ë²•ë ¹ ì¡°ë¬¸ ì§ì ‘ ê²€ìƒ‰ë„ ë³‘ë ¬ ì‹¤í–‰
                direct_statute_future = None
                if needs_direct_statute:
                    def _search_direct_statute():
                        try:
                            from core.agents.legal_data_connector_v2 import LegalDataConnectorV2
                            data_connector = LegalDataConnectorV2()
                            return data_connector.search_statute_article_direct(original_query, limit=5)
                        except Exception as e:
                            if debug_mode:
                                self.logger.debug(f"Direct statute search error: {e}")
                            return []
                    
                    direct_statute_future = executor.submit(_search_direct_statute)

                # Multi-Query ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”: Multi-Query futures ì¶”ê°€
                multi_query_futures = {}
                if multi_queries_to_process:
                    for mq in multi_queries_to_process:
                        mq_future = executor.submit(
                            self._execute_semantic_search_single,
                            mq,
                            max(5, semantic_k // 3),
                            keywords_copy,
                            None
                        )
                        multi_query_futures[mq_future] = ('multi_query', mq[:30])
                
                # as_completedë¥¼ ì‚¬ìš©í•˜ì—¬ ë¨¼ì € ì™„ë£Œë˜ëŠ” ì‘ì—…ë¶€í„° ì²˜ë¦¬ (ì„±ëŠ¥ ìµœì í™”)
                futures_map = {
                    semantic_future: ('semantic', 'main'),
                    keyword_future: ('keyword', None)
                }
                if direct_statute_future:
                    futures_map[direct_statute_future] = ('direct_statute', None)
                futures_map.update(multi_query_futures)
                
                completed_count = 0
                direct_statute_results = []
                unfinished_futures = []
                
                # íƒ€ì„ì•„ì›ƒ ì¦ê°€: 10ì´ˆ â†’ 20ì´ˆ (ëŒ€ëŸ‰ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì‹œê°„ í™•ë³´)
                # ë¡œê¹… ìµœì í™”: ì™„ë£Œëœ ì‘ì—…ì„ ëª¨ì•„ì„œ í•œ ë²ˆì— ë¡œê¹…
                completed_tasks = []
                try:
                    for future in as_completed(futures_map.keys(), timeout=20):
                        search_type, query_type = futures_map[future]
                        try:
                            if search_type == 'semantic':
                                if query_type == 'main':
                                    semantic_results, semantic_count = future.result()
                                    completed_tasks.append(('semantic', semantic_count))
                                elif query_type and query_type.startswith('multi_query'):
                                    # Multi-Query ê²°ê³¼ ì²˜ë¦¬
                                    mq_results, mq_count = future.result()
                                    if mq_results:
                                        # ì¤‘ë³µ ì œê±° í›„ ì¶”ê°€
                                        seen_ids = {doc.get("id") or doc.get("doc_id") 
                                                  for doc in semantic_results}
                                        new_results = [
                                            doc for doc in mq_results
                                            if (doc.get("id") or doc.get("doc_id")) not in seen_ids
                                        ]
                                        semantic_results.extend(new_results)
                                        completed_tasks.append(('multi_query', len(new_results)))
                            elif search_type == 'keyword':
                                keyword_results, keyword_count = future.result()
                                completed_tasks.append(('keyword', keyword_count))
                            elif search_type == 'direct_statute':
                                direct_statute_results = future.result()
                                completed_tasks.append(('direct_statute', len(direct_statute_results) if direct_statute_results else 0))
                            
                            completed_count += 1
                            
                            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬: ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ë‚˜ë¨¸ì§€ ì·¨ì†Œ
                            current_total = len(semantic_results) + len(keyword_results)
                            if current_total >= early_exit_threshold:
                                early_exit_triggered = True
                                early_exit_reason = f"Sufficient results: {current_total} >= {early_exit_threshold}"
                                
                                # ë‚˜ë¨¸ì§€ ë¯¸ì™„ë£Œ future ì·¨ì†Œ
                                remaining_futures = [f for f in futures_map.keys() if not f.done()]
                                for remaining_future in remaining_futures:
                                    if remaining_future.running():
                                        remaining_future.cancel()
                                        if self.logger.isEnabledFor(logging.DEBUG):
                                            remaining_type, _ = futures_map[remaining_future]
                                            self.logger.debug(f"Cancelled {remaining_type} search (early exit)")
                                
                                if self.logger.isEnabledFor(logging.DEBUG):
                                    self.logger.debug(f"âš¡ [EARLY EXIT] {early_exit_reason}")
                                break
                                
                        except Exception as e:
                            if search_type == 'direct_statute':
                                if self.logger.isEnabledFor(logging.DEBUG):
                                    self.logger.debug(f"Direct statute search failed: {e}")
                                direct_statute_results = []
                                completed_tasks.append(('direct_statute', 'error', str(e)))
                            else:
                                self.logger.error(f"{search_type} search failed: {e}")
                                if self.logger.isEnabledFor(logging.DEBUG):
                                    self.logger.debug(f"{search_type} search exception: {e}")
                                completed_tasks.append((search_type, 'error', str(e)))
                                if search_type == 'semantic':
                                    semantic_results, semantic_count = [], 0
                                else:
                                    keyword_results, keyword_count = [], 0
                            completed_count += 1
                    
                    # ë¡œê¹… ìµœì í™”: ì™„ë£Œëœ ì‘ì—… í•œ ë²ˆì— ë¡œê¹…
                    if self.logger.isEnabledFor(logging.DEBUG) and completed_tasks:
                        self.logger.debug(f"Completed tasks: {completed_tasks}")
                except TimeoutError:
                    # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ ì™„ë£Œë˜ì§€ ì•Šì€ future ìˆ˜ì§‘
                    unfinished_futures = [f for f in futures_map.keys() if not f.done()]
                    self.logger.warning(
                        f"âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ ë°œìƒ: {len(unfinished_futures)} (of {len(futures_map)}) futures unfinished"
                    )
                
                # íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì™„ë£Œë˜ì§€ ì•Šì€ ì‘ì—… ì²˜ë¦¬ (ë¶€ë¶„ ê²°ê³¼ë¼ë„ ë°˜í™˜)
                expected_count = 3 if needs_direct_statute else 2
                if completed_count < expected_count:
                    # ê° ë¯¸ì™„ë£Œ futureì— ëŒ€í•´ ë” ê¸´ ì‹œê°„(5ì´ˆ) ê¸°ë‹¤ë¦¬ê¸°
                    if not semantic_results and semantic_future.running():
                        try:
                            semantic_results, semantic_count = semantic_future.result(timeout=5)
                            if self.logger.isEnabledFor(logging.DEBUG):
                                self.logger.debug(f"Semantic search completed after timeout: {semantic_count} results")
                        except Exception as e:
                            if self.logger.isEnabledFor(logging.DEBUG):
                                self.logger.debug(f"Semantic search timeout or error: {e}")
                            semantic_results, semantic_count = [], 0
                    
                    if not keyword_results and keyword_future.running():
                        try:
                            keyword_results, keyword_count = keyword_future.result(timeout=5)
                            if self.logger.isEnabledFor(logging.DEBUG):
                                self.logger.debug(f"Keyword search completed after timeout: {keyword_count} results")
                        except Exception as e:
                            if self.logger.isEnabledFor(logging.DEBUG):
                                self.logger.debug(f"Keyword search timeout or error: {e}")
                            keyword_results, keyword_count = [], 0
                    
                    if needs_direct_statute and not direct_statute_results and direct_statute_future and direct_statute_future.running():
                        try:
                            direct_statute_results = direct_statute_future.result(timeout=5)
                            if self.logger.isEnabledFor(logging.DEBUG):
                                self.logger.debug(f"Direct statute search completed after timeout: {len(direct_statute_results)} results")
                        except Exception as e:
                            if self.logger.isEnabledFor(logging.DEBUG):
                                self.logger.debug(f"Direct statute search timeout or error: {e}")
                            direct_statute_results = []
                    
                    # ë¯¸ì™„ë£Œ future ì·¨ì†Œ ì‹œë„
                    for future in unfinished_futures:
                        if future.running():
                            future.cancel()
                            if self.logger.isEnabledFor(logging.DEBUG):
                                search_type, _ = futures_map[future]
                                self.logger.debug(f"Cancelled unfinished {search_type} search")
                
                # ë²•ë ¹ ì¡°ë¬¸ ì§ì ‘ ê²€ìƒ‰ ê²°ê³¼ ë³‘í•©
                if direct_statute_results:
                    keyword_results = direct_statute_results + keyword_results
                    keyword_count += len(direct_statute_results)
                    if self.logger.isEnabledFor(logging.DEBUG):
                        self.logger.debug(f"âš–ï¸ [DIRECT STATUTE] {len(direct_statute_results)}ê°œ ì¡°ë¬¸ ì¶”ê°€ ì™„ë£Œ (ì´ {keyword_count}ê°œ)")
                
                # ì¡°ê¸° ì¢…ë£Œ ë¡œê¹…
                if early_exit_triggered:
                    self.logger.info(
                        f"âš¡ [EARLY EXIT] {early_exit_reason} - "
                        f"Semantic: {len(semantic_results)}, Keyword: {len(keyword_results)}"
                    )

            # ê²€ìƒ‰ ê²°ê³¼ íƒ€ì… ê· í˜• ì¡°ì • (ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ê°€ ë§ì„ ë•Œë§Œ ìˆ˜í–‰)
            total_results = len(semantic_results) + len(keyword_results)
            should_balance = total_results > 20  # ê²°ê³¼ê°€ 20ê°œ ì´ìƒì¼ ë•Œë§Œ ê· í˜• ì¡°ì •
            
            if should_balance:
                try:
                    # numpy íƒ€ì… ë³€í™˜ í•¨ìˆ˜ (ìµœì í™”: í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë³€í™˜)
                    def convert_numpy_types(obj, _depth=0):
                        # ì¬ê·€ ê¹Šì´ ì œí•œìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
                        if _depth > 5:
                            return obj
                        import numpy as np
                        if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
                            return int(obj)
                        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
                            return float(obj)
                        elif isinstance(obj, np.ndarray):
                            return obj.tolist()
                        elif isinstance(obj, dict):
                            return {k: convert_numpy_types(v, _depth + 1) for k, v in obj.items()}
                        elif isinstance(obj, (list, tuple)):
                            return [convert_numpy_types(item, _depth + 1) for item in obj]
                        return obj
                    
                    # ê²€ìƒ‰ ê²°ê³¼ì— numpy íƒ€ì… ë³€í™˜ ì ìš© (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
                    has_numpy = False
                    for doc in semantic_results[:5] + keyword_results[:5]:
                        import numpy as np
                        if any(isinstance(v, (np.integer, np.floating, np.ndarray)) for v in (doc.values() if isinstance(doc, dict) else [])):
                            has_numpy = True
                            break
                    
                    if has_numpy:
                        semantic_results = [convert_numpy_types(doc) for doc in semantic_results]
                        keyword_results = [convert_numpy_types(doc) for doc in keyword_results]
                    
                    # semantic_resultsì™€ keyword_resultsë¥¼ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
                    all_results = semantic_results + keyword_results
                    grouped_results = self.result_balancer.group_results_by_type(all_results)
                    
                    # íƒ€ì…ë³„ ë¶„í¬ í™•ì¸ (ë¡œê¹… ìµœì í™”)
                    type_distribution = {doc_type: len(docs) for doc_type, docs in grouped_results.items()}
                    # ë¡œê¹… ìµœì í™”: íƒ€ì…ë³„ ë¶„í¬ ë°°ì¹˜ ë¡œê¹…
                    if self.logger.isEnabledFor(logging.DEBUG):
                        non_zero_types = {k: v for k, v in type_distribution.items() if v > 0}
                        if non_zero_types:
                            self.logger.debug(f"ğŸ“Š [SEARCH BALANCE] Type distribution: {non_zero_types}")
                    
                    # ë‹¨ì¼ íƒ€ì…ë§Œ ê²€ìƒ‰ëœ ê²½ìš° ê²½ê³  (ë¡œê¹… ìµœì í™”)
                    non_zero_types = [t for t, c in type_distribution.items() if c > 0]
                    if len(non_zero_types) == 1:
                        single_type = non_zero_types[0]
                        self.logger.warning(
                            f"âš ï¸ [TYPE DIVERSITY] ë‹¨ì¼ íƒ€ì…ë§Œ ê²€ìƒ‰ë¨: {single_type} ({type_distribution[single_type]}ê°œ)"
                        )
                    elif len(non_zero_types) == 0:
                        self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                    elif debug_mode:
                        # íƒ€ì… ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚° (ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ)
                        total_docs = sum(type_distribution.values())
                        if total_docs > 0:
                            import math
                            entropy = 0.0
                            for count in type_distribution.values():
                                if count > 0:
                                    p = count / total_docs
                                    entropy -= p * math.log2(p)
                            max_entropy = math.log2(len(non_zero_types)) if len(non_zero_types) > 1 else 1.0
                            diversity_score = entropy / max_entropy if max_entropy > 0 else 0.0
                            self.logger.debug(
                                f"âœ… [TYPE DIVERSITY] íƒ€ì… ë‹¤ì–‘ì„± ì ìˆ˜: {diversity_score:.2f} "
                                f"(ê²€ìƒ‰ëœ íƒ€ì…: {len(non_zero_types)}ê°œ, ì´ ë¬¸ì„œ: {total_docs}ê°œ)"
                            )
                    
                    # ê· í˜• ì¡°ì •ëœ ê²°ê³¼ ìƒì„±
                    semantic_k = search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K)
                    keyword_k = search_params.get("keyword_k", WorkflowConstants.KEYWORD_SEARCH_K)
                    balanced_results = self.result_balancer.balance_search_results(
                        grouped_results,
                        total_limit=semantic_k + keyword_k
                    )
                    
                    # ê· í˜• ì¡°ì •ëœ ê²°ê³¼ë¥¼ semantic_resultsì™€ keyword_resultsë¡œ ì¬ë¶„ë°°
                    if balanced_results:
                        semantic_results_balanced = [
                            doc for doc in balanced_results 
                            if doc.get("relevance_score", 0.0) >= 0.5
                        ]
                        keyword_results_balanced = [
                            doc for doc in balanced_results 
                            if doc.get("relevance_score", 0.0) < 0.5 or doc not in semantic_results_balanced
                        ]
                        
                        # ê¸°ì¡´ ê²°ê³¼ì™€ ë³‘í•© (ì¤‘ë³µ ì œê±° - ì„±ëŠ¥ ìµœì í™”)
                        existing_ids = {id(doc) for doc in semantic_results + keyword_results}
                        
                        semantic_results = semantic_results + [
                            doc for doc in semantic_results_balanced 
                            if id(doc) not in existing_ids
                        ]
                        keyword_results = keyword_results + [
                            doc for doc in keyword_results_balanced 
                            if id(doc) not in existing_ids
                        ]
                        
                        semantic_count = len(semantic_results)
                        keyword_count = len(keyword_results)
                        
                        if debug_mode:
                            self.logger.debug(
                                f"âœ… [SEARCH BALANCE] ê· í˜• ì¡°ì • ì™„ë£Œ: "
                                f"semantic={semantic_count}, keyword={keyword_count}"
                            )
                except Exception as e:
                    if debug_mode:
                        self.logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ê· í˜• ì¡°ì • ì‹¤íŒ¨ (ê¸°ì¡´ ê²°ê³¼ ì‚¬ìš©): {e}")

            ensure_state_group(state, "search")

            if debug_mode:
                self.logger.debug(f"PARALLEL SEARCH: Before save - semantic_results={len(semantic_results)}, keyword_results={len(keyword_results)}")

            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "semantic_count", semantic_count)
            self._set_state_value(state, "keyword_count", keyword_count)
            
            # State êµ¬ì¡° ì¼ê´€ì„± í™•ë³´: retrieved_docsë¥¼ í—¬í¼ í•¨ìˆ˜ë¡œ ì €ì¥
            merged_docs = semantic_results + keyword_results
            set_retrieved_docs(state, merged_docs)

            if debug_mode:
                stored_semantic = self._get_state_value(state, "semantic_results", [])
                stored_keyword = self._get_state_value(state, "keyword_results", [])
                self.logger.debug(f"PARALLEL SEARCH: After save - semantic_results={len(stored_semantic)}, keyword_results={len(stored_keyword)}")

                if "search" in state and isinstance(state.get("search"), dict):
                    direct_semantic = state["search"].get("semantic_results", [])
                    direct_keyword = state["search"].get("keyword_results", [])
                    self.logger.debug(f"PARALLEL SEARCH: Direct state['search'] check - semantic={len(direct_semantic)}, keyword={len(direct_keyword)}")
                else:
                    self.logger.debug(f"PARALLEL SEARCH: state['search'] not found or not dict, state keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")

            self._save_metadata_safely(state, "_last_executed_node", "execute_searches_parallel")
            self._update_processing_time(state, start_time)

            elapsed_time = time.time() - start_time

            self.logger.info(
                f"âœ… [PARALLEL SEARCH] Completed in {elapsed_time:.3f}s - "
                f"Semantic: {semantic_count} results, Keyword: {keyword_count} results"
            )

            if debug_mode:
                self.logger.debug(f"PARALLEL SEARCH: Semantic={semantic_count}, Keyword={keyword_count}")

                if semantic_results:
                    semantic_scores = [doc.get("relevance_score", 0.0) for doc in semantic_results[:5]]
                    self.logger.info(
                        f"ğŸ” [DEBUG] Semantic search details: "
                        f"Top scores: {semantic_scores}, "
                        f"Sample sources: {[doc.get('source', 'Unknown')[:30] for doc in semantic_results[:3]]}"
                    )
                else:
                    self.logger.warning("âš ï¸ [DEBUG] Semantic search returned 0 results")

                if keyword_results:
                    keyword_scores = [doc.get("relevance_score", doc.get("score", 0.0)) for doc in keyword_results[:5]]
                    self.logger.info(
                        f"ğŸ” [DEBUG] Keyword search details: "
                        f"Top scores: {keyword_scores}, "
                        f"Sample sources: {[doc.get('source', 'Unknown')[:30] for doc in keyword_results[:3]]}"
                    )
                else:
                    self.logger.warning("âš ï¸ [DEBUG] Keyword search returned 0 results")

        except TimeoutError as timeout_err:
            self.logger.warning(f"âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ ë°œìƒ: {timeout_err}")
            self.logger.info("ğŸ”„ ìˆœì°¨ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± ì‹œë„ ì¤‘...")
            try:
                return self.fallback_sequential_search(state)
            except Exception as fallback_err:
                self.logger.error(f"âŒ ìˆœì°¨ ê²€ìƒ‰ í´ë°±ë„ ì‹¤íŒ¨: {fallback_err}", exc_info=True)
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ ê²°ê³¼ë¡œë¼ë„ ê³„ì† ì§„í–‰
                self.logger.warning("âš ï¸ ìµœì†Œí•œì˜ ê²°ê³¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                semantic_results, semantic_count = [], 0
                keyword_results, keyword_count = [], 0
                # ë¯¸ì™„ë£Œëœ future ì·¨ì†Œ ì‹œë„
                try:
                    if 'semantic_future' in locals() and not semantic_future.done():
                        semantic_future.cancel()
                    if 'keyword_future' in locals() and not keyword_future.done():
                        keyword_future.cancel()
                except Exception:
                    pass
                # ë¹ˆ ê²°ê³¼ë¼ë„ stateì— ì €ì¥í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ê°€ ê³„ì† ì§„í–‰ë˜ë„ë¡ í•¨
                ensure_state_group(state, "search")
                state["search"]["semantic_results"] = semantic_results
                state["search"]["keyword_results"] = keyword_results
                state["search"]["semantic_count"] = semantic_count
                state["search"]["keyword_count"] = keyword_count
                return state
        except Exception as e:
            self.logger.error(f"âŒ ë³‘ë ¬ ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            self.logger.info("ğŸ”„ ìˆœì°¨ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± ì‹œë„ ì¤‘...")
            try:
                return self.fallback_sequential_search(state)
            except Exception as fallback_err:
                self.logger.error(f"âŒ ìˆœì°¨ ê²€ìƒ‰ í´ë°±ë„ ì‹¤íŒ¨: {fallback_err}", exc_info=True)
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ ê²°ê³¼ë¡œë¼ë„ ê³„ì† ì§„í–‰
                self.logger.warning("âš ï¸ ìµœì†Œí•œì˜ ê²°ê³¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                semantic_results, semantic_count = [], 0
                keyword_results, keyword_count = [], 0
                ensure_state_group(state, "search")
                state["search"]["semantic_results"] = semantic_results
                state["search"]["keyword_results"] = keyword_results
                state["search"]["semantic_count"] = semantic_count
                state["search"]["keyword_count"] = keyword_count
                return state

        debug_mode = os.getenv("DEBUG_SEARCH", "false").lower() == "true"

        if debug_mode:
            if "search" in state and isinstance(state.get("search"), dict):
                final_search = state["search"]
                final_semantic = len(final_search.get("semantic_results", []))
                final_keyword = len(final_search.get("keyword_results", []))
                self.logger.debug(f"[DEBUG] execute_searches_parallel: Returning state with search group - semantic_results={final_semantic}, keyword_results={final_keyword}")
                self.logger.debug(f"[DEBUG] execute_searches_parallel: Returning state keys={list(state.keys()) if isinstance(state, dict) else 'N/A'}")
            else:
                self.logger.debug(f"[DEBUG] execute_searches_parallel: WARNING - Returning state WITHOUT search group!")
                self.logger.debug(f"[DEBUG] execute_searches_parallel: Returning state keys={list(state.keys()) if isinstance(state, dict) else 'N/A'}")

        return state

    def _execute_semantic_search_single(
        self,
        query: str,
        k: int,
        extracted_keywords: Optional[List[str]] = None,
        original_query: Optional[str] = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """ë‹¨ì¼ semantic ê²€ìƒ‰ ì‹¤í–‰ (Multi-Queryìš©)"""
        if not query or not query.strip():
            return [], 0
        
        try:
            results, count = self.search_handler.semantic_search(
                query,
                k=k,
                extracted_keywords=extracted_keywords
            )
            return results, count
        except Exception as e:
            self.logger.warning(f"Single semantic search failed for '{query[:30]}...': {e}")
            return [], 0

    def execute_semantic_search(
        self,
        optimized_queries: Dict[str, Any],
        search_params: Dict[str, Any],
        original_query: str = "",
        extracted_keywords: Optional[List[str]] = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤í–‰"""
        self.logger.info("ğŸ” [EXECUTE_SEMANTIC_SEARCH] ë©”ì„œë“œ í˜¸ì¶œë¨")
        self.logger.info(f"ğŸ” [EXECUTE_SEMANTIC_SEARCH] original_query: {original_query[:50] if original_query else 'N/A'}...")
        semantic_results = []
        semantic_count = 0

        semantic_query = optimized_queries.get("semantic_query", "") if optimized_queries else ""
        semantic_k = search_params.get("semantic_k", WorkflowConstants.SEMANTIC_SEARCH_K) if search_params else WorkflowConstants.SEMANTIC_SEARCH_K
        expanded_keywords = optimized_queries.get("expanded_keywords", []) if optimized_queries else []
        
        # ë¹ˆ ì¿¼ë¦¬ ê²€ì¦: semantic_queryê°€ ë¹„ì–´ìˆìœ¼ë©´ original_query ì‚¬ìš©, ë‘˜ ë‹¤ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if not semantic_query or not str(semantic_query).strip():
            if original_query and original_query.strip():
                semantic_query = original_query
                if optimized_queries:
                    optimized_queries["semantic_query"] = original_query
                self.logger.info(f"ğŸ” [EXECUTE_SEMANTIC_SEARCH] semantic_queryê°€ ë¹„ì–´ìˆì–´ original_query ì‚¬ìš©: '{original_query[:50]}...'")
            else:
                self.logger.warning("âš ï¸ [EXECUTE_SEMANTIC_SEARCH] semantic_queryì™€ original_queryê°€ ëª¨ë‘ ë¹„ì–´ìˆì–´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return [], 0
        
        # ê°œì„ : textToSQL ë¼ìš°íŒ… í™•ì¸ ë° ì ìš© (ìš°ì„ ìˆœìœ„ 1)
        if original_query and original_query.strip():
            from core.agents.legal_data_connector_v2 import route_query, LegalDataConnectorV2
            route = route_query(original_query)
            self.logger.info(f"ğŸ” [TEXT2SQL SEMANTIC] route_query result: '{route}' for query: '{original_query[:50]}...'")
            if route == "text2sql":
                self.logger.info(f"ğŸ” [TEXT2SQL SEMANTIC] Detected text2sql route for semantic search: '{original_query[:50]}...'")
                try:
                    data_connector = LegalDataConnectorV2()
                    text2sql_results = data_connector.search_documents(original_query, limit=semantic_k)
                    if text2sql_results:
                        semantic_results.extend(text2sql_results)
                        semantic_count += len(text2sql_results)
                        self.logger.info(f"âœ… [TEXT2SQL SEMANTIC] {len(text2sql_results)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì„±ê³µ (semantic_resultsì— ì¶”ê°€)")
                    else:
                        self.logger.warning(f"âš ï¸ [TEXT2SQL SEMANTIC] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ [TEXT2SQL SEMANTIC] ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        if extracted_keywords is None:
            extracted_keywords = []

        self.logger.info(
            f"ğŸ” [QUERY USAGE] semantic_query from optimized_queries: '{semantic_query[:100]}...' "
            f"(length={len(semantic_query)}, expanded_keywords_count={len(expanded_keywords) if expanded_keywords else 0})"
        )
        if expanded_keywords:
            self.logger.info(
                f"ğŸ” [QUERY USAGE] expanded_keywords: {expanded_keywords[:10]} "
                f"(total={len(expanded_keywords)}, included_in_query={len([t for t in expanded_keywords if t in semantic_query])})"
            )

        self.logger.info(
            f"ğŸ” [DEBUG] _execute_semantic_search_internal received: extracted_keywords={len(extracted_keywords)} (type: {type(extracted_keywords).__name__}), query='{semantic_query[:50]}...', k={semantic_k}"
        )

        self.logger.info(
            f"ğŸ” [DEBUG] Executing semantic search: query='{semantic_query[:50]}...', k={semantic_k}, original_query='{original_query[:50] if original_query else 'N/A'}...', extracted_keywords={len(extracted_keywords)}"
        )

        enhanced_semantic_query = semantic_query
        if extracted_keywords and len(extracted_keywords) > 0:
            core_keywords = []
            for kw in extracted_keywords[:5]:
                if isinstance(kw, str):
                    if any(term in kw for term in ["ë²•", "ì¡°", "ì œ", "ë¯¼ë²•", "í˜•ë²•", "ìƒë²•", "ì„ëŒ€ì°¨", "ê³„ì•½"]):
                        core_keywords.insert(0, kw)
                    else:
                        core_keywords.append(kw)

            if core_keywords:
                query_keywords = set(semantic_query.split())
                new_keywords = [kw for kw in core_keywords if kw not in query_keywords]
                if new_keywords:
                    enhanced_semantic_query = f"{semantic_query} {' '.join(new_keywords[:3])}"
                    self.logger.info(
                        f"ğŸ” [QUERY ENHANCEMENT] Enhanced semantic query: "
                        f"original='{semantic_query[:80]}...', "
                        f"enhanced='{enhanced_semantic_query[:100]}...', "
                        f"added_keywords={new_keywords[:3]}"
                    )
                else:
                    self.logger.info(
                        f"ğŸ” [QUERY ENHANCEMENT] No new keywords to add (all keywords already in query): "
                        f"query='{semantic_query[:80]}...'"
                    )
            else:
                self.logger.debug(f"ğŸ” [QUERY ENHANCEMENT] No core keywords extracted from extracted_keywords")
        else:
            self.logger.info(
                f"ğŸ” [QUERY ENHANCEMENT] Using original semantic_query (no extracted_keywords): "
                f"'{semantic_query[:100]}...'"
            )

        main_semantic, main_count = self.search_handler.semantic_search(
            enhanced_semantic_query,
            k=semantic_k,
            extracted_keywords=extracted_keywords
        )
        semantic_results.extend(main_semantic)
        semantic_count += main_count

        self.logger.info(
            f"ğŸ” [DEBUG] Main semantic search: {main_count} results (query: '{enhanced_semantic_query[:50]}...')"
        )

        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬: ë©”ì¸ ê²€ìƒ‰ë§Œìœ¼ë¡œ ì¶©ë¶„í•œ ê²½ìš°
        max_results_threshold = semantic_k * 3
        if len(semantic_results) >= max_results_threshold:
            self.logger.info(
                f"â­ï¸ [EARLY EXIT] Main semantic search sufficient: "
                f"{len(semantic_results)} >= {max_results_threshold}, skipping additional searches"
            )
            return semantic_results, semantic_count

        if original_query and original_query.strip():
            enhanced_original_query = original_query
            if extracted_keywords and len(extracted_keywords) > 0:
                core_keywords = [str(kw) for kw in extracted_keywords[:3] if isinstance(kw, str)]
                if core_keywords:
                    query_keywords = set(original_query.split())
                    new_keywords = [kw for kw in core_keywords if kw not in query_keywords]
                    if new_keywords:
                        enhanced_original_query = f"{original_query} {' '.join(new_keywords[:2])}"

            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬: original_query ê²€ìƒ‰ ì „ í™•ì¸
            if len(semantic_results) >= max_results_threshold:
                self.logger.info(
                    f"â­ï¸ [EARLY EXIT] Skipping original query search: "
                    f"{len(semantic_results)} >= {max_results_threshold}"
                )
            else:
                original_semantic, original_count = self.search_handler.semantic_search(
                    enhanced_original_query,
                    k=semantic_k // 2,
                    extracted_keywords=extracted_keywords
                )
                semantic_results.extend(original_semantic)
                semantic_count += original_count
                self.logger.info(
                    f"ğŸ” [DEBUG] Original query semantic search: {original_count} results (query: '{enhanced_original_query[:50]}...')"
                )
                print(f"[DEBUG] _execute_semantic_search_internal: Added {original_count} results from original query search")
                
                # ë‹¤ì‹œ ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                if len(semantic_results) >= max_results_threshold:
                    self.logger.info(
                        f"â­ï¸ [EARLY EXIT] After original query search: "
                        f"{len(semantic_results)} >= {max_results_threshold}, skipping multi-query"
                    )
                    return semantic_results, semantic_count

        # Multi-Query Retrieval ì ìš© (LLM ê¸°ë°˜ ì§ˆë¬¸ ì¬ì‘ì„±)
        # ê°œì„ : ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ë° ë³‘ë ¬ ì‹¤í–‰
        multi_queries = optimized_queries.get("multi_queries", [])
        min_results_threshold = semantic_k  # ìµœì†Œ ê²°ê³¼ ìˆ˜
        max_results_threshold = semantic_k * 3  # ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ
        
        # ì¡°ê¸° ì¢…ë£Œ: ì´ë¯¸ ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë©€í‹° ì¿¼ë¦¬ ìŠ¤í‚µ
        if len(semantic_results) >= max_results_threshold:
            self.logger.info(f"â­ï¸ [MULTI-QUERY] Skipping multi-query: already have {len(semantic_results)} results (threshold: {max_results_threshold})")
        elif multi_queries and len(multi_queries) > 1:
            # ê°œì„ : Multi-Query ë³‘ë ¬ ì‹¤í–‰
            # ê°œì„ : Multi-Query ë³‘ë ¬ ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ìŠ¤í‚µ)
            max_semantic_results_before_multi = semantic_k * 2  # Multi-Query ì „ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            if len(semantic_results) >= max_semantic_results_before_multi:
                self.logger.info(
                    f"âš¡ [PERFORMANCE] Skipping multi-query search "
                    f"(already have {len(semantic_results)} results, threshold={max_semantic_results_before_multi})"
                )
                multi_queries_to_process = []
            else:
                multi_queries_to_process = multi_queries[1:]  # ì²« ë²ˆì§¸ëŠ” ì´ë¯¸ ì²˜ë¦¬ë¨
                # ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜ ì œí•œ (ì„±ëŠ¥ ìµœì í™”)
                max_multi_queries = min(len(multi_queries_to_process), 2)  # ìµœëŒ€ 2ê°œë¡œ ê°ì†Œ (3 â†’ 2)
                multi_queries_to_process = multi_queries_to_process[:max_multi_queries]
            
            if multi_queries_to_process:
                print(f"[MULTI-QUERY] Found {len(multi_queries)} queries, processing {len(multi_queries_to_process)} in parallel...", flush=True, file=sys.stdout)
                self.logger.info(f"ğŸ” [MULTI-QUERY] Found {len(multi_queries)} queries, processing {len(multi_queries_to_process)} in parallel...")
                
                # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ seen_ids ë° ë‚´ìš© ìœ ì‚¬ë„ ì¶”ì 
                seen_ids = set()
                seen_contents = {}  # content_hash -> doc
                
                # ì›ë³¸ ì¿¼ë¦¬ ê²°ê³¼ì˜ IDì™€ ë‚´ìš© í•´ì‹œ ìˆ˜ì§‘
                for doc in semantic_results:
                    doc_id = doc.get("id") or doc.get("doc_id") or doc.get("document_id")
                    if doc_id:
                        seen_ids.add(doc_id)
                    # ë‚´ìš© í•´ì‹œë¡œë„ ì¤‘ë³µ í™•ì¸
                    content = doc.get("content") or doc.get("text", "")
                    if content:
                        import hashlib
                        content_hash = hashlib.md5(content[:200].encode('utf-8')).hexdigest()
                        if content_hash not in seen_contents:
                            seen_contents[content_hash] = doc
                from concurrent.futures import as_completed
                import threading
                
                # ìŠ¤ë ˆë“œ ì•ˆì „ì„ ìœ„í•œ ë½
                results_lock = threading.Lock()
                
                def process_multi_query(mq: str) -> List[Dict[str, Any]]:
                    """ë‹¨ì¼ Multi-Query ì²˜ë¦¬ í•¨ìˆ˜"""
                    if not mq or not mq.strip() or mq == semantic_query:
                        return []
                    
                    try:
                        # ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ ìˆ˜ ì œí•œ
                        mq_semantic, mq_count = self.search_handler.semantic_search(
                            mq,
                            k=max(5, semantic_k // 3),  # ìµœì†Œ 5ê°œ, ìµœëŒ€ semantic_k // 3
                            extracted_keywords=extracted_keywords
                        )
                        return mq_semantic
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ [MULTI-QUERY] Query '{mq[:30]}...' failed: {e}")
                        return []
                
                # ë³‘ë ¬ ì‹¤í–‰
                multi_query_results = []
                with ThreadPoolExecutor(max_workers=min(len(multi_queries_to_process), 4)) as executor:
                    # ëª¨ë“  Multi-Query ì‘ì—… ì œì¶œ
                    future_to_query = {
                        executor.submit(process_multi_query, mq): mq 
                        for mq in multi_queries_to_process
                    }
                    
                    # ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
                    for future in as_completed(future_to_query, timeout=20):
                        query = future_to_query[future]
                        try:
                            mq_semantic = future.result()
                            if mq_semantic:
                                with results_lock:
                                    # ì¡°ê¸° ì¢…ë£Œ í™•ì¸
                                    if len(semantic_results) >= max_results_threshold:
                                        self.logger.info(
                                            f"â­ï¸ [MULTI-QUERY] Early exit: {len(semantic_results)} results "
                                            f"(threshold: {max_results_threshold})"
                                        )
                                        break
                                    
                                    # ì¤‘ë³µ ì œê±° ë° ê²°ê³¼ ì¶”ê°€
                                    added_count = 0
                                    for doc in mq_semantic:
                                        doc_id = doc.get("id") or doc.get("doc_id") or doc.get("document_id")
                                        content = doc.get("content") or doc.get("text", "")
                                        
                                        # ID ê¸°ë°˜ ì¤‘ë³µ í™•ì¸
                                        if doc_id and doc_id in seen_ids:
                                            continue
                                        
                                        # ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ í™•ì¸
                                        is_duplicate = False
                                        if content:
                                            import hashlib
                                            content_hash = hashlib.md5(content[:200].encode('utf-8')).hexdigest()
                                            if content_hash in seen_contents:
                                                existing_doc = seen_contents[content_hash]
                                                existing_content = existing_doc.get("content") or existing_doc.get("text", "")
                                                if len(content) > 0 and len(existing_content) > 0:
                                                    common_chars = len(set(content[:100]) & set(existing_content[:100]))
                                                    similarity = common_chars / max(len(set(content[:100])), len(set(existing_content[:100])), 1)
                                                    if similarity > 0.8:
                                                        is_duplicate = True
                                            else:
                                                seen_contents[content_hash] = doc
                                        
                                        if not is_duplicate:
                                            semantic_results.append(doc)
                                            if doc_id:
                                                seen_ids.add(doc_id)
                                            added_count += 1
                                    
                                    if added_count > 0:
                                        self.logger.info(
                                            f"ğŸ” [MULTI-QUERY] Query '{query[:30]}...' added {added_count} unique results"
                                        )
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ [MULTI-QUERY] Query '{query[:30]}...' processing failed: {e}")
            
            # ìµœì¢… ê²°ê³¼ ìˆ˜ ì—…ë°ì´íŠ¸
            semantic_count = len(semantic_results)
            self.logger.info(
                f"âœ… [MULTI-QUERY] Parallel processing completed: {semantic_count} total results "
                f"(from {len(multi_queries_to_process)} queries)"
            )
        
        # í‚¤ì›Œë“œ ì¿¼ë¦¬ë¡œ ì¶”ê°€ ì˜ë¯¸ì  ê²€ìƒ‰ (Multi-Queryê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•œ ê²½ìš°)
        # ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ ìˆ˜ê°€ ì´ë¯¸ ì¶©ë¶„í•˜ë©´ ìŠ¤í‚µ
        max_semantic_results = semantic_k * 3  # ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ (ì˜ˆ: 12 * 3 = 36)
        if len(semantic_results) < max_semantic_results and (not multi_queries or len(multi_queries) <= 1):
            keyword_queries = optimized_queries.get("keyword_queries", [])[:1]  # 2ê°œ â†’ 1ê°œë¡œ ê°ì†Œ
            for i, kw_query in enumerate(keyword_queries, 1):
                if kw_query and kw_query.strip() and kw_query != semantic_query:
                    # ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ ìˆ˜ ì œí•œ
                    kw_semantic, kw_count = self.search_handler.semantic_search(
                        kw_query,
                        k=max(5, semantic_k // 3),  # ìµœì†Œ 5ê°œ, ìµœëŒ€ semantic_k // 3
                        extracted_keywords=extracted_keywords
                    )
                    semantic_results.extend(kw_semantic)
                    semantic_count += kw_count
                    self.logger.info(
                        f"ğŸ” [DEBUG] Keyword-based semantic search #{i}: {kw_count} results (query: '{kw_query[:50]}...')"
                    )
                    print(f"[DEBUG] _execute_semantic_search_internal: Added {kw_count} results from keyword query #{i}")
                    
                    # ì„±ëŠ¥ ìµœì í™”: ê²°ê³¼ ìˆ˜ê°€ ì´ë¯¸ ì¶©ë¶„í•˜ë©´ ì¤‘ë‹¨
                    if len(semantic_results) >= max_semantic_results:
                        self.logger.info(f"âš¡ [PERFORMANCE] Stopping keyword-based search (already have {len(semantic_results)} results)")
                        break

        # Phase 1 + Phase 2: íƒ€ì…ë³„ ë³„ë„ ê²€ìƒ‰ ìˆ˜í–‰ ë° ì¿¼ë¦¬ ë‹¤ë³€í™” ì ìš© (íƒ€ì… ë‹¤ì–‘ì„± ê°œì„ )
        # ê°œì„ : íƒ€ì…ë³„ ê²€ìƒ‰ ì „ ì¡°ê±´ ì²´í¬ (ì¡°ê¸° ìŠ¤í‚µ)
        max_semantic_results = semantic_k * 3  # ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ
        
        # ì¡°ê±´ 1: ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ìŠ¤í‚µ (60% ì´ìƒìœ¼ë¡œ ì™„í™”í•˜ì—¬ ë” ë¹¨ë¦¬ ìŠ¤í‚µ)
        should_skip_type_diversity = False
        if len(semantic_results) >= max_semantic_results * 0.6:
            self.logger.info(
                f"âš¡ [PERFORMANCE] Skipping type diversity search "
                f"(already have {len(semantic_results)} results, threshold={max_semantic_results * 0.6:.0f})"
            )
            should_skip_type_diversity = True
        else:
            # ì¡°ê±´ 2: íƒ€ì… ë¶„í¬ í™•ì¸
            def _calculate_type_distribution(docs):
                """íƒ€ì… ë¶„í¬ ê³„ì‚°"""
                type_counts = {}
                for doc in docs:
                    doc_type = (
                        doc.get("type") or
                        doc.get("source_type") or
                        (doc.get("metadata", {}).get("source_type") if isinstance(doc.get("metadata"), dict) else "") or
                        "unknown"
                    )
                    # íƒ€ì… ë§¤í•‘
                    type_mapping = {
                        "statute_article": "statute",
                        "case_paragraph": "case",
                        "decision_paragraph": "decision",
                        "interpretation_paragraph": "interpretation"
                    }
                    mapped_type = type_mapping.get(doc_type, doc_type)
                    type_counts[mapped_type] = type_counts.get(mapped_type, 0) + 1
                return type_counts
            
            type_distribution = _calculate_type_distribution(semantic_results)
            
            # ì´ë¯¸ 2ê°œ ì´ìƒ íƒ€ì…ì´ë©´ ìŠ¤í‚µ (3ê°œ â†’ 2ê°œë¡œ ì™„í™”)
            if len(type_distribution) >= 2:
                self.logger.info(
                    f"âš¡ [PERFORMANCE] Skipping type diversity search "
                    f"(sufficient type diversity: {len(type_distribution)} types)"
                )
                should_skip_type_diversity = True
        
        type_specific_results = {}
        type_specific_count = 0
        
        if not should_skip_type_diversity:
            document_types = {
                "statute_article": "statute",
                "case_paragraph": "case",
                "decision_paragraph": "decision",
                "interpretation_paragraph": "interpretation"
            }
            
            # semantic_search_engine í™•ì¸ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
            semantic_engine = None
        print(f"[TYPE DIVERSITY] semantic_search_engine í™•ì¸ ì‹œì‘")
        print(f"[TYPE DIVERSITY] self.semantic_search_engine: {self.semantic_search_engine is not None}")
        self.logger.info(f"ğŸ” [TYPE DIVERSITY] semantic_search_engine í™•ì¸ ì‹œì‘")
        self.logger.info(f"ğŸ” [TYPE DIVERSITY] self.semantic_search_engine: {self.semantic_search_engine is not None}")
        
        # SemanticSearchEngineV2 ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        def is_semantic_search_engine(obj):
            """SemanticSearchEngineV2 ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸"""
            if obj is None:
                return False
            # íƒ€ì… ì´ë¦„ìœ¼ë¡œ í™•ì¸ (import ì—†ì´)
            type_name = type(obj).__name__
            if type_name == 'SemanticSearchEngineV2':
                return True
            # hasattrë¡œ search ë©”ì„œë“œ í™•ì¸
            if hasattr(obj, 'search') and callable(getattr(obj, 'search', None)):
                # í•¨ìˆ˜ê°€ ì•„ë‹Œ ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸
                if not callable(obj) or hasattr(obj, '__class__'):
                    return True
            return False
        
        if self.semantic_search_engine and is_semantic_search_engine(self.semantic_search_engine):
            semantic_engine = self.semantic_search_engine
            self.logger.info(f"âœ… [TYPE DIVERSITY] semantic_search_engine from self: {type(semantic_engine).__name__}")
        elif hasattr(self.search_handler, 'semantic_search_engine') and self.search_handler.semantic_search_engine:
            candidate = self.search_handler.semantic_search_engine
            if is_semantic_search_engine(candidate):
                semantic_engine = candidate
                self.logger.info(f"âœ… [TYPE DIVERSITY] semantic_search_engine from search_handler: {type(semantic_engine).__name__}")
            else:
                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler.semantic_search_engine is not a valid engine: {type(candidate).__name__}")
        elif hasattr(self.search_handler, 'semantic_search') and self.search_handler.semantic_search:
            candidate = self.search_handler.semantic_search
            # semantic_searchê°€ í•¨ìˆ˜ì¸ì§€ í™•ì¸
            if callable(candidate) and not is_semantic_search_engine(candidate):
                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler.semantic_search is a function, not an engine instance")
            elif is_semantic_search_engine(candidate):
                semantic_engine = candidate
                self.logger.info(f"âœ… [TYPE DIVERSITY] semantic_search_engine from search_handler.semantic_search: {type(semantic_engine).__name__}")
            else:
                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler.semantic_search is not a valid engine: {type(candidate).__name__}")
        else:
            self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] semantic_search_engine not found")
            self.logger.warning(f"   - self.semantic_search_engine: {self.semantic_search_engine} ({type(self.semantic_search_engine).__name__ if self.semantic_search_engine else 'None'})")
            self.logger.warning(f"   - search_handler.semantic_search_engine: {getattr(self.search_handler, 'semantic_search_engine', 'N/A')}")
            self.logger.warning(f"   - search_handler.semantic_search: {getattr(self.search_handler, 'semantic_search', 'N/A')}")
        
        print(f"[TYPE DIVERSITY] semantic_engine í™•ì¸ ê²°ê³¼: {semantic_engine is not None}")
        self.logger.info(f"ğŸ” [TYPE DIVERSITY] semantic_engine í™•ì¸ ê²°ê³¼: {semantic_engine is not None}")
        
        if semantic_engine and not should_skip_type_diversity:
            print(f"[TYPE DIVERSITY] semantic_engine ë°œê²¬, íƒ€ì…ë³„ ê²€ìƒ‰ ì§„í–‰")
            self.logger.info("âœ… [TYPE DIVERSITY] semantic_engine ë°œê²¬, íƒ€ì…ë³„ ê²€ìƒ‰ ì§„í–‰")
            # Phase 2: QueryDiversifierë¡œ íƒ€ì…ë³„ ì¿¼ë¦¬ ìƒì„±
            try:
                diversified_queries = self.query_diversifier.diversify_search_queries(original_query or enhanced_semantic_query)
                self.logger.info(
                    f"ğŸ” [TYPE DIVERSITY] ë‹¤ë³€í™”ëœ ì¿¼ë¦¬ ìƒì„±: "
                    f"statute={len(diversified_queries.get('statute', []))}, "
                    f"case={len(diversified_queries.get('case', []))}, "
                    f"decision={len(diversified_queries.get('decision', []))}, "
                    f"interpretation={len(diversified_queries.get('interpretation', []))}"
                )
            except Exception as e:
                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] ì¿¼ë¦¬ ë‹¤ë³€í™” ì‹¤íŒ¨: {e}")
                diversified_queries = {}
            
            print(f"[TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì‹œì‘ (ë³‘ë ¬ ì‹¤í–‰)")
            print(f"[TYPE DIVERSITY] ê²€ìƒ‰í•  íƒ€ì…: {list(document_types.keys())}")
            self.logger.info("ğŸ” [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì‹œì‘ (ë³‘ë ¬ ì‹¤í–‰)")
            self.logger.info(f"ğŸ” [TYPE DIVERSITY] ê²€ìƒ‰í•  íƒ€ì…: {list(document_types.keys())}")
            
            # íƒ€ì…ë³„ ê²€ìƒ‰ ë³‘ë ¬í™”
            def search_by_type(doc_type, query_type):
                """íƒ€ì…ë³„ ê²€ìƒ‰ í•¨ìˆ˜ (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
                print(f"[TYPE DIVERSITY] {doc_type} ê²€ìƒ‰ ì‹œì‘ (query_type={query_type})")
                self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type} ê²€ìƒ‰ ì‹œì‘ (query_type={query_type})")
                try:
                    # Phase 2: íƒ€ì…ë³„ ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©
                    type_queries = diversified_queries.get(query_type, [])
                    search_query = enhanced_semantic_query  # ê¸°ë³¸ ì¿¼ë¦¬
                    
                    # íƒ€ì…ë³„ ìµœì í™”ëœ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                    if type_queries:
                        search_query = type_queries[0]  # ì²« ë²ˆì§¸ ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©: '{search_query[:50]}...'")
                    else:
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: '{search_query[:50]}...'")
                    
                    # ìš°ì„ ìˆœìœ„ 5: íƒ€ì…ë³„ ê²€ìƒ‰ ê°•í™” (ì„±ëŠ¥ ìµœì í™”: k ê°’ ê°ì†Œ)
                    k_per_type = 15  # 20 â†’ 15ë¡œ ê°ì†Œ (ì„±ëŠ¥ ê°œì„ )
                    min_score_by_type = {
                        "statute_article": 0.4,  # ë²•ë ¹ ì¡°ë¬¸: ë‚®ì€ ì„ê³„ê°’
                        "case_paragraph": 0.5,
                        "decision_paragraph": 0.5,
                        "interpretation_paragraph": 0.5
                    }
                    min_score = min_score_by_type.get(doc_type, 0.5)
                    
                    self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type} ê²€ìƒ‰ ì‹œì‘ (k={k_per_type}, threshold={min_score}, source_types=[{doc_type}])")
                    type_results = semantic_engine.search(
                        search_query,
                        k=k_per_type,  # k * 2 â†’ kë¡œ ê°ì†Œ (ì„±ëŠ¥ ê°œì„ )
                        source_types=[doc_type],  # íƒ€ì…ë³„ í•„í„° ì ìš©
                        similarity_threshold=min_score,  # íƒ€ì…ë³„ ìµœì†Œ ì ìˆ˜
                        min_results=1,  # ìµœì†Œ 1ê°œëŠ” ë³´ì¥
                        disable_retry=False  # ì¬ì‹œë„ ë¡œì§ í™œì„±í™” (ì„ê³„ê°’ ìë™ ì¡°ì •)
                    )
                    
                    # í’ˆì§ˆ í•„í„°ë§ (íƒ€ì…ë³„ ìµœì†Œ ì ìˆ˜)
                    if type_results:
                        filtered_results = [
                            doc for doc in type_results
                            if doc.get("similarity", doc.get("relevance_score", 0.0)) >= min_score
                        ]
                        # ìƒìœ„ k_per_typeê°œ ì„ íƒ
                        type_results = filtered_results[:k_per_type]
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: {len(type_results)}ê°œ ê²°ê³¼ (í•„í„°ë§ í›„)")
                    
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë” ì¼ë°˜ì ì¸ ì¿¼ë¦¬ë¡œ ì¬ì‹œë„
                    if not type_results:
                        # ì›ë³¸ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì—¬ ì¬ì‹œë„
                        core_keywords = original_query.split()[:3] if original_query else search_query.split()[:3]
                        fallback_query = " ".join(core_keywords)
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: í´ë°± ì¿¼ë¦¬ë¡œ ì¬ì‹œë„: '{fallback_query}'")
                        try:
                            type_results = semantic_engine.search(
                                fallback_query,
                                k=20,
                                source_types=[doc_type],
                                similarity_threshold=0.0,  # ìµœì†Œ ì„ê³„ê°’
                                min_results=1,
                                disable_retry=False  # ì¬ì‹œë„ ë¡œì§ í™œì„±í™”
                            )
                            if type_results:
                                self.logger.info(f"âœ… [TYPE DIVERSITY] {doc_type}: í´ë°± ì¿¼ë¦¬ë¡œ {len(type_results)}ê°œ ê²€ìƒ‰ ì„±ê³µ")
                        except Exception as e:
                            self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type} í´ë°± ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    
                    # ê°œì„ : ì¼ë°˜ í‚¤ì›Œë“œ ì¬ì‹œë„ ì œê±° (3ë‹¨ê³„ â†’ 2ë‹¨ê³„ë¡œ ë‹¨ìˆœí™”)
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´
                    self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ìµœì¢… {len(type_results)}ê°œ ê²€ìƒ‰ë¨")
                    
                    # ìµœì¢… ë°©ì•ˆ: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ íƒ€ì…ë³„ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´
                    if not type_results:
                        self.logger.info(f"ğŸ” [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´ ì‹œë„")
                        try:
                            type_results = self._get_type_sample(semantic_engine, doc_type, k=2)
                            if type_results:
                                print(f"[TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ìœ¼ë¡œ {len(type_results)}ê°œ ê°€ì ¸ì˜´")
                                self.logger.info(f"âœ… [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ìœ¼ë¡œ {len(type_results)}ê°œ ê°€ì ¸ì˜´")
                                # ìƒ˜í”Œë§ëœ ë¬¸ì„œ ìƒì„¸ ë¡œê·¸
                                for idx, sample_doc in enumerate(type_results, 1):
                                    self.logger.debug(
                                        f"   ìƒ˜í”Œ {idx}: id={sample_doc.get('id')}, "
                                        f"source_type={sample_doc.get('source_type')}, "
                                        f"type={sample_doc.get('type')}, "
                                        f"relevance_score={sample_doc.get('relevance_score')}"
                                    )
                            else:
                                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ ê²°ê³¼ë„ ì—†ìŒ")
                        except Exception as e:
                            self.logger.error(f"âŒ [TYPE DIVERSITY] {doc_type} ìƒ˜í”Œë§ ì‹¤íŒ¨: {e}")
                            import traceback
                            self.logger.debug(f"ìƒ˜í”Œë§ ì˜ˆì™¸ ìƒì„¸: {traceback.format_exc()}")
                    
                    return doc_type, type_results
                except Exception as e:
                    self.logger.error(f"âŒ [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì‹¤íŒ¨ ({doc_type}): {e}")
                    import traceback
                    self.logger.debug(f"íƒ€ì…ë³„ ê²€ìƒ‰ ì˜ˆì™¸ ìƒì„¸: {traceback.format_exc()}")
                    return doc_type, []
            
            # ë³‘ë ¬ ì‹¤í–‰
            with ThreadPoolExecutor(max_workers=len(document_types)) as executor:
                futures = {
                    executor.submit(search_by_type, doc_type, query_type): doc_type
                    for doc_type, query_type in document_types.items()
                }
                
                for future in futures:
                    doc_type = futures[future]
                    try:
                        result_doc_type, type_results = future.result(timeout=15)  # 30ì´ˆ â†’ 15ì´ˆë¡œ ìµœì í™”
                        if type_results:
                            type_specific_results[result_doc_type] = type_results
                            semantic_results.extend(type_results)
                            type_specific_count += len(type_results)
                            print(f"[TYPE DIVERSITY] {result_doc_type}: {len(type_results)}ê°œ ê²€ìƒ‰ ì„±ê³µ (ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€ë¨, ì´ semantic_results: {len(semantic_results)}ê°œ)")
                            self.logger.info(
                                f"âœ… [TYPE DIVERSITY] {result_doc_type}: {len(type_results)}ê°œ ê²€ìƒ‰ ì„±ê³µ "
                                f"(ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€ë¨, ì´ semantic_results: {len(semantic_results)}ê°œ)"
                            )
                        else:
                            print(f"[TYPE DIVERSITY] {result_doc_type}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                            self.logger.warning(
                                f"âš ï¸ [TYPE DIVERSITY] {result_doc_type}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë°ì´í„° ì—†ìŒ ë˜ëŠ” ì¿¼ë¦¬ ê´€ë ¨ì„± ë‚®ìŒ)"
                            )
                    except Exception as e:
                        self.logger.error(f"âŒ [TYPE DIVERSITY] {doc_type} ë³‘ë ¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                
                # íƒ€ì…ë³„ ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
                if 'type_specific_count' in locals() and type_specific_count > 0:
                    self.logger.info(
                        f"âœ… [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì™„ë£Œ: ì´ {type_specific_count}ê°œ ì¶”ê°€ "
                        f"(ì´ semantic_results: {len(semantic_results)}ê°œ)"
                    )
        elif not semantic_engine:
            self.logger.warning("âš ï¸ [TYPE DIVERSITY] semantic_search_engineì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íƒ€ì…ë³„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] semantic_search_engine í™•ì¸: self.semantic_search_engine={self.semantic_search_engine is not None}")
            if hasattr(self, 'search_handler'):
                self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler í™•ì¸: {self.search_handler is not None}")
                if self.search_handler:
                    self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler.semantic_search í™•ì¸: {hasattr(self.search_handler, 'semantic_search')}")
                    if hasattr(self.search_handler, 'semantic_search_engine'):
                        self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] search_handler.semantic_search_engine í™•ì¸: {self.search_handler.semantic_search_engine is not None}")
        
        semantic_count += type_specific_count
        
        if type_specific_count > 0:
            type_distribution = dict((k, len(v)) for k, v in type_specific_results.items())
            self.logger.info(
                f"âœ… [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ì™„ë£Œ: ì´ {type_specific_count}ê°œ ì¶”ê°€ "
                f"(íƒ€ì…ë³„ ë¶„í¬: {type_distribution})"
            )
            # interpretation_paragraph í™•ì¸
            if "interpretation_paragraph" in type_specific_results:
                self.logger.info(
                    f"âœ… [TYPE DIVERSITY] interpretation_paragraph: {len(type_specific_results['interpretation_paragraph'])}ê°œ "
                    f"ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë¨"
                )
            else:
                self.logger.warning(
                    f"âš ï¸ [TYPE DIVERSITY] interpretation_paragraph: ê²€ìƒ‰ ê²°ê³¼ì— ì—†ìŒ "
                    f"(type_specific_results keys: {list(type_specific_results.keys())})"
                )
        else:
            self.logger.info("âš ï¸ [TYPE DIVERSITY] íƒ€ì…ë³„ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë°ì´í„° ë¶ˆê· í˜• ë˜ëŠ” ê²€ìƒ‰ ì‹¤íŒ¨)")

        self.logger.info(
            f"ğŸ” [DEBUG] Total semantic search results: {semantic_count} (unique: {len(semantic_results)})"
        )
        print(f"[DEBUG] SEMANTIC SEARCH INTERNAL: Total={semantic_count}, Unique={len(semantic_results)}")

        search_queries_used = []
        if semantic_query:
            search_queries_used.append(f"semantic_query({len(semantic_query)} chars)")
        if original_query:
            search_queries_used.append(f"original_query({len(original_query)} chars)")
        keyword_queries_used = optimized_queries.get("keyword_queries", [])[:2]
        if keyword_queries_used:
            search_queries_used.append(f"keyword_queries({len(keyword_queries_used)} queries)")
        print(f"[DEBUG] SEMANTIC SEARCH INTERNAL: Queries used: {', '.join(search_queries_used)}")

        return semantic_results, semantic_count

    def execute_keyword_search(
        self,
        optimized_queries: Dict[str, Any],
        search_params: Dict[str, Any],
        query_type_str: str,
        legal_field: str,
        extracted_keywords: List[str],
        original_query: str = ""
    ) -> Tuple[List[Dict[str, Any]], int]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰"""
        keyword_results = []
        keyword_count = 0

        keyword_queries = optimized_queries.get("keyword_queries", [])
        keyword_limit = search_params.get("keyword_limit", WorkflowConstants.CATEGORY_SEARCH_LIMIT)

        self.logger.info(
            f"ğŸ” [DEBUG] Executing keyword search: {len(keyword_queries)} queries, "
            f"limit={keyword_limit}, field={legal_field}, "
            f"keywords={extracted_keywords[:5] if extracted_keywords else []}, "
            f"original_query='{original_query[:50] if original_query else 'N/A'}...'"
        )

        # ê°œì„ : textToSQL ë¼ìš°íŒ… í™•ì¸ ë° ì ìš©
        from core.agents.legal_data_connector_v2 import route_query, LegalDataConnectorV2
        
        # original_queryì— ëŒ€í•´ ë¼ìš°íŒ… í™•ì¸
        print(f"[TEXT2SQL DEBUG] original_query='{original_query[:50] if original_query else 'EMPTY'}...', has_query={bool(original_query and original_query.strip())}", flush=True, file=sys.stdout)
        self.logger.info(f"ğŸ” [TEXT2SQL DEBUG] original_query='{original_query[:50] if original_query else 'EMPTY'}...', has_query={bool(original_query and original_query.strip())}")
        if original_query and original_query.strip():
            route = route_query(original_query)
            print(f"[TEXT2SQL DEBUG] route_query result: '{route}' for query: '{original_query[:50]}...'", flush=True, file=sys.stdout)
            self.logger.info(f"ğŸ” [TEXT2SQL DEBUG] route_query result: '{route}' for query: '{original_query[:50]}...'")
            if route == "text2sql":
                # textToSQL ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰
                print(f"[TEXT2SQL] Detected text2sql route for query: '{original_query[:50]}...'", flush=True, file=sys.stdout)
                self.logger.info(f"ğŸ” [TEXT2SQL] Detected text2sql route for query: '{original_query[:50]}...'")
                try:
                    data_connector = LegalDataConnectorV2()
                    text2sql_results = data_connector.search_documents(original_query, limit=keyword_limit)
                    if text2sql_results:
                        keyword_results.extend(text2sql_results)
                        keyword_count += len(text2sql_results)
                        print(f"[TEXT2SQL] {len(text2sql_results)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì„±ê³µ", flush=True, file=sys.stdout)
                        self.logger.info(f"âœ… [TEXT2SQL] {len(text2sql_results)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì„±ê³µ")
                    else:
                        print(f"[TEXT2SQL] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", flush=True, file=sys.stdout)
                        self.logger.warning(f"âš ï¸ [TEXT2SQL] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                except Exception as e:
                    print(f"[TEXT2SQL] ê²€ìƒ‰ ì‹¤íŒ¨: {e}", flush=True, file=sys.stdout)
                    self.logger.warning(f"âš ï¸ [TEXT2SQL] ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ê¸°ì¡´ keyword_search_func ë¡œì§ë„ ìœ ì§€ (í•˜ì´ë¸Œë¦¬ë“œ)
            if self.keyword_search_func:
                original_kw_results, original_kw_count = self.keyword_search_func(
                    query=original_query,
                    query_type_str=query_type_str,
                    limit=keyword_limit,
                    legal_field=legal_field,
                    extracted_keywords=extracted_keywords
                )
                keyword_results.extend(original_kw_results)
                keyword_count += original_kw_count
                self.logger.info(
                    f"ğŸ” [DEBUG] Original query keyword search: {original_kw_count} results (query: '{original_query[:50]}...')"
                )

        # keyword_queriesì— ëŒ€í•´ì„œë„ ë¼ìš°íŒ… í™•ì¸
        for i, kw_query in enumerate(keyword_queries, 1):
            if kw_query and kw_query.strip() and kw_query != original_query:
                route = route_query(kw_query)
                if route == "text2sql":
                    self.logger.info(f"ğŸ” [TEXT2SQL] Detected text2sql route for keyword query #{i}: '{kw_query[:50]}...'")
                    try:
                        data_connector = LegalDataConnectorV2()
                        text2sql_results = data_connector.search_documents(kw_query, limit=keyword_limit)
                        if text2sql_results:
                            keyword_results.extend(text2sql_results)
                            keyword_count += len(text2sql_results)
                            self.logger.info(f"âœ… [TEXT2SQL] Query #{i}: {len(text2sql_results)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì„±ê³µ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ [TEXT2SQL] Query #{i} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                
                # ê¸°ì¡´ keyword_search_func ë¡œì§ë„ ìœ ì§€ (í•˜ì´ë¸Œë¦¬ë“œ)
                if self.keyword_search_func:
                    kw_results, kw_count = self.keyword_search_func(
                        query=kw_query,
                        query_type_str=query_type_str,
                        limit=keyword_limit,
                        legal_field=legal_field,
                        extracted_keywords=extracted_keywords
                    )
                    keyword_results.extend(kw_results)
                    keyword_count += kw_count
                    self.logger.info(
                        f"ğŸ” [DEBUG] Keyword search #{i}: {kw_count} results (query: '{kw_query[:50]}...')"
                    )

        self.logger.info(
            f"ğŸ” [DEBUG] Total keyword search results: {keyword_count} (unique: {len(keyword_results)})"
        )
        print(f"[DEBUG] KEYWORD SEARCH INTERNAL: Total={keyword_count}, Unique={len(keyword_results)}")

        return keyword_results, keyword_count

    def fallback_sequential_search(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ìˆœì°¨ ê²€ìƒ‰ ì‹¤í–‰ (í´ë°±) - ì•ˆì „í•œ ì˜¤ë¥˜ ì²˜ë¦¬"""
        semantic_results, semantic_count = [], 0
        keyword_results, keyword_count = [], 0
        
        try:
            self.logger.info("ğŸ”„ ìˆœì°¨ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")

            optimized_queries = self._get_state_value(state, "optimized_queries", {})
            search_params = self._get_state_value(state, "search_params", {})
            query_type_str = self._get_query_type_str(self._get_state_value(state, "query_type", ""))
            legal_field = self._get_state_value(state, "legal_field", "")
            extracted_keywords = optimized_queries.get("expanded_keywords", [])

            original_query = self._get_state_value(state, "query", "")
            if not original_query and "input" in state and isinstance(state.get("input"), dict):
                original_query = state["input"].get("query", "")
            
            # ë¹ˆ ì¿¼ë¦¬ ê²€ì¦ ì¶”ê°€
            if not original_query or not original_query.strip():
                self.logger.error("âŒ ìˆœì°¨ ê²€ìƒ‰: queryê°€ ë¹„ì–´ìˆì–´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.logger.warning("âš ï¸ ìˆœì°¨ ê²€ìƒ‰: queryë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. state êµ¬ì¡° í™•ì¸:")
                self.logger.debug(f"   state keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")
                if "input" in state:
                    self.logger.debug(f"   input type: {type(state.get('input'))}")
                    if isinstance(state.get("input"), dict):
                        self.logger.debug(f"   input keys: {list(state.get('input', {}).keys())}")
                # ë¹ˆ ê²°ê³¼ ë°˜í™˜
                self._set_state_value(state, "semantic_results", [])
                self._set_state_value(state, "keyword_results", [])
                self._set_state_value(state, "semantic_count", 0)
                self._set_state_value(state, "keyword_count", 0)
                return state

            extracted_keywords_for_search = self._get_state_value(state, "extracted_keywords", [])
            
            # ì˜ë¯¸ì  ê²€ìƒ‰ ì‹œë„ (ì˜¤ë¥˜ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰)
            try:
                semantic_results, semantic_count = self.execute_semantic_search(
                    optimized_queries, search_params, original_query, extracted_keywords_for_search
                )
                self.logger.info(f"âœ… ìˆœì°¨ ê²€ìƒ‰: ì˜ë¯¸ì  ê²€ìƒ‰ ì™„ë£Œ ({semantic_count}ê°œ ê²°ê³¼)")
            except Exception as semantic_err:
                self.logger.warning(f"âš ï¸ ìˆœì°¨ ê²€ìƒ‰: ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤íŒ¨: {semantic_err}")
                self.logger.debug(f"   ì˜ë¯¸ì  ê²€ìƒ‰ ì˜¤ë¥˜ ìƒì„¸: {semantic_err}", exc_info=True)
                semantic_results, semantic_count = [], 0

            # í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œë„ (ì˜¤ë¥˜ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰)
            try:
                keyword_results, keyword_count = self.execute_keyword_search(
                    optimized_queries, search_params, query_type_str, legal_field, extracted_keywords, original_query
                )
                self.logger.info(f"âœ… ìˆœì°¨ ê²€ìƒ‰: í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ ({keyword_count}ê°œ ê²°ê³¼)")
            except Exception as keyword_err:
                self.logger.warning(f"âš ï¸ ìˆœì°¨ ê²€ìƒ‰: í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {keyword_err}")
                self.logger.debug(f"   í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜ ìƒì„¸: {keyword_err}", exc_info=True)
                keyword_results, keyword_count = [], 0

            # ê²°ê³¼ ì €ì¥ (ì¼ë¶€ë¼ë„ ì„±ê³µí•˜ë©´ ì €ì¥)
            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "semantic_count", semantic_count)
            self._set_state_value(state, "keyword_count", keyword_count)
            
            # State êµ¬ì¡° ì¼ê´€ì„± í™•ë³´: retrieved_docsë¥¼ í—¬í¼ í•¨ìˆ˜ë¡œ ì €ì¥
            merged_docs = semantic_results + keyword_results
            set_retrieved_docs(state, merged_docs)

            total_results = semantic_count + keyword_count
            if total_results > 0:
                self.logger.info(f"âœ… ìˆœì°¨ ê²€ìƒ‰ ì™„ë£Œ: ì˜ë¯¸ì  {semantic_count}ê°œ, í‚¤ì›Œë“œ {keyword_count}ê°œ (ì´ {total_results}ê°œ)")
            else:
                self.logger.warning(f"âš ï¸ ìˆœì°¨ ê²€ìƒ‰ ì™„ë£Œ: ê²°ê³¼ ì—†ìŒ (ì˜ë¯¸ì  {semantic_count}ê°œ, í‚¤ì›Œë“œ {keyword_count}ê°œ)")

        except Exception as e:
            self.logger.error(f"âŒ ìˆœì°¨ ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            self._handle_error(state, str(e), "ìˆœì°¨ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            # ìµœì†Œí•œì˜ ê²°ê³¼ë¼ë„ ì €ì¥
            self._set_state_value(state, "semantic_results", semantic_results)
            self._set_state_value(state, "keyword_results", keyword_results)
            self._set_state_value(state, "semantic_count", semantic_count)
            self._set_state_value(state, "keyword_count", keyword_count)
            
            # State êµ¬ì¡° ì¼ê´€ì„± í™•ë³´: retrieved_docsë¥¼ í—¬í¼ í•¨ìˆ˜ë¡œ ì €ì¥
            merged_docs = semantic_results + keyword_results
            set_retrieved_docs(state, merged_docs)

        return state

    def _get_state_value(self, state: LegalWorkflowState, key: str, default: Any = None) -> Any:
        """Stateì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
        if self._get_state_value_func:
            return self._get_state_value_func(state, key, default)
        if isinstance(state, dict):
            if key in state:
                return state[key]
            if "search" in state and isinstance(state.get("search"), dict) and key in state["search"]:
                return state["search"][key]
        return default

    def _set_state_value(self, state: LegalWorkflowState, key: str, value: Any) -> None:
        """Stateì— ê°’ ì„¤ì •"""
        if self._set_state_value_func:
            self._set_state_value_func(state, key, value)
        elif isinstance(state, dict):
            if "search" not in state or not isinstance(state.get("search"), dict):
                state["search"] = {}
            state["search"][key] = value

    def _get_query_type_str(self, query_type) -> str:
        """QueryTypeì„ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if self._get_query_type_str_func:
            return self._get_query_type_str_func(query_type)
        if isinstance(query_type, str):
            return query_type
        if hasattr(query_type, 'value'):
            return query_type.value
        return str(query_type) if query_type else ""

    def _determine_search_parameters(
        self,
        query_type: str,
        query_complexity: int,
        keyword_count: int,
        is_retry: bool
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ê²°ì •"""
        if self._determine_search_parameters_func:
            return self._determine_search_parameters_func(query_type, query_complexity, keyword_count, is_retry)
        return {
            "semantic_k": WorkflowConstants.SEMANTIC_SEARCH_K,
            "keyword_limit": WorkflowConstants.CATEGORY_SEARCH_LIMIT,
            "min_relevance": self.config.similarity_threshold if hasattr(self.config, 'similarity_threshold') else 0.5,
            "max_results": WorkflowConstants.MAX_DOCUMENTS
        }

    def _save_metadata_safely(self, state: LegalWorkflowState, key: str, value: Any) -> None:
        """ë©”íƒ€ë°ì´í„° ì•ˆì „í•˜ê²Œ ì €ì¥"""
        if self._save_metadata_safely_func:
            self._save_metadata_safely_func(state, key, value)
        elif isinstance(state, dict):
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            state["metadata"][key] = value

    def _update_processing_time(self, state: LegalWorkflowState, start_time: float) -> None:
        """ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        if self._update_processing_time_func:
            self._update_processing_time_func(state, start_time)
        elif isinstance(state, dict):
            elapsed = time.time() - start_time
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            if "processing_time" not in state["metadata"]:
                state["metadata"]["processing_time"] = 0.0
            state["metadata"]["processing_time"] += elapsed

    def _get_type_sample(self, semantic_engine, doc_type: str, k: int = 2) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • íƒ€ì…ì˜ ëœë¤ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸° (ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        
        Args:
            semantic_engine: SemanticSearchEngineV2 ì¸ìŠ¤í„´ìŠ¤
            doc_type: ë¬¸ì„œ íƒ€ì…
            k: ê°€ì ¸ì˜¬ ìƒ˜í”Œ ìˆ˜
            
        Returns:
            List[Dict[str, Any]]: ìƒ˜í”Œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # DBì—ì„œ í•´ë‹¹ íƒ€ì…ì˜ ëœë¤ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ì„±ëŠ¥ ìµœì í™”: ì¸ë±ìŠ¤ í™œìš©)
            conn = semantic_engine._get_connection()
            
            # ë¨¼ì € í•´ë‹¹ íƒ€ì…ì˜ ë¬¸ì„œ ìˆ˜ í™•ì¸
            count_cursor = conn.execute(
                "SELECT COUNT(*) as count FROM text_chunks WHERE source_type = ? AND text IS NOT NULL AND LENGTH(text) > 50",
                (doc_type,)
            )
            count_row = count_cursor.fetchone()
            total_count = count_row['count'] if count_row else 0
            
            if total_count == 0:
                self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§í•  ë¬¸ì„œ ì—†ìŒ (ì´ 0ê°œ)")
                return []
            
            # ëœë¤ ìƒ˜í”Œë§ (ì„±ëŠ¥ ìµœì í™”: LIMIT ì‚¬ìš©)
            cursor = conn.execute(
                """
                SELECT id, text, source_id, source_type
                FROM text_chunks
                WHERE source_type = ? AND text IS NOT NULL AND LENGTH(text) > 50
                ORDER BY RANDOM()
                LIMIT ?
                """,
                (doc_type, k)
            )
            rows = cursor.fetchall()
            
            if not rows:
                self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] {doc_type}: ìƒ˜í”Œë§ ê²°ê³¼ ì—†ìŒ (ì´ {total_count}ê°œ ì¤‘)")
                return []
            
            self.logger.info(f"âœ… [TYPE DIVERSITY] {doc_type}: {len(rows)}ê°œ ìƒ˜í”Œë§ ì„±ê³µ (ì´ {total_count}ê°œ ì¤‘)")
            
            # ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            samples = []
            for row in rows:
                chunk_id = row['id']
                text = row['text'] or ""
                source_id = row['source_id']
                
                # ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)
                source_meta = {}
                try:
                    if hasattr(semantic_engine, '_get_source_metadata'):
                        source_meta = semantic_engine._get_source_metadata(conn, doc_type, source_id)
                        if not source_meta:
                            # ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ ì‹œ text_chunksì—ì„œ ê¸°ë³¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                            cursor_meta = conn.execute(
                                "SELECT source_type, source_id, text FROM text_chunks WHERE id = ?",
                                (chunk_id,)
                            )
                            row_meta = cursor_meta.fetchone()
                            if row_meta:
                                source_meta = {
                                    "source_type": row_meta['source_type'],
                                    "source_id": row_meta['source_id'],
                                    "text": row_meta['text']
                                }
                except Exception as e:
                    self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ ({doc_type}, source_id={source_id}): {e}")
                    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
                    source_meta = {
                        "source_type": doc_type,
                        "source_id": source_id
                    }
                
                # UnifiedSourceFormatterë¡œ ì¶œì²˜ ì •ë³´ ìƒì„± (ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê°œì„ )
                try:
                    from core.generation.formatters.unified_source_formatter import UnifiedSourceFormatter
                    formatter = UnifiedSourceFormatter()
                    source_info = formatter.format_source(doc_type, source_meta)
                    source_name = source_info.name
                    source_url = source_info.url
                    
                    # source_nameì´ ë¹„ì–´ìˆê±°ë‚˜ ê¸°ë³¸ê°’ì´ë©´ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ ì‹œë„
                    if not source_name or source_name == doc_type:
                        if doc_type == "statute_article":
                            source_name = source_meta.get("statute_name") or source_meta.get("name") or "ë²•ë ¹ ì¡°ë¬¸"
                        elif doc_type == "case_paragraph":
                            source_name = source_meta.get("casenames") or source_meta.get("doc_id") or "íŒë¡€"
                        elif doc_type == "decision_paragraph":
                            source_name = f"{source_meta.get('org', '')} {source_meta.get('doc_id', '')}".strip() or "ê²°ì •ë¡€"
                        elif doc_type == "interpretation_paragraph":
                            source_name = f"{source_meta.get('org', '')} {source_meta.get('title', '')}".strip() or "í•´ì„ë¡€"
                except Exception as e:
                    self.logger.debug(f"âš ï¸ [TYPE DIVERSITY] ì¶œì²˜ ì •ë³´ ìƒì„± ì‹¤íŒ¨ ({doc_type}): {e}")
                    source_name = doc_type
                    source_url = ""
                
                # ê³ ìœ í•œ ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)
                unique_id = f"sample_{doc_type}_{chunk_id}_{source_id}"
                samples.append({
                    "id": unique_id,  # ê³ ìœ  IDë¡œ ë³€ê²½
                    "content_id": unique_id,  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ëŒ€ì²´ ID
                    "text": text,
                    "content": text,
                    "score": 0.3,  # ë‚®ì€ ì ìˆ˜ (ê°•ì œ ìƒ˜í”Œë§)
                    "similarity": 0.3,
                    "type": doc_type,
                    "source_type": doc_type,
                    "source": source_name,
                    "source_url": source_url,
                    "source_id": source_id,
                    "metadata": {
                        "chunk_id": chunk_id,
                        "source_type": doc_type,
                        "source_id": source_id,
                        "text": text,
                        "is_sample": True,  # ìƒ˜í”Œë§ëœ ë¬¸ì„œ í‘œì‹œ
                        "search_type": "type_sample",  # ë©”íƒ€ë°ì´í„°ì—ë„ ì¶”ê°€
                        **source_meta
                    },
                    "relevance_score": 0.3,
                    "search_type": "type_sample"
                })
                self.logger.debug(f"ğŸ” [TYPE DIVERSITY] ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±: id={unique_id}, doc_type={doc_type}, chunk_id={chunk_id}")
            
            return samples
        except Exception as e:
            self.logger.warning(f"âš ï¸ [TYPE DIVERSITY] íƒ€ì… ìƒ˜í”Œë§ ì‹¤íŒ¨ ({doc_type}): {e}")
            import traceback
            self.logger.debug(f"íƒ€ì… ìƒ˜í”Œë§ ì˜ˆì™¸ ìƒì„¸: {traceback.format_exc()}")
            return []

    def _handle_error(self, state: LegalWorkflowState, error_msg: str, context: str) -> None:
        """ì—ëŸ¬ ì²˜ë¦¬"""
        if self._handle_error_func:
            self._handle_error_func(state, error_msg, context)
        else:
            self.logger.error(f"{context}: {error_msg}")
            if isinstance(state, dict):
                if "errors" not in state:
                    state["errors"] = []
                state["errors"].append(f"{context}: {error_msg}")

