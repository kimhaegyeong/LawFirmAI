# -*- coding: utf-8 -*-
"""
Multi-Query Search Agent Node
ë©€í‹° ì§ˆì˜ ìƒì„± + ì—ì´ì „íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ë…¸ë“œ
"""

import json
import time
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    from lawfirm_langgraph.core.utils.logger import get_logger
except ImportError:
    from core.utils.logger import get_logger

try:
    from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
except ImportError:
    from core.workflow.state.state_definitions import LegalWorkflowState

# LangChain imports
try:
    from langchain.tools import tool
    from langchain.agents import AgentExecutor, create_openai_tools_agent
    try:
        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    except ImportError:
        from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    # Mock for when LangChain is not available
    def tool(*args, **kwargs):
        def decorator(func):
            return func
        return decorator

logger = get_logger(__name__)


class MultiQuerySearchAgentNode:
    """ë©€í‹° ì§ˆì˜ ìƒì„± + ì—ì´ì „íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ë…¸ë“œ"""
    
    def __init__(self, workflow_instance, logger_instance=None):
        self.workflow = workflow_instance
        self.logger = logger_instance or logger
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
        self.semantic_search = None
        self.keyword_search = None
        self.hybrid_query_processor = None
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
        self.agentic_agent = None
        self.search_tools = []
        self.llm = None
    
    def _initialize_search_engines(self):
        """ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)"""
        if self.semantic_search is None:
            try:
                from core.search.engines.semantic_search_engine_v2 import SemanticSearchEngineV2
                self.semantic_search = SemanticSearchEngineV2()
                self.logger.debug("âœ… SemanticSearchEngineV2 initialized")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Failed to initialize SemanticSearchEngineV2: {e}")
        
        if self.keyword_search is None:
            try:
                from core.search.connectors.legal_data_connector_v2 import LegalDataConnectorV2
                self.keyword_search = LegalDataConnectorV2()
                self.logger.debug("âœ… LegalDataConnectorV2 initialized")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Failed to initialize LegalDataConnectorV2: {e}")
        
        if self.hybrid_query_processor is None and self.workflow:
            self.hybrid_query_processor = getattr(self.workflow, 'hybrid_query_processor', None)
    
    def _create_postgresql_keyword_search_tool(self):
        """PostgreSQL í‚¤ì›Œë“œ ê²€ìƒ‰ ë„êµ¬"""
        @tool
        def search_postgresql_keywords(query: str, limit: int = 10) -> str:
            '''
            PostgreSQL í‚¤ì›Œë“œ ê²€ìƒ‰: ë²•ë ¹ëª…, ì¡°ë¬¸, íŒë¡€ëª… ë“± ì •í™•í•œ í‚¤ì›Œë“œ ê²€ìƒ‰
            
            ì‚¬ìš© ì‹œê¸°:
            - ì •í™•í•œ ë²•ë ¹ëª…ì´ë‚˜ ì¡°ë¬¸ë²ˆí˜¸ê°€ í¬í•¨ëœ ì§ˆë¬¸
            - íŒë¡€ëª…ì´ë‚˜ ì‚¬ê±´ë²ˆí˜¸ê°€ í¬í•¨ëœ ì§ˆë¬¸
            - íŠ¹ì • í‚¤ì›Œë“œë¡œ ì •í™•íˆ ë§¤ì¹­í•´ì•¼ í•˜ëŠ” ê²½ìš°
            
            Args:
                query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "ë¯¼ë²• ì œ750ì¡°", "ëŒ€ë²•ì› 2020ë‹¤12345")
                limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)
            
            Returns:
                JSON í˜•ì‹ì˜ ê²€ìƒ‰ ê²°ê³¼
            '''
            try:
                if not self.keyword_search:
                    self._initialize_search_engines()
                
                if not self.keyword_search:
                    return json.dumps({
                        "success": False,
                        "error": "Keyword search engine not available",
                        "search_type": "postgresql_keyword"
                    })
                
                results = self.keyword_search.search_documents(query, limit=limit, force_fts=True)
                return json.dumps({
                    "success": True,
                    "search_type": "postgresql_keyword",
                    "query": query,
                    "results": results,
                    "count": len(results)
                }, ensure_ascii=False)
            except Exception as e:
                self.logger.error(f"âŒ [POSTGRESQL-KEYWORD] Error: {e}", exc_info=True)
                return json.dumps({
                    "success": False,
                    "error": str(e),
                    "search_type": "postgresql_keyword"
                })
        
        return search_postgresql_keywords
    
    def _create_vector_index_search_tool(self):
        """ë²¡í„° ì¸ë±ìŠ¤ ê²€ìƒ‰ ë„êµ¬"""
        @tool
        def search_vector_index(query: str, limit: int = 10) -> str:
            '''
            ë²¡í„° ì˜ë¯¸ ê²€ìƒ‰: ì§ˆë¬¸ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ì—¬ ìœ ì‚¬í•œ ë²•ë¥  ë¬¸ì„œ ê²€ìƒ‰
            
            ì‚¬ìš© ì‹œê¸°:
            - ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°
            - í‚¤ì›Œë“œê°€ ëª…í™•í•˜ì§€ ì•Šì§€ë§Œ ì˜ë„ë¥¼ ì´í•´í•´ì•¼ í•˜ëŠ” ê²½ìš°
            - ìœ ì‚¬í•œ ë²•ë¥  ê°œë…ì„ ì°¾ì•„ì•¼ í•˜ëŠ” ê²½ìš°
            
            Args:
                query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "ê³„ì•½ í•´ì§€ ì‚¬ìœ ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”")
                limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)
            
            Returns:
                JSON í˜•ì‹ì˜ ê²€ìƒ‰ ê²°ê³¼
            '''
            try:
                if not self.semantic_search:
                    self._initialize_search_engines()
                
                if not self.semantic_search:
                    return json.dumps({
                        "success": False,
                        "error": "Vector search engine not available",
                        "search_type": "vector_semantic"
                    })
                
                results = self.semantic_search.search(query, k=limit)
                return json.dumps({
                    "success": True,
                    "search_type": "vector_semantic",
                    "query": query,
                    "results": results,
                    "count": len(results)
                }, ensure_ascii=False)
            except Exception as e:
                self.logger.error(f"âŒ [VECTOR-INDEX] Error: {e}", exc_info=True)
                return json.dumps({
                    "success": False,
                    "error": str(e),
                    "search_type": "vector_semantic"
                })
        
        return search_vector_index
    
    def _create_hybrid_search_tool(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë„êµ¬"""
        @tool
        def search_hybrid(query: str, limit: int = 10) -> str:
            '''
            í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ë²¡í„° ê²€ìƒ‰ì„ ëª¨ë‘ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë³‘í•©
            
            ì‚¬ìš© ì‹œê¸°:
            - ì •í™•ì„±ê³¼ í¬ê´„ì„±ì„ ëª¨ë‘ í•„ìš”ë¡œ í•˜ëŠ” ê²½ìš°
            - ë³µì¡í•œ ë²•ë¥  ì§ˆë¬¸
            - ì—¬ëŸ¬ ê´€ì ì—ì„œ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°
            
            Args:
                query: ê²€ìƒ‰ ì¿¼ë¦¬
                limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)
            
            Returns:
                JSON í˜•ì‹ì˜ í†µí•© ê²€ìƒ‰ ê²°ê³¼
            '''
            try:
                if not self.keyword_search or not self.semantic_search:
                    self._initialize_search_engines()
                
                if not self.keyword_search or not self.semantic_search:
                    return json.dumps({
                        "success": False,
                        "error": "Search engines not available",
                        "search_type": "hybrid"
                    })
                
                # ë³‘ë ¬ ê²€ìƒ‰
                with ThreadPoolExecutor(max_workers=2) as executor:
                    keyword_future = executor.submit(
                        self.keyword_search.search_documents, query, limit=limit, force_fts=True
                    )
                    vector_future = executor.submit(
                        self.semantic_search.search, query, k=limit
                    )
                    
                    keyword_results = keyword_future.result(timeout=10.0)
                    vector_results = vector_future.result(timeout=10.0)
                
                # ê²°ê³¼ ë³‘í•© ë° ë¦¬ë­í‚¹
                merged_results = self._merge_and_rerank(
                    keyword_results, vector_results, limit
                )
                
                return json.dumps({
                    "success": True,
                    "search_type": "hybrid",
                    "query": query,
                    "keyword_count": len(keyword_results),
                    "vector_count": len(vector_results),
                    "results": merged_results,
                    "count": len(merged_results)
                }, ensure_ascii=False)
            except Exception as e:
                self.logger.error(f"âŒ [HYBRID] Error: {e}", exc_info=True)
                return json.dumps({
                    "success": False,
                    "error": str(e),
                    "search_type": "hybrid"
                })
        
        return search_hybrid
    
    def _create_multi_query_search_tool(self):
        """ë©€í‹° ì§ˆì˜ ê²€ìƒ‰ ë„êµ¬ (í•µì‹¬ ê¸°ëŠ¥)"""
        @tool
        def search_multi_query(original_query: str, max_queries: int = 3, limit_per_query: int = 5) -> str:
            '''
            ë©€í‹° ì§ˆì˜ ê²€ìƒ‰: ì›ë³¸ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê´€ì ì˜ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ê°ê° ê²€ìƒ‰
            
            ì‚¬ìš© ì‹œê¸°:
            - ë³µì¡í•˜ê³  ë‹¤ë©´ì ì¸ ë²•ë¥  ì§ˆë¬¸
            - ì—¬ëŸ¬ ê´€ì ì—ì„œ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°
            - ë‹¨ì¼ ê²€ìƒ‰ìœ¼ë¡œëŠ” ë¶€ì¡±í•œ ê²½ìš°
            
            ì‘ë™ ë°©ì‹:
            1. ì›ë³¸ ì§ˆë¬¸ì„ ì—¬ëŸ¬ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´
            2. ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ë²¡í„° ê²€ìƒ‰ì„ ëª¨ë‘ ìˆ˜í–‰
            3. ëª¨ë“  ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ë¦¬ë­í‚¹
            
            Args:
                original_query: ì›ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬
                max_queries: ìƒì„±í•  ìµœëŒ€ í•˜ìœ„ ì§ˆë¬¸ ìˆ˜ (ê¸°ë³¸ê°’: 3)
                limit_per_query: ê° ì§ˆë¬¸ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)
            
            Returns:
                JSON í˜•ì‹ì˜ í†µí•© ê²€ìƒ‰ ê²°ê³¼
            '''
            try:
                if not self.keyword_search or not self.semantic_search:
                    self._initialize_search_engines()
                
                if not self.keyword_search or not self.semantic_search:
                    return json.dumps({
                        "success": False,
                        "error": "Search engines not available",
                        "search_type": "multi_query"
                    })
                
                # 1. ë©€í‹° ì§ˆì˜ ìƒì„±
                multi_queries = self._generate_multi_queries(original_query, max_queries)
                
                self.logger.info(f"ğŸ” [MULTI-QUERY] Generated {len(multi_queries)} sub-queries: {multi_queries}")
                
                # 2. ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë³‘ë ¬ ê²€ìƒ‰
                all_results = []
                seen_doc_ids = set()
                
                with ThreadPoolExecutor(max_workers=min(len(multi_queries) * 2, 10)) as executor:
                    futures = []
                    
                    for sub_query in multi_queries:
                        # ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ë²¡í„° ê²€ìƒ‰ ëª¨ë‘ ìˆ˜í–‰
                        keyword_future = executor.submit(
                            self.keyword_search.search_documents,
                            sub_query, limit=limit_per_query, force_fts=True
                        )
                        vector_future = executor.submit(
                            self.semantic_search.search,
                            sub_query, k=limit_per_query
                        )
                        futures.append(("keyword", sub_query, keyword_future))
                        futures.append(("vector", sub_query, vector_future))
                    
                    # ê²°ê³¼ ìˆ˜ì§‘
                    for search_type, sub_query, future in futures:
                        try:
                            results = future.result(timeout=10.0)
                            for result in results:
                                # ì¤‘ë³µ ì œê±°
                                doc_id = self._get_doc_id(result)
                                if doc_id and doc_id not in seen_doc_ids:
                                    seen_doc_ids.add(doc_id)
                                    result["sub_query"] = sub_query
                                    result["search_type"] = search_type
                                    result["original_query"] = original_query
                                    all_results.append(result)
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ [MULTI-QUERY] Search failed for '{sub_query}': {e}")
                
                # 3. ê²°ê³¼ ë¦¬ë­í‚¹
                ranked_results = self._rerank_multi_query_results(all_results, original_query)
                
                return json.dumps({
                    "success": True,
                    "search_type": "multi_query",
                    "original_query": original_query,
                    "sub_queries": multi_queries,
                    "sub_query_count": len(multi_queries),
                    "total_results": len(ranked_results),
                    "results": ranked_results[:limit_per_query * max_queries],
                    "count": len(ranked_results)
                }, ensure_ascii=False)
                
            except Exception as e:
                self.logger.error(f"âŒ [MULTI-QUERY] Error: {e}", exc_info=True)
                return json.dumps({
                    "success": False,
                    "error": str(e),
                    "search_type": "multi_query"
                })
        
        return search_multi_query
    
    def _generate_multi_queries(self, query: str, max_queries: int = 3) -> List[str]:
        """ë©€í‹° ì§ˆì˜ ìƒì„±"""
        try:
            # ê¸°ì¡´ HybridQueryProcessor í™œìš©
            if self.hybrid_query_processor:
                query_info = {
                    "query": query,
                    "search_query": query,
                    "query_type": "general",
                    "extracted_keywords": [],
                    "legal_field": None,
                    "complexity": "moderate",
                    "is_retry": False
                }
                
                optimized_queries, _ = self.hybrid_query_processor.process_query_hybrid(
                    query=query_info["query"],
                    search_query=query_info["search_query"],
                    query_type=query_info["query_type"],
                    extracted_keywords=query_info["extracted_keywords"],
                    legal_field=query_info["legal_field"],
                    complexity=query_info["complexity"],
                    is_retry=query_info["is_retry"]
                )
                
                multi_queries = optimized_queries.get("multi_queries", [query])
                if len(multi_queries) > max_queries:
                    multi_queries = multi_queries[:max_queries]
                return multi_queries
            else:
                # í´ë°±: ì›Œí¬í”Œë¡œìš°ì˜ ë©€í‹° ì§ˆì˜ ìƒì„± ë©”ì„œë“œ ì‚¬ìš©
                if self.workflow and hasattr(self.workflow, '_generate_multi_queries_with_llm'):
                    return self.workflow._generate_multi_queries_with_llm(
                        query=query,
                        query_type="general",
                        max_queries=max_queries
                    )
                else:
                    # ìµœì¢… í´ë°±: ê°„ë‹¨í•œ ì§ˆì˜ ë¶„í•´
                    return self._generate_simple_multi_queries(query, max_queries)
        except Exception as e:
            self.logger.warning(f"âš ï¸ [MULTI-QUERY] Failed to generate multi-queries: {e}, using simple method")
            return self._generate_simple_multi_queries(query, max_queries)
    
    def _generate_simple_multi_queries(self, query: str, max_queries: int = 3) -> List[str]:
        """ê°„ë‹¨í•œ ë©€í‹° ì§ˆì˜ ìƒì„± (í´ë°±)"""
        queries = [query]
        
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë¶„í•´
        if "í•´ì§€" in query:
            queries.append(query.replace("í•´ì§€", "í•´ì§€ ì‚¬ìœ "))
            queries.append(query.replace("í•´ì§€", "í•´ì§€ ì ˆì°¨"))
        elif "ìš”ê±´" in query:
            queries.append(query.replace("ìš”ê±´", "ì„±ë¦½ ìš”ê±´"))
            queries.append(query.replace("ìš”ê±´", "íš¨ë ¥ ìš”ê±´"))
        elif "íš¨ê³¼" in query:
            queries.append(query.replace("íš¨ê³¼", "ë²•ì  íš¨ê³¼"))
            queries.append(query.replace("íš¨ê³¼", "ì‹¤ì œ íš¨ê³¼"))
        elif "ê³„ì•½" in query:
            queries.append(query + " ì‚¬ìœ ")
            queries.append(query + " ì ˆì°¨")
        
        return queries[:max_queries]
    
    def _get_doc_id(self, result: Dict[str, Any]) -> Optional[str]:
        """ë¬¸ì„œ ID ì¶”ì¶œ"""
        if isinstance(result, dict):
            metadata = result.get("metadata", {})
            return (metadata.get("id") or 
                   metadata.get("chunk_id") or 
                   metadata.get("source_id") or
                   result.get("id") or
                   result.get("source", ""))
        return None
    
    def _merge_and_rerank(self, keyword_results: List[Dict], vector_results: List[Dict], limit: int) -> List[Dict]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© ë° ë¦¬ë­í‚¹"""
        seen_ids = set()
        merged = []
        
        for result in keyword_results + vector_results:
            doc_id = self._get_doc_id(result)
            if doc_id and doc_id not in seen_ids:
                seen_ids.add(doc_id)
                # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
                if result in keyword_results:
                    current_score = result.get("relevance_score", 0.0) or result.get("score", 0.0)
                    result["relevance_score"] = current_score * 1.2
                merged.append(result)
        
        # relevance_score ê¸°ì¤€ ì •ë ¬
        merged.sort(key=lambda x: x.get("relevance_score", 0.0) or x.get("score", 0.0), reverse=True)
        return merged[:limit]
    
    def _rerank_multi_query_results(self, results: List[Dict], original_query: str) -> List[Dict]:
        """ë©€í‹° ì§ˆì˜ ê²°ê³¼ ë¦¬ë­í‚¹"""
        for result in results:
            base_score = result.get("relevance_score", 0.0) or result.get("score", 0.0)
            # ì›ë³¸ ì§ˆë¬¸ê³¼ì˜ ë§¤ì¹­ë„ ì¶”ê°€ ì ìˆ˜
            text = str(result.get("text", "") or result.get("content", ""))
            if original_query in text:
                base_score *= 1.3
            result["final_score"] = base_score
        
        # ìµœì¢… ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        results.sort(key=lambda x: x.get("final_score", 0.0), reverse=True)
        return results
    
    def _initialize_agent(self):
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        if not LANGCHAIN_AVAILABLE:
            self.logger.error("âŒ LangChain not available. Cannot initialize agent.")
            return False
        
        if not self.llm:
            if self.workflow:
                self.llm = getattr(self.workflow, 'llm', None)
            if not self.llm:
                self.logger.error("âŒ LLM not available. Cannot initialize agent.")
                return False
        
        # ê²€ìƒ‰ ë„êµ¬ ìƒì„±
        self.search_tools = [
            self._create_postgresql_keyword_search_tool(),
            self._create_vector_index_search_tool(),
            self._create_hybrid_search_tool(),
            self._create_multi_query_search_tool()
        ]
        
        # ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸
        agent_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì „ë¬¸ ë²•ë¥  ê²€ìƒ‰ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë²•ë¥  ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ íš¨ê³¼ì ì¸ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì—­í• :
1. ì‚¬ìš©ìì˜ ë²•ë¥  ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
2. ë‹¤ìŒ ê²€ìƒ‰ ë„êµ¬ ì¤‘ì—ì„œ ì ì ˆí•œ ê²ƒì„ ì„ íƒí•˜ì—¬ ì‚¬ìš©:
   - search_postgresql_keywords: ì •í™•í•œ í‚¤ì›Œë“œ, ë²•ë ¹ëª…, ì¡°ë¬¸ë²ˆí˜¸ ê²€ìƒ‰
   - search_vector_index: ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
   - search_hybrid: í‚¤ì›Œë“œ + ë²¡í„° í†µí•© ê²€ìƒ‰
   - search_multi_query: ë³µì¡í•œ ì§ˆë¬¸ì„ ì—¬ëŸ¬ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ê²€ìƒ‰ (ê°€ì¥ ê°•ë ¥í•¨)

ê²€ìƒ‰ ì „ëµ ê°€ì´ë“œ:
- **ë‹¨ìˆœí•˜ê³  ëª…í™•í•œ ì§ˆë¬¸** (ì˜ˆ: "ë¯¼ë²• ì œ750ì¡°") â†’ search_postgresql_keywords
- **ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸** (ì˜ˆ: "ê³„ì•½ í•´ì§€ ì‚¬ìœ ") â†’ search_vector_index
- **ì •í™•ì„±ê³¼ í¬ê´„ì„± ëª¨ë‘ í•„ìš”** â†’ search_hybrid
- **ë³µì¡í•˜ê³  ë‹¤ë©´ì ì¸ ì§ˆë¬¸** (ì˜ˆ: "ê³„ì•½ í•´ì§€ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”") â†’ search_multi_query (ê¶Œì¥)

ì¤‘ìš” ì›ì¹™:
- ë³µì¡í•œ ì§ˆë¬¸ì€ search_multi_queryë¥¼ ìš°ì„  ì‚¬ìš©
- ë‹¨ì¼ ê²€ìƒ‰ìœ¼ë¡œ ë¶€ì¡±í•˜ë©´ ì—¬ëŸ¬ ë„êµ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
- ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ê²€ìƒ‰ì€ í”¼í•¨
- ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ë„êµ¬ë¡œ ì¬ê²€ìƒ‰
"""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        try:
            agent = create_openai_tools_agent(self.llm, self.search_tools, agent_prompt)
            self.agentic_agent = AgentExecutor(
                agent=agent,
                tools=self.search_tools,
                verbose=True,
                max_iterations=3,
                max_execution_time=30,
                handle_parsing_errors=True,
                return_intermediate_steps=True
            )
            self.logger.debug("âœ… Multi-Query Search Agent initialized")
            return True
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize agent: {e}", exc_info=True)
            return False
    
    def execute(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰"""
        try:
            start_time = time.time()
            query = state.get("query", "") or (state.get("input", {}) or {}).get("query", "")
            
            if not query:
                self.logger.error("âŒ [MULTI-QUERY-AGENT] No query found in state")
                state.setdefault("search", {})["results"] = []
                state.setdefault("retrieved_docs", [])
                return state
            
            # ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
            if self.agentic_agent is None:
                if not self._initialize_agent():
                    # ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ í´ë°±
                    self.logger.warning("âš ï¸ [MULTI-QUERY-AGENT] Agent initialization failed, using direct multi-query search")
                    return self._execute_direct_multi_query(state, query)
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            try:
                result = self.agentic_agent.invoke({"input": query})
                
                # ê²°ê³¼ íŒŒì‹± ë° state ì—…ë°ì´íŠ¸
                search_results = self._parse_agent_results(result)
                
                # retrieved_docs í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                retrieved_docs = self._convert_to_retrieved_docs(search_results)
                
                state["retrieved_docs"] = retrieved_docs
                state["search"] = {
                    "results": search_results,
                    "total_results": len(search_results)
                }
                
                processing_time = time.time() - start_time
                self.logger.info(f"âœ… [MULTI-QUERY-AGENT] Completed in {processing_time:.2f}s, {len(retrieved_docs)} docs")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ [MULTI-QUERY-AGENT] Agent execution failed: {e}, using direct multi-query search")
                return self._execute_direct_multi_query(state, query)
            
            return state
            
        except Exception as e:
            self.logger.error(f"âŒ [MULTI-QUERY-AGENT] Error: {e}", exc_info=True)
            state.setdefault("search", {})["results"] = []
            state.setdefault("retrieved_docs", [])
            return state
    
    def _get_source_types_from_query_type(self, query_type: Optional[str]) -> Optional[List[str]]:
        """
        ì§ˆì˜ íƒ€ì…ì— ë”°ë¼ ê²€ìƒ‰í•  ë¬¸ì„œ íƒ€ì… ê²°ì •
        
        Args:
            query_type: ì§ˆì˜ íƒ€ì… (law_inquiry, precedent_search, general_question ë“±)
        
        Returns:
            ê²€ìƒ‰í•  ë¬¸ì„œ íƒ€ì… ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  íƒ€ì… ê²€ìƒ‰)
        """
        if not query_type:
            return None
        
        query_type_lower = query_type.lower()
        
        # ì§ˆì˜ íƒ€ì…ë³„ ë¬¸ì„œ íƒ€ì… ë§¤í•‘
        type_mapping = {
            "law_inquiry": ["statute_article"],  # ë²•ë ¹ ì§ˆì˜ â†’ ë²•ë ¹ ì¡°ë¬¸ë§Œ ê²€ìƒ‰
            "precedent_search": ["precedent_content"],  # íŒë¡€ ê²€ìƒ‰ â†’ íŒë¡€ë§Œ ê²€ìƒ‰
            "general_question": None,  # ì¼ë°˜ ì§ˆì˜ â†’ ëª¨ë“  íƒ€ì… ê²€ìƒ‰
            "legal_advice": None,  # ë²•ë¥  ì¡°ì–¸ â†’ ëª¨ë“  íƒ€ì… ê²€ìƒ‰
        }
        
        source_types = type_mapping.get(query_type_lower)
        
        if source_types:
            self.logger.info(f"ğŸ” [SEARCH TYPE FILTER] ì§ˆì˜ íƒ€ì… '{query_type}' â†’ ë¬¸ì„œ íƒ€ì…: {source_types}")
        else:
            self.logger.info(f"ğŸ” [SEARCH TYPE FILTER] ì§ˆì˜ íƒ€ì… '{query_type}' â†’ ëª¨ë“  íƒ€ì… ê²€ìƒ‰")
        
        return source_types
    
    def _search_keywords_with_type_filter(
        self, 
        query: str, 
        source_types: Optional[List[str]], 
        limit: int = 5
    ) -> List[Dict[str, Any]]:
        """
        íƒ€ì… í•„í„°ë§ì„ ì ìš©í•œ í‚¤ì›Œë“œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            source_types: ê²€ìƒ‰í•  ë¬¸ì„œ íƒ€ì… ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  íƒ€ì…)
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.keyword_search:
            return []
        
        # source_typesê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ íƒ€ì…ë§Œ ê²€ìƒ‰
        if source_types:
            results = []
            for doc_type in source_types:
                if doc_type == "statute_article":
                    # ë²•ë ¹ ì¡°ë¬¸ ê²€ìƒ‰
                    statute_results = self.keyword_search.search_statutes_fts(query, limit=limit)
                    results.extend(statute_results)
                elif doc_type == "precedent_content":
                    # íŒë¡€ ê²€ìƒ‰
                    case_results = self.keyword_search.search_cases_fts(query, limit=limit)
                    results.extend(case_results)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            seen_ids = set()
            unique_results = []
            for doc in results:
                doc_id = doc.get("id") or doc.get("chunk_id") or doc.get("document_id")
                if doc_id and doc_id not in seen_ids:
                    seen_ids.add(doc_id)
                    unique_results.append(doc)
            
            # relevance_score ê¸°ì¤€ ì •ë ¬
            unique_results.sort(
                key=lambda x: x.get("relevance_score", 0.0) or x.get("score", 0.0) or 0.0,
                reverse=True
            )
            
            return unique_results[:limit]
        else:
            # ëª¨ë“  íƒ€ì… ê²€ìƒ‰
            return self.keyword_search.search_documents(query, limit=limit, force_fts=True)
    
    def _execute_direct_multi_query(self, state: LegalWorkflowState, query: str) -> LegalWorkflowState:
        """ì—ì´ì „íŠ¸ ì—†ì´ ì§ì ‘ ë©€í‹° ì§ˆì˜ ê²€ìƒ‰ ì‹¤í–‰ (í´ë°±)"""
        try:
            self._initialize_search_engines()
            
            if not self.keyword_search or not self.semantic_search:
                self.logger.error("âŒ [MULTI-QUERY] Search engines not available")
                state.setdefault("search", {})["results"] = []
                state.setdefault("retrieved_docs", [])
                return state
            
            # ğŸ”¥ ê°œì„ : ì§ˆì˜ íƒ€ì…ì— ë”°ë¼ ê²€ìƒ‰í•  ë¬¸ì„œ íƒ€ì… ê²°ì •
            query_type = None
            if self.workflow:
                # workflowì—ì„œ ì§ˆì˜ íƒ€ì… ê°€ì ¸ì˜¤ê¸°
                query_type_raw = self.workflow._get_state_value(state, "query_type", "")
                if query_type_raw:
                    if hasattr(query_type_raw, 'value'):
                        query_type = query_type_raw.value
                    else:
                        query_type = str(query_type_raw)
            
            source_types = self._get_source_types_from_query_type(query_type)
            
            # ë©€í‹° ì§ˆì˜ ìƒì„±
            multi_queries = self._generate_multi_queries(query, max_queries=3)
            
            # ê° ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰
            all_results = []
            seen_doc_ids = set()
            
            with ThreadPoolExecutor(max_workers=6) as executor:
                futures = []
                for sub_query in multi_queries:
                    # ğŸ”¥ ê°œì„ : source_typesì— ë”°ë¼ í‚¤ì›Œë“œ ê²€ìƒ‰ë„ í•„í„°ë§
                    # source_typesê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ íƒ€ì…ë§Œ ê²€ìƒ‰
                    keyword_future = executor.submit(
                        self._search_keywords_with_type_filter,
                        sub_query, source_types, limit=5
                    )
                    # ğŸ”¥ ê°œì„ : source_types íŒŒë¼ë¯¸í„° ì „ë‹¬
                    vector_future = executor.submit(
                        self.semantic_search.search,
                        sub_query, k=5, source_types=source_types
                    )
                    futures.append(("keyword", sub_query, keyword_future))
                    futures.append(("vector", sub_query, vector_future))
                
                for search_type, sub_query, future in futures:
                    try:
                        results = future.result(timeout=10.0)
                        for result in results:
                            doc_id = self._get_doc_id(result)
                            if doc_id and doc_id not in seen_doc_ids:
                                seen_doc_ids.add(doc_id)
                                result["sub_query"] = sub_query
                                result["search_type"] = search_type
                                result["original_query"] = query
                                all_results.append(result)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ [MULTI-QUERY] Search failed for '{sub_query}': {e}")
            
            # ë¦¬ë­í‚¹
            ranked_results = self._rerank_multi_query_results(all_results, query)
            retrieved_docs = self._convert_to_retrieved_docs(ranked_results)
            
            # ğŸ”¥ LangGraph state ì—…ë°ì´íŠ¸: ì§ì ‘ ì„¤ì • (LangGraphëŠ” ë°˜í™˜ëœ stateë¥¼ ë³‘í•©í•¨)
            # ìµœìƒìœ„ ë ˆë²¨ì— ì €ì¥
            state["retrieved_docs"] = retrieved_docs
            state["semantic_results"] = ranked_results
            state["semantic_count"] = len(ranked_results)
            
            # ğŸ”¥ ê°œì„ : search ê·¸ë£¹ì—ë„ ì €ì¥ (State Reduction ì†ì‹¤ ë°©ì§€)
            if "search" not in state:
                state["search"] = {}
            state["search"]["retrieved_docs"] = retrieved_docs
            state["search"]["semantic_results"] = ranked_results
            state["search"]["semantic_count"] = len(ranked_results)
            
            # common ê·¸ë£¹ì—ë„ ì €ì¥
            if "common" not in state:
                state["common"] = {}
            if "search" not in state["common"]:
                state["common"]["search"] = {}
            state["common"]["search"]["retrieved_docs"] = retrieved_docs
            state["common"]["search"]["semantic_results"] = ranked_results
            state["common"]["search"]["semantic_count"] = len(ranked_results)
            
            # search ê·¸ë£¹ì—ë„ ì €ì¥ (ì—¬ëŸ¬ ìœ„ì¹˜ì— ì €ì¥í•˜ì—¬ ì•ˆì „ì„± í™•ë³´)
            if "search" not in state:
                state["search"] = {}
            state["search"]["results"] = ranked_results
            state["search"]["total_results"] = len(ranked_results)
            state["search"]["semantic_results"] = ranked_results
            state["search"]["semantic_count"] = len(ranked_results)
            
            # common ê·¸ë£¹ì—ë„ ì €ì¥ (ë³µêµ¬ë¥¼ ìœ„í•´)
            if "common" not in state:
                state["common"] = {}
            if "search" not in state["common"]:
                state["common"]["search"] = {}
            state["common"]["search"]["semantic_results"] = ranked_results
            state["common"]["search"]["semantic_count"] = len(ranked_results)
            
            # ğŸ”¥ ë””ë²„ê·¸: state ì €ì¥ í™•ì¸
            self.logger.info(f"âœ… [MULTI-QUERY] Direct search completed, {len(retrieved_docs)} docs")
            self.logger.info(f"ğŸ“¥ [MULTI-QUERY] State ì €ì¥ í™•ì¸ - semantic_results: {len(state.get('semantic_results', []))}, search.results: {len(state.get('search', {}).get('results', []))}, search.semantic_results: {len(state.get('search', {}).get('semantic_results', []))}")
            return state
            
        except Exception as e:
            self.logger.error(f"âŒ [MULTI-QUERY] Direct search error: {e}", exc_info=True)
            state.setdefault("search", {})["results"] = []
            state.setdefault("retrieved_docs", [])
            return state
    
    def _parse_agent_results(self, agent_result: Dict) -> List[Dict]:
        """ì—ì´ì „íŠ¸ ê²°ê³¼ íŒŒì‹±"""
        search_results = []
        
        if "intermediate_steps" in agent_result:
            for step in agent_result["intermediate_steps"]:
                action, observation = step
                tool_name = action.tool if hasattr(action, 'tool') else str(action)
                
                if tool_name in ["search_postgresql_keywords", "search_vector_index", 
                                "search_hybrid", "search_multi_query"]:
                    try:
                        tool_result = json.loads(observation)
                        if tool_result.get("success") and tool_result.get("results"):
                            search_results.extend(tool_result["results"])
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Failed to parse tool result: {e}")
        
        # ì¤‘ë³µ ì œê±°
        seen_ids = set()
        unique_results = []
        for result in search_results:
            doc_id = self._get_doc_id(result)
            if doc_id and doc_id not in seen_ids:
                seen_ids.add(doc_id)
                unique_results.append(result)
        
        return unique_results
    
    def _convert_to_retrieved_docs(self, search_results: List[Dict]) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ retrieved_docs í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        retrieved_docs = []
        for result in search_results:
            doc = {
                "text": result.get("text", "") or result.get("content", ""),
                "metadata": result.get("metadata", {}),
                "source": result.get("source", ""),
                "relevance_score": result.get("relevance_score", 0.0) or result.get("score", 0.0),
                "search_type": result.get("search_type", "unknown"),
                "sub_query": result.get("sub_query", ""),
                "original_query": result.get("original_query", "")
            }
            retrieved_docs.append(doc)
        
        return retrieved_docs

