# -*- coding: utf-8 -*-
"""
Streaming Callback Handler
LangGraph ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ êµ¬í˜„
"""

import asyncio
import logging
from typing import Any, Dict, List, Optional

try:
    from langchain_core.callbacks import BaseCallbackHandler
    from langchain_core.outputs import LLMResult
    LANCHAIN_CALLBACKS_AVAILABLE = True
except ImportError:
    try:
        from langchain.callbacks.base import BaseCallbackHandler
        from langchain.schema import LLMResult
        LANCHAIN_CALLBACKS_AVAILABLE = True
    except ImportError:
        LANCHAIN_CALLBACKS_AVAILABLE = False
        BaseCallbackHandler = object
        LLMResult = None

logger = logging.getLogger(__name__)


class StreamingCallbackHandler(BaseCallbackHandler):
    """ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ - on_llm_stream ì´ë²¤íŠ¸ë¥¼ íì— ì €ì¥"""
    
    def __init__(self, queue: Optional[asyncio.Queue] = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            queue: ì²­í¬ë¥¼ ì €ì¥í•  asyncio.Queue. Noneì´ë©´ ìë™ ìƒì„±
        """
        if not LANCHAIN_CALLBACKS_AVAILABLE:
            logger.warning("LangChain callbacks not available. Streaming may not work properly.")
        
        super().__init__()
        self.queue = queue if queue is not None else asyncio.Queue()
        self.streaming_active = False
        self.chunk_count = 0
        self.total_chunks = 0
        self.node_name = None
        
    def on_llm_start(
        self,
        serialized: Dict[str, Any],
        prompts: List[str],
        **kwargs: Any
    ) -> None:
        """LLM ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        self.streaming_active = True
        self.chunk_count = 0
        self.node_name = kwargs.get("name", "unknown")
        logger.debug(f"ğŸ“¡ [CALLBACK] on_llm_start: node={self.node_name}, prompts={len(prompts)}")
    
    def on_llm_stream(self, chunk: Any, **kwargs: Any) -> None:
        """LLM ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ìˆ˜ì‹  ì‹œ í˜¸ì¶œ"""
        if not self.streaming_active:
            return
        
        self.chunk_count += 1
        self.total_chunks += 1
        
        # ì²­í¬ ë‚´ìš© ì¶”ì¶œ
        chunk_content = self._extract_chunk_content(chunk)
        
        if chunk_content:
            try:
                # íì— ì²­í¬ ì¶”ê°€ (ë¹„ë™ê¸°)
                if self.queue:
                    # íê°€ ê°€ë“ ì°¬ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ non-blocking ì‹œë„
                    try:
                        self.queue.put_nowait({
                            "type": "chunk",
                            "content": chunk_content,
                            "chunk_index": self.chunk_count,
                            "node_name": self.node_name,
                            "timestamp": asyncio.get_event_loop().time() if hasattr(asyncio, 'get_event_loop') else None
                        })
                    except asyncio.QueueFull:
                        logger.warning(f"âš ï¸ [CALLBACK] Queue full, dropping chunk #{self.chunk_count}")
                    except RuntimeError:
                        # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
                        logger.debug(f"ğŸ“¡ [CALLBACK] on_llm_stream (sync): chunk #{self.chunk_count}, content={chunk_content[:50]}...")
                
                # ë””ë²„ê·¸ ë¡œê¹… (ì²˜ìŒ 10ê°œ ì²­í¬ë§Œ)
                if self.chunk_count <= 10:
                    logger.info(
                        f"ğŸ“¡ [CALLBACK] on_llm_stream: chunk #{self.chunk_count}, "
                        f"content={chunk_content[:50]}..., node={self.node_name}, "
                        f"queue_size={self.queue.qsize() if self.queue else 0}"
                    )
            except Exception as e:
                logger.error(f"âŒ [CALLBACK] Error processing chunk: {e}")
    
    def on_chat_model_stream(self, chunk: Any, **kwargs: Any) -> None:
        """Chat Model ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ìˆ˜ì‹  ì‹œ í˜¸ì¶œ (ChatGoogleGenerativeAI ë“±)"""
        # on_llm_streamê³¼ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
        self.on_llm_stream(chunk, **kwargs)
    
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLM ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        self.streaming_active = False
        logger.debug(
            f"âœ… [CALLBACK] on_llm_end: node={self.node_name}, "
            f"total_chunks={self.chunk_count}, "
            f"generations={len(response.generations) if response and hasattr(response, 'generations') else 0}"
        )
        
        # ì¢…ë£Œ ì‹ í˜¸ë¥¼ íì— ì¶”ê°€
        if self.queue:
            try:
                self.queue.put_nowait({
                    "type": "end",
                    "node_name": self.node_name,
                    "total_chunks": self.chunk_count,
                    "timestamp": asyncio.get_event_loop().time() if hasattr(asyncio, 'get_event_loop') else None
                })
            except (asyncio.QueueFull, RuntimeError):
                pass
    
    def on_llm_error(self, error: Exception, **kwargs: Any) -> None:
        """LLM ì˜¤ë¥˜ ì‹œ í˜¸ì¶œ"""
        self.streaming_active = False
        logger.error(f"âŒ [CALLBACK] on_llm_error: node={self.node_name}, error={error}")
        
        # ì˜¤ë¥˜ ì‹ í˜¸ë¥¼ íì— ì¶”ê°€
        if self.queue:
            try:
                self.queue.put_nowait({
                    "type": "error",
                    "node_name": self.node_name,
                    "error": str(error),
                    "timestamp": asyncio.get_event_loop().time() if hasattr(asyncio, 'get_event_loop') else None
                })
            except (asyncio.QueueFull, RuntimeError):
                pass
    
    def _extract_chunk_content(self, chunk: Any) -> str:
        """ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ"""
        if chunk is None:
            return ""
        
        # AIMessageChunk ë˜ëŠ” ìœ ì‚¬í•œ ê°ì²´ ì²˜ë¦¬
        if hasattr(chunk, "content"):
            content = chunk.content
            if isinstance(content, str):
                return content
            elif isinstance(content, list) and len(content) > 0:
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì¶”ì¶œ
                first_item = content[0]
                if isinstance(first_item, str):
                    return first_item
                elif hasattr(first_item, "text"):
                    return first_item.text
                else:
                    return str(first_item)
            else:
                return str(content) if content else ""
        
        # ë¬¸ìì—´ì¸ ê²½ìš°
        if isinstance(chunk, str):
            return chunk
        
        # text ì†ì„±ì´ ìˆëŠ” ê²½ìš°
        if hasattr(chunk, "text"):
            return chunk.text
        
        # delta í˜•ì‹ (LangGraph v2)
        if isinstance(chunk, dict):
            delta = chunk.get("delta", {})
            if isinstance(delta, dict):
                return delta.get("content", delta.get("text", ""))
            elif isinstance(delta, str):
                return delta
            return chunk.get("content", chunk.get("text", ""))
        
        # ê¸°íƒ€ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
        return str(chunk)
    
    def reset(self) -> None:
        """í•¸ë“¤ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”"""
        self.streaming_active = False
        self.chunk_count = 0
        self.node_name = None
        # íëŠ” ë¹„ìš°ì§€ ì•ŠìŒ (ì´ë¯¸ ì²˜ë¦¬ëœ ì²­í¬ ë³´ì¡´)
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            "streaming_active": self.streaming_active,
            "chunk_count": self.chunk_count,
            "total_chunks": self.total_chunks,
            "node_name": self.node_name,
            "queue_size": self.queue.qsize() if self.queue else 0
        }

