# -*- coding: utf-8 -*-
"""
ëª¨ë¸ ìºì‹œ ë§¤ë‹ˆì € (ì‹±ê¸€í†¤ íŒ¨í„´)
ë™ì¼í•œ ëª¨ë¸ì„ ì¤‘ë³µ ë¡œë“œí•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬
"""

import threading
import logging
from typing import Optional, Dict, Any

try:
    from lawfirm_langgraph.core.utils.logger import get_logger
except ImportError:
    from core.utils.logger import get_logger

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

logger = get_logger(__name__)


class ModelCacheManager:
    """ëª¨ë¸ ìºì‹œ ë§¤ë‹ˆì € (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    
    _instance: Optional['ModelCacheManager'] = None
    _lock = threading.Lock()
    
    def __new__(cls):
        """ì‹±ê¸€í†¤ íŒ¨í„´ êµ¬í˜„"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰)"""
        if self._initialized:
            return
        
        self._models: Dict[str, Any] = {}
        self._model_locks: Dict[str, threading.Lock] = {}
        self._cache_lock = threading.Lock()
        self._initialized = True
        logger.debug("âœ… [MODEL CACHE] ModelCacheManager initialized")
    
    def get_model(
        self,
        model_name: str,
        fallback_model_name: Optional[str] = None,
        device: Optional[str] = None,
        model_kwargs: Optional[Dict[str, Any]] = None
    ) -> Optional[Any]:
        """
        ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œì—ì„œ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ë¡œë“œ)
        
        Args:
            model_name: ëª¨ë¸ëª…
            fallback_model_name: í´ë°± ëª¨ë¸ëª… (ê¸°ë³¸ê°’: paraphrase-multilingual-MiniLM-L12-v2)
            device: ë””ë°”ì´ìŠ¤ ("cpu", "cuda" ë“±, Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            model_kwargs: ëª¨ë¸ ë¡œë”© ì‹œ ì¶”ê°€ ì˜µì…˜ (dict)
            
        Returns:
            SentenceTransformer ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            logger.warning("âš ï¸ [MODEL CACHE] SentenceTransformers not available")
            return None
        
        # ìºì‹œ í™•ì¸
        with self._cache_lock:
            if model_name in self._models:
                logger.trace(f"âœ… [MODEL CACHE] Cache hit for model: {model_name}")
                return self._models[model_name]
        
        # ëª¨ë¸ë³„ ë½ ìƒì„± (ì—†ìœ¼ë©´)
        if model_name not in self._model_locks:
            with self._cache_lock:
                if model_name not in self._model_locks:
                    self._model_locks[model_name] = threading.Lock()
        
        # ëª¨ë¸ ë¡œë“œ (ë™ì‹œ ë¡œë“œ ë°©ì§€)
        with self._model_locks[model_name]:
            # ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸ (ë‹¤ë¥¸ ìŠ¤ë ˆë“œê°€ ë¡œë“œí–ˆì„ ìˆ˜ ìˆìŒ)
            if model_name in self._models:
                logger.trace(f"âœ… [MODEL CACHE] Cache hit (after lock) for model: {model_name}")
                return self._models[model_name]
            
            try:
                logger.info(f"ğŸ”„ [MODEL CACHE] Loading model: {model_name}")
                
                # deviceì™€ model_kwargsë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¡œë“œ
                if device and model_kwargs:
                    model = SentenceTransformer(model_name, device=device, model_kwargs=model_kwargs)
                elif device:
                    model = SentenceTransformer(model_name, device=device)
                elif model_kwargs:
                    model = SentenceTransformer(model_name, model_kwargs=model_kwargs)
                else:
                    model = SentenceTransformer(model_name)
                
                with self._cache_lock:
                    self._models[model_name] = model
                
                logger.info(f"âœ… [MODEL CACHE] Successfully loaded and cached model: {model_name}")
                return model
                
            except Exception as e:
                logger.warning(f"âš ï¸ [MODEL CACHE] Failed to load {model_name}: {e}")
                
                # í´ë°± ëª¨ë¸ ì‹œë„
                fallback = fallback_model_name or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                
                if fallback in self._models:
                    logger.info(f"âœ… [MODEL CACHE] Using cached fallback model: {fallback}")
                    return self._models[fallback]
                
                try:
                    logger.info(f"ğŸ”„ [MODEL CACHE] Loading fallback model: {fallback}")
                    
                    # í´ë°± ëª¨ë¸ë„ ë™ì¼í•œ ì˜µì…˜ìœ¼ë¡œ ë¡œë“œ ì‹œë„
                    if device and model_kwargs:
                        fallback_model = SentenceTransformer(fallback, device=device, model_kwargs=model_kwargs)
                    elif device:
                        fallback_model = SentenceTransformer(fallback, device=device)
                    elif model_kwargs:
                        fallback_model = SentenceTransformer(fallback, model_kwargs=model_kwargs)
                    else:
                        fallback_model = SentenceTransformer(fallback)
                    
                    with self._cache_lock:
                        self._models[fallback] = fallback_model
                    
                    logger.info(f"âœ… [MODEL CACHE] Successfully loaded and cached fallback model: {fallback}")
                    return fallback_model
                    
                except Exception as e2:
                    logger.error(f"âŒ [MODEL CACHE] Failed to load fallback model {fallback}: {e2}")
                    return None
    
    def clear_cache(self, model_name: Optional[str] = None):
        """
        ìºì‹œ ì‚­ì œ
        
        Args:
            model_name: ì‚­ì œí•  ëª¨ë¸ëª… (Noneì´ë©´ ì „ì²´ ì‚­ì œ)
        """
        with self._cache_lock:
            if model_name:
                if model_name in self._models:
                    del self._models[model_name]
                    logger.info(f"ğŸ—‘ï¸ [MODEL CACHE] Cleared cache for model: {model_name}")
            else:
                self._models.clear()
                logger.info("ğŸ—‘ï¸ [MODEL CACHE] Cleared all model cache")
    
    def get_cached_models(self) -> list:
        """ìºì‹œëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        with self._cache_lock:
            return list(self._models.keys())


def get_model_cache_manager() -> ModelCacheManager:
    """ModelCacheManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return ModelCacheManager()

