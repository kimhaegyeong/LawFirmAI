# -*- coding: utf-8 -*-
"""
ëª¨ë¸ ìºì‹œ ë§¤ë‹ˆì € (ì‹±ê¸€í†¤ íŒ¨í„´)
ë™ì¼í•œ ëª¨ë¸ì„ ì¤‘ë³µ ë¡œë“œí•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬
"""

import threading
import logging
import os
import re
from typing import Optional, Dict, Any

try:
    from lawfirm_langgraph.core.utils.logger import get_logger
except ImportError:
    from core.utils.logger import get_logger

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# Transformers ëª¨ë¸ ì§€ì›
TRANSFORMERS_AVAILABLE = False
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

logger = get_logger(__name__)


def _normalize_model_name(model_name: str) -> str:
    """
    ëª¨ë¸ ì´ë¦„ ì •ê·œí™” (ë”°ì˜´í‘œ ì œê±° ë° ê³µë°± ì œê±°)
    
    Args:
        model_name: ì›ë³¸ ëª¨ë¸ ì´ë¦„ (ì˜ˆ: '"woong0322/ko-legal-sbert-finetuned"')
        
    Returns:
        ì •ê·œí™”ëœ ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "woong0322/ko-legal-sbert-finetuned")
    """
    # ì•ë’¤ ë”°ì˜´í‘œ ë° ê³µë°± ì œê±°
    normalized = model_name.strip().strip('"').strip("'")
    return normalized


def _normalize_model_name_for_cache(model_name: str) -> str:
    """
    ëª¨ë¸ ì´ë¦„ì„ ìºì‹œ ë””ë ‰í† ë¦¬ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì •ê·œí™”
    
    Args:
        model_name: ì›ë³¸ ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "woong0322/ko-legal-sbert-finetuned")
        
    Returns:
        ì •ê·œí™”ëœ ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "woong0322_ko_legal_sbert_finetuned")
    """
    # ë¨¼ì € ëª¨ë¸ ì´ë¦„ ì •ê·œí™” (ë”°ì˜´í‘œ ì œê±°)
    normalized = _normalize_model_name(model_name)
    # ìŠ¬ë˜ì‹œì™€ í•˜ì´í”ˆì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
    normalized = re.sub(r'[/-]', '_', normalized)
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì•ŒíŒŒë²³, ìˆ«ì, ì–¸ë”ìŠ¤ì½”ì–´ë§Œ í—ˆìš©)
    normalized = re.sub(r'[^a-zA-Z0-9_]', '', normalized)
    return normalized


def _filter_model_kwargs(model_kwargs: Dict[str, Any]) -> Dict[str, Any]:
    """
    SentenceTransformerê°€ ì§€ì›í•˜ëŠ” model_kwargsë§Œ í•„í„°ë§
    
    Args:
        model_kwargs: ì›ë³¸ model_kwargs
        
    Returns:
        í•„í„°ë§ëœ model_kwargs
    """
    # SentenceTransformerê°€ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° ì œê±°
    # ì´ íŒŒë¼ë¯¸í„°ë“¤ì€ ë‚´ë¶€ transformers ëª¨ë¸ì—ë§Œ ì „ë‹¬ë˜ì§€ë§Œ,
    # SentenceTransformerëŠ” ì´ë¥¼ ì§ì ‘ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì§€ ì•ŠìŒ
    unsupported_params = {
        'low_cpu_mem_usage',  # SentenceTransformerê°€ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ
        'device_map',  # SentenceTransformerëŠ” device íŒŒë¼ë¯¸í„° ì‚¬ìš©
        'dtype',  # SentenceTransformerê°€ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ (ì¼ë¶€ ë²„ì „ì—ì„œ ì—ëŸ¬ ë°œìƒ)
        'torch_dtype',  # SentenceTransformerê°€ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ
        'use_safetensors',  # SentenceTransformerê°€ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ (ì¼ë¶€ ë²„ì „ì—ì„œ ì—ëŸ¬ ë°œìƒ)
        'trust_remote_code',  # SentenceTransformerê°€ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ (ì¼ë¶€ ë²„ì „ì—ì„œ ì—ëŸ¬ ë°œìƒ)
        'local_files_only',  # SentenceTransformerê°€ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ (ì¼ë¶€ ë²„ì „ì—ì„œ ì—ëŸ¬ ë°œìƒ)
    }
    
    filtered_kwargs = {
        k: v for k, v in model_kwargs.items()
        if k not in unsupported_params
    }
    
    if filtered_kwargs != model_kwargs:
        removed = set(model_kwargs.keys()) - set(filtered_kwargs.keys())
        logger.debug(f"ğŸ”§ [MODEL CACHE] Filtered unsupported params: {removed}")
    
    return filtered_kwargs


def _get_cache_folder(model_name: str, base_cache_dir: Optional[str] = None) -> str:
    """
    ëª¨ë¸ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ ìƒì„±
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        base_cache_dir: ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        
    Returns:
        ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    if base_cache_dir is None:
        # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬: ~/.cache/huggingface/transformers ë˜ëŠ” ./model_cache
        default_cache = os.getenv("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
        base_cache_dir = os.getenv("MODEL_CACHE_DIR", os.path.join(default_cache, "sentence_transformers"))
    
    # ëª¨ë¸ ì´ë¦„ ì •ê·œí™”
    normalized_name = _normalize_model_name_for_cache(model_name)
    cache_folder = os.path.join(base_cache_dir, normalized_name)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(cache_folder, exist_ok=True)
    
    return cache_folder


class ModelCacheManager:
    """ëª¨ë¸ ìºì‹œ ë§¤ë‹ˆì € (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    
    _instance: Optional['ModelCacheManager'] = None
    _lock = threading.Lock()
    
    def __new__(cls):
        """ì‹±ê¸€í†¤ íŒ¨í„´ êµ¬í˜„"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰)"""
        if self._initialized:
            return
        
        self._models: Dict[str, Any] = {}
        self._transformers_models: Dict[str, Dict[str, Any]] = {}  # {model_name: {"model": model, "tokenizer": tokenizer}}
        self._model_locks: Dict[str, threading.Lock] = {}
        self._cache_lock = threading.Lock()
        self._initialized = True
        logger.debug("âœ… [MODEL CACHE] ModelCacheManager initialized")
    
    def get_model(
        self,
        model_name: str,
        fallback_model_name: Optional[str] = None,
        device: Optional[str] = None,
        model_kwargs: Optional[Dict[str, Any]] = None,
        cache_folder: Optional[str] = None
    ) -> Optional[Any]:
        """
        ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œì—ì„œ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ë¡œë“œ)
        
        Args:
            model_name: ëª¨ë¸ëª…
            fallback_model_name: í´ë°± ëª¨ë¸ëª… (ê¸°ë³¸ê°’: paraphrase-multilingual-MiniLM-L12-v2)
            device: ë””ë°”ì´ìŠ¤ ("cpu", "cuda" ë“±, Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            model_kwargs: ëª¨ë¸ ë¡œë”© ì‹œ ì¶”ê°€ ì˜µì…˜ (dict)
            cache_folder: ëª…ì‹œì  ìºì‹œ í´ë” ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            SentenceTransformer ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            logger.warning("âš ï¸ [MODEL CACHE] SentenceTransformers not available")
            return None
        
        # ëª¨ë¸ ì´ë¦„ ì •ê·œí™” (ë”°ì˜´í‘œ ì œê±°)
        normalized_model_name = _normalize_model_name(model_name)
        
        # ìºì‹œ í™•ì¸ (ì •ê·œí™”ëœ ì´ë¦„ ì‚¬ìš©)
        with self._cache_lock:
            if normalized_model_name in self._models:
                logger.trace(f"âœ… [MODEL CACHE] Cache hit for model: {normalized_model_name}")
                return self._models[normalized_model_name]
        
        # ëª¨ë¸ë³„ ë½ ìƒì„± (ì—†ìœ¼ë©´) - ì •ê·œí™”ëœ ì´ë¦„ ì‚¬ìš©
        if normalized_model_name not in self._model_locks:
            with self._cache_lock:
                if normalized_model_name not in self._model_locks:
                    self._model_locks[normalized_model_name] = threading.Lock()
        
        # ëª¨ë¸ ë¡œë“œ (ë™ì‹œ ë¡œë“œ ë°©ì§€)
        with self._model_locks[normalized_model_name]:
            # ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸ (ë‹¤ë¥¸ ìŠ¤ë ˆë“œê°€ ë¡œë“œí–ˆì„ ìˆ˜ ìˆìŒ)
            if normalized_model_name in self._models:
                logger.trace(f"âœ… [MODEL CACHE] Cache hit (after lock) for model: {normalized_model_name}")
                return self._models[normalized_model_name]
            
            try:
                logger.info(f"ğŸ”„ [MODEL CACHE] Loading model: {normalized_model_name} (original: {model_name})")
                
                # ìºì‹œ í´ë” ì„¤ì •
                if cache_folder is None:
                    cache_folder = _get_cache_folder(normalized_model_name)
                
                # model_kwargs í•„í„°ë§ ë° cache_folder ì¶”ê°€
                if model_kwargs is None:
                    model_kwargs = {}
                else:
                    # SentenceTransformerê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° í•„í„°ë§
                    model_kwargs = _filter_model_kwargs(model_kwargs)
                
                # cache_folderê°€ ì´ë¯¸ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
                if 'cache_folder' not in model_kwargs:
                    model_kwargs = {**model_kwargs, 'cache_folder': cache_folder}
                
                logger.debug(f"ğŸ“ [MODEL CACHE] Using cache folder: {cache_folder}")
                
                # deviceì™€ model_kwargsë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¡œë“œ (ì •ê·œí™”ëœ ì´ë¦„ ì‚¬ìš©)
                if device:
                    model = SentenceTransformer(normalized_model_name, device=device, **model_kwargs)
                else:
                    model = SentenceTransformer(normalized_model_name, **model_kwargs)
                
                with self._cache_lock:
                    self._models[normalized_model_name] = model
                
                logger.info(f"âœ… [MODEL CACHE] Successfully loaded and cached model: {normalized_model_name} (original: {model_name})")
                return model
                
            except Exception as e:
                logger.warning(f"âš ï¸ [MODEL CACHE] Failed to load model '{normalized_model_name}' (original: '{model_name}'): {e}")
                
                # í´ë°± ëª¨ë¸ ì‹œë„
                fallback_raw = fallback_model_name or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                fallback = _normalize_model_name(fallback_raw)
                
                if fallback in self._models:
                    logger.info(f"âœ… [MODEL CACHE] Using cached fallback model: {fallback} (original: {fallback_raw})")
                    return self._models[fallback]
                
                # í´ë°± ëª¨ë¸ ë¡œë”© ì‹œë„ (ìµœëŒ€ 2íšŒ: í•„í„°ë§ëœ íŒŒë¼ë¯¸í„°, ìµœì†Œ íŒŒë¼ë¯¸í„°)
                fallback_cache_folder = cache_folder or _get_cache_folder(fallback)
                
                # ì²« ë²ˆì§¸ ì‹œë„: í•„í„°ë§ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
                fallback_model_kwargs = {**model_kwargs} if model_kwargs else {}
                fallback_model_kwargs = _filter_model_kwargs(fallback_model_kwargs)
                if 'cache_folder' not in fallback_model_kwargs:
                    fallback_model_kwargs['cache_folder'] = fallback_cache_folder
                
                try:
                    logger.info(f"ğŸ”„ [MODEL CACHE] Loading fallback model: {fallback} (original: {fallback_raw})")
                    
                    # í´ë°± ëª¨ë¸ë„ ë™ì¼í•œ ì˜µì…˜ìœ¼ë¡œ ë¡œë“œ ì‹œë„
                    if device:
                        fallback_model = SentenceTransformer(fallback, device=device, **fallback_model_kwargs)
                    else:
                        fallback_model = SentenceTransformer(fallback, **fallback_model_kwargs)
                    
                    with self._cache_lock:
                        self._models[fallback] = fallback_model
                    
                    logger.info(f"âœ… [MODEL CACHE] Successfully loaded and cached fallback model: {fallback} (original: {fallback_raw})")
                    return fallback_model
                    
                except Exception as e2:
                    logger.warning(f"âš ï¸ [MODEL CACHE] Failed to load fallback model '{fallback}' (original: '{fallback_raw}') with filtered kwargs: {e2}")
                    
                    # ë‘ ë²ˆì§¸ ì‹œë„: ìµœì†Œ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš© (cache_folderë§Œ)
                    try:
                        logger.info(f"ğŸ”„ [MODEL CACHE] Retrying fallback model with minimal kwargs: {fallback}")
                        minimal_kwargs = {'cache_folder': fallback_cache_folder}
                        
                        if device:
                            fallback_model = SentenceTransformer(fallback, device=device, **minimal_kwargs)
                        else:
                            fallback_model = SentenceTransformer(fallback, **minimal_kwargs)
                        
                        with self._cache_lock:
                            self._models[fallback] = fallback_model
                        
                        logger.info(f"âœ… [MODEL CACHE] Successfully loaded fallback model with minimal kwargs: {fallback} (original: {fallback_raw})")
                        return fallback_model
                        
                    except Exception as e3:
                        logger.error(f"âŒ [MODEL CACHE] Failed to load fallback model '{fallback}' (original: '{fallback_raw}') even with minimal kwargs: {e3}")
                        return None
    
    def clear_cache(self, model_name: Optional[str] = None):
        """
        ìºì‹œ ì‚­ì œ
        
        Args:
            model_name: ì‚­ì œí•  ëª¨ë¸ëª… (Noneì´ë©´ ì „ì²´ ì‚­ì œ)
        """
        with self._cache_lock:
            if model_name:
                if model_name in self._models:
                    del self._models[model_name]
                    logger.info(f"ğŸ—‘ï¸ [MODEL CACHE] Cleared cache for model: {model_name}")
            else:
                self._models.clear()
                logger.info("ğŸ—‘ï¸ [MODEL CACHE] Cleared all model cache")
    
    def get_cached_models(self) -> list:
        """ìºì‹œëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        with self._cache_lock:
            return list(self._models.keys())
    
    def get_transformers_model(
        self,
        model_name: str,
        model_type: str = "AutoModelForSequenceClassification",
        device: Optional[str] = None,
        cache_dir: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """
        Transformers ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œì—ì„œ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ë¡œë“œ)
        
        Args:
            model_name: ëª¨ë¸ëª… (ì˜ˆ: "monologg/kobert")
            model_type: ëª¨ë¸ íƒ€ì… ("AutoModelForSequenceClassification", "AutoModel" ë“±)
            device: ë””ë°”ì´ìŠ¤ ("cpu", "cuda" ë“±, Noneì´ë©´ ìë™ ê°ì§€)
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            
        Returns:
            {"model": model, "tokenizer": tokenizer} ë˜ëŠ” None
        """
        if not TRANSFORMERS_AVAILABLE:
            logger.warning("âš ï¸ [MODEL CACHE] Transformers not available")
            return None
        
        # ëª¨ë¸ ì´ë¦„ ì •ê·œí™”
        normalized_model_name = _normalize_model_name(model_name)
        
        # ìºì‹œ í™•ì¸
        with self._cache_lock:
            if normalized_model_name in self._transformers_models:
                logger.trace(f"âœ… [MODEL CACHE] Cache hit for transformers model: {normalized_model_name}")
                return self._transformers_models[normalized_model_name]
        
        # ëª¨ë¸ë³„ ë½ ìƒì„±
        if normalized_model_name not in self._model_locks:
            with self._cache_lock:
                if normalized_model_name not in self._model_locks:
                    self._model_locks[normalized_model_name] = threading.Lock()
        
        # ëª¨ë¸ ë¡œë“œ (ë™ì‹œ ë¡œë“œ ë°©ì§€)
        with self._model_locks[normalized_model_name]:
            # ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸
            if normalized_model_name in self._transformers_models:
                logger.trace(f"âœ… [MODEL CACHE] Cache hit (after lock) for transformers model: {normalized_model_name}")
                return self._transformers_models[normalized_model_name]
            
            try:
                logger.info(f"ğŸ”„ [MODEL CACHE] Loading transformers model: {normalized_model_name} (type: {model_type})")
                
                # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
                if cache_dir is None:
                    cache_dir = os.getenv("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
                
                # Tokenizer ë¡œë“œ
                tokenizer = AutoTokenizer.from_pretrained(
                    normalized_model_name,
                    cache_dir=cache_dir,
                    trust_remote_code=True  # KoBERT ë“± ì»¤ìŠ¤í…€ ì½”ë“œ ëª¨ë¸ ì§€ì›
                )
                
                # Model ë¡œë“œ
                if model_type == "AutoModelForSequenceClassification":
                    model = AutoModelForSequenceClassification.from_pretrained(
                        normalized_model_name,
                        cache_dir=cache_dir,
                        trust_remote_code=True  # KoBERT ë“± ì»¤ìŠ¤í…€ ì½”ë“œ ëª¨ë¸ ì§€ì›
                    )
                else:
                    from transformers import AutoModel
                    model = AutoModel.from_pretrained(
                        normalized_model_name,
                        cache_dir=cache_dir,
                        trust_remote_code=True  # KoBERT ë“± ì»¤ìŠ¤í…€ ì½”ë“œ ëª¨ë¸ ì§€ì›
                    )
                
                # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
                model.eval()
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì •
                if device is None:
                    device = "cuda" if torch.cuda.is_available() else "cpu"
                
                if device == "cuda" and torch.cuda.is_available():
                    model = model.cuda()
                    logger.debug(f"ğŸ“± [MODEL CACHE] Model loaded on GPU: {normalized_model_name}")
                else:
                    logger.debug(f"ğŸ“± [MODEL CACHE] Model loaded on CPU: {normalized_model_name}")
                
                # ìºì‹œì— ì €ì¥
                model_dict = {
                    "model": model,
                    "tokenizer": tokenizer,
                    "device": device
                }
                
                with self._cache_lock:
                    self._transformers_models[normalized_model_name] = model_dict
                
                logger.info(f"âœ… [MODEL CACHE] Successfully loaded and cached transformers model: {normalized_model_name}")
                return model_dict
                
            except Exception as e:
                logger.warning(f"âš ï¸ [MODEL CACHE] Failed to load transformers model '{normalized_model_name}': {e}")
                return None
    
    def clear_transformers_cache(self, model_name: Optional[str] = None):
        """
        Transformers ëª¨ë¸ ìºì‹œ ì‚­ì œ
        
        Args:
            model_name: ì‚­ì œí•  ëª¨ë¸ëª… (Noneì´ë©´ ì „ì²´ ì‚­ì œ)
        """
        with self._cache_lock:
            if model_name:
                normalized_name = _normalize_model_name(model_name)
                if normalized_name in self._transformers_models:
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    model_dict = self._transformers_models[normalized_name]
                    if "model" in model_dict:
                        del model_dict["model"]
                    if "tokenizer" in model_dict:
                        del model_dict["tokenizer"]
                    del self._transformers_models[normalized_name]
                    logger.info(f"ğŸ—‘ï¸ [MODEL CACHE] Cleared transformers cache for model: {normalized_name}")
            else:
                # ì „ì²´ ì‚­ì œ
                for normalized_name, model_dict in self._transformers_models.items():
                    if "model" in model_dict:
                        del model_dict["model"]
                    if "tokenizer" in model_dict:
                        del model_dict["tokenizer"]
                self._transformers_models.clear()
                logger.info("ğŸ—‘ï¸ [MODEL CACHE] Cleared all transformers model cache")
    
    def get_cached_transformers_models(self) -> list:
        """ìºì‹œëœ Transformers ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        with self._cache_lock:
            return list(self._transformers_models.keys())


def get_model_cache_manager() -> ModelCacheManager:
    """ModelCacheManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return ModelCacheManager()

