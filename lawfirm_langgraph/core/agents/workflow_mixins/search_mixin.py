# -*- coding: utf-8 -*-
"""
Search Mixin
ê²€ìƒ‰ ê´€ë ¨ ë…¸ë“œ ë° ë©”ì„œë“œë“¤ì„ ì œê³µí•˜ëŠ” Mixin í´ë˜ìŠ¤
"""

import re
import sys
import time
from typing import Any, Dict, List, Optional, Tuple

from core.workflow.state.state_definitions import LegalWorkflowState
from core.workflow.utils.workflow_constants import WorkflowConstants, RetryConfig
from core.workflow.state.workflow_types import QueryComplexity
from core.shared.wrappers.node_wrappers import with_state_optimization
from core.generation.validators.quality_validators import SearchValidator
from core.workflow.state.state_utils import prune_retrieved_docs, MAX_RETRIEVED_DOCS, MAX_DOCUMENT_CONTENT_LENGTH

try:
    from langfuse import observe
except ImportError:
    def observe(**kwargs):
        def decorator(func):
            return func
        return decorator

# ì„±ëŠ¥ ìµœì í™”: ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼ (ëª¨ë“ˆ ë ˆë²¨)
LAW_PATTERN = re.compile(r'[ê°€-í£]+ë²•\s*ì œ?\s*\d+\s*ì¡°')
PRECEDENT_PATTERN = re.compile(r'ëŒ€ë²•ì›|ë²•ì›.*\d{4}[ë‹¤ë‚˜ë§ˆ]\d+')


class SearchMixin:
    """ê²€ìƒ‰ ê´€ë ¨ ë…¸ë“œ ë° ë©”ì„œë“œë“¤ì„ ì œê³µí•˜ëŠ” Mixin í´ë˜ìŠ¤"""
    
    # ============================================================================
    # prepare_search_query í—¬í¼ ë©”ì„œë“œë“¤
    # ============================================================================
    
    def _get_query_info_for_optimization(
        self,
        state: LegalWorkflowState
    ) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ë° ê²€ì¦ (ì¤‘ë³µ ì½”ë“œ ì œê±°)"""
        query = None
        
        if "input" in state and isinstance(state["input"], dict):
            query = state["input"].get("query", "")
        
        if not query or not str(query).strip():
            query = self._get_state_value(state, "query", "")
        
        if not query or not str(query).strip():
            if isinstance(state, dict) and "query" in state:
                query = state["query"]
        
        search_query = self._get_state_value(state, "search_query") or query
        
        if not query or not str(query).strip():
            self.logger.error(f"prepare_search_query: query is empty! State keys: {list(state.keys()) if isinstance(state, dict) else 'N/A'}")
            if "input" in state:
                self.logger.error(f"prepare_search_query: state['input'] = {state['input']}")
            return {
                "query": None,
                "search_query": None,
                "query_type_str": "",
                "extracted_keywords": [],
                "legal_field": ""
            }
        
        query_type_raw = self._get_state_value(state, "query_type", "")
        query_type_str = self._get_query_type_str(query_type_raw)
        query_type_str = self._normalize_query_type_for_prompt(query_type_str)
        
        extracted_keywords_raw = self._get_state_value(state, "extracted_keywords", [])
        if not isinstance(extracted_keywords_raw, list):
            self.logger.warning(f"extracted_keywords is not a list: {type(extracted_keywords_raw)}, converting to empty list")
            extracted_keywords = []
        else:
            extracted_keywords = [kw for kw in extracted_keywords_raw if kw and isinstance(kw, str) and len(str(kw).strip()) > 0]
        
        legal_field_raw = self._get_state_value(state, "legal_field", "")
        legal_field = str(legal_field_raw).strip() if legal_field_raw else ""
        
        self.logger.debug(
            f"ğŸ“‹ [PREPARE SEARCH QUERY] Data for query optimization:\n"
            f"   query: '{query[:50]}{'...' if len(query) > 50 else ''}'\n"
            f"   search_query: '{search_query[:50]}{'...' if len(search_query) > 50 else ''}'\n"
            f"   query_type (raw): '{query_type_raw}' â†’ (normalized): '{query_type_str}'\n"
            f"   extracted_keywords: {len(extracted_keywords)} items {extracted_keywords[:5] if extracted_keywords else '[]'}\n"
            f"   legal_field: '{legal_field}'"
        )
        
        return {
            "query": query,
            "search_query": search_query,
            "query_type_str": query_type_str,
            "extracted_keywords": extracted_keywords,
            "legal_field": legal_field
        }
    
    def _optimize_query_with_cache(
        self,
        search_query: str,
        query_type_str: str,
        extracted_keywords: List[str],
        legal_field: str,
        is_retry: bool
    ) -> Tuple[Dict[str, Any], bool]:
        """ì¿¼ë¦¬ ìµœì í™” (ìºì‹± í¬í•¨) (ì¤‘ë³µ ì½”ë“œ ì œê±°)"""
        import hashlib
        
        optimized_queries = None
        cache_hit = False
        
        if not is_retry:
            cache_key_parts = [
                search_query,
                query_type_str,
                ",".join(sorted(extracted_keywords)) if extracted_keywords else "",
                legal_field
            ]
            cache_key = hashlib.md5(":".join(cache_key_parts).encode('utf-8')).hexdigest()
            
            try:
                cached_result = self.performance_optimizer.cache.get_cached_answer(
                    f"query_opt:{cache_key}", query_type_str
                )
                if cached_result and isinstance(cached_result, dict) and "optimized_queries" in cached_result:
                    optimized_queries = cached_result.get("optimized_queries")
                    cache_hit = True
                    self.logger.info(f"âœ… [CACHE HIT] ì¿¼ë¦¬ ìµœì í™” ê²°ê³¼ ìºì‹œ íˆíŠ¸: {cache_key[:16]}...")
            except Exception as e:
                self.logger.debug(f"ìºì‹œ í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
        
        if not optimized_queries:
            optimized_queries = self._optimize_search_query(
                query=search_query,
                query_type=query_type_str,
                extracted_keywords=extracted_keywords,
                legal_field=legal_field
            )
            
            if not is_retry:
                try:
                    cache_key_parts = [
                        search_query,
                        query_type_str,
                        ",".join(sorted(extracted_keywords)) if extracted_keywords else "",
                        legal_field
                    ]
                    cache_key = hashlib.md5(":".join(cache_key_parts).encode('utf-8')).hexdigest()
                    self.performance_optimizer.cache.cache_answer(
                        f"query_opt:{cache_key}",
                        query_type_str,
                        {"optimized_queries": optimized_queries},
                        confidence=1.0,
                        sources=[]
                    )
                    self.logger.debug(f"âœ… [CACHE STORE] ì¿¼ë¦¬ ìµœì í™” ê²°ê³¼ ìºì‹œ ì €ì¥: {cache_key[:16]}...")
                except Exception as e:
                    self.logger.debug(f"ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
        
        return optimized_queries, cache_hit
    
    def _validate_and_fix_optimized_queries(
        self,
        state: LegalWorkflowState,
        optimized_queries: Dict[str, Any],
        query: str
    ) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì¿¼ë¦¬ ê²€ì¦ ë° ìˆ˜ì • (ì¤‘ë³µ ì½”ë“œ ì œê±°)"""
        semantic_query_created = optimized_queries.get("semantic_query", "")
        if not semantic_query_created or not str(semantic_query_created).strip():
            self.logger.warning(f"semantic_query is empty, using base query: '{query[:50]}...'")
            optimized_queries["semantic_query"] = query
            semantic_query_created = query
            self._set_state_value(state, "optimized_queries", optimized_queries)
        
        keyword_queries_created = optimized_queries.get("keyword_queries", [])
        if not keyword_queries_created or len(keyword_queries_created) == 0:
            self.logger.warning("keyword_queries is empty, using base query")
            optimized_queries["keyword_queries"] = [query]
            keyword_queries_created = [query]
            self._set_state_value(state, "optimized_queries", optimized_queries)
        
        return {
            "optimized_queries": optimized_queries,
            "semantic_query_created": semantic_query_created,
            "keyword_queries_created": keyword_queries_created
        }
    
    @observe(name="prepare_search_query")
    @with_state_optimization("prepare_search_query", enable_reduction=False)
    def prepare_search_query(self, state: LegalWorkflowState) -> LegalWorkflowState:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ ë° ìµœì í™” ì „ìš© ë…¸ë“œ (Part 2)"""
        try:
            start_time = time.time()

            preserved = self._preserve_metadata(state, ["query_complexity", "needs_search"])
            if "metadata" not in state or not isinstance(state.get("metadata"), dict):
                state["metadata"] = {}
            state["metadata"] = dict(state["metadata"])
            state["metadata"].update(preserved)
            state["metadata"]["_last_executed_node"] = "prepare_search_query"
            
            if "common" not in state or not isinstance(state.get("common"), dict):
                state["common"] = {}
            if "metadata" not in state["common"]:
                state["common"]["metadata"] = {}
            state["common"]["metadata"]["_last_executed_node"] = "prepare_search_query"

            self._ensure_input_group(state)
            query_value, session_id_value = self._restore_query_from_state(state)
            if query_value:
                query_value = self._normalize_query_encoding(query_value)
                state["input"]["query"] = query_value
                if session_id_value:
                    state["input"]["session_id"] = session_id_value

            # ì¬ì‹œë„ ì¹´ìš´í„° ê´€ë¦¬
            metadata = state.get("metadata", {}) if isinstance(state.get("metadata"), dict) else {}
            if not isinstance(metadata, dict):
                metadata = {}

            # ì¤‘ìš”: state.get("common")ì´ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            common_state = state.get("common")
            if common_state and isinstance(common_state, dict):
                common_metadata = common_state.get("metadata", {})
                if isinstance(common_metadata, dict):
                    metadata = {**metadata, **common_metadata}

            last_executed_node = metadata.get("_last_executed_node", "")
            is_retry_from_generation = (last_executed_node == "generate_answer_enhanced")
            is_retry_from_validation = (last_executed_node == "validate_answer_quality")

            # ì¬ì‹œë„ ì¹´ìš´í„° ì¦ê°€
            if is_retry_from_generation:
                if self.retry_manager.should_allow_retry(state, "generation"):
                    self.retry_manager.increment_retry_count(state, "generation")

            if is_retry_from_validation:
                if self.retry_manager.should_allow_retry(state, "validation"):
                    self.retry_manager.increment_retry_count(state, "validation")

            # ì¬ì‹œë„ íšŸìˆ˜ ì²´í¬
            retry_counts = self.retry_manager.get_retry_counts(state)
            if retry_counts["total"] >= RetryConfig.MAX_TOTAL_RETRIES:
                self.logger.error("Maximum total retry count reached")
                if not self._get_state_value(state, "answer", ""):
                    query = self._get_state_value(state, "query", "")
                    self._set_answer_safely(state,
                        f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ '{query}'ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤.")
                return state

            query_info = self._get_query_info_for_optimization(state)
            if not query_info["query"]:
                self._set_answer_safely(state, "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")
                return state
            
            query = query_info["query"]
            search_query = query_info["search_query"]
            query_type_str = query_info["query_type_str"]
            extracted_keywords = query_info["extracted_keywords"]
            legal_field = query_info["legal_field"]

            is_retry = (last_executed_node == "validate_answer_quality")

            optimized_queries, cache_hit_optimization = self._optimize_query_with_cache(
                search_query=search_query,
                query_type_str=query_type_str,
                extracted_keywords=extracted_keywords,
                legal_field=legal_field,
                is_retry=is_retry
            )

            if is_retry:
                quality_feedback = self.answer_generator.get_quality_feedback_for_retry(state)
                improved_query = self._improve_search_query_for_retry(
                    optimized_queries["semantic_query"],
                    quality_feedback,
                    state
                )
                if improved_query != optimized_queries["semantic_query"]:
                    self.logger.info(
                        f"ğŸ” [SEARCH RETRY] Improved query: '{optimized_queries['semantic_query']}' â†’ '{improved_query}'"
                    )
                    optimized_queries["semantic_query"] = improved_query
                    optimized_queries["keyword_queries"][0] = improved_query

            search_params = self._determine_search_parameters(
                query_type=query_type_str,
                query_complexity=len(query),
                keyword_count=len(extracted_keywords),
                is_retry=is_retry
            )

            self._set_state_value(state, "optimized_queries", optimized_queries)
            self._set_state_value(state, "search_params", search_params)
            self._set_state_value(state, "is_retry_search", is_retry)
            self._set_state_value(state, "search_start_time", start_time)

            validated_queries = self._validate_and_fix_optimized_queries(state, optimized_queries, query)
            optimized_queries = validated_queries["optimized_queries"]
            semantic_query_created = validated_queries["semantic_query_created"]
            keyword_queries_created = validated_queries["keyword_queries_created"]

            self._set_state_value(state, "search_query", semantic_query_created)

            # ìºì‹œ í™•ì¸ (ì¬ì‹œë„ ì‹œì—ëŠ” ìºì‹œ ìš°íšŒ)
            cache_hit = False
            if not is_retry:
                cached_documents = self.performance_optimizer.cache.get_cached_documents(
                    optimized_queries["semantic_query"],
                    query_type_str
                )
                if cached_documents:
                    self._set_state_value(state, "retrieved_docs", cached_documents)
                    self._set_state_value(state, "search_cache_hit", True)
                    cache_hit = True
                    self._add_step(state, "ìºì‹œ íˆíŠ¸", f"ìºì‹œ íˆíŠ¸: {len(cached_documents)}ê°œ ë¬¸ì„œ")

            self._set_state_value(state, "search_cache_hit", cache_hit)
            self._save_metadata_safely(state, "_last_executed_node", "prepare_search_query")
            self._update_processing_time(state, start_time)
            self._add_step(state, "ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„", f"ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ ì™„ë£Œ: {semantic_query_created[:50]}...")

            if cache_hit:
                self.logger.info(f"âœ… [CACHE HIT] ìºì‹œ íˆíŠ¸: {len(cached_documents)}ê°œ ë¬¸ì„œ, ê²€ìƒ‰ ìŠ¤í‚µ")
            else:
                self.logger.info(
                    f"âœ… [PREPARE SEARCH QUERY] "
                    f"semantic_query: '{semantic_query_created[:50]}...', "
                    f"keyword_queries: {len(keyword_queries_created)}ê°œ, "
                    f"search_params: k={search_params.get('semantic_k', 'N/A')}"
                )

        except Exception as e:
            self._handle_error(state, str(e), "ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

        self._ensure_input_group(state)
        query_value, session_id_value = self._restore_query_from_state(state)
        if query_value:
            state["input"]["query"] = query_value
            if session_id_value:
                state["input"]["session_id"] = session_id_value

        return state

