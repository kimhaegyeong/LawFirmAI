# -*- coding: utf-8 -*-
"""
ì¿¼ë¦¬ í™•ì¥ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

ìƒˆë¡œ êµ¬í˜„í•œ LangGraph ë…¸ë“œ â†’ ì„œë¸Œë…¸ë“œ â†’ íƒœìŠ¤í¬ êµ¬ì¡° ê°œì„  ì‚¬í•­ í…ŒìŠ¤íŠ¸
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€ (í•˜ìœ„ í´ë”ë¡œ ì´ë™í•˜ì—¬ parent í•˜ë‚˜ ì¶”ê°€)
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# lawfirm_langgraph ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
lawfirm_langgraph_path = project_root / "lawfirm_langgraph"
sys.path.insert(0, str(lawfirm_langgraph_path))

import logging
try:
    from lawfirm_langgraph.core.utils.logger import get_logger
except ImportError:
    from core.utils.logger import get_logger
from typing import List, Dict, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = get_logger(__name__)


def test_semantic_search_engine_query_expansion():
    """SemanticSearchEngineV2ì˜ ì¿¼ë¦¬ í™•ì¥ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        from lawfirm_langgraph.core.search.engines.semantic_search_engine_v2 import SemanticSearchEngineV2
        from lawfirm_langgraph.core.utils.config import Config
        
        logger.info("=" * 80)
        logger.info("í…ŒìŠ¤íŠ¸ 1: SemanticSearchEngineV2 ì¿¼ë¦¬ í™•ì¥ ê²€ìƒ‰")
        logger.info("=" * 80)
        
        config = Config()
        db_path = config.database_path
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        search_engine = SemanticSearchEngineV2(db_path=db_path)
        logger.info(f"âœ… SemanticSearchEngineV2 ì´ˆê¸°í™” ì™„ë£Œ: {db_path}")
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_query = "ì†í•´ë°°ìƒ ì²­êµ¬"
        expanded_keywords = ["ë¶ˆë²•í–‰ìœ„", "ê³¼ì‹¤", "ê³ ì˜", "ì†í•´", "ë°°ìƒ"]
        
        logger.info(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")
        logger.info(f"ğŸ“ í™•ì¥ í‚¤ì›Œë“œ: {expanded_keywords}")
        
        # 1. ê¸°ì¡´ ê²€ìƒ‰ ë°©ì‹
        logger.info("\n1ï¸âƒ£ ê¸°ì¡´ ê²€ìƒ‰ ë°©ì‹ í…ŒìŠ¤íŠ¸...")
        results_old = search_engine.search(
            query=test_query,
            k=5,
            similarity_threshold=0.5
        )
        logger.info(f"   ê²°ê³¼: {len(results_old)}ê°œ ë¬¸ì„œ")
        if results_old:
            logger.info(f"   ìƒìœ„ 3ê°œ ì ìˆ˜: {[r.get('relevance_score', 0.0) for r in results_old[:3]]}")
        
        # 2. ì¿¼ë¦¬ í™•ì¥ ê²€ìƒ‰ ë°©ì‹
        logger.info("\n2ï¸âƒ£ ì¿¼ë¦¬ í™•ì¥ ê²€ìƒ‰ ë°©ì‹ í…ŒìŠ¤íŠ¸...")
        results_new = search_engine.search_with_query_expansion(
            query=test_query,
            k=5,
            similarity_threshold=0.5,
            expanded_keywords=expanded_keywords,
            use_query_variations=True
        )
        logger.info(f"   ê²°ê³¼: {len(results_new)}ê°œ ë¬¸ì„œ")
        if results_new:
            logger.info(f"   ìƒìœ„ 3ê°œ ì ìˆ˜: {[r.get('relevance_score', 0.0) for r in results_new[:3]]}")
            logger.info(f"   ì¿¼ë¦¬ ë³€í˜• íƒ€ì…: {[r.get('query_variation', 'unknown') for r in results_new[:3]]}")
        
        # 3. ì¿¼ë¦¬ ë³€í˜• ìƒì„± í…ŒìŠ¤íŠ¸
        logger.info("\n3ï¸âƒ£ ì¿¼ë¦¬ ë³€í˜• ìƒì„± í…ŒìŠ¤íŠ¸...")
        variations = search_engine._generate_simple_query_variations(test_query, expanded_keywords)
        logger.info(f"   ìƒì„±ëœ ë³€í˜• ìˆ˜: {len(variations)}")
        for i, var in enumerate(variations, 1):
            logger.info(f"   ë³€í˜• {i}: {var['type']} - '{var['query'][:50]}...' (ê°€ì¤‘ì¹˜: {var['weight']})")
        
        # 4. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        logger.info("\n4ï¸âƒ£ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
        core_keywords = search_engine._extract_core_keywords_simple(test_query)
        logger.info(f"   ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ: {core_keywords}")
        
        logger.info("\nâœ… SemanticSearchEngineV2 ì¿¼ë¦¬ í™•ì¥ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return False


def test_query_expansion_subnode():
    """ì¿¼ë¦¬ í™•ì¥ ì„œë¸Œë…¸ë“œ í…ŒìŠ¤íŠ¸"""
    try:
        from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
        from lawfirm_langgraph.core.workflow.legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
        try:
            from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        except ImportError:
            from lawfirm_langgraph.core.utils.langgraph_config import LangGraphConfig
        
        logger.info("\n" + "=" * 80)
        logger.info("í…ŒìŠ¤íŠ¸ 2: ì¿¼ë¦¬ í™•ì¥ ì„œë¸Œë…¸ë“œ")
        logger.info("=" * 80)
        
        # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        config = LangGraphConfig()
        workflow = EnhancedLegalQuestionWorkflow(config)
        logger.info("âœ… EnhancedLegalQuestionWorkflow ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ State ìƒì„±
        test_state: LegalWorkflowState = {
            "input": {
                "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
                "session_id": "test_session"
            },
            "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
            "query_type": "legal_question",
            "legal_field": "civil",
            "optimized_queries": {
                "expanded_keywords": ["ë¶ˆë²•í–‰ìœ„", "ê³¼ì‹¤", "ê³ ì˜", "ì†í•´", "ë°°ìƒ"]
            },
            "search": {},
            "common": {
                "processing_time": 0.0,
                "tokens_used": 0
            }
        }
        
        logger.info(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_state['query']}")
        logger.info(f"ğŸ“ í™•ì¥ í‚¤ì›Œë“œ: {test_state['optimized_queries']['expanded_keywords']}")
        
        # ì¿¼ë¦¬ í™•ì¥ ì„œë¸Œë…¸ë“œ ì‹¤í–‰
        logger.info("\nğŸ”„ ì¿¼ë¦¬ í™•ì¥ ì„œë¸Œë…¸ë“œ ì‹¤í–‰ ì¤‘...")
        result_state = workflow.query_expansion_subnode(test_state)
        
        # ê²°ê³¼ í™•ì¸
        expanded_queries = result_state.get("expanded_queries", {})
        logger.info(f"\nâœ… ì¿¼ë¦¬ í™•ì¥ ì™„ë£Œ:")
        logger.info(f"   - ë³€í˜• ìˆ˜: {len(expanded_queries.get('variations', []))}")
        logger.info(f"   - ì—°ê´€ í‚¤ì›Œë“œ ìˆ˜: {len(expanded_queries.get('related_keywords', []))}")
        logger.info(f"   - ì •ê·œí™”ëœ ì¿¼ë¦¬ ìˆ˜: {len(expanded_queries.get('normalized', []))}")
        logger.info(f"   - ì „ì²´ ì¿¼ë¦¬ ìˆ˜: {len(expanded_queries.get('all_queries', []))}")
        
        if expanded_queries.get('variations'):
            logger.info("\n   ìƒì„±ëœ ì¿¼ë¦¬ ë³€í˜•:")
            for i, var in enumerate(expanded_queries['variations'][:5], 1):
                logger.info(f"   {i}. {var['type']}: '{var['query'][:60]}...' (ê°€ì¤‘ì¹˜: {var['weight']})")
        
        if expanded_queries.get('related_keywords'):
            logger.info(f"\n   ì—°ê´€ í‚¤ì›Œë“œ: {expanded_queries['related_keywords'][:10]}")
        
        logger.info("\nâœ… ì¿¼ë¦¬ í™•ì¥ ì„œë¸Œë…¸ë“œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return False


def test_semantic_search_variations_subnode():
    """ì˜ë¯¸ì  ê²€ìƒ‰ ë³€í˜• ì„œë¸Œë…¸ë“œ í…ŒìŠ¤íŠ¸"""
    try:
        from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
        from lawfirm_langgraph.core.workflow.legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
        try:
            from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        except ImportError:
            from lawfirm_langgraph.core.utils.langgraph_config import LangGraphConfig
        
        logger.info("\n" + "=" * 80)
        logger.info("í…ŒìŠ¤íŠ¸ 3: ì˜ë¯¸ì  ê²€ìƒ‰ ë³€í˜• ì„œë¸Œë…¸ë“œ")
        logger.info("=" * 80)
        
        # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        config = LangGraphConfig()
        workflow = EnhancedLegalQuestionWorkflow(config)
        logger.info("âœ… EnhancedLegalQuestionWorkflow ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ State ìƒì„± (ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼ í¬í•¨)
        test_state: LegalWorkflowState = {
            "input": {
                "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
                "session_id": "test_session"
            },
            "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
            "query_type": "legal_question",
            "legal_field": "civil",
            "expanded_queries": {
                "original": "ì†í•´ë°°ìƒ ì²­êµ¬",
                "variations": [
                    {"query": "ì†í•´ë°°ìƒ ì²­êµ¬", "type": "original", "weight": 1.0, "priority": 1},
                    {"query": "ì†í•´ë°°ìƒ ì²­êµ¬ ë¶ˆë²•í–‰ìœ„ ê³¼ì‹¤ ê³ ì˜", "type": "keyword_expanded", "weight": 0.9, "priority": 2},
                    {"query": "ì†í•´ë°°ìƒ", "type": "core_keywords", "weight": 0.85, "priority": 2}
                ],
                "all_queries": ["ì†í•´ë°°ìƒ ì²­êµ¬", "ì†í•´ë°°ìƒ ì²­êµ¬ ë¶ˆë²•í–‰ìœ„ ê³¼ì‹¤ ê³ ì˜", "ì†í•´ë°°ìƒ"]
            },
            "search_params": {
                "semantic_k": 5,
                "similarity_threshold": 0.5
            },
            "search": {},
            "common": {
                "processing_time": 0.0,
                "tokens_used": 0
            }
        }
        
        logger.info(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_state['query']}")
        logger.info(f"ğŸ“ ì¿¼ë¦¬ ë³€í˜• ìˆ˜: {len(test_state['expanded_queries']['variations'])}")
        
        # ì˜ë¯¸ì  ê²€ìƒ‰ ë³€í˜• ì„œë¸Œë…¸ë“œ ì‹¤í–‰
        logger.info("\nğŸ”„ ì˜ë¯¸ì  ê²€ìƒ‰ ë³€í˜• ì„œë¸Œë…¸ë“œ ì‹¤í–‰ ì¤‘...")
        result_state = workflow.semantic_search_variations_subnode(test_state)
        
        # ê²°ê³¼ í™•ì¸
        semantic_results = result_state.get("semantic_results", [])
        semantic_count = result_state.get("semantic_count", 0)
        
        logger.info(f"\nâœ… ì˜ë¯¸ì  ê²€ìƒ‰ ì™„ë£Œ:")
        logger.info(f"   - ê²°ê³¼ ìˆ˜: {semantic_count}")
        logger.info(f"   - ê³ ìœ  ê²°ê³¼ ìˆ˜: {len(semantic_results)}")
        
        if semantic_results:
            logger.info("\n   ìƒìœ„ 5ê°œ ê²°ê³¼:")
            for i, result in enumerate(semantic_results[:5], 1):
                query_type = result.get('query_type', 'unknown')
                relevance = result.get('relevance_score', 0.0)
                source = result.get('source', 'Unknown')[:30]
                logger.info(f"   {i}. [{query_type}] ì ìˆ˜: {relevance:.3f}, ì†ŒìŠ¤: {source}...")
        
        logger.info("\nâœ… ì˜ë¯¸ì  ê²€ìƒ‰ ë³€í˜• ì„œë¸Œë…¸ë“œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return False


def test_keyword_search_subnode():
    """í‚¤ì›Œë“œ ê²€ìƒ‰ ì„œë¸Œë…¸ë“œ í…ŒìŠ¤íŠ¸"""
    try:
        from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
        from lawfirm_langgraph.core.workflow.legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
        try:
            from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        except ImportError:
            from lawfirm_langgraph.core.utils.langgraph_config import LangGraphConfig
        
        logger.info("\n" + "=" * 80)
        logger.info("í…ŒìŠ¤íŠ¸ 4: í‚¤ì›Œë“œ ê²€ìƒ‰ ì„œë¸Œë…¸ë“œ")
        logger.info("=" * 80)
        
        # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        config = LangGraphConfig()
        workflow = EnhancedLegalQuestionWorkflow(config)
        logger.info("âœ… EnhancedLegalQuestionWorkflow ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ State ìƒì„±
        test_state: LegalWorkflowState = {
            "input": {
                "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
                "session_id": "test_session"
            },
            "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
            "query_type": "legal_question",
            "legal_field": "civil",
            "expanded_queries": {
                "original": "ì†í•´ë°°ìƒ ì²­êµ¬",
                "all_queries": ["ì†í•´ë°°ìƒ ì²­êµ¬", "ì†í•´ë°°ìƒ"],
                "related_keywords": ["ë¶ˆë²•í–‰ìœ„", "ê³¼ì‹¤", "ê³ ì˜"]
            },
            "search_params": {
                "keyword_limit": 5
            },
            "search": {},
            "common": {
                "processing_time": 0.0,
                "tokens_used": 0
            }
        }
        
        logger.info(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_state['query']}")
        logger.info(f"ğŸ“ í‚¤ì›Œë“œ ì¿¼ë¦¬ ìˆ˜: {len(test_state['expanded_queries']['all_queries'])}")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ì„œë¸Œë…¸ë“œ ì‹¤í–‰
        logger.info("\nğŸ”„ í‚¤ì›Œë“œ ê²€ìƒ‰ ì„œë¸Œë…¸ë“œ ì‹¤í–‰ ì¤‘...")
        result_state = workflow.keyword_search_subnode(test_state)
        
        # ê²°ê³¼ í™•ì¸
        keyword_results = result_state.get("keyword_results", [])
        keyword_count = result_state.get("keyword_count", 0)
        
        logger.info(f"\nâœ… í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ:")
        logger.info(f"   - ê²°ê³¼ ìˆ˜: {keyword_count}")
        logger.info(f"   - ê³ ìœ  ê²°ê³¼ ìˆ˜: {len(keyword_results)}")
        
        if keyword_results:
            logger.info("\n   ìƒìœ„ 5ê°œ ê²°ê³¼:")
            for i, result in enumerate(keyword_results[:5], 1):
                source_type = result.get('source_type', 'unknown')
                relevance = result.get('relevance_score', 0.0)
                source = result.get('source', 'Unknown')[:30]
                logger.info(f"   {i}. [{source_type}] ì ìˆ˜: {relevance:.3f}, ì†ŒìŠ¤: {source}...")
        
        logger.info("\nâœ… í‚¤ì›Œë“œ ê²€ìƒ‰ ì„œë¸Œë…¸ë“œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return False


def test_result_merger_subnode():
    """ê²°ê³¼ í†µí•© ì„œë¸Œë…¸ë“œ í…ŒìŠ¤íŠ¸"""
    try:
        from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
        from lawfirm_langgraph.core.workflow.legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
        try:
            from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        except ImportError:
            from lawfirm_langgraph.core.utils.langgraph_config import LangGraphConfig
        
        logger.info("\n" + "=" * 80)
        logger.info("í…ŒìŠ¤íŠ¸ 5: ê²°ê³¼ í†µí•© ì„œë¸Œë…¸ë“œ")
        logger.info("=" * 80)
        
        # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        config = LangGraphConfig()
        workflow = EnhancedLegalQuestionWorkflow(config)
        logger.info("âœ… EnhancedLegalQuestionWorkflow ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ State ìƒì„± (ì˜ë¯¸ì  ê²€ìƒ‰ ë° í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨)
        test_state: LegalWorkflowState = {
            "input": {
                "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
                "session_id": "test_session"
            },
            "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
            "semantic_results": [
                {"id": "1", "content": "ì†í•´ë°°ìƒì— ê´€í•œ ë²•ë¥ ", "relevance_score": 0.9, "metadata": {"chunk_id": "1"}},
                {"id": "2", "content": "ë¶ˆë²•í–‰ìœ„ ì±…ì„", "relevance_score": 0.8, "metadata": {"chunk_id": "2"}},
            ],
            "keyword_results": [
                {"id": "1", "content": "ì†í•´ë°°ìƒì— ê´€í•œ ë²•ë¥ ", "relevance_score": 0.85, "metadata": {"chunk_id": "1"}},
                {"id": "3", "content": "ë¯¼ë²• ì œ750ì¡°", "relevance_score": 0.75, "metadata": {"chunk_id": "3"}},
            ],
            "search": {},
            "common": {
                "processing_time": 0.0,
                "tokens_used": 0
            }
        }
        
        logger.info(f"\nğŸ“ ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼: {len(test_state['semantic_results'])}ê°œ")
        logger.info(f"ğŸ“ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {len(test_state['keyword_results'])}ê°œ")
        
        # ê²°ê³¼ í†µí•© ì„œë¸Œë…¸ë“œ ì‹¤í–‰
        logger.info("\nğŸ”„ ê²°ê³¼ í†µí•© ì„œë¸Œë…¸ë“œ ì‹¤í–‰ ì¤‘...")
        result_state = workflow.result_merger_subnode(test_state)
        
        # ê²°ê³¼ í™•ì¸
        merged_documents = result_state.get("merged_documents", [])
        retrieved_docs = result_state.get("retrieved_docs", [])
        
        logger.info(f"\nâœ… ê²°ê³¼ í†µí•© ì™„ë£Œ:")
        logger.info(f"   - í†µí•©ëœ ë¬¸ì„œ ìˆ˜: {len(merged_documents)}")
        logger.info(f"   - retrieved_docs ìˆ˜: {len(retrieved_docs)}")
        
        if merged_documents:
            logger.info("\n   í†µí•©ëœ ìƒìœ„ 5ê°œ ê²°ê³¼:")
            for i, doc in enumerate(merged_documents[:5], 1):
                search_method = doc.get('search_method', 'unknown')
                final_score = doc.get('final_weighted_score', doc.get('relevance_score', 0.0))
                logger.info(f"   {i}. [{search_method}] ìµœì¢… ì ìˆ˜: {final_score:.3f}")
        
        logger.info("\nâœ… ê²°ê³¼ í†µí•© ì„œë¸Œë…¸ë“œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return False


def test_expanded_queries_missing():
    """expanded_queriesê°€ ì—†ëŠ” ê²½ìš° í…ŒìŠ¤íŠ¸"""
    try:
        from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
        from lawfirm_langgraph.core.workflow.legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
        try:
            from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        except ImportError:
            from lawfirm_langgraph.core.utils.langgraph_config import LangGraphConfig
        
        logger.info("\n" + "=" * 80)
        logger.info("í…ŒìŠ¤íŠ¸ 6: expanded_queriesê°€ ì—†ëŠ” ê²½ìš°")
        logger.info("=" * 80)
        
        # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        config = LangGraphConfig()
        workflow = EnhancedLegalQuestionWorkflow(config)
        logger.info("âœ… EnhancedLegalQuestionWorkflow ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ State ìƒì„± (expanded_queries ì—†ìŒ)
        test_state: LegalWorkflowState = {
            "input": {
                "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
                "session_id": "test_session"
            },
            "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
            "query_type": "legal_question",
            "legal_field": "civil",
            "search_params": {
                "semantic_k": 5,
                "similarity_threshold": 0.5
            },
            "search": {},
            "common": {
                "processing_time": 0.0,
                "tokens_used": 0
            }
        }
        
        logger.info(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_state['query']}")
        logger.info("ğŸ“ expanded_queries: ì—†ìŒ (í´ë°± ë¡œì§ í…ŒìŠ¤íŠ¸)")
        
        # _get_expanded_queries í—¬í¼ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
        logger.info("\nğŸ”„ _get_expanded_queries í—¬í¼ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸...")
        expanded_queries = workflow._get_expanded_queries(test_state, default_query=test_state['query'])
        logger.info(f"âœ… expanded_queries ìƒì„±ë¨:")
        logger.info(f"   - original: {expanded_queries.get('original', 'N/A')}")
        logger.info(f"   - all_queries: {expanded_queries.get('all_queries', [])}")
        logger.info(f"   - variations: {len(expanded_queries.get('variations', []))}ê°œ")
        
        # _validate_expanded_queries í—¬í¼ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
        logger.info("\nğŸ”„ _validate_expanded_queries í—¬í¼ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸...")
        validated_queries = workflow._validate_expanded_queries(expanded_queries, test_state['query'])
        logger.info(f"âœ… expanded_queries ê²€ì¦ ì™„ë£Œ:")
        logger.info(f"   - í•„ìˆ˜ í•„ë“œ í™•ì¸: original={bool(validated_queries.get('original'))}, "
                   f"all_queries={bool(validated_queries.get('all_queries'))}, "
                   f"variations={'variations' in validated_queries}, "
                   f"related_keywords={'related_keywords' in validated_queries}")
        
        # ì˜ë¯¸ì  ê²€ìƒ‰ ë³€í˜• ì„œë¸Œë…¸ë“œ ì‹¤í–‰ (expanded_queries ì—†ìŒ)
        logger.info("\nğŸ”„ ì˜ë¯¸ì  ê²€ìƒ‰ ë³€í˜• ì„œë¸Œë…¸ë“œ ì‹¤í–‰ (expanded_queries ì—†ìŒ)...")
        result_state = workflow.semantic_search_variations_subnode(test_state)
        
        # ê²°ê³¼ í™•ì¸
        semantic_results = result_state.get("semantic_results", [])
        semantic_count = result_state.get("semantic_count", 0)
        
        logger.info(f"\nâœ… ì˜ë¯¸ì  ê²€ìƒ‰ ì™„ë£Œ (í´ë°± ë¡œì§):")
        logger.info(f"   - ê²°ê³¼ ìˆ˜: {semantic_count}")
        logger.info(f"   - ê³ ìœ  ê²°ê³¼ ìˆ˜: {len(semantic_results)}")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ì„œë¸Œë…¸ë“œ ì‹¤í–‰ (expanded_queries ì—†ìŒ)
        logger.info("\nğŸ”„ í‚¤ì›Œë“œ ê²€ìƒ‰ ì„œë¸Œë…¸ë“œ ì‹¤í–‰ (expanded_queries ì—†ìŒ)...")
        result_state = workflow.keyword_search_subnode(test_state)
        
        keyword_results = result_state.get("keyword_results", [])
        keyword_count = result_state.get("keyword_count", 0)
        
        logger.info(f"\nâœ… í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ (í´ë°± ë¡œì§):")
        logger.info(f"   - ê²°ê³¼ ìˆ˜: {keyword_count}")
        logger.info(f"   - ê³ ìœ  ê²°ê³¼ ìˆ˜: {len(keyword_results)}")
        
        logger.info("\nâœ… expanded_queriesê°€ ì—†ëŠ” ê²½ìš° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return False


def test_parallel_search_failure():
    """ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    try:
        from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
        from lawfirm_langgraph.core.workflow.legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
        try:
            from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        except ImportError:
            from lawfirm_langgraph.core.utils.langgraph_config import LangGraphConfig
        
        logger.info("\n" + "=" * 80)
        logger.info("í…ŒìŠ¤íŠ¸ 7: ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤")
        logger.info("=" * 80)
        
        # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        config = LangGraphConfig()
        workflow = EnhancedLegalQuestionWorkflow(config)
        logger.info("âœ… EnhancedLegalQuestionWorkflow ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ State ìƒì„±
        test_state: LegalWorkflowState = {
            "input": {
                "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
                "session_id": "test_session"
            },
            "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
            "query_type": "legal_question",
            "legal_field": "civil",
            "expanded_queries": {
                "original": "ì†í•´ë°°ìƒ ì²­êµ¬",
                "all_queries": ["ì†í•´ë°°ìƒ ì²­êµ¬"],
                "variations": []
            },
            "search_params": {
                "semantic_k": 5,
                "similarity_threshold": 0.5,
                "keyword_limit": 5
            },
            "search": {},
            "common": {
                "processing_time": 0.0,
                "tokens_used": 0
            }
        }
        
        logger.info(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_state['query']}")
        logger.info("ğŸ“ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ (í´ë°± ë¡œì§ í™•ì¸)")
        
        # execute_searches_parallel ë…¸ë“œ ì‹¤í–‰
        logger.info("\nğŸ”„ execute_searches_parallel ë…¸ë“œ ì‹¤í–‰ ì¤‘...")
        result_state = workflow.execute_searches_parallel(test_state)
        
        # ê²°ê³¼ í™•ì¸
        semantic_results = result_state.get("semantic_results", [])
        keyword_results = result_state.get("keyword_results", [])
        semantic_count = result_state.get("semantic_count", 0)
        keyword_count = result_state.get("keyword_count", 0)
        merged_documents = result_state.get("merged_documents", [])
        retrieved_docs = result_state.get("retrieved_docs", [])
        
        logger.info(f"\nâœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ:")
        logger.info(f"   - ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼: {semantic_count}ê°œ")
        logger.info(f"   - í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {keyword_count}ê°œ")
        logger.info(f"   - í†µí•©ëœ ë¬¸ì„œ: {len(merged_documents)}ê°œ")
        logger.info(f"   - retrieved_docs: {len(retrieved_docs)}ê°œ")
        
        # í´ë°± ë¡œì§ í™•ì¸
        if semantic_count == 0 and keyword_count == 0:
            logger.warning("âš ï¸ ë‘ ê²€ìƒ‰ ëª¨ë‘ ê²°ê³¼ê°€ ì—†ìŒ (í´ë°± ë¡œì§ í™•ì¸ í•„ìš”)")
        elif semantic_count == 0:
            logger.info("â„¹ï¸ ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤íŒ¨, í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš© (í´ë°± ë¡œì§ ì‘ë™)")
        elif keyword_count == 0:
            logger.info("â„¹ï¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨, ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš© (í´ë°± ë¡œì§ ì‘ë™)")
        else:
            logger.info("âœ… ë‘ ê²€ìƒ‰ ëª¨ë‘ ì„±ê³µ")
        
        logger.info("\nâœ… ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return False


def test_state_validation_failure():
    """State ê²€ì¦ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    try:
        from lawfirm_langgraph.core.workflow.state.state_definitions import LegalWorkflowState
        from lawfirm_langgraph.core.workflow.legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
        try:
            from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        except ImportError:
            from lawfirm_langgraph.core.utils.langgraph_config import LangGraphConfig
        
        logger.info("\n" + "=" * 80)
        logger.info("í…ŒìŠ¤íŠ¸ 8: State ê²€ì¦ ì‹¤íŒ¨ ì¼€ì´ìŠ¤")
        logger.info("=" * 80)
        
        # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        config = LangGraphConfig()
        workflow = EnhancedLegalQuestionWorkflow(config)
        logger.info("âœ… EnhancedLegalQuestionWorkflow ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: expanded_queriesê°€ ë¹„ì–´ìˆëŠ” ë”•ì…”ë„ˆë¦¬
        logger.info("\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: expanded_queriesê°€ ë¹„ì–´ìˆëŠ” ë”•ì…”ë„ˆë¦¬")
        test_state_1: LegalWorkflowState = {
            "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
            "expanded_queries": {},
            "search": {},
            "common": {"processing_time": 0.0, "tokens_used": 0}
        }
        
        expanded_queries_1 = workflow._get_expanded_queries(test_state_1, default_query="ì†í•´ë°°ìƒ ì²­êµ¬")
        validated_queries_1 = workflow._validate_expanded_queries(expanded_queries_1, "ì†í•´ë°°ìƒ ì²­êµ¬")
        logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ: original={validated_queries_1.get('original')}, "
                   f"all_queries={len(validated_queries_1.get('all_queries', []))}ê°œ")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: expanded_queriesì— í•„ìˆ˜ í•„ë“œê°€ ì—†ìŒ
        logger.info("\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: expanded_queriesì— í•„ìˆ˜ í•„ë“œê°€ ì—†ìŒ")
        test_state_2: LegalWorkflowState = {
            "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
            "expanded_queries": {
                "variations": [{"query": "í…ŒìŠ¤íŠ¸", "type": "test"}]
                # original, all_queries ë“± í•„ìˆ˜ í•„ë“œ ì—†ìŒ
            },
            "search": {},
            "common": {"processing_time": 0.0, "tokens_used": 0}
        }
        
        expanded_queries_2 = workflow._get_expanded_queries(test_state_2, default_query="ì†í•´ë°°ìƒ ì²­êµ¬")
        validated_queries_2 = workflow._validate_expanded_queries(expanded_queries_2, "ì†í•´ë°°ìƒ ì²­êµ¬")
        logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ: original={validated_queries_2.get('original')}, "
                   f"all_queries={len(validated_queries_2.get('all_queries', []))}ê°œ, "
                   f"variations={len(validated_queries_2.get('variations', []))}ê°œ")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: expanded_queriesê°€ None
        logger.info("\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: expanded_queriesê°€ None")
        test_state_3: LegalWorkflowState = {
            "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
            "expanded_queries": None,
            "search": {},
            "common": {"processing_time": 0.0, "tokens_used": 0}
        }
        
        expanded_queries_3 = workflow._get_expanded_queries(test_state_3, default_query="ì†í•´ë°°ìƒ ì²­êµ¬")
        validated_queries_3 = workflow._validate_expanded_queries(expanded_queries_3, "ì†í•´ë°°ìƒ ì²­êµ¬")
        logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ: original={validated_queries_3.get('original')}, "
                   f"all_queries={len(validated_queries_3.get('all_queries', []))}ê°œ")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: expanded_queriesê°€ ë¬¸ìì—´ (ì˜ëª»ëœ íƒ€ì…)
        logger.info("\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: expanded_queriesê°€ ë¬¸ìì—´ (ì˜ëª»ëœ íƒ€ì…)")
        test_state_4: LegalWorkflowState = {
            "query": "ì†í•´ë°°ìƒ ì²­êµ¬",
            "expanded_queries": "invalid_type",
            "search": {},
            "common": {"processing_time": 0.0, "tokens_used": 0}
        }
        
        expanded_queries_4 = workflow._get_expanded_queries(test_state_4, default_query="ì†í•´ë°°ìƒ ì²­êµ¬")
        validated_queries_4 = workflow._validate_expanded_queries(expanded_queries_4, "ì†í•´ë°°ìƒ ì²­êµ¬")
        logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ: original={validated_queries_4.get('original')}, "
                   f"all_queries={len(validated_queries_4.get('all_queries', []))}ê°œ")
        
        logger.info("\nâœ… State ê²€ì¦ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return False


def run_all_tests():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("\n" + "=" * 80)
    logger.info("ì¿¼ë¦¬ í™•ì¥ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 80)
    
    results = []
    
    # í…ŒìŠ¤íŠ¸ 1: SemanticSearchEngineV2 ì¿¼ë¦¬ í™•ì¥ ê²€ìƒ‰
    results.append(("SemanticSearchEngineV2 ì¿¼ë¦¬ í™•ì¥ ê²€ìƒ‰", test_semantic_search_engine_query_expansion()))
    
    # í…ŒìŠ¤íŠ¸ 2: ì¿¼ë¦¬ í™•ì¥ ì„œë¸Œë…¸ë“œ
    results.append(("ì¿¼ë¦¬ í™•ì¥ ì„œë¸Œë…¸ë“œ", test_query_expansion_subnode()))
    
    # í…ŒìŠ¤íŠ¸ 3: ì˜ë¯¸ì  ê²€ìƒ‰ ë³€í˜• ì„œë¸Œë…¸ë“œ
    results.append(("ì˜ë¯¸ì  ê²€ìƒ‰ ë³€í˜• ì„œë¸Œë…¸ë“œ", test_semantic_search_variations_subnode()))
    
    # í…ŒìŠ¤íŠ¸ 4: í‚¤ì›Œë“œ ê²€ìƒ‰ ì„œë¸Œë…¸ë“œ
    results.append(("í‚¤ì›Œë“œ ê²€ìƒ‰ ì„œë¸Œë…¸ë“œ", test_keyword_search_subnode()))
    
    # í…ŒìŠ¤íŠ¸ 5: ê²°ê³¼ í†µí•© ì„œë¸Œë…¸ë“œ
    results.append(("ê²°ê³¼ í†µí•© ì„œë¸Œë…¸ë“œ", test_result_merger_subnode()))
    
    # í…ŒìŠ¤íŠ¸ 6: expanded_queriesê°€ ì—†ëŠ” ê²½ìš°
    results.append(("expanded_queriesê°€ ì—†ëŠ” ê²½ìš°", test_expanded_queries_missing()))
    
    # í…ŒìŠ¤íŠ¸ 7: ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤
    results.append(("ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤", test_parallel_search_failure()))
    
    # í…ŒìŠ¤íŠ¸ 8: State ê²€ì¦ ì‹¤íŒ¨ ì¼€ì´ìŠ¤
    results.append(("State ê²€ì¦ ì‹¤íŒ¨ ì¼€ì´ìŠ¤", test_state_validation_failure()))
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "=" * 80)
    logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logger.info("=" * 80)
    
    passed = 0
    failed = 0
    
    for test_name, result in results:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        logger.info(f"{status}: {test_name}")
        if result:
            passed += 1
        else:
            failed += 1
    
    logger.info(f"\nì´ {len(results)}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {passed}ê°œ í†µê³¼, {failed}ê°œ ì‹¤íŒ¨")
    
    return failed == 0


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)

