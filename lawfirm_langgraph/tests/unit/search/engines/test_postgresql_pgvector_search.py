# -*- coding: utf-8 -*-
"""
PostgreSQL ë¬¸ì„œ ê²€ìƒ‰ ë° pgvector ê²€ìƒ‰ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
"""

import pytest
import os

# conftest.pyì—ì„œ project_root fixture ìë™ ì‚¬ìš© ê°€ëŠ¥

from lawfirm_langgraph.core.search.connectors.legal_data_connector import LegalDataConnectorV2
from lawfirm_langgraph.core.search.engines.semantic_search_engine_v2 import SemanticSearchEngineV2
from lawfirm_langgraph.core.utils.config import Config
from lawfirm_langgraph.core.utils.logger import get_logger

logger = get_logger(__name__)


@pytest.fixture
def config():
    """ì„¤ì • ë¡œë“œ"""
    return Config()


@pytest.fixture
def legal_connector(config):
    """LegalDataConnectorV2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return LegalDataConnectorV2()


@pytest.fixture
def semantic_engine(config):
    """SemanticSearchEngineV2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    # pgvector ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
    os.environ['VECTOR_SEARCH_METHOD'] = 'pgvector'
    engine = SemanticSearchEngineV2()
    return engine


class TestPostgreSQLDocumentSearch:
    """PostgreSQL ë¬¸ì„œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    
    def test_statute_search_fts(self, legal_connector):
        """ë²•ë ¹ FTS ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        query = "ê³„ì•½ í•´ì§€"
        limit = 10
        
        results = legal_connector.search_statutes_fts(query, limit=limit)
        
        assert isinstance(results, list), "ê²°ê³¼ëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤"
        assert len(results) <= limit, f"ê²°ê³¼ëŠ” {limit}ê°œ ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
        
        if results:
            result = results[0]
            assert 'id' in result, "ê²°ê³¼ì— 'id'ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            assert 'type' in result, "ê²°ê³¼ì— 'type'ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            assert 'content' in result, "ê²°ê³¼ì— 'content'ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            assert 'source' in result, "ê²°ê³¼ì— 'source'ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            assert 'metadata' in result, "ê²°ê³¼ì— 'metadata'ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            assert result['type'] == 'statute_article', "íƒ€ì…ì€ 'statute_article'ì´ì–´ì•¼ í•©ë‹ˆë‹¤"
            
            logger.info(f"âœ… ë²•ë ¹ ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼")
            logger.info(f"   ì²« ë²ˆì§¸ ê²°ê³¼: {result.get('source', 'N/A')}")
        else:
            logger.warning(f"âš ï¸ ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: query='{query}'")
    
    def test_case_search_fts(self, legal_connector):
        """íŒë¡€ FTS ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        query = "ê³„ì•½ í•´ì§€"
        limit = 10
        
        results = legal_connector.search_cases_fts(query, limit=limit)
        
        assert isinstance(results, list), "ê²°ê³¼ëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤"
        assert len(results) <= limit, f"ê²°ê³¼ëŠ” {limit}ê°œ ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
        
        if results:
            result = results[0]
            assert 'id' in result, "ê²°ê³¼ì— 'id'ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            assert 'type' in result, "ê²°ê³¼ì— 'type'ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            assert 'content' in result, "ê²°ê³¼ì— 'content'ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            assert 'source' in result, "ê²°ê³¼ì— 'source'ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            assert 'metadata' in result, "ê²°ê³¼ì— 'metadata'ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            
            logger.info(f"âœ… íŒë¡€ ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼")
            logger.info(f"   ì²« ë²ˆì§¸ ê²°ê³¼: {result.get('source', 'N/A')}")
        else:
            logger.warning(f"âš ï¸ íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: query='{query}'")
    
    def test_parallel_search(self, legal_connector):
        """ë³‘ë ¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ë²•ë ¹ + íŒë¡€)"""
        query = "ê³„ì•½ í•´ì§€"
        limit = 10
        
        results = legal_connector._search_documents_parallel(query, limit=limit)
        
        assert isinstance(results, list), "ê²°ê³¼ëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤"
        assert len(results) <= limit * 2, f"ê²°ê³¼ëŠ” {limit * 2}ê°œ ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
        
        if results:
            logger.info(f"âœ… ë³‘ë ¬ ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼")
            type_counts = {}
            for result in results:
                result_type = result.get('type', 'unknown')
                type_counts[result_type] = type_counts.get(result_type, 0) + 1
            logger.info(f"   íƒ€ì…ë³„ ë¶„í¬: {type_counts}")
        else:
            logger.warning(f"âš ï¸ ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: query='{query}'")


class TestPgVectorSearch:
    """pgvector ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    
    def test_pgvector_weighted_search(self, semantic_engine):
        """pgvector ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        query = "ê³„ì•½ í•´ì§€ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
        k = 10
        
        # ê²€ìƒ‰ ì‹¤í–‰
        results = semantic_engine.search(
            query=query,
            k=k,
            similarity_threshold=0.5
        )
        
        assert isinstance(results, list), "ê²°ê³¼ëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤"
        assert len(results) <= k, f"ê²°ê³¼ëŠ” {k}ê°œ ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
        
        if results:
            logger.info(f"âœ… pgvector ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼")
            
            # íƒ€ì…ë³„ ë¶„í¬ í™•ì¸
            type_counts = {}
            for result in results:
                if isinstance(result, dict):
                    source_type = result.get('source_type', 'unknown')
                    type_counts[source_type] = type_counts.get(source_type, 0) + 1
                elif isinstance(result, tuple) and len(result) >= 3:
                    source_type = result[2] if len(result) > 2 else 'unknown'
                    type_counts[source_type] = type_counts.get(source_type, 0) + 1
            
            logger.info(f"   íƒ€ì…ë³„ ë¶„í¬: {type_counts}")
            
            # ê²°ê³¼ êµ¬ì¡° í™•ì¸
            first_result = results[0]
            if isinstance(first_result, dict):
                assert 'chunk_id' in first_result or 'id' in first_result, "ê²°ê³¼ì— IDê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            elif isinstance(first_result, tuple):
                assert len(first_result) >= 2, "ê²°ê³¼ íŠœí”Œì€ ìµœì†Œ 2ê°œ ìš”ì†Œë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤"
        else:
            logger.warning(f"âš ï¸ pgvector ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: query='{query}'")
    
    def test_pgvector_table_detection(self, semantic_engine):
        """pgvector í…Œì´ë¸” ìë™ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        available_tables = semantic_engine._get_available_vector_tables()
        
        assert isinstance(available_tables, list), "ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”ì€ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤"
        
        logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ë²¡í„° í…Œì´ë¸”: {len(available_tables)}ê°œ")
        for table in available_tables:
            logger.info(f"   - {table.get('source_type', 'unknown')}: {table.get('table_name', 'N/A')}")
        
        # ìµœì†Œí•œ precedent_content ë˜ëŠ” statute_articleì´ ìˆì–´ì•¼ í•¨
        source_types = [t.get('source_type') for t in available_tables]
        assert 'precedent_content' in source_types or 'statute_article' in source_types, \
            "ìµœì†Œí•œ precedent_content ë˜ëŠ” statute_article í…Œì´ë¸”ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
    
    def test_pgvector_search_with_source_types(self, semantic_engine):
        """íŠ¹ì • ì†ŒìŠ¤ íƒ€ì…ìœ¼ë¡œ pgvector ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        query = "ê³„ì•½ í•´ì§€"
        k = 5
        source_types = ['precedent_content']  # íŒë¡€ë§Œ ê²€ìƒ‰
        
        results = semantic_engine.search(
            query=query,
            k=k,
            source_types=source_types,
            similarity_threshold=0.5
        )
        
        assert isinstance(results, list), "ê²°ê³¼ëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤"
        
        if results:
            logger.info(f"âœ… íŠ¹ì • íƒ€ì… ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼ (íƒ€ì…: {source_types})")
        else:
            logger.warning(f"âš ï¸ íŠ¹ì • íƒ€ì… ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: query='{query}', types={source_types}")


class TestSearchIntegration:
    """ê²€ìƒ‰ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def test_hybrid_search_flow(self, legal_connector, semantic_engine):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í”Œë¡œìš° í…ŒìŠ¤íŠ¸ (FTS + pgvector)"""
        query = "ê³„ì•½ í•´ì§€ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
        
        # 1. FTS ê²€ìƒ‰
        fts_results = legal_connector._search_documents_parallel(query, limit=5)
        logger.info(f"ğŸ“Š FTS ê²€ìƒ‰ ê²°ê³¼: {len(fts_results)}ê°œ")
        
        # 2. pgvector ê²€ìƒ‰
        vector_results = semantic_engine.search(
            query=query,
            k=5,
            similarity_threshold=0.5
        )
        logger.info(f"ğŸ“Š pgvector ê²€ìƒ‰ ê²°ê³¼: {len(vector_results)}ê°œ")
        
        # 3. ê²°ê³¼ í†µí•© í™•ì¸
        total_results = len(fts_results) + len(vector_results)
        logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: ì´ {total_results}ê°œ ê²°ê³¼")
        
        assert total_results >= 0, "ì´ ê²°ê³¼ ìˆ˜ëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])

