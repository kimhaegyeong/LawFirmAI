# -*- coding: utf-8 -*-
"""
LangGraph ì§ˆì˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (astream_events ì‚¬ìš©)

Usage:
    python lawfirm_langgraph/tests/runners/run_query_test_astream_events.py "ì§ˆì˜ ë‚´ìš©"
    python lawfirm_langgraph/tests/runners/run_query_test_astream_events.py  # ê¸°ë³¸ ì§ˆì˜ ì‚¬ìš©
"""

import sys
import os
import asyncio
import logging
import signal
import atexit
import time
from pathlib import Path
from datetime import datetime
from typing import Optional

# TRACE ë ˆë²¨ ì¶”ê°€ (DEBUGë³´ë‹¤ ë‚®ì€ ë ˆë²¨, ê°’: 5)
if not hasattr(logging, 'TRACE'):
    logging.TRACE = 5
    logging.addLevelName(logging.TRACE, "TRACE")
    
    # Logger í´ë˜ìŠ¤ì— trace ë©”ì„œë“œ ì¶”ê°€
    def trace(self, message, *args, **kwargs):
        """TRACE ë ˆë²¨ ë¡œê·¸"""
        if self.isEnabledFor(logging.TRACE):
            self._log(logging.TRACE, message, args, **kwargs)
    
    logging.Logger.trace = trace

# UTF-8 ì¸ì½”ë”© ì„¤ì • (Windows í˜¸í™˜)
os.environ['PYTHONIOENCODING'] = 'utf-8'
if sys.platform == 'win32':
    os.environ['PYTHONLEGACYWINDOWSSTDIO'] = 'utf-8'

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
script_dir = Path(__file__).parent
runners_dir = script_dir.parent
tests_dir = runners_dir.parent
lawfirm_langgraph_dir = tests_dir.parent
project_root = lawfirm_langgraph_dir

if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
if str(lawfirm_langgraph_dir) not in sys.path:
    sys.path.insert(0, str(lawfirm_langgraph_dir))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from utils.env_loader import ensure_env_loaded
    ensure_env_loaded(project_root)
except ImportError:
    pass

# run_query_test.pyì˜ ëª¨ë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ import
# (LineBufferedFileHandler, SafeStreamHandler, Tee, setup_logging ë“±)
# íŒŒì¼ì´ ë„ˆë¬´ ê¸¸ì–´ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ë³µì‚¬í•˜ê±°ë‚˜ import ì‚¬ìš©
# ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ run_query_testì˜ í•¨ìˆ˜ë“¤ì„ ì¬ì‚¬ìš©

# run_query_test.pyì˜ setup_loggingê³¼ ë‹¤ë¥¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ import
import importlib.util
run_query_test_path = script_dir / "run_query_test.py"
spec = importlib.util.spec_from_file_location("run_query_test", run_query_test_path)
run_query_test_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(run_query_test_module)

# í•„ìš”í•œ í•¨ìˆ˜ë“¤ ê°€ì ¸ì˜¤ê¸°
setup_logging = run_query_test_module.setup_logging
get_query_from_args = run_query_test_module.get_query_from_args
flush_all_log_handlers = run_query_test_module.flush_all_log_handlers


async def test_langgraph_query_astream_events(query: str, logger: logging.Logger):
    """LangGraph ì§ˆì˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (astream_events ì‚¬ìš©)
    
    Args:
        query: í…ŒìŠ¤íŠ¸í•  ì§ˆì˜
        logger: ë¡œê±°
    """
    logger.info("=" * 80)
    logger.info("LangGraph ì§ˆì˜ í…ŒìŠ¤íŠ¸ (astream_events ì‚¬ìš©)")
    logger.info("=" * 80)
    logger.info(f"ì§ˆì˜: {query}")
    
    try:
        # ì´ˆê¸°í™” ì‹œê°„ ì¸¡ì • ì‹œì‘
        import time
        total_start_time = time.time()
        
        # ì„¤ì • ë¡œë“œ
        logger.info("1. ì„¤ì • ë¡œë“œ ì¤‘...")
        setup_start = time.time()
        from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        from lawfirm_langgraph.config.app_config import Config as AppConfig
        
        config = LangGraphConfig.from_env()
        config.enable_checkpoint = False
        setup_time = time.time() - setup_start
        logger.info(f"   LangGraph í™œì„±í™”: {config.langgraph_enabled}")
        logger.info(f"   ì²´í¬í¬ì¸íŠ¸: {config.enable_checkpoint}")
        logger.info(f"   ì„¤ì • ë¡œë“œ ì‹œê°„: {setup_time:.3f}ì´ˆ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë° ë²¡í„° ê²€ìƒ‰ ì„¤ì • í™•ì¸
        logger.info("\n1.1. ë°ì´í„°ë² ì´ìŠ¤ ë° ë²¡í„° ê²€ìƒ‰ ì„¤ì • í™•ì¸...")
        db_check_start = time.time()
        app_config = AppConfig()
        
        # SQLite URL ê²€ì¦ (í…ŒìŠ¤íŠ¸ ì‹œì‘ ì „)
        if app_config.database_url.startswith("sqlite://"):
            logger.error("   âŒ SQLiteëŠ” ë” ì´ìƒ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. PostgreSQLì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            raise ValueError("SQLite is no longer supported. Please configure PostgreSQL.")
        
        logger.info(f"   âœ… Database URL ì„¤ì •ë¨ (PostgreSQL)")
        logger.info(f"   VECTOR_SEARCH_METHOD: {app_config.vector_search_method}")
        
        # DatabaseAdapter í™•ì¸
        try:
            from lawfirm_langgraph.core.data.db_adapter import DatabaseAdapter
            if app_config.database_url:
                db_adapter = DatabaseAdapter(app_config.database_url)
                logger.info(f"   âœ… DatabaseAdapter ì´ˆê¸°í™” ì„±ê³µ: type={db_adapter.db_type}")
        except Exception as e:
            logger.error(f"   âŒ DatabaseAdapter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
        
        db_check_time = time.time() - db_check_start
        logger.info(f"   ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ ì‹œê°„: {db_check_time:.3f}ì´ˆ")
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        logger.info("\n2. LangGraphWorkflowService ì´ˆê¸°í™” ì¤‘...")
        service_start = time.time()
        
        try:
            from lawfirm_langgraph.core.workflow.workflow_service import LangGraphWorkflowService
            
            service = LangGraphWorkflowService(config)
            service_time = time.time() - service_start
            logger.info(f"   ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (ì´ ì‹œê°„: {service_time:.3f}ì´ˆ)")
            
            init_total_time = time.time() - total_start_time
            logger.info(f"\nì´ˆê¸°í™” ì™„ë£Œ (ì´ ì‹œê°„: {init_total_time:.3f}ì´ˆ)")
                
        except Exception as e:
            logger.error(f"   âŒ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {type(e).__name__}: {e}")
            logger.debug("ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True)
            raise
        
        # ì§ˆì˜ ì²˜ë¦¬ (astream_events ì‚¬ìš©)
        logger.info("\n3. ì§ˆì˜ ì²˜ë¦¬ ì¤‘ (astream_events ì‚¬ìš©)...")
        logger.info("   (ì´ ì‘ì—…ì€ ëª‡ ì´ˆì—ì„œ ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        query_start_time = time.time()
        
        try:
            logger.info("   ğŸ”„ process_query ì‹¤í–‰ ì‹œì‘ (use_astream_events=True)...")
            
            # ğŸ”¥ CRITICAL: astream_events ì‚¬ìš© ëª…ì‹œ
            result = await service.process_query(
                query=query,
                session_id="test_langgraph_query_astream_events",
                enable_checkpoint=False,
                use_astream_events=True  # astream_events ì‚¬ìš©
            )
            
            logger.info("   âœ… process_query ì‹¤í–‰ ì™„ë£Œ")
                
        except Exception as query_error:
            logger.error(f"   âŒ ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {type(query_error).__name__}: {query_error}")
            logger.debug("   ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True)
            raise
        
        query_end_time = time.time()
        query_elapsed_time = query_end_time - query_start_time
        total_elapsed_time = query_end_time - total_start_time
        logger.info(f"   ì§ˆì˜ ì²˜ë¦¬ ì™„ë£Œ (ì§ˆì˜ ì²˜ë¦¬ ì‹œê°„: {query_elapsed_time:.2f}ì´ˆ, ì´ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ)")
        
        # ğŸ”¥ ê°œì„ : resultê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
        if result is None:
            logger.error("   âŒ ì§ˆì˜ ì²˜ë¦¬ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            raise ValueError("Query processing returned None result")
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info("\n4. ê²°ê³¼:")
        logger.info("=" * 80)
        
        # ë‹µë³€ ì¶”ì¶œ
        answer = result.get("answer", "")
        if isinstance(answer, dict):
            answer = answer.get("answer", "") or answer.get("content", "") or ""
        answer = str(answer).strip() if answer else ""
        
        if not answer:
            output = result.get("output", {})
            if isinstance(output, dict):
                answer = output.get("answer", "") or output.get("content", "")
        answer = str(answer).strip() if answer else ""
        
        if answer:
            logger.info(f"\në‹µë³€ ({len(answer)}ì):")
            logger.info("-" * 80)
            logger.info(answer)
            flush_all_log_handlers()
        else:
            logger.warning("\në‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤!")
            flush_all_log_handlers()
        
        # ê²€ìƒ‰ ê²°ê³¼ (type ì •ë³´ í™•ì¸)
        retrieved_docs = result.get("retrieved_docs", [])
        if retrieved_docs:
            logger.info(f"\nê²€ìƒ‰ëœ ì°¸ê³ ìë£Œ ({len(retrieved_docs)}ê°œ):")
            
            # type ì •ë³´ í†µê³„
            type_stats = {}
            unknown_count = 0
            
            for i, doc in enumerate(retrieved_docs[:10], 1):
                if isinstance(doc, dict):
                    # type ì •ë³´ ì¶”ì¶œ
                    doc_type = (
                        doc.get("type") or 
                        doc.get("source_type") or 
                        (doc.get("metadata", {}).get("type") if isinstance(doc.get("metadata"), dict) else None) or
                        (doc.get("metadata", {}).get("source_type") if isinstance(doc.get("metadata"), dict) else None) or
                        "unknown"
                    )
                    
                    # í†µê³„ ìˆ˜ì§‘
                    if doc_type == "unknown":
                        unknown_count += 1
                    type_stats[doc_type] = type_stats.get(doc_type, 0) + 1
                    
                    # íƒ€ì… ì´ë¦„ ë³€í™˜
                    type_names = {
                        "statute_article": "ë²•ë ¹",
                        "precedent_content": "íŒë¡€",
                        "unknown": "ì•Œ ìˆ˜ ì—†ìŒ"
                    }
                    doc_type_display = type_names.get(doc_type, doc_type)
                    
                    # ì œëª© ì¶”ì¶œ
                    title = (
                        doc.get("title") or 
                        doc.get("name") or 
                        doc.get("source") or
                        (doc.get("content", "")[:100] if doc.get("content") else "") or
                        (doc.get("text", "")[:100] if doc.get("text") else "") or
                        "ì œëª© ì—†ìŒ"
                    )
                    
                    # ì ìˆ˜ ì¶”ì¶œ
                    score = (
                        doc.get("relevance_score") or 
                        doc.get("final_weighted_score") or
                        doc.get("score") or 
                        doc.get("similarity") or 
                        0.0
                    )
                    score_display = f"{score:.3f}" if isinstance(score, (int, float)) else str(score)
                    
                    logger.info(f"   {i}. [{doc_type_display}] {title}")
                    logger.info(f"       ì ìˆ˜: {score_display}, type={doc_type}")
                    
                    # typeì´ unknownì¸ ê²½ìš° ìƒì„¸ ì •ë³´ ë¡œê¹…
                    if doc_type == "unknown":
                        logger.warning(f"       âš ï¸  type=unknown ê°ì§€!")
                        logger.debug(f"       - doc.type: {doc.get('type')}")
                        logger.debug(f"       - doc.source_type: {doc.get('source_type')}")
                        logger.debug(f"       - metadata.type: {doc.get('metadata', {}).get('type') if isinstance(doc.get('metadata'), dict) else 'N/A'}")
                        logger.debug(f"       - metadata.source_type: {doc.get('metadata', {}).get('source_type') if isinstance(doc.get('metadata'), dict) else 'N/A'}")
                        logger.debug(f"       - doc keys: {list(doc.keys())[:20]}")
                        logger.debug(f"       - metadata keys: {list(doc.get('metadata', {}).keys())[:20] if isinstance(doc.get('metadata'), dict) else 'N/A'}")
                else:
                    logger.info(f"   {i}. {str(doc)[:100]}")
            
            if len(retrieved_docs) > 10:
                logger.info(f"   ... (ì´ {len(retrieved_docs)}ê°œ)")
            
            # type í†µê³„ ì¶œë ¥
            logger.info(f"\nğŸ“Š Type í†µê³„:")
            for doc_type, count in sorted(type_stats.items(), key=lambda x: x[1], reverse=True):
                type_names = {
                    "statute_article": "ë²•ë ¹",
                    "precedent_content": "íŒë¡€",
                    "unknown": "ì•Œ ìˆ˜ ì—†ìŒ"
                }
                doc_type_display = type_names.get(doc_type, doc_type)
                logger.info(f"   - {doc_type_display}: {count}ê°œ")
            
            if unknown_count > 0:
                logger.warning(f"\nâš ï¸  type=unknownì¸ ë¬¸ì„œê°€ {unknown_count}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                logger.info(f"\nâœ… ëª¨ë“  ë¬¸ì„œì˜ type ì •ë³´ê°€ ì •ìƒì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            flush_all_log_handlers()
        else:
            logger.warning("\nê²€ìƒ‰ëœ ì°¸ê³ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤!")
            flush_all_log_handlers()
        
        # ì²˜ë¦¬ ì‹œê°„
        processing_time = result.get("processing_time", 0.0)
        if processing_time:
            logger.info(f"\nì²˜ë¦¬ ì‹œê°„ (ê²°ê³¼): {processing_time:.2f}ì´ˆ")
        if 'query_elapsed_time' in locals():
            logger.info(f"ì²˜ë¦¬ ì‹œê°„ (ì¸¡ì •): {query_elapsed_time:.2f}ì´ˆ")
        flush_all_log_handlers()
        
        # ì˜¤ë¥˜ í™•ì¸
        errors = result.get("errors", [])
        if errors:
            logger.warning(f"\nì˜¤ë¥˜ ë°œìƒ ({len(errors)}ê°œ):")
            for i, error in enumerate(errors[:5], 1):
                logger.warning(f"   {i}. {error}")
            flush_all_log_handlers()
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("\n5. ê²°ê³¼ ìš”ì•½:")
        logger.info("=" * 80)
        
        summary = {
            "ì§ˆì˜": query,
            "ë‹µë³€ ê¸¸ì´": len(answer) if answer else 0,
            "ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜": len(retrieved_docs) if retrieved_docs else 0,
            "type=unknown ë¬¸ì„œ ìˆ˜": unknown_count if 'unknown_count' in locals() else 0,
            "ì²˜ë¦¬ ì‹œê°„": f"{processing_time:.2f}ì´ˆ" if processing_time else "N/A",
            "ì˜¤ë¥˜ ìˆ˜": len(errors) if errors else 0
        }
        
        logger.info("   ìš”ì•½ ì •ë³´:")
        for key, value in summary.items():
            logger.info(f"   - {key}: {value}")
        
        flush_all_log_handlers()
        
        logger.info("\n" + "=" * 80)
        logger.info("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info("=" * 80)
        
        flush_all_log_handlers()
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        try:
            if hasattr(service, 'cleanup'):
                service.cleanup()
        except Exception as e:
            logger.debug(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
        
        return result
        
    except Exception as e:
        logger.error(f"\nì˜¤ë¥˜ ë°œìƒ: {type(e).__name__}: {e}")
        logger.debug("ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True)
        flush_all_log_handlers()
        raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger = None
    log_file_path = None
    
    try:
        # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ê²°ì •
        log_dir_env = os.getenv("TEST_LOG_DIR")
        if log_dir_env:
            log_dir = Path(log_dir_env)
        else:
            log_dir = project_root / "logs" / "langgraph"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        log_file_env = os.getenv("TEST_LOG_FILE")
        if log_file_env:
            log_file_path = str(Path(log_file_env))
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file_path = str(log_dir / f"test_astream_events_{timestamp}.log")
        
        # ë¡œê¹… ì„¤ì •
        logger = setup_logging(log_file_path=log_file_path)
        
        if logger:
            logger.info("=" * 80)
            logger.info("í…ŒìŠ¤íŠ¸ ì‹œì‘ (astream_events ì‚¬ìš©)")
            logger.info("=" * 80)
            flush_all_log_handlers()
        
        # ì§ˆì˜ ê°€ì ¸ì˜¤ê¸°
        query = get_query_from_args()
        
        if not query:
            if logger:
                logger.error("ì§ˆì˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                logger.info("\nì‚¬ìš©ë²•:")
                logger.info("  python run_query_test_astream_events.py \"ì§ˆì˜ ë‚´ìš©\"")
                logger.info("  python run_query_test_astream_events.py 0  # ê¸°ë³¸ ì§ˆì˜ ì„ íƒ")
                logger.info("  $env:TEST_QUERY='ì§ˆì˜ë‚´ìš©'; python run_query_test_astream_events.py")
            return 1
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        flush_all_log_handlers()
        
        try:
            asyncio.run(test_langgraph_query_astream_events(query, logger))
        except KeyboardInterrupt:
            flush_all_log_handlers()
            if logger:
                logger.warning("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            raise
        except Exception as async_error:
            flush_all_log_handlers()
            if logger:
                logger.error(f"\n\në¹„ë™ê¸° ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {type(async_error).__name__}: {async_error}")
                logger.debug("   ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True)
            flush_all_log_handlers()
            raise
        finally:
            flush_all_log_handlers()
        
        # í…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì¶œë ¥
        if log_file_path:
            print(f"\n[í…ŒìŠ¤íŠ¸ ì™„ë£Œ]")
            print(f"  ë¡œê·¸ íŒŒì¼: {log_file_path}")
            print(f"  ë¡œê·¸ íŒŒì¼ì„ í™•ì¸í•˜ì—¬ type ì •ë³´ ë³´ì¡´ ì—¬ë¶€ë¥¼ ê²€ì¦í•˜ì„¸ìš”.")
        
        flush_all_log_handlers()
        
        return 0
        
    except KeyboardInterrupt:
        if logger:
            logger.warning("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        flush_all_log_handlers()
        return 1
    except Exception as e:
        if logger:
            logger.error(f"\n\ní…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            logger.debug("ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True)
        flush_all_log_handlers()
        return 1
    finally:
        flush_all_log_handlers()


if __name__ == "__main__":
    sys.exit(main())

