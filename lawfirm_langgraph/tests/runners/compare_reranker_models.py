# -*- coding: utf-8 -*-
"""
Cross-Encoder ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ë‘ ê°œì˜ Cross-Encoder ëª¨ë¸ì„ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

Usage:
    python lawfirm_langgraph/tests/runners/compare_reranker_models.py "ì§ˆì˜ ë‚´ìš©"
    python lawfirm_langgraph/tests/runners/compare_reranker_models.py  # ê¸°ë³¸ ì§ˆì˜ ì‚¬ìš©
"""

import sys
import os
import asyncio
import logging
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# UTF-8 ì¸ì½”ë”© ì„¤ì • (Windows í˜¸í™˜)
os.environ['PYTHONIOENCODING'] = 'utf-8'
if sys.platform == 'win32':
    os.environ['PYTHONLEGACYWINDOWSSTDIO'] = 'utf-8'

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
script_dir = Path(__file__).parent
runners_dir = script_dir.parent
tests_dir = runners_dir.parent
lawfirm_langgraph_dir = tests_dir.parent
project_root = lawfirm_langgraph_dir

if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
if str(lawfirm_langgraph_dir) not in sys.path:
    sys.path.insert(0, str(lawfirm_langgraph_dir))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from utils.env_loader import ensure_env_loaded
    ensure_env_loaded(project_root)
except ImportError:
    pass

# ë¡œê±° ì„¤ì •
from lawfirm_langgraph.core.utils.logger import get_logger
logger = get_logger(__name__)

# í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡
MODELS_TO_COMPARE = [
    "Dongjin-kr/ko-reranker",
    "dragonkue/bge-reranker-v2-m3-ko"
]

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆì˜
DEFAULT_QUERY = "ê³„ì•½ í•´ì§€ ì‚¬ìœ ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"


def compare_models_on_query(query: str, test_documents: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ë‘ ëª¨ë¸ì„ ë™ì¼í•œ ì¿¼ë¦¬ì™€ ë¬¸ì„œì— ëŒ€í•´ ë¹„êµ
    
    Args:
        query: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_documents: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    from lawfirm_langgraph.core.search.processors.result_merger import ResultRanker
    
    results = {
        "query": query,
        "num_documents": len(test_documents),
        "models": {}
    }
    
    for model_name in MODELS_TO_COMPARE:
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ” Testing model: {model_name}")
        logger.info(f"{'='*80}\n")
        
        try:
            # ëª¨ë¸ ë¡œë“œ ì‹œê°„ ì¸¡ì •
            load_start = time.time()
            ranker = ResultRanker(
                use_cross_encoder=True,
                cross_encoder_model_name=model_name
            )
            
            # ëª¨ë¸ ê°•ì œ ë¡œë“œ
            if ranker._ensure_cross_encoder_loaded():
                load_time = time.time() - load_start
                logger.info(f"âœ… Model loaded in {load_time:.2f} seconds")
            else:
                logger.error(f"âŒ Failed to load model: {model_name}")
                results["models"][model_name] = {
                    "status": "failed",
                    "error": "Model loading failed"
                }
                continue
            
            # ê° ë¬¸ì„œì— ëŒ€í•´ ì ìˆ˜ ê³„ì‚°
            scores = []
            score_start = time.time()
            
            for i, doc in enumerate(test_documents):
                doc_text = doc.get("text") or doc.get("content", "")
                if not doc_text:
                    continue
                
                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
                processed_text = ranker._preprocess_text_for_cross_encoder(doc_text, max_length=512)
                
                if not processed_text:
                    continue
                
                # Cross-Encoder ì ìˆ˜ ê³„ì‚°
                try:
                    from sentence_transformers import CrossEncoder
                    pairs = [[query, processed_text]]
                    doc_scores = ranker.cross_encoder.predict(pairs, batch_size=1, show_progress_bar=False)
                    raw_score = float(doc_scores[0])
                    
                    scores.append({
                        "doc_index": i,
                        "doc_type": doc.get("type", "unknown"),
                        "raw_score": raw_score,
                        "text_preview": processed_text[:100] + "..." if len(processed_text) > 100 else processed_text,
                        "original_relevance_score": doc.get("relevance_score", 0.0),
                        "original_rank_score": doc.get("rank_score", 0.0)
                    })
                    
                    logger.debug(
                        f"  Document {i+1}/{len(test_documents)}: "
                        f"type={doc.get('type', 'unknown')}, "
                        f"score={raw_score:.4f}"
                    )
                except Exception as e:
                    logger.warning(f"  Failed to score document {i+1}: {e}")
                    continue
            
            score_time = time.time() - score_start
            
            # í†µê³„ ê³„ì‚°
            if scores:
                raw_scores = [s["raw_score"] for s in scores]
                avg_score = sum(raw_scores) / len(raw_scores)
                max_score = max(raw_scores)
                min_score = min(raw_scores)
                
                # íŒë¡€ ë¬¸ì„œ ì ìˆ˜ í†µê³„
                precedent_scores = [s["raw_score"] for s in scores if "precedent" in s.get("doc_type", "").lower()]
                precedent_avg = sum(precedent_scores) / len(precedent_scores) if precedent_scores else 0.0
                
                # ë²•ë ¹ ë¬¸ì„œ ì ìˆ˜ í†µê³„
                statute_scores = [s["raw_score"] for s in scores if "statute" in s.get("doc_type", "").lower()]
                statute_avg = sum(statute_scores) / len(statute_scores) if statute_scores else 0.0
                
                results["models"][model_name] = {
                    "status": "success",
                    "load_time": load_time,
                    "score_time": score_time,
                    "num_scored": len(scores),
                    "scores": scores,
                    "statistics": {
                        "avg_score": avg_score,
                        "max_score": max_score,
                        "min_score": min_score,
                        "precedent_avg": precedent_avg,
                        "num_precedents": len(precedent_scores),
                        "statute_avg": statute_avg,
                        "num_statutes": len(statute_scores)
                    }
                }
                
                logger.info(f"\nğŸ“Š Statistics for {model_name}:")
                logger.info(f"  Average score: {avg_score:.4f}")
                logger.info(f"  Max score: {max_score:.4f}")
                logger.info(f"  Min score: {min_score:.4f}")
                logger.info(f"  Precedent average: {precedent_avg:.4f} ({len(precedent_scores)} documents)")
                logger.info(f"  Scoring time: {score_time:.2f} seconds")
            else:
                results["models"][model_name] = {
                    "status": "failed",
                    "error": "No documents scored"
                }
                
        except Exception as e:
            logger.error(f"âŒ Error testing model {model_name}: {e}", exc_info=True)
            results["models"][model_name] = {
                "status": "failed",
                "error": str(e)
            }
    
    return results


def print_comparison_summary(results: Dict[str, Any]):
    """ë¹„êµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    logger.info(f"\n{'='*80}")
    logger.info("ğŸ“Š COMPARISON SUMMARY")
    logger.info(f"{'='*80}\n")
    
    logger.info(f"Query: {results['query']}")
    logger.info(f"Number of documents: {results['num_documents']}\n")
    
    # ê° ëª¨ë¸ë³„ ê²°ê³¼ ì¶œë ¥
    for model_name in MODELS_TO_COMPARE:
        if model_name not in results["models"]:
            continue
        
        model_result = results["models"][model_name]
        
        if model_result["status"] == "success":
            stats = model_result["statistics"]
            logger.info(f"\nğŸ” {model_name}:")
            logger.info(f"  Status: âœ… Success")
            logger.info(f"  Load time: {model_result['load_time']:.2f}s")
            logger.info(f"  Score time: {model_result['score_time']:.2f}s")
            logger.info(f"  Average score: {stats['avg_score']:.4f}")
            logger.info(f"  Max score: {stats['max_score']:.4f}")
            logger.info(f"  Min score: {stats['min_score']:.4f}")
            logger.info(f"  Statute average: {stats['statute_avg']:.4f} ({stats['num_statutes']} documents)")
            logger.info(f"  Precedent average: {stats['precedent_avg']:.4f} ({stats['num_precedents']} documents)")
        else:
            logger.info(f"\nâŒ {model_name}:")
            logger.info(f"  Status: Failed")
            logger.info(f"  Error: {model_result.get('error', 'Unknown error')}")
    
    # ë¹„êµ ë¶„ì„
    successful_models = {
        name: result for name, result in results["models"].items()
        if result["status"] == "success"
    }
    
    if len(successful_models) >= 2:
        logger.info(f"\n{'='*80}")
        logger.info("ğŸ“ˆ COMPARISON ANALYSIS")
        logger.info(f"{'='*80}\n")
        
        model_names = list(successful_models.keys())
        model1_name = model_names[0]
        model2_name = model_names[1]
        
        model1_stats = successful_models[model1_name]["statistics"]
        model2_stats = successful_models[model2_name]["statistics"]
        
        logger.info(f"Average Score Comparison:")
        logger.info(f"  {model1_name}: {model1_stats['avg_score']:.4f}")
        logger.info(f"  {model2_name}: {model2_stats['avg_score']:.4f}")
        diff = model2_stats['avg_score'] - model1_stats['avg_score']
        logger.info(f"  Difference: {diff:+.4f} ({diff/model1_stats['avg_score']*100:+.1f}%)")
        
        logger.info(f"\nStatute Score Comparison:")
        logger.info(f"  {model1_name}: {model1_stats['statute_avg']:.4f}")
        logger.info(f"  {model2_name}: {model2_stats['statute_avg']:.4f}")
        statute_diff = model2_stats['statute_avg'] - model1_stats['statute_avg']
        if model1_stats['statute_avg'] > 0:
            logger.info(f"  Difference: {statute_diff:+.4f} ({statute_diff/model1_stats['statute_avg']*100:+.1f}%)")
        
        logger.info(f"\nPrecedent Score Comparison:")
        logger.info(f"  {model1_name}: {model1_stats['precedent_avg']:.4f}")
        logger.info(f"  {model2_name}: {model2_stats['precedent_avg']:.4f}")
        precedent_diff = model2_stats['precedent_avg'] - model1_stats['precedent_avg']
        if model1_stats['precedent_avg'] > 0:
            logger.info(f"  Difference: {precedent_diff:+.4f} ({precedent_diff/model1_stats['precedent_avg']*100:+.1f}%)")
        
        logger.info(f"\nPerformance Comparison:")
        logger.info(f"  {model1_name}: Load={successful_models[model1_name]['load_time']:.2f}s, Score={successful_models[model1_name]['score_time']:.2f}s")
        logger.info(f"  {model2_name}: Load={successful_models[model2_name]['load_time']:.2f}s, Score={successful_models[model2_name]['score_time']:.2f}s")


async def get_test_documents_from_query(query: str) -> List[Dict[str, Any]]:
    """
    ì‹¤ì œ ê²€ìƒ‰ì„ í†µí•´ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ê²€ìƒ‰ ì»¤ë„¥í„° ì§ì ‘ ì‚¬ìš©)
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
    
    Returns:
        í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    try:
        from lawfirm_langgraph.core.search.connectors.legal_data_connector_v2 import LegalDataConnectorV2
        
        logger.info(f"ğŸ” Fetching test documents for query: {query}")
        
        connector = LegalDataConnectorV2()
        
        # ğŸ”¥ ìˆ˜ì •: limit íŒŒë¼ë¯¸í„° ì‚¬ìš© (top_kê°€ ì•„ë‹˜)
        # ë²•ë ¹ ê²€ìƒ‰
        statute_results = connector.search_statutes_fts(query, limit=5)
        
        # íŒë¡€ ê²€ìƒ‰
        precedent_results = connector.search_cases_fts(query, limit=5)
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        test_documents = []
        
        # ë²•ë ¹ ë¬¸ì„œ ì¶”ê°€
        for result in statute_results:
            doc = {
                "text": result.get("text") or result.get("content", ""),
                "type": "statute_article",
                "metadata": result.get("metadata", {}),
                "relevance_score": result.get("relevance_score", result.get("rank_score", 0.0)),
                "rank_score": result.get("rank_score", 0.0)
            }
            if doc["text"]:
                test_documents.append(doc)
        
        # íŒë¡€ ë¬¸ì„œ ì¶”ê°€
        for result in precedent_results:
            doc = {
                "text": result.get("text") or result.get("content", ""),
                "type": "precedent_content",
                "metadata": result.get("metadata", {}),
                "relevance_score": result.get("relevance_score", result.get("rank_score", 0.0)),
                "rank_score": result.get("rank_score", 0.0)
            }
            if doc["text"]:
                test_documents.append(doc)
        
        logger.info(f"âœ… Retrieved {len(test_documents)} test documents ({len(statute_results)} statutes, {len(precedent_results)} precedents)")
        return test_documents
        
    except Exception as e:
        logger.error(f"Failed to fetch test documents: {e}", exc_info=True)
        # í´ë°±: ìƒ˜í”Œ ë¬¸ì„œ ì‚¬ìš©
        logger.warning("Using fallback sample documents")
        return [
            {
                "text": "ã€ì‹  ì²­ ì¸ã€‘ <br/>ã€í”¼ì‹ ì²­ì¸ã€‘ ì£¼ì‹íšŒì‚¬ ì‹ í•œì€í–‰ì™¸ 1ì¸ (ì†Œì†¡ëŒ€ë¦¬ì¸ ë²•ë¬´ë²•ì¸ ìœ¨ì´Œì™¸ 5ì¸)<br/>ã€ì£¼    ë¬¸ã€‘<br/>1. ì‹ ì²­ì¸ì´ í”¼ì‹ ì²­ì¸ ì£¼ì‹íšŒì‚¬ ì‹ í•œì€í–‰ì„ ìœ„í•œ ë‹´ë³´",
                "type": "precedent_content",
                "relevance_score": 0.0,
                "rank_score": 0.0
            },
            {
                "text": "ê³„ì•½ì˜ í•´ì§€ëŠ” ë‹¹ì‚¬ì ì¼ë°©ì˜ ì˜ì‚¬í‘œì‹œë¡œ ê³„ì•½ì„ ì†Œê¸‰í•˜ì—¬ ì†Œë©¸ì‹œí‚¤ëŠ” í–‰ìœ„ë¥¼ ë§í•œë‹¤. ê³„ì•½ í•´ì§€ì˜ ì‚¬ìœ ë¡œëŠ” ê³„ì•½ ìœ„ë°˜, ë¶ˆê°€ëŠ¥, ëª©ì  ë‹¬ì„± ë¶ˆê°€ ë“±ì´ ìˆë‹¤.",
                "type": "statute_article",
                "relevance_score": 0.0,
                "rank_score": 0.0
            },
            {
                "text": "ê³„ì•½ í•´ì§€ì˜ íš¨ê³¼ëŠ” ê³„ì•½ì´ ì†Œê¸‰í•˜ì—¬ ì†Œë©¸í•˜ëŠ” ê²ƒì´ë©°, ì´ë¯¸ ì´í–‰ëœ ê¸‰ë¶€ì— ëŒ€í•˜ì—¬ëŠ” ì›ìƒíšŒë³µì˜ë¬´ê°€ ë°œìƒí•œë‹¤.",
                "type": "statute_article",
                "relevance_score": 0.0,
                "rank_score": 0.0
            }
        ]


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì§ˆì˜ ê°€ì ¸ì˜¤ê¸°
    query = sys.argv[1] if len(sys.argv) > 1 else DEFAULT_QUERY
    
    logger.info(f"\n{'='*80}")
    logger.info("ğŸš€ Cross-Encoder Model Comparison Test")
    logger.info(f"{'='*80}\n")
    logger.info(f"Query: {query}\n")
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    test_documents = await get_test_documents_from_query(query)
    
    if not test_documents:
        logger.error("No test documents available")
        return
    
    logger.info(f"Using {len(test_documents)} test documents\n")
    
    # ëª¨ë¸ ë¹„êµ ì‹¤í–‰
    results = compare_models_on_query(query, test_documents)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print_comparison_summary(results)
    
    logger.info(f"\n{'='*80}")
    logger.info("âœ… Comparison test completed")
    logger.info(f"{'='*80}\n")


if __name__ == "__main__":
    asyncio.run(main())

