# -*- coding: utf-8 -*-
"""
ê°€ì¤‘ì¹˜ ì„¤ì • ê²€ì¦ ë° ìµœì í™” ìŠ¤í¬ë¦½íŠ¸

ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•˜ê³  í‰ê°€ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ì—¬ ìµœì ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

Usage:
    python lawfirm_langgraph/tests/runners/validate_weight_configurations.py
    python lawfirm_langgraph/tests/runners/validate_weight_configurations.py --query-type law_inquiry
    python lawfirm_langgraph/tests/runners/validate_weight_configurations.py --quick  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ì ì€ ì¡°í•©)
"""

import sys
import os
import json
import re
import asyncio
import argparse
import gc
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import statistics
import logging
try:
    from lawfirm_langgraph.core.utils.logger import get_logger
except ImportError:
    from core.utils.logger import get_logger

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
script_dir = Path(__file__).parent
runners_dir = script_dir.parent
tests_dir = runners_dir.parent
lawfirm_langgraph_dir = tests_dir.parent
project_root = lawfirm_langgraph_dir.parent
sys.path.insert(0, str(lawfirm_langgraph_dir))
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from utils.env_loader import ensure_env_loaded
    ensure_env_loaded(project_root)
except ImportError:
    pass

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = get_logger(__name__)

# MLflow í†µí•©
try:
    import mlflow
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    logger.warning("MLflow not available. Install with: pip install mlflow")


@dataclass
class WeightConfig:
    """ê°€ì¤‘ì¹˜ ì„¤ì •"""
    name: str
    hybrid_law: Dict[str, float]
    hybrid_case: Dict[str, float]
    hybrid_general: Dict[str, float]
    doc_type_boost: Dict[str, float]
    quality_weight: float
    keyword_adjustment: float


@dataclass
class EvaluationMetrics:
    """í‰ê°€ ë©”íŠ¸ë¦­"""
    # ê²€ìƒ‰ ê´€ë ¨ ë©”íŠ¸ë¦­
    avg_relevance_score: float = 0.0
    min_relevance_score: float = 0.0
    max_relevance_score: float = 0.0
    keyword_coverage: float = 0.0
    
    # ë¬¸ì„œ í™œìš© ë©”íŠ¸ë¦­
    retrieved_docs_count: int = 0
    used_docs_count: int = 0
    document_utilization_rate: float = 0.0  # used_docs / retrieved_docs
    
    # ë‹µë³€ í’ˆì§ˆ ë©”íŠ¸ë¦­
    answer_length: int = 0
    answer_quality_score: float = 0.0  # 0-100
    has_sources: bool = False
    source_count: int = 0
    
    # ì†ŒìŠ¤ ê´€ë ¨ì„± ë©”íŠ¸ë¦­
    source_relevance_avg: float = 0.0
    source_coverage: float = 0.0  # ë‹µë³€ì´ ì†ŒìŠ¤ì— ê¸°ë°˜í•˜ëŠ” ì •ë„
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    total_time: float = 0.0
    search_time: float = 0.0
    generation_time: float = 0.0
    
    # ì¢…í•© ì ìˆ˜
    overall_score: float = 0.0  # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê³„ì‚°


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    config: WeightConfig
    query: str
    query_type: str
    metrics: EvaluationMetrics
    timestamp: str
    success: bool
    error: Optional[str] = None


class WeightConfigGenerator:
    """ê°€ì¤‘ì¹˜ ì¡°í•© ìƒì„±ê¸°"""
    
    @staticmethod
    def generate_weight_combinations(query_type: str = "all", quick: bool = False) -> List[WeightConfig]:
        """
        ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ ì¡°í•© ìƒì„±
        
        Args:
            query_type: í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ìœ í˜• ("law_inquiry", "precedent_search", "general", "all")
            quick: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì ì€ ì¡°í•©)
        """
        combinations = []
        
        if quick:
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: í•µì‹¬ ì¡°í•©ë§Œ
            law_semantic_values = [0.3, 0.4, 0.45, 0.5]
            case_semantic_values = [0.6, 0.65, 0.7, 0.75]
            general_semantic_values = [0.5]
        else:
            # ì „ì²´ í…ŒìŠ¤íŠ¸: ë” ë§ì€ ì¡°í•©
            law_semantic_values = [0.2, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]
            case_semantic_values = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]
            general_semantic_values = [0.4, 0.45, 0.5, 0.55, 0.6]
        
        config_id = 0
        
        # ë²•ë ¹ ì¡°íšŒ ê°€ì¤‘ì¹˜ ì¡°í•©
        if query_type in ["law_inquiry", "all"]:
            for law_semantic in law_semantic_values:
                law_keyword = 1.0 - law_semantic
                config_id += 1
                combinations.append(WeightConfig(
                    name=f"law_sem{law_semantic:.2f}_kw{law_keyword:.2f}",
                    hybrid_law={"semantic": law_semantic, "keyword": law_keyword},
                    hybrid_case={"semantic": 0.65, "keyword": 0.35},  # ê¸°ë³¸ê°’
                    hybrid_general={"semantic": 0.5, "keyword": 0.5},  # ê¸°ë³¸ê°’
                    doc_type_boost={"statute": 1.2, "case": 1.15},
                    quality_weight=0.2,
                    keyword_adjustment=1.8
                ))
        
        # íŒë¡€ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì¡°í•©
        if query_type in ["precedent_search", "all"]:
            for case_semantic in case_semantic_values:
                case_keyword = 1.0 - case_semantic
                config_id += 1
                combinations.append(WeightConfig(
                    name=f"case_sem{case_semantic:.2f}_kw{case_keyword:.2f}",
                    hybrid_law={"semantic": 0.45, "keyword": 0.55},  # ê¸°ë³¸ê°’
                    hybrid_case={"semantic": case_semantic, "keyword": case_keyword},
                    hybrid_general={"semantic": 0.5, "keyword": 0.5},  # ê¸°ë³¸ê°’
                    doc_type_boost={"statute": 1.2, "case": 1.15},
                    quality_weight=0.2,
                    keyword_adjustment=1.8
                ))
        
        # ì¼ë°˜ ì§ˆë¬¸ ê°€ì¤‘ì¹˜ ì¡°í•©
        if query_type in ["general", "all"]:
            for general_semantic in general_semantic_values:
                general_keyword = 1.0 - general_semantic
                config_id += 1
                combinations.append(WeightConfig(
                    name=f"general_sem{general_semantic:.2f}_kw{general_keyword:.2f}",
                    hybrid_law={"semantic": 0.45, "keyword": 0.55},  # ê¸°ë³¸ê°’
                    hybrid_case={"semantic": 0.65, "keyword": 0.35},  # ê¸°ë³¸ê°’
                    hybrid_general={"semantic": general_semantic, "keyword": general_keyword},
                    doc_type_boost={"statute": 1.2, "case": 1.15},
                    quality_weight=0.2,
                    keyword_adjustment=1.8
                ))
        
        # í˜„ì¬ ì„¤ì • ì¶”ê°€ (ë² ì´ìŠ¤ë¼ì¸)
        combinations.insert(0, WeightConfig(
            name="current_baseline",
            hybrid_law={"semantic": 0.45, "keyword": 0.55},
            hybrid_case={"semantic": 0.65, "keyword": 0.35},
            hybrid_general={"semantic": 0.5, "keyword": 0.5},
            doc_type_boost={"statute": 1.2, "case": 1.15},
            quality_weight=0.2,
            keyword_adjustment=1.8
        ))
        
        return combinations


class TestQuerySet:
    """í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„¸íŠ¸"""
    
    @staticmethod
    def get_queries(query_type: str = "all") -> Dict[str, List[str]]:
        """
        ì§ˆë¬¸ ìœ í˜•ë³„ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë°˜í™˜
        
        Args:
            query_type: ì§ˆë¬¸ ìœ í˜• ("law_inquiry", "precedent_search", "general", "all")
        """
        queries = {
            "law_inquiry": [
                # ë¯¼ë²• ì¡°ë¬¸ ì¡°íšŒ (10ê°œ)
                "ë¯¼ë²• ì œ750ì¡° ì†í•´ë°°ìƒì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ê³„ì•½ ìœ„ì•½ê¸ˆì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ë¯¼ë²• ì œ103ì¡° ë¶ˆê³µì •í•œ ë²•ë¥ í–‰ìœ„ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ë¯¼ë²• ì œ563ì¡° ë§¤ë§¤ê³„ì•½ì˜ í•´ì œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ë¯¼ë²• ì œ105ì¡° ì‚¬ê¸°Â·ê°•ë°•ì— ì˜í•œ ì˜ì‚¬í‘œì‹œì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                "ë¯¼ë²• ì œ110ì¡° ëŒ€ë¦¬ê¶Œì˜ ë²”ìœ„ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ë¯¼ë²• ì œ213ì¡° ì†Œìœ ê¶Œì˜ ë‚´ìš©ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                "ë¯¼ë²• ì œ618ì¡° ì„ëŒ€ì°¨ì˜ ì˜ì˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ë¯¼ë²• ì œ543ì¡° ê³„ì•½ì˜ í•´ì œì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                "ë¯¼ë²• ì œ390ì¡° ì±„ë¬´ë¶ˆì´í–‰ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                # ë¯¼ì‚¬ë²• ê°œë… ì¡°íšŒ (8ê°œ)
                "ì†í•´ë°°ìƒì˜ ë²”ìœ„ëŠ” ì–´ë–»ê²Œ ê²°ì •ë˜ë‚˜ìš”?",
                "ê³„ì•½ í•´ì§€ ì‚¬ìœ ì—ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?",
                "ë¶ˆë²•í–‰ìœ„ê°€ ì„±ë¦½í•˜ë ¤ë©´ ì–´ë–¤ ìš”ê±´ì´ í•„ìš”í•œê°€ìš”?",
                "ëª…ì˜ˆí›¼ì†ì´ ì„±ë¦½í•˜ë ¤ë©´ ì–´ë–¤ ì¡°ê±´ì´ í•„ìš”í•œê°€ìš”?",
                "ì„ëŒ€ì°¨ ê³„ì•½ì˜ íš¨ë ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ê³„ì•½ ìœ„ì•½ê¸ˆì˜ ë²•ì  íš¨ë ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì†Œìœ ê¶Œ ì´ì „ì˜ ìš”ê±´ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "ì±„ê¶Œ ì–‘ë„ì˜ ì œí•œì‚¬í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                # ë¯¼ì‚¬ë²• ì ˆì°¨ ì¡°íšŒ (7ê°œ)
                "ì†í•´ë°°ìƒ ì²­êµ¬ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "ê³„ì•½ í•´ì§€ ì ˆì°¨ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ì„ëŒ€ì°¨ ê³„ì•½ í•´ì§€ ì ˆì°¨ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "ëª…ì˜ˆí›¼ì† ê³ ì†Œ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì†Œìœ ê¶Œ ì´ì „ ë“±ê¸° ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "ê³„ì•½ ìœ„ì•½ê¸ˆ ì²­êµ¬ ì ˆì°¨ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ì±„ê¶Œ ì¶”ì‹¬ ì ˆì°¨ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            ],
            "precedent_search": [
                # íŠ¹ì • ì‚¬ê±´ ê²€ìƒ‰ (10ê°œ)
                "ê³„ì•½ í•´ì§€ ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ì†í•´ë°°ìƒ ì²­êµ¬ ì‚¬ë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì„ëŒ€ì°¨ ê³„ì•½ í•´ì§€ íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ê³„ì•½ ìœ„ì•½ê¸ˆ ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ëª…ì˜ˆí›¼ì† íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ê³„ì•½ í•´ì„ ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ì†Œìœ ê¶Œ ì´ì „ íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì±„ê¶Œ ì–‘ë„ ë¬´íš¨ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ë¶ˆë²•í–‰ìœ„ ì†í•´ë°°ìƒ íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ê³„ì•½ ì²´ê²° ë¬´íš¨ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                # ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (8ê°œ)
                "ê³„ì•½ í•´ì§€ ì‚¬ìœ ê°€ ë¶ˆëª…í™•í•œ ê²½ìš° íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ì†í•´ë°°ìƒ ë²”ìœ„ ì‚°ì • ê´€ë ¨ íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì„ëŒ€ì°¨ ê³„ì•½ í•´ì§€ ì‚¬ìœ  íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ê³„ì•½ ìœ„ì•½ê¸ˆ ê³¼ë‹¤ ê°ì•¡ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ëª…ì˜ˆí›¼ì† ê³µì—°ì„± ìš”ê±´ íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì†Œìœ ê¶Œ ì´ì „ ë“±ê¸° ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ì±„ê¶Œ ì¶”ì‹¬ ê´€ë ¨ íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ë¶ˆë²•í–‰ìœ„ ì¸ê³¼ê´€ê³„ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                # ë²•ì›ë³„ íŒë¡€ ê²€ìƒ‰ (7ê°œ)
                "ëŒ€ë²•ì› ê³„ì•½ í•´ì§€ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ê³ ë“±ë²•ì› ì†í•´ë°°ìƒ íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì§€ë°©ë²•ì› ì„ëŒ€ì°¨ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ëŒ€ë²•ì› ì„ëŒ€ì°¨ íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ê³ ë“±ë²•ì› ê³„ì•½ ìœ„ì•½ê¸ˆ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ëŒ€ë²•ì› ëª…ì˜ˆí›¼ì† íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì§€ë°©ë²•ì› ì†Œìœ ê¶Œ ì´ì „ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”"
            ],
            "general": [
                # ë¯¼ì‚¬ë²• ìë¬¸ (7ê°œ) - ì •ë³´ ì¡°íšŒ ë° êµìœ¡ ê´€ë ¨ ì§ˆë¬¸ ì œì™¸
                "ë¯¼ì‚¬ë²• ìë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤",
                "ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                "ë¯¼ì‚¬ë²• ìš©ì–´ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ë¯¼ì‚¬ ì†Œì†¡ ì ˆì°¨ì— ëŒ€í•´ ì•ˆë‚´í•´ì£¼ì„¸ìš”",
                "ë¯¼ì‚¬ë²• ìƒë‹´ì´ í•„ìš”í•©ë‹ˆë‹¤",
                "ê³„ì•½ ë¶„ìŸ í•´ê²° ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì†í•´ë°°ìƒ ì²­êµ¬ ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
            ]
        }
        
        if query_type == "all":
            return queries
        elif query_type in queries:
            return {query_type: queries[query_type]}
        else:
            return {query_type: queries.get("general", [])}


class WeightConfigUpdater:
    """ê°€ì¤‘ì¹˜ ì„¤ì • ì—…ë°ì´íŠ¸"""
    
    def __init__(self, config_file: Path):
        self.config_file = config_file
        self.original_content = None
    
    def backup(self):
        """ì›ë³¸ ì„¤ì • ë°±ì—…"""
        if not self.config_file.exists():
            raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.config_file}")
        
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                self.original_content = f.read()
            if not self.original_content:
                raise ValueError(f"ì„¤ì • íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {self.config_file}")
        except Exception as e:
            raise ValueError(f"ì„¤ì • íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def update(self, config: WeightConfig):
        """ê°€ì¤‘ì¹˜ ì„¤ì • ì—…ë°ì´íŠ¸ - ìˆ«ì ê°’ë§Œ êµì²´í•˜ëŠ” ì•ˆì „í•œ ë°©ì‹"""
        if not self.original_content:
            self.backup()
        
        if not self.original_content:
            raise ValueError(f"ì„¤ì • íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.config_file}")
        
        content = self.original_content
        
        # ìˆ«ì ê°’ë§Œ ì •í™•í•˜ê²Œ êµì²´ (ë“¤ì—¬ì“°ê¸° ìœ ì§€)
        # hybrid_law semantic
        content = re.sub(
            r'"hybrid_law":\s*\{\s*"semantic":\s*[\d.]+',
            f'"hybrid_law": {{"semantic": {config.hybrid_law["semantic"]}',
            content
        )
        # hybrid_law keyword
        content = re.sub(
            r'"hybrid_law":\s*\{\s*"semantic":\s*[\d.]+\s*,\s*"keyword":\s*[\d.]+',
            f'"hybrid_law": {{"semantic": {config.hybrid_law["semantic"]}, "keyword": {config.hybrid_law["keyword"]}',
            content
        )
        
        # hybrid_case semantic
        content = re.sub(
            r'"hybrid_case":\s*\{\s*"semantic":\s*[\d.]+',
            f'"hybrid_case": {{"semantic": {config.hybrid_case["semantic"]}',
            content
        )
        # hybrid_case keyword
        content = re.sub(
            r'"hybrid_case":\s*\{\s*"semantic":\s*[\d.]+\s*,\s*"keyword":\s*[\d.]+',
            f'"hybrid_case": {{"semantic": {config.hybrid_case["semantic"]}, "keyword": {config.hybrid_case["keyword"]}',
            content
        )
        
        # hybrid_general semantic
        content = re.sub(
            r'"hybrid_general":\s*\{\s*"semantic":\s*[\d.]+',
            f'"hybrid_general": {{"semantic": {config.hybrid_general["semantic"]}',
            content
        )
        # hybrid_general keyword
        content = re.sub(
            r'"hybrid_general":\s*\{\s*"semantic":\s*[\d.]+\s*,\s*"keyword":\s*[\d.]+',
            f'"hybrid_general": {{"semantic": {config.hybrid_general["semantic"]}, "keyword": {config.hybrid_general["keyword"]}',
            content
        )
        
        # doc_type_boost statute
        content = re.sub(
            r'"doc_type_boost":\s*\{\s*"statute":\s*[\d.]+',
            f'"doc_type_boost": {{"statute": {config.doc_type_boost["statute"]}',
            content
        )
        # doc_type_boost case
        content = re.sub(
            r'"doc_type_boost":\s*\{\s*"statute":\s*[\d.]+\s*,\s*"case":\s*[\d.]+',
            f'"doc_type_boost": {{"statute": {config.doc_type_boost["statute"]}, "case": {config.doc_type_boost["case"]}',
            content
        )
        
        # quality_weight
        content = re.sub(
            r'"quality_weight":\s*[\d.]+',
            f'"quality_weight": {config.quality_weight}',
            content
        )
        
        # keyword_adjustment
        content = re.sub(
            r'"keyword_adjustment":\s*[\d.]+',
            f'"keyword_adjustment": {config.keyword_adjustment}',
            content
        )
        
        # íŒŒì¼ ì €ì¥
        with open(self.config_file, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def restore(self):
        """ì›ë³¸ ì„¤ì • ë³µì›"""
        if self.original_content:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                f.write(self.original_content)


class QueryTestRunner:
    """ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        # run_query_test í•¨ìˆ˜ë¥¼ ì§ì ‘ import
        try:
            script_dir = Path(__file__).parent
            run_query_test_path = script_dir / "run_query_test.py"
            if not run_query_test_path.exists():
                # ê°™ì€ ë””ë ‰í† ë¦¬ì— ì—†ìœ¼ë©´ runners ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
                run_query_test_path = script_dir / "run_query_test.py"
            
            if not run_query_test_path.exists():
                raise FileNotFoundError(f"run_query_test.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {run_query_test_path}")
            
            # ëª¨ë“ˆì„ ë™ì ìœ¼ë¡œ import
            import importlib.util
            spec = importlib.util.spec_from_file_location("run_query_test", run_query_test_path)
            run_query_test_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(run_query_test_module)
            
            self.run_query_test_func = run_query_test_module.run_query_test
            self._extract_and_normalize_answer = run_query_test_module._extract_and_normalize_answer
            self._evaluate_answer_quality = run_query_test_module._evaluate_answer_quality
            
        except Exception as e:
            logger.error(f"run_query_test ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    async def run_test(self, query: str) -> Tuple[Dict[str, Any], str]:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - result ë”•ì…”ë„ˆë¦¬ì™€ ë¡œê·¸ ì¶œë ¥ ë°˜í™˜"""
        try:
            # run_query_test í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ
            result = await self.run_query_test_func(query, enable_profiling=False, enable_memory_monitoring=False)
            
            # ë¡œê·¸ í•¸ë“¤ëŸ¬ì—ì„œ ì¶œë ¥ ìº¡ì²˜
            if isinstance(result, dict):
                result_keys = list(result.keys())
                # í° ë°ì´í„°ëŠ” í•„ìš”ì‹œì—ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
                if "retrieved_docs" in result and len(result["retrieved_docs"]) > 20:
                    # ë„ˆë¬´ ë§ì€ ë¬¸ì„œëŠ” ìƒ˜í”Œë§Œ ìœ ì§€
                    result["retrieved_docs"] = result["retrieved_docs"][:20]
            else:
                result_keys = []
            output_str = f"Query: {query}\nResult keys: {result_keys}"
            
            return result, output_str
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {}, str(e)
        finally:
            # í…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„ ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()


class MetricsExtractor:
    """ë©”íŠ¸ë¦­ ì¶”ì¶œê¸°"""
    
    @staticmethod
    def extract_metrics_from_result(result: Dict[str, Any], query: str) -> EvaluationMetrics:
        """result ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§ì ‘ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        metrics = EvaluationMetrics()
        
        if not isinstance(result, dict):
            logger.warning(f"   âš ï¸  resultê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(result)}")
            return metrics
        
        # ë‹µë³€ ì •ë³´
        answer = result.get("answer", "")
        if isinstance(answer, str):
            metrics.answer_length = len(answer)
        else:
            # answerê°€ ë”•ì…”ë„ˆë¦¬ì¼ ìˆ˜ ìˆìŒ
            answer_str = str(answer) if answer else ""
            metrics.answer_length = len(answer_str)
        
        # ë‹µë³€ í’ˆì§ˆ í‰ê°€
        retrieved_docs = result.get("retrieved_docs", [])
        sources = result.get("sources", [])
        
        # ê°„ë‹¨í•œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        answer_quality_score = 0.0
        if answer and metrics.answer_length > 0:
            answer_quality_score += 25  # ë‹µë³€ ì¡´ì¬
        if metrics.answer_length >= 100:  # MIN_ANSWER_LENGTH
            answer_quality_score += 25  # ìµœì†Œ ê¸¸ì´ ì¶©ì¡±
        if not any(pattern in str(answer) for pattern in ["ì£„ì†¡í•©ë‹ˆë‹¤", "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", "ì‹œìŠ¤í…œ ì˜¤ë¥˜"]):
            answer_quality_score += 25  # ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ìŒ
        if len(retrieved_docs) > 0 or len(sources) > 0:
            answer_quality_score += 25  # ì°¸ê³ ìë£Œ ì¡´ì¬
        
        metrics.answer_quality_score = answer_quality_score
        
        # ë¬¸ì„œ í™œìš© ë©”íŠ¸ë¦­
        metrics.retrieved_docs_count = len(retrieved_docs) if retrieved_docs else 0
        metrics.used_docs_count = len(sources) if sources else 0
        metrics.source_count = metrics.used_docs_count
        metrics.has_sources = metrics.source_count > 0
        
        if metrics.retrieved_docs_count > 0:
            metrics.document_utilization_rate = metrics.used_docs_count / metrics.retrieved_docs_count
        else:
            metrics.document_utilization_rate = 0.0
        
        # ê²€ìƒ‰ ê´€ë ¨ì„± ì ìˆ˜ (retrieved_docsì˜ scoreì—ì„œ ê³„ì‚°)
        if retrieved_docs:
            scores = []
            for doc in retrieved_docs:
                if isinstance(doc, dict):
                    score = doc.get("relevance_score") or doc.get("score") or doc.get("similarity_score")
                    if score is not None:
                        try:
                            scores.append(float(score))
                        except (ValueError, TypeError):
                            pass
            
            if scores:
                metrics.avg_relevance_score = sum(scores) / len(scores)
                metrics.min_relevance_score = min(scores)
                metrics.max_relevance_score = max(scores)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        processing_time = result.get("processing_time", 0.0)
        if processing_time:
            metrics.total_time = float(processing_time)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        metrics.overall_score = MetricsExtractor._calculate_overall_score(metrics)
        
        return metrics
    
    @staticmethod
    def extract_metrics(output: str, query: str) -> EvaluationMetrics:
        """ì¶œë ¥ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        metrics = EvaluationMetrics()
        
        if not output or len(output.strip()) == 0:
            logger.warning("   âš ï¸  ì¶œë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë©”íŠ¸ë¦­ ì¶”ì¶œ ë¶ˆê°€.")
            return metrics
        
        # ê²€ìƒ‰ ê´€ë ¨ ë©”íŠ¸ë¦­
        # run_query_test.pyëŠ” Avg Relevanceë¥¼ ì§ì ‘ ì¶œë ¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, retrieved_docsì˜ scoreì—ì„œ ê³„ì‚°
        # ë˜ëŠ” ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¶”ì¶œ ì‹œë„
        avg_match = re.search(r'Avg Relevance: ([\d.]+)', output)
        if avg_match:
            metrics.avg_relevance_score = float(avg_match.group(1))
        else:
            # retrieved_docsì˜ scoreì—ì„œ í‰ê·  ê³„ì‚° ì‹œë„
            score_matches = re.findall(r'score=([\d.]+)', output)
            if score_matches:
                scores = [float(s) for s in score_matches]
                metrics.avg_relevance_score = sum(scores) / len(scores) if scores else 0.0
                metrics.min_relevance_score = min(scores) if scores else 0.0
                metrics.max_relevance_score = max(scores) if scores else 0.0
            else:
                # ìœ ì‚¬ë„ ì ìˆ˜ ë¶„í¬ì—ì„œ ì¶”ì¶œ ì‹œë„
                avg_score_match = re.search(r'í‰ê· =([\d.]+)', output)
                if avg_score_match:
                    metrics.avg_relevance_score = float(avg_score_match.group(1))
        
        min_match = re.search(r'Min: ([\d.]+)', output)
        if min_match:
            metrics.min_relevance_score = float(min_match.group(1))
        
        max_match = re.search(r'Max: ([\d.]+)', output)
        if max_match:
            metrics.max_relevance_score = float(max_match.group(1))
        
        keyword_match = re.search(r'Keyword Coverage: ([\d.]+)', output)
        if keyword_match:
            metrics.keyword_coverage = float(keyword_match.group(1))
        
        # ë¬¸ì„œ í™œìš© ë©”íŠ¸ë¦­
        # run_query_test.py í˜•ì‹: "ğŸ” ê²€ìƒ‰ëœ ì°¸ê³ ìë£Œ (retrieved_docs) ({count}ê°œ):"
        retrieved_match = re.search(r'ê²€ìƒ‰ëœ ì°¸ê³ ìë£Œ.*?\((\d+)ê°œ\)', output)
        if not retrieved_match:
            retrieved_match = re.search(r'ê²€ìƒ‰ëœ ë¬¸ì„œ.*?(\d+)ê°œ', output)
        if retrieved_match:
            metrics.retrieved_docs_count = int(retrieved_match.group(1))
        
        # sources ê°œìˆ˜
        sources_match = re.search(r'ì†ŒìŠ¤ \(sources\)\s*\((\d+)ê°œ\)', output)
        if sources_match:
            metrics.used_docs_count = int(sources_match.group(1))
            metrics.source_count = metrics.used_docs_count
            metrics.has_sources = metrics.source_count > 0
        
        # ì‹¤ì œ ì‚¬ìš© ë¬¸ì„œ ìˆ˜ (sourcesê°€ ì—†ìœ¼ë©´ retrieved_docs ì‚¬ìš©)
        if metrics.retrieved_docs_count > 0:
            if metrics.used_docs_count == 0:
                # sourcesê°€ ì—†ìœ¼ë©´ retrieved_docsë¥¼ ì‚¬ìš©ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                metrics.used_docs_count = metrics.retrieved_docs_count
            metrics.document_utilization_rate = metrics.used_docs_count / metrics.retrieved_docs_count
        
        # ë‹µë³€ í’ˆì§ˆ ë©”íŠ¸ë¦­
        # run_query_test.py í˜•ì‹: "ğŸ“ ë‹µë³€ ({length}ì)"
        answer_length_match = re.search(r'ë‹µë³€\s*\((\d+)ì\)', output)
        if answer_length_match:
            metrics.answer_length = int(answer_length_match.group(1))
        
        # run_query_test.py í˜•ì‹: "í’ˆì§ˆ ì ìˆ˜: {score}/100"
        quality_match = re.search(r'í’ˆì§ˆ ì ìˆ˜:\s*(\d+)/100', output)
        if quality_match:
            metrics.answer_quality_score = float(quality_match.group(1))
        
        # ì°¸ê³ ìë£Œ ê°œìˆ˜
        if not metrics.has_sources:
            source_match = re.search(r'ì°¸ê³ ìë£Œ ì¡´ì¬.*?(\d+)ê°œ', output)
            if source_match:
                metrics.source_count = int(source_match.group(1))
                metrics.has_sources = metrics.source_count > 0
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        # run_query_test.py í˜•ì‹: "â±ï¸  ì²˜ë¦¬ ì‹œê°„: {time}ì´ˆ"
        time_match = re.search(r'ì²˜ë¦¬ ì‹œê°„:\s*([\d.]+)ì´ˆ', output)
        if not time_match:
            time_match = re.search(r'ì´ ì†Œìš” ì‹œê°„.*?([\d.]+)ì´ˆ', output)
        if time_match:
            metrics.total_time = float(time_match.group(1))
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        metrics.overall_score = MetricsExtractor._calculate_overall_score(metrics)
        
        return metrics
    
    @staticmethod
    def _calculate_overall_score(metrics: EvaluationMetrics) -> float:
        """
        ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        
        ê°€ì¤‘ì¹˜:
        - ë‹µë³€ í’ˆì§ˆ: 30%
        - ë¬¸ì„œ í™œìš©ë¥ : 25%
        - ì†ŒìŠ¤ ê´€ë ¨ì„±: 20%
        - ê²€ìƒ‰ ì ìˆ˜: 15%
        - ì„±ëŠ¥: 10%
        """
        # ë‹µë³€ í’ˆì§ˆ ì ìˆ˜ (0-100) â†’ 0-1 ì •ê·œí™”
        quality_score = metrics.answer_quality_score / 100.0
        
        # ë¬¸ì„œ í™œìš©ë¥  (0-1)
        utilization_score = metrics.document_utilization_rate
        
        # ì†ŒìŠ¤ ê´€ë ¨ì„± (ì†ŒìŠ¤ê°€ ìˆìœ¼ë©´ 1.0, ì—†ìœ¼ë©´ 0.0)
        source_score = 1.0 if metrics.has_sources else 0.0
        
        # ê²€ìƒ‰ ì ìˆ˜ (0-1 ì •ê·œí™”, avg_relevance_scoreê°€ ì´ë¯¸ 0-1 ë²”ìœ„ë¼ê³  ê°€ì •)
        search_score = metrics.avg_relevance_score
        
        # ì„±ëŠ¥ ì ìˆ˜ (ë¹ ë¥¼ìˆ˜ë¡ ë†’ìŒ, 10ì´ˆ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”)
        performance_score = max(0.0, 1.0 - (metrics.total_time / 10.0))
        
        # ê°€ì¤‘ í‰ê· 
        overall = (
            0.30 * quality_score +
            0.25 * utilization_score +
            0.20 * source_score +
            0.15 * search_score +
            0.10 * performance_score
        )
        
        return overall


class MLflowTracker:
    """MLflow ì¶”ì ê¸°"""
    
    def __init__(self, experiment_name: str = "weight_validation"):
        self.experiment_name = experiment_name
        self.mlflow_available = MLFLOW_AVAILABLE
        self.parent_run_id = None
        
        if not self.mlflow_available:
            logger.warning("MLflow not available. Tracking disabled.")
            return
        
        try:
            # MLflow ì„¤ì • (SQLite ë°±ì—”ë“œ ì‚¬ìš©)
            tracking_uri = os.getenv("MLFLOW_TRACKING_URI")
            if not tracking_uri:
                mlflow_db_path = project_root / "mlflow" / "mlflow.db"
                mlflow_db_path.parent.mkdir(parents=True, exist_ok=True)
                tracking_uri = f"sqlite:///{str(mlflow_db_path).replace(os.sep, '/')}"
            
            mlflow.set_tracking_uri(tracking_uri)
            mlflow.set_experiment(experiment_name)
            
            # ë¶€ëª¨ run ì‹œì‘ (ì „ì²´ ê²€ì¦ ì„¸ì…˜)
            run_name = f"weight_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            mlflow.start_run(run_name=run_name, nested=False)
            self.parent_run_id = mlflow.active_run().info.run_id
            
            logger.info(f"âœ… MLflow ì‹¤í—˜ ì‹œì‘: {experiment_name} (run_id: {self.parent_run_id})")
            
        except Exception as e:
            logger.warning(f"MLflow ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. Tracking disabled.")
            self.mlflow_available = False
    
    def log_config_run(self, config: WeightConfig, query: str, query_type: str, 
                      metrics: EvaluationMetrics, success: bool) -> Optional[str]:
        """ê°œë³„ ê°€ì¤‘ì¹˜ ì„¤ì • í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…"""
        if not self.mlflow_available:
            return None
        
        try:
            run_name = f"{config.name}_{query_type}_{datetime.now().strftime('%H%M%S')}"
            
            with mlflow.start_run(run_name=run_name, nested=True):
                # íƒœê·¸ ì„¤ì •
                mlflow.set_tags({
                    "config_name": config.name,
                    "query_type": query_type,
                    "query": query[:100],  # ì¿¼ë¦¬ ì¼ë¶€ë§Œ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦¼)
                    "success": str(success)
                })
                
                # íŒŒë¼ë¯¸í„° ë¡œê¹… (ê°€ì¤‘ì¹˜ ì„¤ì •)
                mlflow.log_params({
                    "hybrid_law_semantic": config.hybrid_law["semantic"],
                    "hybrid_law_keyword": config.hybrid_law["keyword"],
                    "hybrid_case_semantic": config.hybrid_case["semantic"],
                    "hybrid_case_keyword": config.hybrid_case["keyword"],
                    "hybrid_general_semantic": config.hybrid_general["semantic"],
                    "hybrid_general_keyword": config.hybrid_general["keyword"],
                    "doc_type_boost_statute": config.doc_type_boost["statute"],
                    "doc_type_boost_case": config.doc_type_boost["case"],
                    "quality_weight": config.quality_weight,
                    "keyword_adjustment": config.keyword_adjustment
                })
                
                # ë©”íŠ¸ë¦­ ë¡œê¹…
                if success:
                    mlflow.log_metrics({
                        "overall_score": metrics.overall_score,
                        "answer_quality_score": metrics.answer_quality_score,
                        "document_utilization_rate": metrics.document_utilization_rate,
                        "avg_relevance_score": metrics.avg_relevance_score,
                        "keyword_coverage": metrics.keyword_coverage,
                        "answer_length": float(metrics.answer_length),
                        "source_count": float(metrics.source_count),
                        "retrieved_docs_count": float(metrics.retrieved_docs_count),
                        "used_docs_count": float(metrics.used_docs_count),
                        "total_time": metrics.total_time
                    })
                
                run_id = mlflow.active_run().info.run_id
                return run_id
        
        except Exception as e:
            logger.warning(f"MLflow ë¡œê¹… ì‹¤íŒ¨: {e}")
            return None
    
    def log_summary(self, analysis: Dict[str, Any], best_config_name: str):
        """ê²€ì¦ ê²°ê³¼ ìš”ì•½ ë¡œê¹…"""
        if not self.mlflow_available:
            return
        
        try:
            # ë¶€ëª¨ runì— ìš”ì•½ ë©”íŠ¸ë¦­ ë¡œê¹…
            if "best_config" in analysis:
                best = analysis["best_config"]
                mlflow.log_metrics({
                    "best_overall_score": best["metrics"]["avg_score"],
                    "best_median_score": best["metrics"]["median_score"],
                    "best_min_score": best["metrics"]["min_score"],
                    "best_max_score": best["metrics"]["max_score"],
                    "best_std_dev": best["metrics"]["std_dev"]
                })
            
            if "summary" in analysis:
                mlflow.log_metrics({
                    "total_tests": float(analysis.get("total_tests", 0)),
                    "successful_tests": float(analysis.get("successful_tests", 0))
                })
            
            # ìµœì  ì„¤ì •ì„ íƒœê·¸ë¡œ ì €ì¥
            mlflow.set_tags({
                "best_config": best_config_name,
                "validation_completed": "true"
            })
            
        except Exception as e:
            logger.warning(f"MLflow ìš”ì•½ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    def log_artifacts(self, output_file: Path):
        """ì•„í‹°íŒ©íŠ¸ ë¡œê¹…"""
        if not self.mlflow_available:
            return
        
        try:
            if output_file.exists():
                mlflow.log_artifact(str(output_file), "validation_results")
                logger.info(f"âœ… MLflow ì•„í‹°íŒ©íŠ¸ ë¡œê¹…: {output_file}")
        except Exception as e:
            logger.warning(f"MLflow ì•„í‹°íŒ©íŠ¸ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    def end_run(self):
        """MLflow run ì¢…ë£Œ"""
        if self.mlflow_available:
            try:
                mlflow.end_run()
                logger.info(f"âœ… MLflow run ì¢…ë£Œ: {self.parent_run_id}")
            except Exception as e:
                logger.warning(f"MLflow run ì¢…ë£Œ ì‹¤íŒ¨: {e}")


class WeightValidationRunner:
    """ê°€ì¤‘ì¹˜ ê²€ì¦ ì‹¤í–‰ê¸°"""
    
    def __init__(self, quick: bool = False, use_mlflow: bool = True, max_workers: int = None):
        self.quick = quick
        # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ ì„¤ì • (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)
        if max_workers is None:
            import multiprocessing
            self.max_workers = min(multiprocessing.cpu_count(), 4)  # ìµœëŒ€ 4ê°œë¡œ ì œí•œ (ë©”ëª¨ë¦¬ ê³ ë ¤)
        else:
            self.max_workers = max_workers
        
        # ì„¤ì • íŒŒì¼ ê²½ë¡œ í™•ì¸ ë° ìˆ˜ì •
        # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
        possible_paths = [
            lawfirm_langgraph_dir / "core" / "search" / "processors" / "search_result_processor.py",
            project_root / "lawfirm_langgraph" / "core" / "search" / "processors" / "search_result_processor.py",
            Path(__file__).parent.parent.parent / "core" / "search" / "processors" / "search_result_processor.py"
        ]
        
        config_file = None
        for path in possible_paths:
            if path.exists():
                config_file = path.resolve()
                break
        
        if not config_file:
            raise FileNotFoundError(
                "ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ê²½ë¡œ:\n" + 
                "\n".join([f"  - {p}" for p in possible_paths])
            )
        
        self.config_file = config_file
        self.config_updater = WeightConfigUpdater(self.config_file)
        self.test_runner = QueryTestRunner()
        self.metrics_extractor = MetricsExtractor()
        self.mlflow_tracker = MLflowTracker() if use_mlflow and MLFLOW_AVAILABLE else None
        
        logger.info(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜: {self.max_workers}")
    
    async def _run_single_test(self, config: WeightConfig, query: str, q_type: str, 
                               current_test: int, total_tests: int) -> TestResult:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        try:
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (result ë”•ì…”ë„ˆë¦¬ ë°˜í™˜)
            result, output_str = await self.test_runner.run_test(query)
            
            # resultì—ì„œ ì§ì ‘ ë©”íŠ¸ë¦­ ì¶”ì¶œ
            metrics = self.metrics_extractor.extract_metrics_from_result(result, query)
            
            test_result = TestResult(
                config=config,
                query=query,
                query_type=q_type,
                metrics=metrics,
                timestamp=datetime.now().isoformat(),
                success=True
            )
            
            # MLflow ë¡œê¹…
            if self.mlflow_tracker:
                self.mlflow_tracker.log_config_run(
                    config, query, q_type, metrics, True
                )
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del result, output_str, metrics
            gc.collect()
            
            return test_result
            
        except Exception as e:
            logger.error(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            test_result = TestResult(
                config=config,
                query=query,
                query_type=q_type,
                metrics=EvaluationMetrics(),
                timestamp=datetime.now().isoformat(),
                success=False,
                error=str(e)
            )
            
            # MLflow ë¡œê¹… (ì‹¤íŒ¨í•œ ê²½ìš°ë„)
            if self.mlflow_tracker:
                self.mlflow_tracker.log_config_run(
                    config, query, q_type, EvaluationMetrics(), False
                )
            
            gc.collect()
            return test_result
    
    async def run_validation(self, query_type: str = "all") -> List[TestResult]:
        """ê²€ì¦ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì ìš©)"""
        import time
        
        start_time = time.time()
        logger.info("ğŸš€ ê°€ì¤‘ì¹˜ ê²€ì¦ ì‹œì‘")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì„¤ì • ìµœì í™”
        gc.set_threshold(700, 10, 10)  # ë” ì ê·¹ì ì¸ GC
        
        # ê°€ì¤‘ì¹˜ ì¡°í•© ìƒì„±
        configs = WeightConfigGenerator.generate_weight_combinations(query_type, self.quick)
        queries_dict = TestQuerySet.get_queries(query_type)
        
        total_configs = len(configs)
        total_queries = sum(len(q) for q in queries_dict.values())
        total_tests = total_configs * total_queries
        
        logger.info(f"   ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {total_tests}ê°œ (ê°€ì¤‘ì¹˜ ì¡°í•©: {total_configs}ê°œ Ã— ì¿¼ë¦¬: {total_queries}ê°œ)")
        logger.info(f"   ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜: {self.max_workers}")
        
        # ì›ë³¸ ì„¤ì • ë°±ì—…
        self.config_updater.backup()
        
        all_results = []
        
        try:
            current_test = 0
            
            for config in configs:
                # ê°€ì¤‘ì¹˜ ì„¤ì • ì—…ë°ì´íŠ¸
                self.config_updater.update(config)
                
                # ê° ì§ˆë¬¸ ìœ í˜•ë³„ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
                for q_type, queries in queries_dict.items():
                    # ë³‘ë ¬ ì²˜ë¦¬: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‹¤í–‰
                    batch_size = self.max_workers
                    for batch_start in range(0, len(queries), batch_size):
                        batch_queries = queries[batch_start:batch_start + batch_size]
                        
                        # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì‹¤í–‰
                        tasks = []
                        for query in batch_queries:
                            current_test += 1
                            task = self._run_single_test(
                                config, query, q_type, current_test, total_tests
                            )
                            tasks.append(task)
                        
                        # ë³‘ë ¬ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
                        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                        
                        # ê²°ê³¼ ì²˜ë¦¬
                        for result in batch_results:
                            if isinstance(result, Exception):
                                logger.error(f"   âŒ ë°°ì¹˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {result}")
                                # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„±
                                all_results.append(TestResult(
                                    config=config,
                                    query="unknown",
                                    query_type=q_type,
                                    metrics=EvaluationMetrics(),
                                    timestamp=datetime.now().isoformat(),
                                    success=False,
                                    error=str(result)
                                ))
                            else:
                                all_results.append(result)
                        
                        # ì§„í–‰ë¥  í‘œì‹œ
                        progress = (current_test / total_tests) * 100
                        elapsed_time = time.time() - start_time
                        avg_time_per_test = elapsed_time / current_test if current_test > 0 else 0
                        remaining_tests = total_tests - current_test
                        estimated_remaining = (remaining_tests * avg_time_per_test) / 60 if avg_time_per_test > 0 else 0
                        
                        logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {current_test}/{total_tests} ({progress:.1f}%) | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining:.1f}ë¶„")
                        
                        # ë°°ì¹˜ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                        del tasks, batch_results
                        gc.collect()
                
                # ì„¤ì •ë³„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
        
        finally:
            # ì›ë³¸ ì„¤ì • ë³µì›
            self.config_updater.restore()
            
            # ìµœì¢… í†µê³„
            total_elapsed = time.time() - start_time
            successful_tests = sum(1 for r in all_results if r.success)
            
            logger.info(f"\nâœ… ê²€ì¦ ì™„ë£Œ: {len(all_results)}/{total_tests} ì™„ë£Œ (ì„±ê³µ: {successful_tests}ê°œ, ì†Œìš” ì‹œê°„: {total_elapsed/60:.1f}ë¶„)")
            logger.info("   ì›ë³¸ ì„¤ì • ë³µì› ì™„ë£Œ")
            
            # MLflow run ì¢…ë£Œ
            if self.mlflow_tracker:
                self.mlflow_tracker.end_run()
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
        
        return all_results


class ResultsAnalyzer:
    """ê²°ê³¼ ë¶„ì„ê¸°"""
    
    @staticmethod
    def analyze_results(results: List[TestResult]) -> Dict[str, Any]:
        """ê²°ê³¼ ë¶„ì„"""
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
        successful_results = [r for r in results if r.success]
        
        if not successful_results:
            return {"error": "ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ì„¤ì •ë³„ ê·¸ë£¹í™”
        config_groups = {}
        for result in successful_results:
            config_name = result.config.name
            if config_name not in config_groups:
                config_groups[config_name] = []
            config_groups[config_name].append(result)
        
        # ì„¤ì •ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
        config_scores = {}
        for config_name, config_results in config_groups.items():
            scores = [r.metrics.overall_score for r in config_results]
            config_scores[config_name] = {
                "avg_score": statistics.mean(scores),
                "median_score": statistics.median(scores),
                "min_score": min(scores),
                "max_score": max(scores),
                "std_dev": statistics.stdev(scores) if len(scores) > 1 else 0.0,
                "test_count": len(scores)
            }
        
        # ìµœê³  ì„±ëŠ¥ ì„¤ì • ì°¾ê¸°
        best_config = max(config_scores.items(), key=lambda x: x[1]["avg_score"])
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„ì„
        query_type_analysis = {}
        for q_type in ["law_inquiry", "precedent_search", "general"]:
            type_results = [r for r in successful_results if r.query_type == q_type]
            if type_results:
                type_scores = [r.metrics.overall_score for r in type_results]
                query_type_analysis[q_type] = {
                    "avg_score": statistics.mean(type_scores),
                    "test_count": len(type_scores)
                }
        
        return {
            "total_tests": len(results),
            "successful_tests": len(successful_results),
            "config_scores": config_scores,
            "best_config": {
                "name": best_config[0],
                "metrics": best_config[1]
            },
            "query_type_analysis": query_type_analysis,
            "summary": {
                "best_overall_score": best_config[1]["avg_score"],
                "config_count": len(config_scores)
            }
        }


class ReportGenerator:
    """ë¦¬í¬íŠ¸ ìƒì„±ê¸°"""
    
    @staticmethod
    def generate_report(analysis: Dict[str, Any], results: List[TestResult], output_file: Path):
        """ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "analysis": analysis,
            "detailed_results": [asdict(r) for r in results],
            "recommendations": ReportGenerator._generate_recommendations(analysis)
        }
        
        # JSON ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
        text_report_file = output_file.with_suffix('.txt')
        ReportGenerator._generate_text_report(analysis, text_report_file)
        
        return report
    
    @staticmethod
    def _generate_recommendations(analysis: Dict[str, Any]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if "best_config" in analysis:
            best = analysis["best_config"]
            recommendations.append(
                f"ìµœì  ê°€ì¤‘ì¹˜ ì„¤ì •: {best['name']} (í‰ê·  ì ìˆ˜: {best['metrics']['avg_score']:.3f})"
            )
        
        if "query_type_analysis" in analysis:
            for q_type, metrics in analysis["query_type_analysis"].items():
                recommendations.append(
                    f"{q_type} ì§ˆë¬¸ ìœ í˜• í‰ê·  ì ìˆ˜: {metrics['avg_score']:.3f}"
                )
        
        return recommendations
    
    @staticmethod
    def _generate_text_report(analysis: Dict[str, Any], output_file: Path):
        """í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("ê°€ì¤‘ì¹˜ ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸\n")
            f.write("="*80 + "\n\n")
            
            f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            if "best_config" in analysis:
                best = analysis["best_config"]
                f.write("ìµœì  ê°€ì¤‘ì¹˜ ì„¤ì •:\n")
                f.write(f"  ì´ë¦„: {best['name']}\n")
                f.write(f"  í‰ê·  ì ìˆ˜: {best['metrics']['avg_score']:.3f}\n")
                f.write(f"  ì¤‘ì•™ê°’: {best['metrics']['median_score']:.3f}\n")
                f.write(f"  í‘œì¤€í¸ì°¨: {best['metrics']['std_dev']:.3f}\n")
                f.write(f"  í…ŒìŠ¤íŠ¸ ìˆ˜: {best['metrics']['test_count']}\n\n")
            
            if "config_scores" in analysis:
                f.write("ì„¤ì •ë³„ ì ìˆ˜ (ìƒìœ„ 10ê°œ):\n")
                sorted_configs = sorted(
                    analysis["config_scores"].items(),
                    key=lambda x: x[1]["avg_score"],
                    reverse=True
                )[:10]
                
                for config_name, metrics in sorted_configs:
                    f.write(f"  {config_name}: {metrics['avg_score']:.3f} "
                           f"(ì¤‘ì•™ê°’: {metrics['median_score']:.3f}, "
                           f"í‘œì¤€í¸ì°¨: {metrics['std_dev']:.3f})\n")
                f.write("\n")
            
            if "query_type_analysis" in analysis:
                f.write("ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„ì„:\n")
                for q_type, metrics in analysis["query_type_analysis"].items():
                    f.write(f"  {q_type}: í‰ê·  {metrics['avg_score']:.3f} "
                           f"({metrics['test_count']}ê°œ í…ŒìŠ¤íŠ¸)\n")
                f.write("\n")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ê°€ì¤‘ì¹˜ ì„¤ì • ê²€ì¦ ë° ìµœì í™”")
    parser.add_argument("--query-type", choices=["law_inquiry", "precedent_search", "general", "all"],
                       default="all", help="í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ìœ í˜•")
    parser.add_argument("--quick", action="store_true", help="ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì ì€ ì¡°í•©)")
    parser.add_argument("--output-dir", type=str, default=None, help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--max-workers", type=int, default=None, 
                       help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜, ìµœëŒ€ 4)")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = project_root / "logs" / "test" / "weight_validation"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ê²€ì¦ ì‹¤í–‰
    runner = WeightValidationRunner(quick=args.quick, max_workers=args.max_workers)
    results = await runner.run_validation(args.query_type)
    
    # ê²°ê³¼ ë¶„ì„
    analyzer = ResultsAnalyzer()
    analysis = analyzer.analyze_results(results)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = output_dir / f"weight_validation_{timestamp}.json"
    
    report_generator = ReportGenerator()
    report = report_generator.generate_report(analysis, results, output_file)
    
    # MLflow ìš”ì•½ ë¡œê¹…
    best_config_name = analysis.get("best_config", {}).get("name", "unknown")
    if runner.mlflow_tracker:
        runner.mlflow_tracker.log_summary(analysis, best_config_name)
        runner.mlflow_tracker.log_artifacts(output_file)
        runner.mlflow_tracker.log_artifacts(output_file.with_suffix('.txt'))
        if runner.mlflow_tracker.parent_run_id:
            logger.info(f"\nğŸ“Š MLflow ì‹¤í—˜ ID: {runner.mlflow_tracker.parent_run_id}")
            logger.info(f"   MLflow UI: mlflow ui --backend-store-uri {mlflow.get_tracking_uri()}")
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info("\n" + "="*80)
    logger.info("ê²€ì¦ ê²°ê³¼ ìš”ì•½")
    logger.info("="*80)
    
    if "best_config" in analysis:
        best = analysis["best_config"]
        logger.info(f"\nâœ… ìµœì  ê°€ì¤‘ì¹˜ ì„¤ì •: {best['name']}")
        logger.info(f"   í‰ê·  ì ìˆ˜: {best['metrics']['avg_score']:.3f}")
        logger.info(f"   ì¤‘ì•™ê°’: {best['metrics']['median_score']:.3f}")
        logger.info(f"   í‘œì¤€í¸ì°¨: {best['metrics']['std_dev']:.3f}")
    
    logger.info(f"\nğŸ“Š ê²°ê³¼ ì €ì¥: {output_file}")
    logger.info(f"ğŸ“„ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸: {output_file.with_suffix('.txt')}")
    
    if "recommendations" in report:
        logger.info("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in report["recommendations"]:
            logger.info(f"   - {rec}")


if __name__ == "__main__":
    asyncio.run(main())

