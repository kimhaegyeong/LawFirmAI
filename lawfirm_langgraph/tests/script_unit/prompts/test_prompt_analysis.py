# -*- coding: utf-8 -*-
"""
ì‹¤ì œ LLM í”„ë¡¬í”„íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
- ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ í™•ì¸
- ê²€ìƒ‰ëœ ë¬¸ì„œì™€ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ë¬¸ì„œ ë¹„êµ
- ëˆ„ë½ëœ ë¬¸ì„œ í™•ì¸
- í”„ë¡¬í”„íŠ¸ ê°œì„  ì‚¬í•­ ë„ì¶œ
"""

import sys
import os
import json
import re
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.insert(0, project_root)

def extract_documents_from_prompt(prompt: str) -> Dict[str, List[Dict[str, Any]]]:
    """í”„ë¡¬í”„íŠ¸ì—ì„œ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ"""
    documents = {
        "statute_article": [],
        "case_paragraph": [],
        "decision_paragraph": [],
        "interpretation_paragraph": []
    }
    
    # íƒ€ì…ë³„ ì„¹ì…˜ ì°¾ê¸°
    statute_section = re.search(r'### ğŸ“œ ë²•ë ¹ ì¡°ë¬¸\n\n(.*?)(?=###|$)', prompt, re.DOTALL)
    case_section = re.search(r'### âš–ï¸ íŒë¡€\n\n(.*?)(?=###|$)', prompt, re.DOTALL)
    decision_section = re.search(r'### ğŸ“‹ ê²°ì •ë¡€\n\n(.*?)(?=###|$)', prompt, re.DOTALL)
    interpretation_section = re.search(r'### ğŸ“– í•´ì„ë¡€\n\n(.*?)(?=###|$)', prompt, re.DOTALL)
    
    # ê° ì„¹ì…˜ì—ì„œ ë¬¸ì„œ ì¶”ì¶œ
    if statute_section:
        doc_matches = re.finditer(r'\*\*ë¬¸ì„œ (\d+)\*\*: (.*?) \(ê´€ë ¨ë„: ([\d.]+)\)', statute_section.group(1))
        for match in doc_matches:
            documents["statute_article"].append({
                "number": int(match.group(1)),
                "title": match.group(2),
                "relevance": float(match.group(3))
            })
    
    if case_section:
        doc_matches = re.finditer(r'\*\*ë¬¸ì„œ (\d+)\*\*: (.*?) \(ê´€ë ¨ë„: ([\d.]+)\)', case_section.group(1))
        for match in doc_matches:
            documents["case_paragraph"].append({
                "number": int(match.group(1)),
                "title": match.group(2),
                "relevance": float(match.group(3))
            })
    
    if decision_section:
        doc_matches = re.finditer(r'\*\*ë¬¸ì„œ (\d+)\*\*: (.*?) \(ê´€ë ¨ë„: ([\d.]+)\)', decision_section.group(1))
        for match in doc_matches:
            documents["decision_paragraph"].append({
                "number": int(match.group(1)),
                "title": match.group(2),
                "relevance": float(match.group(3))
            })
    
    if interpretation_section:
        doc_matches = re.finditer(r'\*\*ë¬¸ì„œ (\d+)\*\*: (.*?) \(ê´€ë ¨ë„: ([\d.]+)\)', interpretation_section.group(1))
        for match in doc_matches:
            documents["interpretation_paragraph"].append({
                "number": int(match.group(1)),
                "title": match.group(2),
                "relevance": float(match.group(3))
            })
    
    return documents

def analyze_prompt_improvements(prompt: str, retrieved_docs: List[Dict], structured_docs: List[Dict]) -> Dict[str, Any]:
    """í”„ë¡¬í”„íŠ¸ ê°œì„  ì‚¬í•­ ë¶„ì„"""
    improvements = {
        "missing_documents": [],
        "document_count_issues": [],
        "prompt_structure_issues": [],
        "data_quality_issues": []
    }
    
    # 1. ëˆ„ë½ëœ ë¬¸ì„œ í™•ì¸
    retrieved_doc_ids = {doc.get("document_id") or doc.get("doc_id") for doc in retrieved_docs if doc}
    
    # í”„ë¡¬í”„íŠ¸ì—ì„œ ë¬¸ì„œ ID ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
    prompt_doc_ids = set()
    for doc_type in ["statute_article", "case_paragraph", "decision_paragraph", "interpretation_paragraph"]:
        # í”„ë¡¬í”„íŠ¸ì—ì„œ í•´ë‹¹ íƒ€ì…ì˜ ë¬¸ì„œ ì°¾ê¸°
        pattern = rf'### .*?{doc_type.replace("_", " ")}\n\n(.*?)(?=###|$)'
        section = re.search(pattern, prompt, re.DOTALL | re.IGNORECASE)
        if section:
            # ë¬¸ì„œ IDë‚˜ ì œëª© ì¶”ì¶œ ì‹œë„
            pass
    
    # 2. ë¬¸ì„œ ìˆ˜ í™•ì¸
    prompt_docs = extract_documents_from_prompt(prompt)
    total_prompt_docs = sum(len(docs) for docs in prompt_docs.values())
    total_retrieved = len(retrieved_docs)
    total_structured = len(structured_docs)
    
    if total_prompt_docs < total_retrieved * 0.8:
        improvements["document_count_issues"].append(
            f"í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ë¬¸ì„œê°€ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ 80% ë¯¸ë§Œì…ë‹ˆë‹¤ "
            f"(í”„ë¡¬í”„íŠ¸: {total_prompt_docs}ê°œ, ê²€ìƒ‰: {total_retrieved}ê°œ)"
        )
    
    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì¡° í™•ì¸
    if "## ğŸ” ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ" not in prompt:
        improvements["prompt_structure_issues"].append("ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
    
    # íƒ€ì…ë³„ ì„¹ì…˜ í™•ì¸
    type_sections = {
        "ë²•ë ¹ ì¡°ë¬¸": "ğŸ“œ ë²•ë ¹ ì¡°ë¬¸" in prompt,
        "íŒë¡€": "âš–ï¸ íŒë¡€" in prompt,
        "ê²°ì •ë¡€": "ğŸ“‹ ê²°ì •ë¡€" in prompt,
        "í•´ì„ë¡€": "ğŸ“– í•´ì„ë¡€" in prompt
    }
    
    missing_types = [t for t, exists in type_sections.items() if not exists]
    if missing_types:
        improvements["prompt_structure_issues"].append(
            f"ë‹¤ìŒ íƒ€ì…ì˜ ë¬¸ì„œ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_types)}"
        )
    
    # 4. ë°ì´í„° í’ˆì§ˆ í™•ì¸
    if total_prompt_docs == 0:
        improvements["data_quality_issues"].append("í”„ë¡¬í”„íŠ¸ì— ë¬¸ì„œê°€ ì „í˜€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # ê´€ë ¨ë„ ë¶„í¬ í™•ì¸
    relevance_scores = []
    for doc_type, docs in prompt_docs.items():
        for doc in docs:
            relevance_scores.append(doc.get("relevance", 0.0))
    
    if relevance_scores:
        min_relevance = min(relevance_scores)
        if min_relevance < 0.2:
            improvements["data_quality_issues"].append(
                f"ê´€ë ¨ë„ê°€ 0.2 ë¯¸ë§Œì¸ ë¬¸ì„œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (ìµœì†Œ: {min_relevance:.3f})"
            )
    
    return improvements

def test_prompt_analysis():
    """í”„ë¡¬í”„íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("ì‹¤ì œ LLM í”„ë¡¬í”„íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    try:
        from lawfirm_langgraph.core.workflow.legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
        from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        
        # ì„¤ì • ë¡œë“œ
        config = LangGraphConfig()
        
        # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        workflow = EnhancedLegalQuestionWorkflow(config)
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_query = "ì „ì„¸ê¸ˆ ë°˜í™˜ ë³´ì¦ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")
        print(f"\nğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
        
        # ì´ˆê¸° ìƒíƒœ ìƒì„±
        initial_state = {
            "query": test_query,
            "session_id": "test_session_prompt_analysis",
            "metadata": {}
        }
        
        # í”„ë¡¬í”„íŠ¸ë¥¼ ìº¡ì²˜í•˜ê¸° ìœ„í•´ UnifiedPromptManagerì˜ _build_final_promptë¥¼ ëª¨ë‹ˆí„°ë§
        # ë˜ëŠ” generate_answer_enhanced ì‹¤í–‰ í›„ í”„ë¡¬í”„íŠ¸ í™•ì¸
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        state = workflow.generate_answer_enhanced(initial_state)
        
        # ê²°ê³¼ í™•ì¸
        retrieved_docs = state.get("retrieved_docs", [])
        structured_docs_dict = state.get("structured_documents", {})
        structured_docs = structured_docs_dict.get("documents", []) if isinstance(structured_docs_dict, dict) else []
        
        print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼:")
        print(f"   - ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")
        print(f"   - structured_documents ë¬¸ì„œ ìˆ˜: {len(structured_docs)}ê°œ")
        
        # íƒ€ì…ë³„ ë¶„í¬ í™•ì¸
        if retrieved_docs:
            type_distribution = {}
            for doc in retrieved_docs:
                doc_type = (
                    doc.get("type") or
                    doc.get("source_type") or
                    doc.get("metadata", {}).get("type") if isinstance(doc.get("metadata"), dict) else None or
                    "unknown"
                )
                type_distribution[doc_type] = type_distribution.get(doc_type, 0) + 1
            
            print(f"\nğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬:")
            for doc_type, count in type_distribution.items():
                print(f"   - {doc_type}: {count}ê°œ")
        
        # í”„ë¡¬í”„íŠ¸ í™•ì¸ì„ ìœ„í•´ answer_generatorì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
        # ë˜ëŠ” ë¡œê·¸ì—ì„œ í”„ë¡¬í”„íŠ¸ í™•ì¸
        
        # ì‹¤ì œ í”„ë¡¬í”„íŠ¸ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ UnifiedPromptManagerë¥¼ ì§ì ‘ í˜¸ì¶œ
        from lawfirm_langgraph.core.services.unified_prompt_manager import UnifiedPromptManager
        from lawfirm_langgraph.core.classification.classifiers.question_classifier import QuestionType
        
        prompt_manager = UnifiedPromptManager()
        
        # context êµ¬ì„±
        context = {
            "structured_documents": {
                "documents": structured_docs if structured_docs else retrieved_docs,
                "total_count": len(structured_docs) if structured_docs else len(retrieved_docs)
            },
            "document_count": len(structured_docs) if structured_docs else len(retrieved_docs)
        }
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        base_prompt = "í…ŒìŠ¤íŠ¸"
        final_prompt = prompt_manager._build_final_prompt(
            base_prompt=base_prompt,
            query=test_query,
            context=context,
            question_type=QuestionType.TERM_EXPLANATION
        )
        
        # í”„ë¡¬í”„íŠ¸ ë¶„ì„
        prompt_docs = extract_documents_from_prompt(final_prompt)
        improvements = analyze_prompt_improvements(final_prompt, retrieved_docs, structured_docs)
        
        print(f"\nğŸ“‹ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ë¬¸ì„œ:")
        total_prompt_docs = 0
        for doc_type, docs in prompt_docs.items():
            if docs:
                print(f"   - {doc_type}: {len(docs)}ê°œ")
                total_prompt_docs += len(docs)
                for doc in docs[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    print(f"     * ë¬¸ì„œ {doc['number']}: {doc['title'][:50]}... (ê´€ë ¨ë„: {doc['relevance']:.3f})")
        
        print(f"\n   ì´ í¬í•¨ëœ ë¬¸ì„œ: {total_prompt_docs}ê°œ")
        
        # ê°œì„  ì‚¬í•­ ì¶œë ¥
        print(f"\nğŸ” í”„ë¡¬í”„íŠ¸ ê°œì„  ì‚¬í•­:")
        
        if improvements["missing_documents"]:
            print(f"\n   âŒ ëˆ„ë½ëœ ë¬¸ì„œ:")
            for issue in improvements["missing_documents"]:
                print(f"      - {issue}")
        
        if improvements["document_count_issues"]:
            print(f"\n   âš ï¸ ë¬¸ì„œ ìˆ˜ ë¬¸ì œ:")
            for issue in improvements["document_count_issues"]:
                print(f"      - {issue}")
        
        if improvements["prompt_structure_issues"]:
            print(f"\n   âš ï¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ë¬¸ì œ:")
            for issue in improvements["prompt_structure_issues"]:
                print(f"      - {issue}")
        
        if improvements["data_quality_issues"]:
            print(f"\n   âš ï¸ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ:")
            for issue in improvements["data_quality_issues"]:
                print(f"      - {issue}")
        
        if not any(improvements.values()):
            print(f"\n   âœ… íŠ¹ë³„í•œ ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í”„ë¡¬í”„íŠ¸ ì €ì¥
        prompt_file = "test_prompt_analysis_prompt.txt"
        with open(prompt_file, "w", encoding="utf-8") as f:
            f.write(final_prompt)
        print(f"\nğŸ’¾ í”„ë¡¬í”„íŠ¸ê°€ {prompt_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²°ê³¼ ì €ì¥
        result_file = "test_prompt_analysis_result.json"
        result_data = {
            "query": test_query,
            "retrieved_docs_count": len(retrieved_docs),
            "structured_docs_count": len(structured_docs),
            "prompt_docs_count": total_prompt_docs,
            "prompt_docs_by_type": {k: len(v) for k, v in prompt_docs.items()},
            "improvements": improvements
        }
        
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ê°€ {result_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_prompt_analysis()
    sys.exit(0 if success else 1)

