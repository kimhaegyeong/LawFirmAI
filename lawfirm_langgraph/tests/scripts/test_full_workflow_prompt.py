# -*- coding: utf-8 -*-
"""
ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë° í”„ë¡¬í”„íŠ¸ ë¶„ì„
- ê²€ìƒ‰ ë‹¨ê³„ í¬í•¨ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
- ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ í™•ì¸
- ëˆ„ë½ëœ ë¬¸ì„œ í™•ì¸
- í”„ë¡¬í”„íŠ¸ ê°œì„  ì‚¬í•­ ë„ì¶œ
"""

import sys
import os
import json
import re
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.insert(0, project_root)

def extract_documents_from_prompt(prompt: str) -> Dict[str, List[Dict[str, Any]]]:
    """í”„ë¡¬í”„íŠ¸ì—ì„œ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ"""
    documents = {
        "statute_article": [],
        "case_paragraph": [],
        "decision_paragraph": [],
        "interpretation_paragraph": []
    }
    
    # íƒ€ì…ë³„ ì„¹ì…˜ ì°¾ê¸°
    patterns = {
        "statute_article": r'### ğŸ“œ ë²•ë ¹ ì¡°ë¬¸\n\n(.*?)(?=###|$)',
        "case_paragraph": r'### âš–ï¸ íŒë¡€\n\n(.*?)(?=###|$)',
        "decision_paragraph": r'### ğŸ“‹ ê²°ì •ë¡€\n\n(.*?)(?=###|$)',
        "interpretation_paragraph": r'### ğŸ“– í•´ì„ë¡€\n\n(.*?)(?=###|$)'
    }
    
    for doc_type, pattern in patterns.items():
        section = re.search(pattern, prompt, re.DOTALL)
        if section:
            # ë¬¸ì„œ ë²ˆí˜¸ì™€ ê´€ë ¨ë„ ì¶”ì¶œ
            doc_matches = re.finditer(
                r'\*\*ë¬¸ì„œ (\d+)\*\*: (.*?) \(ê´€ë ¨ë„: ([\d.]+)\)',
                section.group(1)
            )
            for match in doc_matches:
                documents[doc_type].append({
                    "number": int(match.group(1)),
                    "title": match.group(2).strip(),
                    "relevance": float(match.group(3))
                })
    
    return documents

def analyze_prompt_improvements(
    prompt: str, 
    retrieved_docs: List[Dict], 
    structured_docs: List[Dict]
) -> Dict[str, Any]:
    """í”„ë¡¬í”„íŠ¸ ê°œì„  ì‚¬í•­ ë¶„ì„"""
    improvements = {
        "missing_documents": [],
        "document_count_issues": [],
        "prompt_structure_issues": [],
        "data_quality_issues": [],
        "prompt_length_issues": [],
        "document_format_issues": []
    }
    
    # 1. ë¬¸ì„œ ìˆ˜ í™•ì¸
    prompt_docs = extract_documents_from_prompt(prompt)
    total_prompt_docs = sum(len(docs) for docs in prompt_docs.values())
    total_retrieved = len(retrieved_docs)
    total_structured = len(structured_docs)
    
    if total_retrieved > 0:
        inclusion_rate = total_prompt_docs / total_retrieved
        if inclusion_rate < 0.8:
            improvements["document_count_issues"].append(
                f"í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ë¬¸ì„œê°€ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ 80% ë¯¸ë§Œì…ë‹ˆë‹¤ "
                f"(í”„ë¡¬í”„íŠ¸: {total_prompt_docs}ê°œ, ê²€ìƒ‰: {total_retrieved}ê°œ, í¬í•¨ë¥ : {inclusion_rate*100:.1f}%)"
            )
    
    # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì¡° í™•ì¸
    if "## ğŸ” ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ" not in prompt:
        improvements["prompt_structure_issues"].append("ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
    
    # íƒ€ì…ë³„ ì„¹ì…˜ í™•ì¸
    type_sections = {
        "ë²•ë ¹ ì¡°ë¬¸": "ğŸ“œ ë²•ë ¹ ì¡°ë¬¸" in prompt,
        "íŒë¡€": "âš–ï¸ íŒë¡€" in prompt,
        "ê²°ì •ë¡€": "ğŸ“‹ ê²°ì •ë¡€" in prompt,
        "í•´ì„ë¡€": "ğŸ“– í•´ì„ë¡€" in prompt
    }
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œ íƒ€ì… í™•ì¸
    retrieved_types = set()
    for doc in retrieved_docs:
        doc_type = (
            doc.get("type") or
            doc.get("source_type") or
            doc.get("metadata", {}).get("type") if isinstance(doc.get("metadata"), dict) else None or
            "unknown"
        )
        if doc_type != "unknown":
            retrieved_types.add(doc_type)
    
    # ê²€ìƒ‰ëœ íƒ€ì…ì´ ìˆì§€ë§Œ í”„ë¡¬í”„íŠ¸ì— ì—†ëŠ” ê²½ìš°
    missing_types = []
    type_mapping = {
        "statute_article": "ë²•ë ¹ ì¡°ë¬¸",
        "case_paragraph": "íŒë¡€",
        "decision_paragraph": "ê²°ì •ë¡€",
        "interpretation_paragraph": "í•´ì„ë¡€"
    }
    
    for retrieved_type in retrieved_types:
        if retrieved_type in type_mapping:
            section_name = type_mapping[retrieved_type]
            if not type_sections[section_name]:
                missing_types.append(section_name)
    
    if missing_types:
        improvements["prompt_structure_issues"].append(
            f"ê²€ìƒ‰ëœ ë¬¸ì„œ íƒ€ì…ì´ ìˆì§€ë§Œ í”„ë¡¬í”„íŠ¸ì— ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_types)}"
        )
    
    # 3. ë°ì´í„° í’ˆì§ˆ í™•ì¸
    if total_prompt_docs == 0 and total_retrieved > 0:
        improvements["data_quality_issues"].append(
            f"ê²€ìƒ‰ëœ ë¬¸ì„œê°€ {total_retrieved}ê°œ ìˆì§€ë§Œ í”„ë¡¬í”„íŠ¸ì— ë¬¸ì„œê°€ ì „í˜€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        )
    
    # ê´€ë ¨ë„ ë¶„í¬ í™•ì¸
    relevance_scores = []
    for doc_type, docs in prompt_docs.items():
        for doc in docs:
            relevance_scores.append(doc.get("relevance", 0.0))
    
    if relevance_scores:
        min_relevance = min(relevance_scores)
        max_relevance = max(relevance_scores)
        avg_relevance = sum(relevance_scores) / len(relevance_scores)
        
        if min_relevance < 0.2:
            improvements["data_quality_issues"].append(
                f"ê´€ë ¨ë„ê°€ 0.2 ë¯¸ë§Œì¸ ë¬¸ì„œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (ìµœì†Œ: {min_relevance:.3f})"
            )
        
        if avg_relevance < 0.5:
            improvements["data_quality_issues"].append(
                f"í‰ê·  ê´€ë ¨ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ (í‰ê· : {avg_relevance:.3f})"
            )
    
    # 4. í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í™•ì¸
    prompt_length = len(prompt)
    if prompt_length > 8000:
        improvements["prompt_length_issues"].append(
            f"í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({prompt_length}ì, ê¶Œì¥: 8000ì ì´í•˜)"
        )
    elif prompt_length < 500:
        improvements["prompt_length_issues"].append(
            f"í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({prompt_length}ì, ìµœì†Œ: 500ì ì´ìƒ)"
        )
    
    # 5. ë¬¸ì„œ í˜•ì‹ í™•ì¸
    # ë¬¸ì„œì— ì œëª©ì´ë‚˜ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°
    for doc_type, docs in prompt_docs.items():
        for doc in docs:
            if not doc.get("title") or len(doc.get("title", "")) < 5:
                improvements["document_format_issues"].append(
                    f"{doc_type} ë¬¸ì„œ {doc.get('number')}ì˜ ì œëª©ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤"
                )
    
    return improvements

def test_full_workflow_prompt():
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë° í”„ë¡¬í”„íŠ¸ ë¶„ì„"""
    print("\n" + "=" * 80)
    print("ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë° í”„ë¡¬í”„íŠ¸ ë¶„ì„")
    print("=" * 80)
    
    try:
        import asyncio
        from lawfirm_langgraph.core.workflow.workflow_service import LangGraphWorkflowService
        from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        
        # ì„¤ì • ë¡œë“œ
        config = LangGraphConfig.from_env()
        config.enable_checkpoint = False  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        
        # ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        workflow_service = LangGraphWorkflowService(config)
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_query = "ì „ì„¸ê¸ˆ ë°˜í™˜ ë³´ì¦ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")
        print(f"\nğŸ”„ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ë¹„ë™ê¸°)
        async def run_workflow():
            return await workflow_service.process_query(
                query=test_query,
                session_id="test_session_full_workflow",
                enable_checkpoint=False
            )
        
        result = asyncio.run(run_workflow())
        
        # ê²°ê³¼ í™•ì¸
        retrieved_docs = result.get("retrieved_docs", [])
        structured_docs_dict = result.get("structured_documents", {})
        structured_docs = structured_docs_dict.get("documents", []) if isinstance(structured_docs_dict, dict) else []
        
        print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼:")
        print(f"   - ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")
        print(f"   - structured_documents ë¬¸ì„œ ìˆ˜: {len(structured_docs)}ê°œ")
        
        # íƒ€ì…ë³„ ë¶„í¬ í™•ì¸
        if retrieved_docs:
            type_distribution = {}
            for doc in retrieved_docs:
                doc_type = (
                    doc.get("type") or
                    doc.get("source_type") or
                    doc.get("metadata", {}).get("type") if isinstance(doc.get("metadata"), dict) else None or
                    "unknown"
                )
                type_distribution[doc_type] = type_distribution.get(doc_type, 0) + 1
            
            print(f"\nğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬:")
            for doc_type, count in type_distribution.items():
                print(f"   - {doc_type}: {count}ê°œ")
        
        # í”„ë¡¬í”„íŠ¸ í™•ì¸ì„ ìœ„í•´ UnifiedPromptManager ì§ì ‘ í˜¸ì¶œ
        from lawfirm_langgraph.core.agents.prompt_builders.unified_prompt_manager import UnifiedPromptManager
        from lawfirm_langgraph.core.agents.prompt_builders.unified_prompt_manager import QuestionType
        
        prompt_manager = UnifiedPromptManager()
        
        # context êµ¬ì„±
        context = {
            "structured_documents": {
                "documents": structured_docs if structured_docs else retrieved_docs,
                "total_count": len(structured_docs) if structured_docs else len(retrieved_docs)
            },
            "document_count": len(structured_docs) if structured_docs else len(retrieved_docs)
        }
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        base_prompt = "í…ŒìŠ¤íŠ¸"
        final_prompt = prompt_manager._build_final_prompt(
            base_prompt=base_prompt,
            query=test_query,
            context=context,
            question_type=QuestionType.TERM_EXPLANATION
        )
        
        # í”„ë¡¬í”„íŠ¸ ë¶„ì„
        prompt_docs = extract_documents_from_prompt(final_prompt)
        improvements = analyze_prompt_improvements(final_prompt, retrieved_docs, structured_docs)
        
        print(f"\nğŸ“‹ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ë¬¸ì„œ:")
        total_prompt_docs = 0
        for doc_type, docs in prompt_docs.items():
            if docs:
                print(f"   - {doc_type}: {len(docs)}ê°œ")
                total_prompt_docs += len(docs)
                for doc in docs[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    print(f"     * ë¬¸ì„œ {doc['number']}: {doc['title'][:50]}... (ê´€ë ¨ë„: {doc['relevance']:.3f})")
        
        print(f"\n   ì´ í¬í•¨ëœ ë¬¸ì„œ: {total_prompt_docs}ê°œ")
        
        # ê°œì„  ì‚¬í•­ ì¶œë ¥
        print(f"\nğŸ” í”„ë¡¬í”„íŠ¸ ê°œì„  ì‚¬í•­:")
        
        all_issues = []
        
        if improvements["missing_documents"]:
            print(f"\n   âŒ ëˆ„ë½ëœ ë¬¸ì„œ:")
            for issue in improvements["missing_documents"]:
                print(f"      - {issue}")
                all_issues.append(("ëˆ„ë½ëœ ë¬¸ì„œ", issue))
        
        if improvements["document_count_issues"]:
            print(f"\n   âš ï¸ ë¬¸ì„œ ìˆ˜ ë¬¸ì œ:")
            for issue in improvements["document_count_issues"]:
                print(f"      - {issue}")
                all_issues.append(("ë¬¸ì„œ ìˆ˜ ë¬¸ì œ", issue))
        
        if improvements["prompt_structure_issues"]:
            print(f"\n   âš ï¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ë¬¸ì œ:")
            for issue in improvements["prompt_structure_issues"]:
                print(f"      - {issue}")
                all_issues.append(("í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ë¬¸ì œ", issue))
        
        if improvements["data_quality_issues"]:
            print(f"\n   âš ï¸ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ:")
            for issue in improvements["data_quality_issues"]:
                print(f"      - {issue}")
                all_issues.append(("ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ", issue))
        
        if improvements["prompt_length_issues"]:
            print(f"\n   âš ï¸ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ë¬¸ì œ:")
            for issue in improvements["prompt_length_issues"]:
                print(f"      - {issue}")
                all_issues.append(("í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ë¬¸ì œ", issue))
        
        if improvements["document_format_issues"]:
            print(f"\n   âš ï¸ ë¬¸ì„œ í˜•ì‹ ë¬¸ì œ:")
            for issue in improvements["document_format_issues"]:
                print(f"      - {issue}")
                all_issues.append(("ë¬¸ì„œ í˜•ì‹ ë¬¸ì œ", issue))
        
        if not all_issues:
            print(f"\n   âœ… íŠ¹ë³„í•œ ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í”„ë¡¬í”„íŠ¸ ì €ì¥
        prompt_file = "test_full_workflow_prompt.txt"
        with open(prompt_file, "w", encoding="utf-8") as f:
            f.write(final_prompt)
        print(f"\nğŸ’¾ í”„ë¡¬í”„íŠ¸ê°€ {prompt_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê²°ê³¼ ì €ì¥
        result_file = "test_full_workflow_prompt_result.json"
        result_data = {
            "query": test_query,
            "retrieved_docs_count": len(retrieved_docs),
            "structured_docs_count": len(structured_docs),
            "prompt_docs_count": total_prompt_docs,
            "prompt_docs_by_type": {k: len(v) for k, v in prompt_docs.items()},
            "prompt_length": len(final_prompt),
            "improvements": improvements,
            "all_issues": all_issues
        }
        
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ê°€ {result_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_full_workflow_prompt()
    sys.exit(0 if success else 1)

