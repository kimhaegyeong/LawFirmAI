# -*- coding: utf-8 -*-
"""
interpretation_paragraphê°€ 0ê°œì¸ ì›ì¸ ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import asyncio
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.insert(0, project_root)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

async def debug_interpretation_paragraph():
    """interpretation_paragraphê°€ 0ê°œì¸ ì›ì¸ ë””ë²„ê¹…"""
    print("\n" + "=" * 80)
    print("interpretation_paragraph ë””ë²„ê¹… ì‹œì‘")
    print("=" * 80)
    
    try:
        from lawfirm_langgraph.core.workflow.workflow_service import LangGraphWorkflowService
        from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        
        config = LangGraphConfig()
        workflow = LangGraphWorkflowService(config)
        
        query = "ì „ì„¸ê¸ˆ ë°˜í™˜ ë³´ì¦ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        print(f"\nğŸ“ ì¿¼ë¦¬: {query}")
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = await workflow.process_query(query)
        
        # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
        retrieved_docs = result.get("retrieved_documents", [])
        print(f"\nğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
        
        # íƒ€ì…ë³„ ë¶„í¬ í™•ì¸
        type_distribution = {}
        interpretation_docs = []
        for doc in retrieved_docs:
            if not isinstance(doc, dict):
                continue
            doc_type = (
                doc.get("type") or
                doc.get("source_type") or
                doc.get("metadata", {}).get("type") if isinstance(doc.get("metadata"), dict) else None or
                doc.get("metadata", {}).get("source_type") if isinstance(doc.get("metadata"), dict) else None or
                "unknown"
            )
            type_distribution[doc_type] = type_distribution.get(doc_type, 0) + 1
            
            if doc_type == "interpretation_paragraph":
                interpretation_docs.append(doc)
                print(f"\nâœ… interpretation_paragraph ë¬¸ì„œ ë°œê²¬:")
                print(f"   - id: {doc.get('id')}")
                print(f"   - source_type: {doc.get('source_type')}")
                print(f"   - type: {doc.get('type')}")
                print(f"   - relevance_score: {doc.get('relevance_score')}")
                print(f"   - is_sample: {doc.get('metadata', {}).get('is_sample', False)}")
                print(f"   - search_type: {doc.get('search_type')}")
        
        print(f"\nğŸ“Š íƒ€ì…ë³„ ë¶„í¬:")
        for doc_type, count in sorted(type_distribution.items()):
            print(f"   - {doc_type}: {count}ê°œ")
        
        print(f"\nğŸ” interpretation_paragraph ë¬¸ì„œ ìˆ˜: {len(interpretation_docs)}")
        
        # í”„ë¡¬í”„íŠ¸ í™•ì¸
        prompt = result.get("prompt", "")
        if "ğŸ“– í•´ì„ë¡€" in prompt:
            print("\nâœ… í”„ë¡¬í”„íŠ¸ì— í•´ì„ë¡€ ì„¹ì…˜ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            # í•´ì„ë¡€ ì„¹ì…˜ ì¶”ì¶œ
            import re
            pattern = r'### ğŸ“– í•´ì„ë¡€\n\n(.*?)(?=###|$)'
            match = re.search(pattern, prompt, re.DOTALL)
            if match:
                interpretation_section = match.group(1)
                print(f"   ì„¹ì…˜ ê¸¸ì´: {len(interpretation_section)}ì")
                print(f"   ë¬¸ì„œ ìˆ˜: {interpretation_section.count('**ë¬¸ì„œ')}")
        else:
            print("\nâŒ í”„ë¡¬í”„íŠ¸ì— í•´ì„ë¡€ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # sources_detail í™•ì¸
        sources_detail = result.get("sources_detail", [])
        interpretation_sources = [s for s in sources_detail if s.get("type") == "interpretation_paragraph"]
        print(f"\nğŸ“‹ sources_detailì˜ interpretation_paragraph ìˆ˜: {len(interpretation_sources)}")
        
        # ê²€ìƒ‰ ë‹¨ê³„ë³„ í™•ì¸
        search_results = result.get("search_results", {})
        semantic_results = search_results.get("semantic_results", [])
        interpretation_semantic = [d for d in semantic_results if (
            d.get("type") == "interpretation_paragraph" or
            d.get("source_type") == "interpretation_paragraph"
        )]
        print(f"\nğŸ” semantic_resultsì˜ interpretation_paragraph ìˆ˜: {len(interpretation_semantic)}")
        
        # ìƒ˜í”Œë§ í™•ì¸
        if interpretation_docs:
            print("\nâœ… interpretation_paragraph ë¬¸ì„œê°€ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            for doc in interpretation_docs:
                if doc.get("metadata", {}).get("is_sample") or doc.get("search_type") == "type_sample":
                    print(f"   - ìƒ˜í”Œë§ëœ ë¬¸ì„œ: {doc.get('id')}")
        else:
            print("\nâŒ interpretation_paragraph ë¬¸ì„œê°€ ê²€ìƒ‰ ê²°ê³¼ì— ì—†ìŠµë‹ˆë‹¤")
            print("   ì›ì¸ ë¶„ì„:")
            print("   1. ìƒ˜í”Œë§ì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ")
            print("   2. ìƒ˜í”Œë§ì€ ë˜ì—ˆì§€ë§Œ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ")
            print("   3. í”„ë¡¬í”„íŠ¸ í•„í„°ë§ì—ì„œ ì œì™¸ë˜ì—ˆì„ ìˆ˜ ìˆìŒ")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(debug_interpretation_paragraph())

