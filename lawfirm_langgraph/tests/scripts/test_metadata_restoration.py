# -*- coding: utf-8 -*-
"""
ë©”íƒ€ë°ì´í„° ë³µì› ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
- ê²€ìƒ‰ ê²°ê³¼ì— ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
"""

import sys
import os
import sqlite3
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
script_dir = Path(__file__).parent
tests_dir = script_dir.parent
lawfirm_langgraph_dir = tests_dir.parent
project_root = lawfirm_langgraph_dir.parent

if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
if str(lawfirm_langgraph_dir) not in sys.path:
    sys.path.insert(0, str(lawfirm_langgraph_dir))

from lawfirm_langgraph.core.search.engines.semantic_search_engine_v2 import SemanticSearchEngineV2
from lawfirm_langgraph.core.utils.config import Config
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_metadata_restoration():
    """ë©”íƒ€ë°ì´í„° ë³µì› ê²€ì¦"""
    logger.info("="*80)
    logger.info("ë©”íƒ€ë°ì´í„° ë³µì› ê²€ì¦ í…ŒìŠ¤íŠ¸")
    logger.info("="*80)
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (IndexIVFPQ ì¸ë±ìŠ¤ ì‚¬ìš©)
    if not os.getenv('USE_EXTERNAL_VECTOR_STORE'):
        os.environ['USE_EXTERNAL_VECTOR_STORE'] = 'true'
    
    if not os.getenv('EXTERNAL_VECTOR_STORE_BASE_PATH'):
        possible_paths = [
            "data/vector_store/v2.0.0-dynamic-dynamic-ivfpq",
            "./data/vector_store/v2.0.0-dynamic-dynamic-ivfpq",
            str(project_root / "data" / "vector_store" / "v2.0.0-dynamic-dynamic-ivfpq"),
        ]
        detected_path = None
        for p in possible_paths:
            if Path(p).exists():
                detected_path = p
                break
        
        if detected_path:
            os.environ['EXTERNAL_VECTOR_STORE_BASE_PATH'] = detected_path
            logger.info(f"âœ… IndexIVFPQ ì¸ë±ìŠ¤ ê²½ë¡œ: {detected_path}")
        else:
            logger.warning("âš ï¸  IndexIVFPQ ì¸ë±ìŠ¤ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # SemanticSearchEngineV2 ì´ˆê¸°í™”
    config = Config()
    search_engine = SemanticSearchEngineV2(
        db_path=config.database_path,
        use_external_index=config.use_external_vector_store,
        external_index_path=config.external_vector_store_base_path
    )
    
    # ê²€ìƒ‰ ì¿¼ë¦¬
    query = "ì„ëŒ€ì°¨ ë³´ì¦ê¸ˆ ë°˜í™˜"
    logger.info(f"\nğŸ“ ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
    
    # ê²€ìƒ‰ ì‹¤í–‰
    results = search_engine.search(query, k=10, similarity_threshold=0.2)
    
    logger.info(f"\nâœ… ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    
    if not results:
        logger.warning("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë©”íƒ€ë°ì´í„° ê²€ì¦
    logger.info("\n" + "="*80)
    logger.info("ê²€ìƒ‰ ê²°ê³¼ ë©”íƒ€ë°ì´í„° ê²€ì¦")
    logger.info("="*80)
    
    metadata_stats = {
        'has_doc_id': 0,
        'has_casenames': 0,
        'has_court': 0,
        'has_org': 0,
        'has_all_metadata': 0,
        'missing_metadata': 0,
        'text_too_short': 0
    }
    
    for i, result in enumerate(results[:10], 1):
        logger.info(f"\n--- ê²°ê³¼ {i} ---")
        # chunk_idëŠ” metadata ì•ˆì— ìˆìŒ
        metadata = result.get('metadata', {})
        chunk_id = metadata.get('chunk_id') or result.get('chunk_id')
        source_type = metadata.get('source_type') or result.get('type') or result.get('source_type')
        logger.info(f"  chunk_id: {chunk_id}")
        logger.info(f"  source_type: {source_type}")
        logger.info(f"  score: {result.get('score', 0):.4f}")
        
        # ë©”íƒ€ë°ì´í„° í™•ì¸ (ìµœìƒìœ„ ë ˆë²¨ê³¼ metadata ëª¨ë‘ í™•ì¸)
        doc_id = result.get('doc_id') or metadata.get('doc_id')
        casenames = result.get('casenames') or metadata.get('casenames')
        court = result.get('court') or metadata.get('court')
        org = result.get('org') or metadata.get('org')
        
        if doc_id:
            metadata_stats['has_doc_id'] += 1
            logger.info(f"  âœ… doc_id: {doc_id}")
        else:
            logger.warning(f"  âš ï¸  doc_id: ì—†ìŒ")
        
        # source_typeì— ë”°ë¼ í•„ìš”í•œ ë©”íƒ€ë°ì´í„° í™•ì¸
        if source_type == 'case_paragraph':
            if casenames:
                metadata_stats['has_casenames'] += 1
                logger.info(f"  âœ… casenames: {casenames[:50]}...")
            else:
                logger.warning(f"  âš ï¸  casenames: ì—†ìŒ")
            
            if court:
                metadata_stats['has_court'] += 1
                logger.info(f"  âœ… court: {court}")
            else:
                logger.warning(f"  âš ï¸  court: ì—†ìŒ")
            
            # case_paragraphëŠ” doc_id, casenames, court ëª¨ë‘ í•„ìš”
            if doc_id and casenames and court:
                metadata_stats['has_all_metadata'] += 1
            else:
                metadata_stats['missing_metadata'] += 1
        elif source_type == 'decision_paragraph':
            if org:
                metadata_stats['has_org'] = metadata_stats.get('has_org', 0) + 1
                logger.info(f"  âœ… org: {org}")
            else:
                logger.warning(f"  âš ï¸  org: ì—†ìŒ")
            
            # decision_paragraphëŠ” doc_idì™€ org í•„ìš”
            if doc_id and org:
                metadata_stats['has_all_metadata'] += 1
            else:
                metadata_stats['missing_metadata'] += 1
        else:
            # ë‹¤ë¥¸ íƒ€ì…ì€ doc_idë§Œ í™•ì¸
            if doc_id:
                metadata_stats['has_all_metadata'] += 1
            else:
                metadata_stats['missing_metadata'] += 1
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
        text = result.get('text') or result.get('content') or ''
        if len(text) < 100:
            metadata_stats['text_too_short'] += 1
            logger.warning(f"  âš ï¸  í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ: {len(text)}ì")
        else:
            logger.info(f"  âœ… í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
    
    # í†µê³„ ì¶œë ¥
    logger.info("\n" + "="*80)
    logger.info("ë©”íƒ€ë°ì´í„° ë³µì› í†µê³„")
    logger.info("="*80)
    logger.info(f"ì „ì²´ ê²°ê³¼: {len(results)}ê°œ")
    logger.info(f"doc_id ìˆìŒ: {metadata_stats['has_doc_id']}ê°œ")
    logger.info(f"casenames ìˆìŒ: {metadata_stats['has_casenames']}ê°œ")
    logger.info(f"court ìˆìŒ: {metadata_stats['has_court']}ê°œ")
    logger.info(f"org ìˆìŒ: {metadata_stats.get('has_org', 0)}ê°œ")
    logger.info(f"ëª¨ë“  ë©”íƒ€ë°ì´í„° ìˆìŒ: {metadata_stats['has_all_metadata']}ê°œ")
    logger.info(f"ë©”íƒ€ë°ì´í„° ëˆ„ë½: {metadata_stats['missing_metadata']}ê°œ")
    logger.info(f"í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ: {metadata_stats['text_too_short']}ê°œ")
    
    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì§ì ‘ í™•ì¸
    logger.info("\n" + "="*80)
    logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì§ì ‘ í™•ì¸")
    logger.info("="*80)
    
    conn = sqlite3.connect(config.database_path)
    conn.row_factory = sqlite3.Row
    
    # ì²« ë²ˆì§¸ ê²°ê³¼ì˜ chunk_id ê°€ì ¸ì˜¤ê¸° (metadata ì•ˆì— ìˆìŒ)
    sample_chunk_id = None
    if results:
        sample_metadata = results[0].get('metadata', {})
        sample_chunk_id = sample_metadata.get('chunk_id') or results[0].get('chunk_id')
    
    if sample_chunk_id:
        cursor = conn.execute("""
            SELECT id, source_type, source_id, meta, LENGTH(text) as text_length
            FROM text_chunks
            WHERE id = ?
        """, (sample_chunk_id,))
        row = cursor.fetchone()
        
        if row:
            logger.info(f"ì²­í¬ ID {sample_chunk_id}:")
            logger.info(f"  source_type: {row['source_type']}")
            logger.info(f"  source_id: {row['source_id']}")
            logger.info(f"  text_length: {row['text_length']}ì")
            
            if row['meta']:
                try:
                    meta_json = json.loads(row['meta'])
                    logger.info(f"  âœ… meta ì»¬ëŸ¼ì— ë©”íƒ€ë°ì´í„° ìˆìŒ:")
                    logger.info(f"    doc_id: {meta_json.get('doc_id', 'ì—†ìŒ')}")
                    logger.info(f"    casenames: {meta_json.get('casenames', 'ì—†ìŒ')[:50]}...")
                    logger.info(f"    court: {meta_json.get('court', 'ì—†ìŒ')}")
                except Exception as e:
                    logger.warning(f"  âš ï¸  meta JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            else:
                logger.warning(f"  âš ï¸  meta ì»¬ëŸ¼ì´ ë¹„ì–´ìˆìŒ")
    
    conn.close()
    
    logger.info("\n" + "="*80)
    logger.info("ê²€ì¦ ì™„ë£Œ")
    logger.info("="*80)

if __name__ == "__main__":
    try:
        test_metadata_restoration()
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)

