#!/usr/bin/env python3
"""
LangGraph ì§ˆì˜ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

LangSmith SDKë¥¼ ì‚¬ìš©í•˜ì—¬ LangGraph ì‹¤í–‰ì„ ë¶„ì„í•˜ê³  ê°œì„  ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python lawfirm_langgraph/tests/scripts/analyze_langgraph_queries.py [ì˜µì…˜]

ì˜µì…˜:
    --hours: ë¶„ì„í•  ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸ê°’: 24)
    --limit: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)
    --run-id: íŠ¹ì • run ID ë¶„ì„
    --output: ê²°ê³¼ ì¶œë ¥ í˜•ì‹ (json, table, summary)
"""

import sys
import os
import argparse
import json
from pathlib import Path
from typing import Optional

project_root = Path(__file__).parent.parent.parent.parent
lawfirm_langgraph_path = project_root / "lawfirm_langgraph"
if lawfirm_langgraph_path.exists():
    sys.path.insert(0, str(lawfirm_langgraph_path))

try:
    from core.utils.langsmith_analyzer import LangGraphQueryAnalyzer
except ImportError:
    print("Error: langsmith_analyzer module not found. Make sure langsmith is installed.")
    print("Install with: pip install langsmith")
    sys.exit(1)


def print_summary(analyzer: LangGraphQueryAnalyzer, hours: int, limit: int):
    """ìš”ì•½ ì •ë³´ ì¶œë ¥"""
    print(f"\n{'='*60}")
    print(f"LangGraph ì§ˆì˜ ë¶„ì„ ìš”ì•½")
    print(f"{'='*60}")
    print(f"í”„ë¡œì íŠ¸: {analyzer.project_name}")
    print(f"ë¶„ì„ ê¸°ê°„: ìµœê·¼ {hours}ì‹œê°„")
    print(f"ìµœëŒ€ ì¡°íšŒ: {limit}ê°œ")
    print(f"{'='*60}\n")
    
    runs = analyzer.get_recent_runs(hours=hours, limit=limit)
    
    if not runs:
        print("âŒ ë¶„ì„í•  ì‹¤í–‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   LangSmith íŠ¸ë ˆì´ì‹±ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    print(f"âœ… ì´ {len(runs)}ê°œì˜ ì‹¤í–‰ ê¸°ë¡ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
    
    patterns = analyzer.analyze_query_patterns(runs)
    
    print("ğŸ“Š ì„±ëŠ¥ í†µê³„:")
    print(f"  - í‰ê·  í† í° ì‚¬ìš©ëŸ‰: {patterns['token_usage']['average_per_run']:.0f} tokens/run")
    print(f"  - ìµœëŒ€ í† í° ì‚¬ìš©ëŸ‰: {patterns['token_usage']['max']} tokens")
    print(f"  - ì´ í† í° ì‚¬ìš©ëŸ‰: {patterns['token_usage']['total']} tokens")
    
    if patterns.get("slow_queries"):
        print(f"\nâ±ï¸ ëŠë¦° ì§ˆì˜ ({len(patterns['slow_queries'])}ê°œ):")
        for query_info in patterns["slow_queries"][:5]:
            print(f"  - {query_info['query'][:60]}... ({query_info['duration']:.2f}ì´ˆ)")
    
    if patterns.get("error_queries"):
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ ì§ˆì˜ ({len(patterns['error_queries'])}ê°œ):")
        for query_info in patterns["error_queries"][:5]:
            print(f"  - {query_info['query'][:60]}...")
            print(f"    ì˜¤ë¥˜: {query_info['error'][:100]}")
    
    if patterns.get("common_nodes"):
        print(f"\nğŸ”„ ìì£¼ ì‹¤í–‰ë˜ëŠ” ë…¸ë“œ:")
        sorted_nodes = sorted(
            patterns["common_nodes"].items(),
            key=lambda x: x[1],
            reverse=True
        )[:10]
        for node_name, count in sorted_nodes:
            avg_duration = patterns["average_durations"].get(node_name, 0)
            print(f"  - {node_name}: {count}íšŒ (í‰ê·  {avg_duration:.2f}ì´ˆ)")


def analyze_single_run(analyzer: LangGraphQueryAnalyzer, run_id: str, show_tree: bool = False):
    """ë‹¨ì¼ run ë¶„ì„"""
    print(f"\n{'='*60}")
    print(f"Run ë¶„ì„: {run_id}")
    print(f"{'='*60}\n")
    
    try:
        if not analyzer._validate_run_id(run_id):
            print(f"âŒ ì˜ëª»ëœ Run ID í˜•ì‹: {run_id}")
            return
        
        run = analyzer.client.read_run(run_id)
        
        tree = None
        if show_tree:
            tree = analyzer.get_run_tree(run_id, show_progress=True)
        
        analysis = analyzer.analyze_run_performance(run, tree=tree)
        
        print(f"ì§ˆì˜: {analysis['query']}")
        print(f"ìƒíƒœ: {analysis['status']}")
        if analysis['start_time']:
            print(f"ì‹œì‘ ì‹œê°„: {analysis['start_time']}")
        if analysis['end_time']:
            print(f"ì¢…ë£Œ ì‹œê°„: {analysis['end_time']}")
        print(f"ì‹¤í–‰ ì‹œê°„: {analysis['duration']:.2f}ì´ˆ" if analysis['duration'] else "N/A")
        print(f"í† í° ì‚¬ìš©ëŸ‰: {analysis['total_tokens']}")
        print(f"ì˜ˆìƒ ë¹„ìš©: ${analysis['total_cost']:.4f}")
        
        state_info = analysis.get('state_info', {})
        if state_info:
            print(f"\nState ì •ë³´:")
            print(f"  - Inputs ì¡´ì¬: {state_info.get('has_inputs', False)}")
            print(f"  - Outputs ì¡´ì¬: {state_info.get('has_outputs', False)}")
            if state_info.get('state_snapshot'):
                print(f"  - State ìŠ¤ëƒ…ìƒ·: {state_info['state_snapshot']}")
            if state_info.get('input_keys'):
                print(f"  - Input í‚¤: {', '.join(state_info['input_keys'][:5])}")
            if state_info.get('output_keys'):
                print(f"  - Output í‚¤: {', '.join(state_info['output_keys'][:5])}")
        
        if show_tree:
            print(f"\n{'='*60}")
            print("RunTree êµ¬ì¡°:")
            print(f"{'='*60}")
            tree_visualization = analyzer.visualize_run_tree(run_id)
            print(tree_visualization)
            
            print(f"\n{'='*60}")
            print("State íë¦„ ë¶„ì„:")
            print(f"{'='*60}")
            state_flow = analyzer.analyze_state_flow(run_id)
            if state_flow:
                summary = state_flow.get('state_changes_summary', {})
                print(f"ì´ ë…¸ë“œ: {summary.get('total_nodes', 0)}")
                print(f"Stateê°€ ìˆëŠ” ë…¸ë“œ: {summary.get('nodes_with_state', 0)}")
                print(f"State ë³€ê²½ì´ ìˆëŠ” ë…¸ë“œ: {summary.get('nodes_with_changes', 0)}")
                print(f"ì´ State ë³€ê²½ íšŸìˆ˜: {summary.get('total_changes', 0)}")
                
                groups_usage = state_flow.get('state_groups_usage', {})
                if groups_usage:
                    print(f"\nState ê·¸ë£¹ ì‚¬ìš© í˜„í™©:")
                    for group, count in sorted(groups_usage.items(), key=lambda x: x[1], reverse=True):
                        print(f"  - {group}: {count}íšŒ")
                
                transitions = state_flow.get('state_transitions', [])
                if transitions:
                    print(f"\nState ì „í™˜ ìƒì„¸ ({len(transitions)}ê°œ):")
                    for i, transition in enumerate(transitions[:10], 1):
                        changes = transition.get('changes', {})
                        print(f"  [{i}] {transition.get('node', 'unknown')}:")
                        print(f"      ì¶”ê°€ëœ í‚¤: {len(changes.get('keys_added', []))}")
                        print(f"      ì œê±°ëœ í‚¤: {len(changes.get('keys_removed', []))}")
                        print(f"      ìˆ˜ì •ëœ í‚¤: {len(changes.get('keys_modified', []))}")
                        groups_modified = changes.get('groups_modified', [])
                        if groups_modified:
                            print(f"      ìˆ˜ì •ëœ ê·¸ë£¹: {[g['group'] for g in groups_modified]}")
                
                # State ì „ë‹¬ ì •ë³´ í‘œì‹œ
                nodes_with_state = state_flow.get('nodes_with_state', [])
                inherited_nodes = [n for n in nodes_with_state if n.get('state_inherited', False)]
                if inherited_nodes:
                    print(f"\nState ì „ë‹¬ í™•ì¸:")
                    print(f"  - Stateë¥¼ ìƒì†ë°›ì€ ë…¸ë“œ: {len(inherited_nodes)}ê°œ")
                    for node in inherited_nodes[:5]:
                        inherited_keys = node.get('inherited_keys', [])
                        print(f"    - {node.get('node_name', 'unknown')}: {len(inherited_keys)}ê°œ í‚¤ ìƒì†")
                        if inherited_keys:
                            print(f"      ìƒì†ëœ í‚¤: {', '.join(inherited_keys[:5])}")
                else:
                    print(f"\nState ì „ë‹¬ í™•ì¸:")
                    print(f"  - Stateë¥¼ ìƒì†ë°›ì€ ë…¸ë“œ: 0ê°œ (ë¶€ëª¨-ìì‹ ê°„ state ì „ë‹¬ì´ í™•ì¸ë˜ì§€ ì•ŠìŒ)")
            
            cache_stats = analyzer.get_cache_stats()
            print(f"\nìºì‹œ í†µê³„:")
            print(f"  - íˆíŠ¸ìœ¨: {cache_stats['hit_rate']:.1f}%")
            print(f"  - íˆíŠ¸: {cache_stats['hits']}, ë¯¸ìŠ¤: {cache_stats['misses']}")
            print(f"  - Run ìºì‹œ í¬ê¸°: {cache_stats['run_cache_size']}")
            print(f"  - Tree ìºì‹œ í¬ê¸°: {cache_stats['tree_cache_size']}")
            
            stats = analyzer.get_run_statistics(run_id, tree=tree)
            if stats:
                print(f"\n{'='*60}")
                print("í†µê³„ ì •ë³´:")
                print(f"{'='*60}")
                print(f"ì´ Runs: {stats['total_runs']}")
                print(f"ìµœëŒ€ ê¹Šì´: {stats['max_depth']}")
                print(f"ì´ ì‹¤í–‰ ì‹œê°„: {stats['total_duration']:.2f}ì´ˆ")
                print(f"í‰ê·  ì‹¤í–‰ ì‹œê°„: {stats['average_duration']:.2f}ì´ˆ")
                print(f"\nRun Typeë³„ ë¶„í¬:")
                for run_type, count in stats['by_type'].items():
                    print(f"  - {run_type}: {count}")
                print(f"\nStatusë³„ ë¶„í¬:")
                for status, count in stats['by_status'].items():
                    print(f"  - {status}: {count}")
                if stats.get('state_updates'):
                    state_updates = stats['state_updates']
                    print(f"\nState ì—…ë°ì´íŠ¸ í†µê³„:")
                    print(f"  - Stateê°€ ìˆëŠ” ë…¸ë“œ: {state_updates.get('nodes_with_state', 0)}")
                    print(f"  - State ì „í™˜ íšŸìˆ˜: {state_updates.get('state_transitions', 0)}")
                if stats.get('node_durations'):
                    print(f"\në…¸ë“œë³„ ì‹¤í–‰ ì‹œê°„ í†µê³„:")
                    for node_name, node_stats in sorted(
                        stats['node_durations'].items(),
                        key=lambda x: x[1]['total'],
                        reverse=True
                    )[:10]:
                        print(f"  - {node_name}:")
                        print(f"    ì‹¤í–‰ íšŸìˆ˜: {node_stats['count']}")
                        print(f"    ì´ ì‹œê°„: {node_stats['total']:.2f}ì´ˆ")
                        print(f"    í‰ê·  ì‹œê°„: {node_stats['average']:.2f}ì´ˆ")
                        print(f"    ìµœì†Œ/ìµœëŒ€: {node_stats['min']:.2f}ì´ˆ / {node_stats['max']:.2f}ì´ˆ")
        
        if analysis.get("nodes"):
            print(f"\në…¸ë“œë³„ ì‹¤í–‰ ì •ë³´ ({len(analysis['nodes'])}ê°œ):")
            for i, node in enumerate(analysis["nodes"], 1):
                duration_str = f"{node['duration']:.2f}ì´ˆ" if node.get('duration') else "N/A"
                tokens_str = f"{node['tokens']} tokens" if node.get('tokens') else "N/A"
                status_str = f" [{node['status']}]" if node.get('status') else ""
                print(f"  [{i}] {node['name']} ({node['run_type']}){status_str}: {duration_str} ({tokens_str})")
        else:
            print(f"\në…¸ë“œ ì •ë³´: ìì‹ runsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if analysis.get("bottlenecks"):
            print(f"\nğŸŒ ë³‘ëª© ì§€ì :")
            for bottleneck in analysis["bottlenecks"]:
                print(f"  - {bottleneck['node']}: {bottleneck['duration']:.2f}ì´ˆ ({bottleneck['tokens']} tokens)")
        
        suggestions = analyzer.get_improvement_suggestions(analysis)
        if suggestions:
            print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ:")
            for suggestion in suggestions:
                print(f"  {suggestion}")
        
        if analysis.get("error"):
            print(f"\nâŒ ì˜¤ë¥˜:")
            print(f"  {analysis['error']}")
    
    except Exception as e:
        print(f"âŒ Run ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


def print_table(analyzer: LangGraphQueryAnalyzer, hours: int, limit: int):
    """í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥"""
    runs = analyzer.get_recent_runs(hours=hours, limit=limit)
    
    if not runs:
        print("âŒ ë¶„ì„í•  ì‹¤í–‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\n{'='*100}")
    print(f"{'ì§ˆì˜':<50} {'ìƒíƒœ':<10} {'ì‹œê°„(ì´ˆ)':<12} {'í† í°':<12}")
    print(f"{'='*100}")
    
    for run in runs[:20]:
        query = analyzer._extract_query(run)
        query_short = query[:48] + "..." if len(query) > 50 else query
        status = run.status or "unknown"
        duration = "N/A"
        if run.start_time and run.end_time:
            duration = f"{(run.end_time - run.start_time).total_seconds():.2f}"
        
        print(f"{query_short:<50} {status:<10} {duration:<12}")


def export_json(analyzer: LangGraphQueryAnalyzer, hours: int, limit: int, output_file: str):
    """JSON í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    runs = analyzer.get_recent_runs(hours=hours, limit=limit)
    
    results = {
        "project": analyzer.project_name,
        "analysis_period_hours": hours,
        "total_runs": len(runs),
        "runs": []
    }
    
    for run in runs:
        analysis = analyzer.analyze_run_performance(run)
        patterns = analyzer.analyze_query_patterns([run])
        suggestions = analyzer.get_improvement_suggestions(analysis, patterns)
        
        results["runs"].append({
            "run_id": str(run.id),
            "analysis": analysis,
            "suggestions": suggestions
        })
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ê²°ê³¼ë¥¼ {output_file}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


def main():
    parser = argparse.ArgumentParser(
        description="LangGraph ì§ˆì˜ ë¶„ì„ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "--hours",
        type=int,
        default=24,
        help="ë¶„ì„í•  ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸ê°’: 24)"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=100,
        help="ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)"
    )
    parser.add_argument(
        "--run-id",
        type=str,
        help="íŠ¹ì • run ID ë¶„ì„"
    )
    parser.add_argument(
        "--output",
        choices=["json", "table", "summary"],
        default="summary",
        help="ê²°ê³¼ ì¶œë ¥ í˜•ì‹ (ê¸°ë³¸ê°’: summary)"
    )
    parser.add_argument(
        "--export",
        type=str,
        help="JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° (íŒŒì¼ ê²½ë¡œ)"
    )
    parser.add_argument(
        "--show-tree",
        action="store_true",
        help="RunTree êµ¬ì¡°ì™€ í†µê³„ ì •ë³´ í‘œì‹œ"
    )
    
    args = parser.parse_args()
    
    try:
        analyzer = LangGraphQueryAnalyzer()
        
        if args.run_id:
            analyze_single_run(analyzer, args.run_id, show_tree=args.show_tree)
        elif args.export:
            export_json(analyzer, args.hours, args.limit, args.export)
        elif args.output == "table":
            print_table(analyzer, args.hours, args.limit)
        else:
            print_summary(analyzer, args.hours, args.limit)
    
    except ValueError as e:
        print(f"âŒ ì„¤ì • ì˜¤ë¥˜: {e}")
        print("\ní™˜ê²½ ë³€ìˆ˜ ì„¤ì •:")
        print("  export LANGSMITH_API_KEY=your-api-key")
        print("  export LANGSMITH_PROJECT=LawFirmAI")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

