# -*- coding: utf-8 -*-
"""
ë¬¸ì„œ í¬í•¨ ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸
- ê´€ë ¨ë„ ì„ê³„ê°’ í•„í„°ë§ (0.2 ë¯¸ë§Œ ì œì™¸)
- íƒ€ì…ë³„ ê· í˜• ì¡°ì •
- ë¬¸ì„œ ìˆ˜ ì¦ê°€ (8ê°œ â†’ 20ê°œ)
"""

import sys
import os
import json
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.insert(0, project_root)

from lawfirm_langgraph.core.agents.prompt_builders.unified_prompt_manager import UnifiedPromptManager
from lawfirm_langgraph.core.agents.prompt_builders.unified_prompt_manager import QuestionType

def create_test_documents() -> List[Dict[str, Any]]:
    """í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ìƒì„±"""
    documents = []
    
    # ë²•ë ¹ ì¡°ë¬¸ (3ê°œ)
    for i in range(3):
        documents.append({
            "type": "statute_article",
            "source_type": "statute_article",
            "relevance_score": 0.8 - i * 0.1,
            "content": f"ë²•ë ¹ ì¡°ë¬¸ {i+1} ë‚´ìš©",
            "law_name": f"í…ŒìŠ¤íŠ¸ë²•ë ¹{i+1}",
            "article_no": f"{i+1}",
            "document_id": f"statute_{i+1}"
        })
    
    # íŒë¡€ (10ê°œ, ê´€ë ¨ë„ ë‹¤ì–‘)
    for i in range(10):
        relevance = 0.85 - i * 0.05
        documents.append({
            "type": "case_paragraph",
            "source_type": "case_paragraph",
            "relevance_score": relevance,
            "content": f"íŒë¡€ {i+1} ë‚´ìš©",
            "source": f"ëŒ€ë²•ì› 202{i}ë‹¤12345",
            "document_id": f"case_{i+1}"
        })
    
    # ê²°ì •ë¡€ (3ê°œ)
    for i in range(3):
        documents.append({
            "type": "decision_paragraph",
            "source_type": "decision_paragraph",
            "relevance_score": 0.7 - i * 0.1,
            "content": f"ê²°ì •ë¡€ {i+1} ë‚´ìš©",
            "source": f"ê²°ì •ë¡€ {i+1}",
            "document_id": f"decision_{i+1}"
        })
    
    # í•´ì„ë¡€ (2ê°œ)
    for i in range(2):
        documents.append({
            "type": "interpretation_paragraph",
            "source_type": "interpretation_paragraph",
            "relevance_score": 0.65 - i * 0.1,
            "content": f"í•´ì„ë¡€ {i+1} ë‚´ìš©",
            "source": f"í•´ì„ë¡€ {i+1}",
            "document_id": f"interpretation_{i+1}"
        })
    
    # ê´€ë ¨ë„ê°€ ë‚®ì€ ë¬¸ì„œ (0.2 ë¯¸ë§Œ, ì œì™¸ë˜ì–´ì•¼ í•¨)
    for i in range(3):
        documents.append({
            "type": "case_paragraph",
            "source_type": "case_paragraph",
            "relevance_score": 0.15 - i * 0.02,  # 0.15, 0.13, 0.11
            "content": f"ë‚®ì€ ê´€ë ¨ë„ íŒë¡€ {i+1}",
            "source": f"ë‚®ì€ ê´€ë ¨ë„ íŒë¡€ {i+1}",
            "document_id": f"low_relevance_case_{i+1}"
        })
    
    return documents

def test_document_filtering_and_balancing():
    """ë¬¸ì„œ í•„í„°ë§ ë° ê· í˜• ì¡°ì • í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("Phase 1 í…ŒìŠ¤íŠ¸: ë¬¸ì„œ í•„í„°ë§ ë° íƒ€ì…ë³„ ê· í˜• ì¡°ì •")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒì„±
    test_documents = create_test_documents()
    print(f"\nğŸ“š í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒì„±: ì´ {len(test_documents)}ê°œ")
    
    # íƒ€ì…ë³„ ë¶„í¬ í™•ì¸
    type_distribution = {}
    for doc in test_documents:
        doc_type = doc.get("type", "unknown")
        type_distribution[doc_type] = type_distribution.get(doc_type, 0) + 1
    
    print(f"\nğŸ“Š ì›ë³¸ ë¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬:")
    for doc_type, count in type_distribution.items():
        print(f"   - {doc_type}: {count}ê°œ")
    
    # ê´€ë ¨ë„ ë¶„í¬ í™•ì¸
    relevance_scores = [doc.get("relevance_score", 0.0) for doc in test_documents]
    print(f"\nğŸ“ˆ ê´€ë ¨ë„ ë¶„í¬:")
    print(f"   - ìµœê³ : {max(relevance_scores):.3f}")
    print(f"   - ìµœì €: {min(relevance_scores):.3f}")
    print(f"   - í‰ê· : {sum(relevance_scores) / len(relevance_scores):.3f}")
    print(f"   - 0.2 ë¯¸ë§Œ: {sum(1 for s in relevance_scores if s < 0.2)}ê°œ")
    
    # UnifiedPromptManager ì´ˆê¸°í™”
    prompt_manager = UnifiedPromptManager()
    
    # _build_final_prompt ë©”ì„œë“œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ context êµ¬ì„±
    context = {
        "structured_documents": {
            "documents": test_documents,
            "total_count": len(test_documents)
        },
        "document_count": len(test_documents)
    }
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¬¸ì„œ ì„¹ì…˜ë§Œ ì¶”ì¶œ)
    try:
        base_prompt = "í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸"
        query = "ì „ì„¸ê¸ˆ ë°˜í™˜ ë³´ì¦ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        
        # _build_final_prompt í˜¸ì¶œ
        final_prompt = prompt_manager._build_final_prompt(
            base_prompt=base_prompt,
            query=query,
            context=context,
            question_type=QuestionType.TERM_EXPLANATION
        )
        
        # ë¬¸ì„œ ì„¹ì…˜ ì¶”ì¶œ
        if "## ğŸ” ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ" in final_prompt:
            print("\nâœ… ë¬¸ì„œ ì„¹ì…˜ì´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # íƒ€ì…ë³„ ë¬¸ì„œ ì„¹ì…˜ í™•ì¸
            type_sections = {
                "ë²•ë ¹ ì¡°ë¬¸": "ğŸ“œ ë²•ë ¹ ì¡°ë¬¸" in final_prompt,
                "íŒë¡€": "âš–ï¸ íŒë¡€" in final_prompt,
                "ê²°ì •ë¡€": "ğŸ“‹ ê²°ì •ë¡€" in final_prompt,
                "í•´ì„ë¡€": "ğŸ“– í•´ì„ë¡€" in final_prompt
            }
            
            print(f"\nğŸ“‹ íƒ€ì…ë³„ ë¬¸ì„œ ì„¹ì…˜ í¬í•¨ ì—¬ë¶€:")
            for section_name, included in type_sections.items():
                status = "âœ…" if included else "âŒ"
                print(f"   {status} {section_name}")
            
            # ë¬¸ì„œ ìˆ˜ í™•ì¸
            # ê° íƒ€ì…ë³„ ë¬¸ì„œ ê°œìˆ˜ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
            statute_count = final_prompt.count("**1. ë²•ë ¹ ì¡°ë¬¸") or final_prompt.count("ğŸ“œ ë²•ë ¹ ì¡°ë¬¸")
            case_count = final_prompt.count("**1. íŒë¡€") or final_prompt.count("âš–ï¸ íŒë¡€")
            decision_count = final_prompt.count("**1. ê²°ì •ë¡€") or final_prompt.count("ğŸ“‹ ê²°ì •ë¡€")
            interpretation_count = final_prompt.count("**1. í•´ì„ë¡€") or final_prompt.count("ğŸ“– í•´ì„ë¡€")
            
            # ë” ì •í™•í•œ ë°©ë²•: ë¬¸ì„œ ë²ˆí˜¸ ì¶”ì¶œ
            import re
            doc_numbers = re.findall(r'\*\*(\d+)\.', final_prompt)
            total_included = len(set(doc_numbers)) if doc_numbers else 0
            
            print(f"\nğŸ“Š í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ë¬¸ì„œ ìˆ˜:")
            print(f"   - ì´ í¬í•¨ëœ ë¬¸ì„œ: {total_included}ê°œ ì´ìƒ")
            print(f"   - ëª©í‘œ: ìµœëŒ€ 20ê°œ (ë²•ë ¹ 5ê°œ, íŒë¡€ 7ê°œ, ê²°ì •ë¡€ 4ê°œ, í•´ì„ë¡€ 4ê°œ)")
            
            # ê´€ë ¨ë„ 0.2 ë¯¸ë§Œ ë¬¸ì„œê°€ ì œì™¸ë˜ì—ˆëŠ”ì§€ í™•ì¸
            low_relevance_docs = [doc for doc in test_documents if doc.get("relevance_score", 0.0) < 0.2]
            print(f"\nğŸ” ê´€ë ¨ë„ 0.2 ë¯¸ë§Œ ë¬¸ì„œ:")
            print(f"   - ì›ë³¸: {len(low_relevance_docs)}ê°œ")
            for doc in low_relevance_docs:
                doc_id = doc.get("document_id", "unknown")
                relevance = doc.get("relevance_score", 0.0)
                if doc_id in final_prompt:
                    print(f"   âš ï¸ ê²½ê³ : {doc_id} (ê´€ë ¨ë„: {relevance:.3f})ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
                else:
                    print(f"   âœ… {doc_id} (ê´€ë ¨ë„: {relevance:.3f})ê°€ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # í”„ë¡¬í”„íŠ¸ ì¼ë¶€ ì¶œë ¥
            print(f"\nğŸ“ í”„ë¡¬í”„íŠ¸ ë¬¸ì„œ ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°:")
            doc_section_start = final_prompt.find("## ğŸ” ê²€ìƒ‰ëœ ë²•ë¥  ë¬¸ì„œ")
            if doc_section_start >= 0:
                doc_section = final_prompt[doc_section_start:doc_section_start+500]
                print(f"   {doc_section}...")
            
            return True
        else:
            print("\nâŒ ë¬¸ì„œ ì„¹ì…˜ì´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_document_filtering_and_balancing()
    sys.exit(0 if success else 1)

