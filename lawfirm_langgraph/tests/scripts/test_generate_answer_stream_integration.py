# -*- coding: utf-8 -*-
"""
generate_answer_stream í†µí•© í…ŒìŠ¤íŠ¸
- ë¬¸ì„œ í•„í„°ë§ ë° ê· í˜• ì¡°ì •ì´ ì‹¤ì œ ì›Œí¬í”Œë¡œìš°ì—ì„œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
- íƒ€ì…ë³„ ë¬¸ì„œ ì„¹ì…˜ì´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
"""

import sys
import os
import json
from typing import Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.insert(0, project_root)

def test_generate_answer_stream_workflow():
    """generate_answer_stream ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸: generate_answer_stream ì›Œí¬í”Œë¡œìš°")
    print("=" * 80)
    
    try:
        from lawfirm_langgraph.core.workflow.legal_workflow_enhanced import EnhancedLegalQuestionWorkflow
        from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        
        # ì„¤ì • ë¡œë“œ
        config = LangGraphConfig()
        
        # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        workflow = EnhancedLegalQuestionWorkflow(config)
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_query = "ì „ì„¸ê¸ˆ ë°˜í™˜ ë³´ì¦ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")
        print(f"\nğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
        
        # ì´ˆê¸° ìƒíƒœ ìƒì„±
        initial_state = {
            "query": test_query,
            "session_id": "test_session_123",
            "metadata": {}
        }
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (generate_answer_stream ë…¸ë“œë§Œ)
        try:
            # generate_answer_stream ë…¸ë“œ ì§ì ‘ í˜¸ì¶œ
            state = workflow.generate_answer_stream(initial_state)
            
            # ê²°ê³¼ í™•ì¸
            answer = state.get("answer", "")
            retrieved_docs = state.get("retrieved_docs", [])
            structured_docs = state.get("structured_documents", {})
            
            print(f"\nâœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ")
            print(f"\nğŸ“Š ê²°ê³¼:")
            print(f"   - ë‹µë³€ ê¸¸ì´: {len(answer)}ì")
            print(f"   - ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")
            
            if isinstance(structured_docs, dict):
                docs_in_structured = structured_docs.get("documents", [])
                print(f"   - structured_documents ë¬¸ì„œ ìˆ˜: {len(docs_in_structured)}ê°œ")
                
                # íƒ€ì…ë³„ ë¶„í¬ í™•ì¸
                if docs_in_structured:
                    type_distribution = {}
                    for doc in docs_in_structured:
                        doc_type = (
                            doc.get("type") or
                            doc.get("source_type") or
                            doc.get("metadata", {}).get("type") if isinstance(doc.get("metadata"), dict) else None or
                            "unknown"
                        )
                        type_distribution[doc_type] = type_distribution.get(doc_type, 0) + 1
                    
                    print(f"\nğŸ“Š structured_documents íƒ€ì…ë³„ ë¶„í¬:")
                    for doc_type, count in type_distribution.items():
                        print(f"   - {doc_type}: {count}ê°œ")
            
            # ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°
            if answer:
                # answerê°€ dictì¸ ê²½ìš° ì²˜ë¦¬
                if isinstance(answer, dict):
                    answer_text = answer.get("answer", str(answer))
                else:
                    answer_text = str(answer)
                
                print(f"\nğŸ“ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°:")
                if isinstance(answer_text, str) and len(answer_text) > 300:
                    preview = answer_text[:300]
                    print(f"   {preview}...")
                else:
                    print(f"   {answer_text}")
            
            # ê²€ì¦
            checks = []
            
            # 1. ë‹µë³€ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if isinstance(answer, dict):
                answer_text = answer.get("answer", str(answer))
            else:
                answer_text = str(answer) if answer else ""
            
            if answer_text and len(answer_text) > 100:
                checks.append(("ë‹µë³€ ìƒì„±", True))
            else:
                checks.append(("ë‹µë³€ ìƒì„±", False, f"ë‹µë³€ ê¸¸ì´: {len(answer_text)}ì"))
            
            # 2. ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
            if retrieved_docs and len(retrieved_docs) > 0:
                checks.append(("ê²€ìƒ‰ëœ ë¬¸ì„œ", True, f"{len(retrieved_docs)}ê°œ"))
            else:
                checks.append(("ê²€ìƒ‰ëœ ë¬¸ì„œ", False))
            
            # 3. structured_documentsì— ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
            if isinstance(structured_docs, dict):
                docs_in_structured = structured_docs.get("documents", [])
                if docs_in_structured and len(docs_in_structured) > 0:
                    checks.append(("structured_documents ë¬¸ì„œ", True, f"{len(docs_in_structured)}ê°œ"))
                else:
                    checks.append(("structured_documents ë¬¸ì„œ", False))
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nâœ… ê²€ì¦ ê²°ê³¼:")
            passed = 0
            failed = 0
            for check in checks:
                if len(check) == 2:
                    check_name, result = check
                    detail = ""
                else:
                    check_name, result, detail = check
                
                status = "âœ…" if result else "âŒ"
                print(f"   {status} {check_name}{f': {detail}' if detail else ''}")
                if result:
                    passed += 1
                else:
                    failed += 1
            
            print(f"\n   ì´ {len(checks)}ê°œ ê²€ì¦ ì¤‘ {passed}ê°œ í†µê³¼, {failed}ê°œ ì‹¤íŒ¨")
            
            # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            result_file = "test_generate_answer_stream_result.json"
            # answer ì²˜ë¦¬
            if isinstance(answer, dict):
                answer_text = answer.get("answer", str(answer))
            else:
                answer_text = str(answer) if answer else ""
            
            result_data = {
                "query": test_query,
                "answer_length": len(answer_text),
                "retrieved_docs_count": len(retrieved_docs),
                "structured_docs_count": len(docs_in_structured) if isinstance(structured_docs, dict) else 0,
                "answer_preview": answer_text[:500] if answer_text else "",
                "checks": [
                    {
                        "name": check[0],
                        "passed": check[1],
                        "detail": check[2] if len(check) > 2 else ""
                    }
                    for check in checks
                ]
            }
            
            with open(result_file, "w", encoding="utf-8") as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ {result_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return failed == 0
            
        except Exception as e:
            print(f"\nâŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return False
            
    except ImportError as e:
        print(f"\nâŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
        print(f"   ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_generate_answer_stream_workflow()
    sys.exit(0 if success else 1)

