# -*- coding: utf-8 -*-
"""
ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- LangGraph ì›Œí¬í”Œë¡œìš° ì „ì²´ í…ŒìŠ¤íŠ¸
- ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í™•ì¸
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
"""

import sys
import os
import asyncio
import time
from pathlib import Path
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
script_dir = Path(__file__).parent
tests_dir = script_dir.parent
lawfirm_langgraph_dir = tests_dir.parent
project_root = lawfirm_langgraph_dir.parent

if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
if str(lawfirm_langgraph_dir) not in sys.path:
    sys.path.insert(0, str(lawfirm_langgraph_dir))

import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_full_workflow(query: str):
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    logger.info("="*80)
    logger.info("ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
    logger.info("="*80)
    logger.info(f"\nğŸ“‹ ì§ˆì˜: {query}\n")
    
    start_time = time.time()
    
    try:
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        if not os.getenv('USE_EXTERNAL_VECTOR_STORE'):
            os.environ['USE_EXTERNAL_VECTOR_STORE'] = 'true'
        
        if not os.getenv('EXTERNAL_VECTOR_STORE_BASE_PATH'):
            possible_paths = [
                "data/vector_store/v2.0.0-dynamic-dynamic-ivfpq",
                "./data/vector_store/v2.0.0-dynamic-dynamic-ivfpq",
                str(project_root / "data" / "vector_store" / "v2.0.0-dynamic-dynamic-ivfpq")
            ]
            for path in possible_paths:
                if Path(path).exists():
                    os.environ['EXTERNAL_VECTOR_STORE_BASE_PATH'] = path
                    logger.info(f"âœ… IndexIVFPQ ì¸ë±ìŠ¤ ê²½ë¡œ: {path}")
                    break
        
        # LangGraph ì„¤ì • ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        from lawfirm_langgraph.core.workflow.workflow_service import LangGraphWorkflowService
        
        config = LangGraphConfig.from_env()
        config.enable_checkpoint = False
        
        logger.info("1ï¸âƒ£  LangGraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        init_start = time.time()
        service = LangGraphWorkflowService(config)
        init_time = time.time() - init_start
        logger.info(f"   âœ… ì´ˆê¸°í™” ì™„ë£Œ ({init_time:.2f}ì´ˆ)")
        
        # ê²€ìƒ‰ ì—”ì§„ ì •ë³´ í™•ì¸
        if hasattr(service, 'workflow') and hasattr(service.workflow, 'semantic_search'):
            search_engine = service.workflow.semantic_search
            if search_engine and hasattr(search_engine, 'index'):
                index_type = type(search_engine.index).__name__ if search_engine.index else 'None'
                index_size = search_engine.index.ntotal if search_engine.index else 0
                logger.info(f"   ğŸ“Š ê²€ìƒ‰ ì—”ì§„: {index_type} ({index_size:,} vectors)")
        
        # ì§ˆì˜ ì²˜ë¦¬
        logger.info("\n2ï¸âƒ£  ì§ˆì˜ ì²˜ë¦¬ ì¤‘...")
        query_start = time.time()
        
        result = await service.process_query(query)
        
        query_time = time.time() - query_start
        total_time = time.time() - start_time
        
        logger.info(f"   âœ… ì§ˆì˜ ì²˜ë¦¬ ì™„ë£Œ ({query_time:.2f}ì´ˆ)")
        
        # ê²°ê³¼ ë¶„ì„
        logger.info("\n3ï¸âƒ£  ê²°ê³¼ ë¶„ì„")
        logger.info("="*80)
        
        if result:
            # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
            retrieved_docs = result.get('retrieved_docs', [])
            logger.info(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼ í†µê³„:")
            logger.info(f"   ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(retrieved_docs)}ê°œ")
            
            if retrieved_docs:
                # ë©”íƒ€ë°ì´í„° ì™„ì „ì„± í™•ì¸
                metadata_stats = {
                    'has_doc_id': 0,
                    'has_casenames': 0,
                    'has_court': 0,
                    'has_org': 0,
                    'has_statute_name': 0,
                    'has_article_no': 0,
                    'has_title': 0,
                    'complete_metadata': 0,
                    'text_lengths': []
                }
                
                source_type_counts = {}
                
                for i, doc in enumerate(retrieved_docs[:10], 1):
                    source_type = doc.get('source_type') or doc.get('type') or doc.get('metadata', {}).get('source_type')
                    if source_type:
                        source_type_counts[source_type] = source_type_counts.get(source_type, 0) + 1
                    
                    # ë©”íƒ€ë°ì´í„° í™•ì¸
                    metadata = doc.get('metadata', {})
                    doc_id = doc.get('doc_id') or metadata.get('doc_id')
                    casenames = doc.get('casenames') or metadata.get('casenames')
                    court = doc.get('court') or metadata.get('court')
                    org = doc.get('org') or metadata.get('org')
                    statute_name = doc.get('statute_name') or metadata.get('statute_name') or doc.get('law_name')
                    article_no = doc.get('article_no') or metadata.get('article_no')
                    title = doc.get('title') or metadata.get('title')
                    
                    if doc_id:
                        metadata_stats['has_doc_id'] += 1
                    if casenames:
                        metadata_stats['has_casenames'] += 1
                    if court:
                        metadata_stats['has_court'] += 1
                    if org:
                        metadata_stats['has_org'] += 1
                    if statute_name:
                        metadata_stats['has_statute_name'] += 1
                    if article_no:
                        metadata_stats['has_article_no'] += 1
                    if title:
                        metadata_stats['has_title'] += 1
                    
                    # source_typeë³„ ì™„ì „ì„± í™•ì¸
                    is_complete = False
                    if source_type == 'case_paragraph':
                        is_complete = bool(doc_id and casenames and court)
                    elif source_type == 'decision_paragraph':
                        is_complete = bool(doc_id and org)
                    elif source_type == 'statute_article':
                        is_complete = bool(statute_name and article_no)
                    elif source_type == 'interpretation_paragraph':
                        is_complete = bool(doc_id and org and title)
                    else:
                        is_complete = bool(doc_id)
                    
                    if is_complete:
                        metadata_stats['complete_metadata'] += 1
                    
                    # í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
                    text = doc.get('text') or doc.get('content') or ''
                    if text:
                        metadata_stats['text_lengths'].append(len(text))
                
                logger.info(f"\nğŸ“‹ ë©”íƒ€ë°ì´í„° ì™„ì „ì„±:")
                logger.info(f"   doc_id: {metadata_stats['has_doc_id']}/{len(retrieved_docs)} ({metadata_stats['has_doc_id']/len(retrieved_docs)*100:.1f}%)")
                if metadata_stats['has_casenames'] > 0:
                    logger.info(f"   casenames: {metadata_stats['has_casenames']}/{len(retrieved_docs)}")
                if metadata_stats['has_court'] > 0:
                    logger.info(f"   court: {metadata_stats['has_court']}/{len(retrieved_docs)}")
                if metadata_stats['has_org'] > 0:
                    logger.info(f"   org: {metadata_stats['has_org']}/{len(retrieved_docs)}")
                if metadata_stats['has_statute_name'] > 0:
                    logger.info(f"   statute_name: {metadata_stats['has_statute_name']}/{len(retrieved_docs)}")
                if metadata_stats['has_article_no'] > 0:
                    logger.info(f"   article_no: {metadata_stats['has_article_no']}/{len(retrieved_docs)}")
                if metadata_stats['has_title'] > 0:
                    logger.info(f"   title: {metadata_stats['has_title']}/{len(retrieved_docs)}")
                logger.info(f"   ì™„ì „í•œ ë©”íƒ€ë°ì´í„°: {metadata_stats['complete_metadata']}/{len(retrieved_docs)} ({metadata_stats['complete_metadata']/len(retrieved_docs)*100:.1f}%)")
                
                logger.info(f"\nğŸ“Š source_type ë¶„í¬:")
                for stype, count in source_type_counts.items():
                    logger.info(f"   {stype}: {count}ê°œ")
                
                if metadata_stats['text_lengths']:
                    avg_length = sum(metadata_stats['text_lengths']) / len(metadata_stats['text_lengths'])
                    min_length = min(metadata_stats['text_lengths'])
                    max_length = max(metadata_stats['text_lengths'])
                    logger.info(f"\nğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´:")
                    logger.info(f"   í‰ê· : {avg_length:.0f}ì")
                    logger.info(f"   ìµœì†Œ: {min_length}ì")
                    logger.info(f"   ìµœëŒ€: {max_length}ì")
                    logger.info(f"   100ì ë¯¸ë§Œ: {sum(1 for l in metadata_stats['text_lengths'] if l < 100)}ê°œ")
            
            # ìµœì¢… ë‹µë³€ í™•ì¸
            final_answer = result.get('final_answer') or result.get('answer') or result.get('response')
            if final_answer:
                logger.info(f"\nğŸ’¬ ìµœì¢… ë‹µë³€:")
                logger.info(f"   ê¸¸ì´: {len(final_answer)}ì")
                logger.info(f"   ë¯¸ë¦¬ë³´ê¸°: {final_answer[:200]}...")
            else:
                logger.warning("   âš ï¸  ìµœì¢… ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ì„±ëŠ¥ í†µê³„
        logger.info(f"\nâ±ï¸  ì„±ëŠ¥ í†µê³„:")
        logger.info(f"   ì´ˆê¸°í™” ì‹œê°„: {init_time:.2f}ì´ˆ")
        logger.info(f"   ì§ˆì˜ ì²˜ë¦¬ ì‹œê°„: {query_time:.2f}ì´ˆ")
        logger.info(f"   ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        logger.info("\n" + "="*80)
        logger.info("í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        logger.info("="*80)
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸')
    parser.add_argument('query', nargs='?', default='ì„ëŒ€ì°¨ ë³´ì¦ê¸ˆ ë°˜í™˜', help='í…ŒìŠ¤íŠ¸í•  ì§ˆì˜')
    
    args = parser.parse_args()
    
    try:
        result = asyncio.run(test_full_workflow(args.query))
        return 0
    except KeyboardInterrupt:
        logger.warning("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 1
    except Exception as e:
        logger.error(f"\n\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())

