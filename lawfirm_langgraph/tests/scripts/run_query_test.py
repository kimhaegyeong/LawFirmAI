# -*- coding: utf-8 -*-
"""
LangGraph ì§ˆì˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „)

Usage:
    python lawfirm_langgraph/tests/scripts/run_query_test.py "ì§ˆì˜ ë‚´ìš©"
    python lawfirm_langgraph/tests/scripts/run_query_test.py 0  # ê¸°ë³¸ ì§ˆì˜ ì„ íƒ
    $env:TEST_QUERY='ì§ˆì˜ë‚´ìš©'; python run_query_test.py  # í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
"""

import sys
import io
import os

# python-dotenv ê²½ê³  ì–µì œ (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
# stderrë¥¼ ì™„ì „íˆ ë¦¬ë‹¤ì´ë ‰íŠ¸í•˜ì§€ ì•Šê³ , warningsë§Œ í•„í„°ë§
_original_stderr = sys.stderr
# stderr ë¦¬ë‹¤ì´ë ‰íŠ¸ ì œê±° - ë¡œê¹… ì˜¤ë¥˜ ë°©ì§€
# try:
#     # Windowsì™€ Unix ëª¨ë‘ ì§€ì›
#     if sys.platform == 'win32':
#         sys.stderr = open('nul', 'w', encoding='utf-8', errors='replace')
#     else:
#         sys.stderr = open('/dev/null', 'w', encoding='utf-8', errors='replace')
# except Exception:
#     # ì‹¤íŒ¨ ì‹œ ì›ë³¸ stderr ìœ ì§€
#     pass

# warnings ëª¨ë“ˆë„ í•„í„°ë§
import warnings  # noqa: E402
warnings.filterwarnings('ignore', message='.*python-dotenv.*')
warnings.filterwarnings('ignore', category=UserWarning, message='.*python-dotenv.*')
warnings.filterwarnings('ignore', category=Warning)

import asyncio  # noqa: E402
import logging  # noqa: E402
from pathlib import Path  # noqa: E402
from datetime import datetime  # noqa: E402
import cProfile  # noqa: E402
import pstats  # noqa: E402
import tracemalloc  # noqa: E402
try:
    import psutil  # noqa: E402
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# osëŠ” ì´ë¯¸ importë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¬import ë¶ˆí•„ìš”

# UTF-8 ì¸ì½”ë”© ì„¤ì • (Windows PowerShell í˜¸í™˜)
_original_stdout = sys.stdout
_original_stderr = sys.stderr

if sys.platform == 'win32':
    import io
    if hasattr(sys.stdout, 'buffer'):
        try:
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)
        except (ValueError, AttributeError):
            pass
    if hasattr(sys.stderr, 'buffer'):
        try:
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=True)
        except (ValueError, AttributeError):
            pass
    os.environ['PYTHONIOENCODING'] = 'utf-8'

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”í•˜ì—¬ generate_answer_final ì‚¬ìš©
# APIì—ì„œëŠ” USE_STREAMING_MODE=trueë¡œ ì„¤ì •í•˜ì—¬ generate_answer_stream ì‚¬ìš©
os.environ['USE_STREAMING_MODE'] = 'false'

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜: lawfirm_langgraph/tests/scripts/run_query_test.py
try:
    script_dir = Path(__file__).parent
except NameError:
    # __file__ì´ ì—†ëŠ” ê²½ìš° (ì˜ˆ: execë¡œ ì‹¤í–‰ëœ ê²½ìš°)
    script_dir = Path.cwd() / "lawfirm_langgraph" / "tests" / "scripts"
tests_dir = script_dir.parent
lawfirm_langgraph_dir = tests_dir.parent
project_root = lawfirm_langgraph_dir.parent

# sys.path ì„¤ì • (ìˆœí™˜ import ë°©ì§€)
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
if str(lawfirm_langgraph_dir) not in sys.path:
    sys.path.insert(0, str(lawfirm_langgraph_dir))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ .env íŒŒì¼ ì‚¬ìš©)
try:
    from utils.env_loader import ensure_env_loaded, load_all_env_files
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ì„ ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œ
    ensure_env_loaded(project_root)
    loaded_files = load_all_env_files(project_root)
    if loaded_files:
        print(f"âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ: {len(loaded_files)}ê°œ .env íŒŒì¼")
    else:
        print("âš ï¸  .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
except ImportError:
    # python-dotenv ì§ì ‘ ì‚¬ìš© (fallback)
    try:
        from dotenv import load_dotenv
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ .env íŒŒì¼ ë¡œë“œ
        root_env = project_root / ".env"
        if root_env.exists():
            load_dotenv(dotenv_path=str(root_env), override=False)
            print(f"âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ: {root_env}")
        else:
            print(f"âš ï¸  .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {root_env}")
    except ImportError:
        print("âš ï¸  python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜: pip install python-dotenv")
except Exception as e:
    print(f"âš ï¸  í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ìƒìˆ˜ ì •ì˜
MIN_ANSWER_LENGTH = 100
ERROR_PATTERNS = [
    "ì£„ì†¡í•©ë‹ˆë‹¤",
    "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
    "ì‹œìŠ¤í…œ ì˜¤ë¥˜",
    "ì…ë ¥ê°’ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤",
    "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
]
MAX_PROCESSING_TIME_WARNING = 300

# SafeStreamHandler í´ë˜ìŠ¤ ì •ì˜
class SafeStreamHandler(logging.StreamHandler):
    """ë²„í¼ ë¶„ë¦¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ëŠ” ì•ˆì „í•œ ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬"""
    
    def __init__(self, stream, original_stdout_ref=None):
        super().__init__(stream)
        self._original_stdout = original_stdout_ref
        self._fallback_stream = None
    
    def _get_safe_stream(self):
        """ì•ˆì „í•œ ìŠ¤íŠ¸ë¦¼ ë°˜í™˜"""
        streams_to_try = []
        if self.stream and hasattr(self.stream, 'write'):
            streams_to_try.append(self.stream)
        if self._original_stdout is not None and hasattr(self._original_stdout, 'write'):
            streams_to_try.append(self._original_stdout)
        if sys.stdout and hasattr(sys.stdout, 'write'):
            streams_to_try.append(sys.stdout)
        if sys.stderr and hasattr(sys.stderr, 'write'):
            streams_to_try.append(sys.stderr)
        
        for stream in streams_to_try:
            try:
                if hasattr(stream, 'buffer') or hasattr(stream, 'write'):
                    return stream
            except (ValueError, AttributeError, OSError):
                continue
        return None
    
    def _is_stream_valid(self, stream):
        """ìŠ¤íŠ¸ë¦¼ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        if stream is None:
            return False
        try:
            if hasattr(stream, 'buffer'):
                buffer = stream.buffer
                if buffer is None:
                    return False
                if hasattr(buffer, 'raw'):
                    raw = buffer.raw
                    if raw is None:
                        return False
            if not hasattr(stream, 'write'):
                return False
            return True
        except (ValueError, AttributeError, OSError):
            return False
    
    def emit(self, record):
        """ì•ˆì „í•œ ë¡œê·¸ ì¶œë ¥ (ë²„í¼ ë¶„ë¦¬ ì˜¤ë¥˜ ë°©ì§€)"""
        try:
            msg = self.format(record) + self.terminator
            safe_stream = self._get_safe_stream()
            if safe_stream is not None:
                try:
                    if hasattr(safe_stream, 'buffer'):
                        try:
                            buffer = safe_stream.buffer
                            if buffer is None:
                                raise ValueError("Buffer is None")
                        except (ValueError, AttributeError):
                            if hasattr(safe_stream, 'write'):
                                safe_stream.write(msg)
                                return
                            else:
                                raise ValueError("No write method")
                    else:
                        safe_stream.write(msg)
                    
                    try:
                        safe_stream.flush()
                    except (ValueError, AttributeError, OSError):
                        pass
                    return
                except (ValueError, AttributeError, OSError) as e:
                    if "detached" in str(e).lower() or "raw stream" in str(e).lower():
                        pass
                    else:
                        pass
            
            try:
                if sys.stderr and hasattr(sys.stderr, 'write'):
                    try:
                        sys.stderr.write(msg)
                        try:
                            sys.stderr.flush()
                        except (ValueError, AttributeError, OSError):
                            pass
                        return
                    except (ValueError, AttributeError, OSError) as e:
                        if "detached" in str(e).lower() or "raw stream" in str(e).lower():
                            pass
            except (ValueError, AttributeError, OSError):
                pass
        except Exception:
            pass
    
    def flush(self):
        """ì•ˆì „í•œ flush (ì˜¤ë¥˜ ë¬´ì‹œ)"""
        try:
            safe_stream = self._get_safe_stream()
            if safe_stream is not None:
                try:
                    safe_stream.flush()
                except (ValueError, AttributeError, OSError):
                    pass
        except (ValueError, AttributeError, OSError):
            pass

# ë¡œê¹… ì„¤ì • (SafeStreamHandler ì‚¬ìš©)
def setup_logging(log_level: str = "DEBUG", log_file: str = None):
    """ë¡œê¹… ì„¤ì • (Windows PowerShell í˜¸í™˜)
    
    Args:
        log_level: ë¡œê·¸ ë ˆë²¨ (INFO, DEBUG, WARNING, ERROR)
        log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
    """
    logger = logging.getLogger("lawfirm_langgraph.tests")
    logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))
    logger.handlers.clear()
    
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    if log_file is None:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸
        log_dir = os.getenv("TEST_LOG_DIR", str(project_root / "logs" / "test"))
        os.makedirs(log_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(log_dir, f"run_query_test_{timestamp}.log")
    
    # ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
    try:
        file_handler = logging.FileHandler(log_file, encoding='utf-8', mode='w')
        file_handler.setLevel(getattr(logging, log_level.upper(), logging.INFO))
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)
        logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file}")
    except Exception as e:
        logger.warning(f"âš ï¸  ë¡œê·¸ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e} (ì½˜ì†” ë¡œê·¸ë§Œ ì‚¬ìš©)")
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ ìƒì„±
    try:
        base_handler = logging.StreamHandler(_original_stdout)
    except (ValueError, AttributeError):
        try:
            base_handler = logging.StreamHandler(sys.stdout)
        except (ValueError, AttributeError):
            base_handler = logging.StreamHandler(sys.stderr)
    
    # SafeStreamHandlerë¡œ êµì²´
    safe_handler = SafeStreamHandler(base_handler.stream, _original_stdout)
    safe_handler.setLevel(getattr(logging, log_level.upper(), logging.INFO))
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    safe_handler.setFormatter(formatter)
    logger.addHandler(safe_handler)
    
    return logger

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ìë™ ìƒì„±)
log_file_path = os.getenv("TEST_LOG_FILE", None)
logger = setup_logging(
    log_level=os.getenv("TEST_LOG_LEVEL", "DEBUG"),
    log_file=log_file_path
)


def get_query_from_args() -> str:
    """ëª…ë ¹ì¤„ ì¸ìì—ì„œ ì§ˆì˜ ì¶”ì¶œ"""
    default_queries = [
        "ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜í•  ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë¯¼ë²• ì œ750ì¡° ì†í•´ë°°ìƒì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "ì„ëŒ€ì°¨ ê³„ì•½ í•´ì§€ ì‹œ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    ]
    
    test_query = os.getenv('TEST_QUERY')
    if test_query and test_query.strip():
        return test_query.strip()
    
    if len(sys.argv) > 1:
        arg = sys.argv[1].strip()
        
        if arg in ['-f', '--file']:
            if len(sys.argv) > 2:
                file_path = sys.argv[2]
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        return f.read().strip()
                except Exception as e:
                    logger.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                    return default_queries[1]
            else:
                logger.error("íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”")
                return default_queries[1]
        
        if arg.isdigit():
            idx = int(arg)
            if 0 <= idx < len(default_queries):
                return default_queries[idx]
        
        return " ".join(sys.argv[1:])
    
    return default_queries[1]


def _setup_mlflow_config():
    """MLflow ì„¤ì • ì´ˆê¸°í™”"""
    if not os.getenv('USE_MLFLOW_INDEX'):
        os.environ['USE_MLFLOW_INDEX'] = 'true'
        logger.info("   ğŸ“Œ USE_MLFLOW_INDEX=true ì„¤ì •ë¨")
    
    if not os.getenv('MLFLOW_TRACKING_URI'):
        mlflow_uri = str(project_root / "mlflow" / "mlruns")
        os.environ['MLFLOW_TRACKING_URI'] = f"file:///{mlflow_uri.replace(chr(92), '/')}"
        logger.info("   ğŸ“Œ MLFLOW_TRACKING_URI ì„¤ì •ë¨")
    
    if not os.getenv('MLFLOW_RUN_ID'):
        logger.info("   ğŸ“Œ MLFLOW_RUN_ID ë¹„ì–´ìˆìŒ - í”„ë¡œë•ì…˜ run ìë™ ì¡°íšŒ ì˜ˆì •")
    else:
        logger.info(f"   ğŸ“Œ MLFLOW_RUN_ID={os.getenv('MLFLOW_RUN_ID')} ì„¤ì •ë¨")


def _check_mlflow_index(config_obj):
    """MLflow ì¸ë±ìŠ¤ ì„¤ì • í™•ì¸"""
    if config_obj.use_mlflow_index:
        logger.info(f"   âœ… MLflow ì¸ë±ìŠ¤ ì‚¬ìš©: run_id={config_obj.mlflow_run_id or 'ìë™ ì¡°íšŒ'}")
        
        try:
            from scripts.rag.mlflow_manager import MLflowFAISSManager
            mlflow_manager = MLflowFAISSManager()
            if mlflow_manager.is_local_filesystem:
                logger.info(f"   âœ… ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ëª¨ë“œ: {mlflow_manager.local_base_path}")
                
                run_id = config_obj.mlflow_run_id or mlflow_manager.get_production_run()
                if run_id:
                    run_info = mlflow_manager.client.get_run(run_id)
                    tags = run_info.data.tags if hasattr(run_info.data, 'tags') else {}
                    version_name = tags.get('version', None)
                    
                    if version_name:
                        vector_store_path = project_root / "data" / "vector_store" / version_name
                        index_path = vector_store_path / "index.faiss"
                        if index_path.exists():
                            logger.info(f"   âœ… data/vector_store ì¸ë±ìŠ¤ ì¡´ì¬: {index_path}")
                        else:
                            logger.info(f"   â„¹ï¸  data/vector_store ì¸ë±ìŠ¤ ì—†ìŒ: {index_path}")
                        
                        artifacts_path = mlflow_manager._get_local_artifact_path(run_id, "faiss_index")
                        mlflow_index_path = artifacts_path / "index.faiss"
                        if mlflow_index_path.exists():
                            logger.info(f"   âœ… MLflow ë¡œì»¬ ê²½ë¡œ ì¸ë±ìŠ¤ ì¡´ì¬: {mlflow_index_path}")
                        else:
                            logger.info(f"   â„¹ï¸  MLflow ë¡œì»¬ ê²½ë¡œ ì¸ë±ìŠ¤ ì—†ìŒ: {mlflow_index_path}")
            else:
                logger.info(f"   ğŸŒ ì›ê²© ì„œë²„ ëª¨ë“œ: {mlflow_manager.tracking_uri}")
        except Exception as e:
            logger.debug(f"   MLflow ë§¤ë‹ˆì € í™•ì¸ ì‹¤íŒ¨: {e}")
    else:
        logger.info("   â„¹ï¸  MLflow ì¸ë±ìŠ¤ ë¯¸ì‚¬ìš© (DB ê¸°ë°˜ ì¸ë±ìŠ¤ ì‚¬ìš©)")


def _extract_and_normalize_answer(result):
    """ë‹µë³€ ì¶”ì¶œ ë° ì •ê·œí™”"""
    answer_raw = result.get("answer", "")
    
    try:
        from lawfirm_langgraph.core.workflow.utils.workflow_utils import WorkflowUtils
    except ImportError:
        try:
            from core.workflow.utils.workflow_utils import WorkflowUtils
        except ImportError:
            WorkflowUtils = None
    
    if WorkflowUtils:
        answer = WorkflowUtils.normalize_answer(answer_raw)
    else:
        if isinstance(answer_raw, dict):
            answer = answer_raw.get("content", answer_raw.get("text", str(answer_raw)))
        else:
            answer = str(answer_raw) if answer_raw else ""
        answer = answer.strip() if isinstance(answer, str) else ""
    
    return answer


def _analyze_retrieved_docs(retrieved_docs):
    """retrieved_docs ë¶„ì„ ë° í†µê³„ ìˆ˜ì§‘"""
    type_counts = {}
    statute_articles = []
    version_counts = {}
    scores = []
    
    for doc in retrieved_docs:
        if isinstance(doc, dict):
            doc_type = doc.get("type") or doc.get("source_type") or doc.get("metadata", {}).get("source_type", "unknown")
            type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
            if doc_type == "statute_article":
                statute_articles.append(doc)
            
            version_id = doc.get("embedding_version_id") or doc.get("metadata", {}).get("embedding_version_id")
            if version_id:
                version_counts[version_id] = version_counts.get(version_id, 0) + 1
            
            score = doc.get("score") or doc.get("similarity") or doc.get("relevance_score")
            if score is not None:
                scores.append(float(score))
    
    return type_counts, statute_articles, version_counts, scores


def _log_retrieved_docs(retrieved_docs):
    """retrieved_docs ë¡œê¹…"""
    if not retrieved_docs:
        logger.warning("\nâš ï¸  ê²€ìƒ‰ëœ ì°¸ê³ ìë£Œ (retrieved_docs)ê°€ ì—†ìŠµë‹ˆë‹¤!")
        logger.warning("   - ë°ì´í„°ë² ì´ìŠ¤/ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜")
        logger.warning("   - ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    
    logger.info(f"\nğŸ” ê²€ìƒ‰ëœ ì°¸ê³ ìë£Œ (retrieved_docs) ({len(retrieved_docs)}ê°œ):")
    
    type_counts, statute_articles, version_counts, scores = _analyze_retrieved_docs(retrieved_docs)
    
    logger.info(f"   íƒ€ì… ë¶„í¬: {type_counts}")
    if statute_articles:
        logger.info(f"   statute_article íƒ€ì… ë¬¸ì„œ: {len(statute_articles)}ê°œ")
    
    if version_counts:
        logger.info(f"   ğŸ“Š Embedding ë²„ì „ ë¶„í¬: {version_counts}")
    else:
        logger.warning("   âš ï¸  ê²€ìƒ‰ ê²°ê³¼ì— embedding_version_idê°€ ì—†ìŠµë‹ˆë‹¤!")
    
    if scores:
        avg_score = sum(scores) / len(scores)
        max_score = max(scores)
        min_score = min(scores)
        logger.info(f"   ğŸ“Š ìœ ì‚¬ë„ ì ìˆ˜ ë¶„í¬: í‰ê· ={avg_score:.3f}, ìµœëŒ€={max_score:.3f}, ìµœì†Œ={min_score:.3f}")
    
    for i, doc in enumerate(retrieved_docs[:10], 1):
        if isinstance(doc, dict):
            doc_id = doc.get("doc_id") or doc.get("id") or doc.get("_id") or f"doc_{i}"
            doc_type = doc.get("type") or doc.get("source_type") or doc.get("metadata", {}).get("source_type", "unknown")
            title = doc.get("title") or doc.get("name") or doc.get("content", "")[:50] or "ì œëª© ì—†ìŒ"
            search_type = doc.get("search_type") or doc.get("search_method") or "unknown"
            logger.info(f"   {i}. [{doc_type}] {title} (ID: {doc_id}, ê²€ìƒ‰ë°©ë²•: {search_type})")
            
            if doc_type == "statute_article":
                statute_name = doc.get("statute_name") or doc.get("law_name") or doc.get("metadata", {}).get("statute_name") or doc.get("metadata", {}).get("law_name")
                article_no = doc.get("article_no") or doc.get("article_number") or doc.get("metadata", {}).get("article_no") or doc.get("metadata", {}).get("article_number")
                clause_no = doc.get("clause_no") or doc.get("metadata", {}).get("clause_no")
                item_no = doc.get("item_no") or doc.get("metadata", {}).get("item_no")
                logger.info(f"      - statute_name: {statute_name}")
                logger.info(f"      - article_no: {article_no}")
                logger.info(f"      - clause_no: {clause_no}")
                logger.info(f"      - item_no: {item_no}")
            
            if doc.get("score"):
                logger.info(f"      - ì ìˆ˜: {doc.get('score'):.4f}")
            
            version_id = doc.get("embedding_version_id") or doc.get("metadata", {}).get("embedding_version_id")
            if version_id:
                logger.info(f"      - embedding_version_id: {version_id}")
            
            if doc.get("metadata") and doc_type != "statute_article":
                logger.info(f"      - ë©”íƒ€ë°ì´í„°: {doc.get('metadata')}")
        else:
            logger.info(f"   {i}. {str(doc)[:100]}")
    
    if len(retrieved_docs) > 10:
        logger.info(f"   ... (ì´ {len(retrieved_docs)}ê°œ)")


def _log_performance_metrics(service):
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š ë¶„ë¥˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ìµœì í™” ê²°ê³¼)")
    logger.info("="*80)
    
    try:
        if hasattr(service, 'workflow') and hasattr(service.workflow, 'stats'):
            stats = service.workflow.stats
            if stats:
                unified_calls = stats.get('unified_classification_calls', 0)
                unified_llm_calls = stats.get('unified_classification_llm_calls', 0)
                avg_unified_time = stats.get('avg_unified_classification_time', 0.0)
                total_unified_time = stats.get('total_unified_classification_time', 0.0)
                
                cache_hits = stats.get('complexity_cache_hits', 0)
                cache_misses = stats.get('complexity_cache_misses', 0)
                total_cache_requests = cache_hits + cache_misses
                cache_hit_rate = (cache_hits / total_cache_requests * 100) if total_cache_requests > 0 else 0
                
                fallback_count = stats.get('complexity_fallback_count', 0)
                
                logger.info("\nâœ… í†µí•© ë¶„ë¥˜ (ë‹¨ì¼ í”„ë¡¬í”„íŠ¸):")
                logger.info(f"   - ì´ í˜¸ì¶œ: {unified_calls}íšŒ")
                logger.info(f"   - LLM í˜¸ì¶œ: {unified_llm_calls}íšŒ (ëª©í‘œ: 1íšŒ/ì¿¼ë¦¬)")
                logger.info(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_unified_time:.3f}ì´ˆ")
                logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {total_unified_time:.3f}ì´ˆ")
                
                if unified_calls > 0:
                    llm_calls_per_query = unified_llm_calls / unified_calls
                    logger.info(f"   - LLM í˜¸ì¶œ/ì¿¼ë¦¬: {llm_calls_per_query:.2f}íšŒ (ëª©í‘œ: 1.0íšŒ)")
                    if llm_calls_per_query > 1.5:
                        logger.warning("   âš ï¸  LLM í˜¸ì¶œì´ ì˜ˆìƒë³´ë‹¤ ë§ìŠµë‹ˆë‹¤! (ëª©í‘œ: 1íšŒ)")
                
                logger.info("\nğŸ’¾ ìºì‹œ ì„±ëŠ¥:")
                logger.info(f"   - ìºì‹œ íˆíŠ¸: {cache_hits}íšŒ")
                logger.info(f"   - ìºì‹œ ë¯¸ìŠ¤: {cache_misses}íšŒ")
                logger.info(f"   - ìºì‹œ íˆíŠ¸ìœ¨: {cache_hit_rate:.1f}%")
                if cache_hit_rate < 50 and total_cache_requests > 5:
                    logger.warning("   âš ï¸  ìºì‹œ íˆíŠ¸ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. ìºì‹œ ì „ëµì„ ê²€í† í•˜ì„¸ìš”.")
                
                logger.info("\nğŸ”„ í´ë°± ì‚¬ìš©:")
                logger.info(f"   - í´ë°± í˜¸ì¶œ: {fallback_count}íšŒ")
                if fallback_count > 0:
                    fallback_rate = (fallback_count / unified_calls * 100) if unified_calls > 0 else 0
                    logger.info(f"   - í´ë°± ë¹„ìœ¨: {fallback_rate:.1f}%")
                    if fallback_rate > 10:
                        logger.warning("   âš ï¸  í´ë°± ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. LLM í˜¸ì¶œ ì‹¤íŒ¨ ì›ì¸ì„ í™•ì¸í•˜ì„¸ìš”.")
                    
                    fallback_reasons = stats.get('fallback_reasons', {})
                    if fallback_reasons:
                        logger.info("\n   ğŸ“‹ í´ë°± ì›ì¸ ë¶„ì„:")
                        for reason, count in sorted(fallback_reasons.items(), key=lambda x: x[1], reverse=True):
                            reason_rate = (count / fallback_count * 100) if fallback_count > 0 else 0
                            logger.info(f"      - {reason}: {count}íšŒ ({reason_rate:.1f}%)")
                            if reason in ["LLM timeout", "Network error", "Rate limit"]:
                                logger.warning(f"         âš ï¸  {reason} - ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ê³ ë ¤ í•„ìš”")
                
                if unified_calls > 0:
                    logger.info("\nğŸ“ˆ ê°œì„  íš¨ê³¼ (ì²´ì¸ ë°©ì‹ ëŒ€ë¹„):")
                    old_llm_calls = unified_calls * 4
                    new_llm_calls = unified_llm_calls
                    reduction = ((old_llm_calls - new_llm_calls) / old_llm_calls * 100) if old_llm_calls > 0 else 0
                    logger.info(f"   - ê¸°ì¡´ LLM í˜¸ì¶œ (ì˜ˆìƒ): {old_llm_calls}íšŒ")
                    logger.info(f"   - í˜„ì¬ LLM í˜¸ì¶œ: {new_llm_calls}íšŒ")
                    logger.info(f"   - LLM í˜¸ì¶œ ê°ì†Œ: {reduction:.1f}%")
                    if reduction >= 70:
                        logger.info("   âœ… ëª©í‘œ ë‹¬ì„±! (75% ê°ì†Œ ëª©í‘œ)")
                    elif reduction >= 50:
                        logger.warning("   âš ï¸  ê°œì„ ë˜ì—ˆì§€ë§Œ ëª©í‘œì— ë¯¸ë‹¬ (75% ëª©í‘œ)")
                    else:
                        logger.warning("   âš ï¸  ê°œì„  íš¨ê³¼ê°€ ë‚®ìŠµë‹ˆë‹¤. ì›ì¸ í™•ì¸ í•„ìš”")
            else:
                logger.warning("   âš ï¸  í†µê³„ê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            logger.warning("   âš ï¸  í†µê³„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.warning(f"   âš ï¸  ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    logger.info("\n" + "="*80)


def _evaluate_answer_quality(answer, answer_length, answer_is_valid, has_error_message, retrieved_docs, sources):
    """ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š ë‹µë³€ í’ˆì§ˆ ì¢…í•© í‰ê°€")
    logger.info("="*80)
    
    answer_quality_score = 0
    quality_checks = []
    
    if answer and answer_length > 0:
        answer_quality_score += 25
        quality_checks.append("âœ… ë‹µë³€ ì¡´ì¬")
    else:
        quality_checks.append("âŒ ë‹µë³€ ì—†ìŒ")
    
    if answer_is_valid:
        answer_quality_score += 25
        quality_checks.append(f"âœ… ìµœì†Œ ê¸¸ì´ ì¶©ì¡± ({answer_length}ì >= {MIN_ANSWER_LENGTH}ì)")
    else:
        quality_checks.append(f"âš ï¸  ìµœì†Œ ê¸¸ì´ ë¯¸ë‹¬ ({answer_length}ì < {MIN_ANSWER_LENGTH}ì)")
    
    if not has_error_message:
        answer_quality_score += 25
        quality_checks.append("âœ… ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ìŒ")
    else:
        quality_checks.append("âŒ ì˜¤ë¥˜ ë©”ì‹œì§€ í¬í•¨")
    
    has_sources = len(retrieved_docs) > 0 or len(sources) > 0
    if has_sources:
        answer_quality_score += 25
        quality_checks.append(f"âœ… ì°¸ê³ ìë£Œ ì¡´ì¬ ({len(retrieved_docs)}ê°œ retrieved_docs, {len(sources)}ê°œ sources)")
    else:
        quality_checks.append("âš ï¸  ì°¸ê³ ìë£Œ ì—†ìŒ")
    
    logger.info(f"\n   í’ˆì§ˆ ì ìˆ˜: {answer_quality_score}/100")
    for check in quality_checks:
        logger.info(f"   {check}")
    
    if answer_quality_score >= 100:
        quality_grade = "ğŸŸ¢ ìš°ìˆ˜"
    elif answer_quality_score >= 75:
        quality_grade = "ğŸŸ¡ ì–‘í˜¸"
    elif answer_quality_score >= 50:
        quality_grade = "ğŸŸ  ë³´í†µ"
    else:
        quality_grade = "ğŸ”´ ë¶ˆëŸ‰"
    
    logger.info(f"\n   ì¢…í•© í‰ê°€: {quality_grade}")
    
    if answer_quality_score < 75:
        logger.warning("\nâš ï¸  ë‹µë³€ í’ˆì§ˆì´ ê¸°ì¤€ ë¯¸ë§Œì…ë‹ˆë‹¤!")
        logger.warning("   ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”:")
        if not answer or answer_length == 0:
            logger.warning("   - ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        if not answer_is_valid:
            logger.warning(f"   - ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ {MIN_ANSWER_LENGTH}ì í•„ìš”)")
        if has_error_message:
            logger.warning("   - ë‹µë³€ì— ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        if not has_sources:
            logger.warning("   - ì°¸ê³ ìë£Œê°€ ì—†ì–´ ë‹µë³€ì˜ ì‹ ë¢°ì„±ì´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    
    return answer_quality_score


async def run_query_test(query: str, enable_profiling: bool = False, enable_memory_monitoring: bool = False):
    """ì§ˆì˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        query: í…ŒìŠ¤íŠ¸í•  ì§ˆì˜
        enable_profiling: í”„ë¡œíŒŒì¼ë§ í™œì„±í™” ì—¬ë¶€
        enable_memory_monitoring: ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í™œì„±í™” ì—¬ë¶€
    """
    logger.info("\n" + "="*80)
    logger.info("LangGraph ì§ˆì˜ í…ŒìŠ¤íŠ¸")
    logger.info("="*80)
    logger.info(f"\nğŸ“‹ ì§ˆì˜: {query}\n")
    
    # í”„ë¡œíŒŒì¼ë§ ì„¤ì •
    profiler = None
    profile_file = None
    if enable_profiling:
        profiler = cProfile.Profile()
        profiler.enable()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        profile_file = str(project_root / "logs" / "test" / f"profile_{timestamp}.prof")
        os.makedirs(os.path.dirname(profile_file), exist_ok=True)
        logger.info(f"ğŸ“Š í”„ë¡œíŒŒì¼ë§ í™œì„±í™”: {profile_file}")
    
    # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì„¤ì •
    memory_snapshots = []
    if enable_memory_monitoring:
        tracemalloc.start()
        if PSUTIL_AVAILABLE:
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss / 1024 / 1024
            memory_snapshots.append(("ì´ˆê¸°", initial_memory))
            logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í™œì„±í™”: ì´ˆê¸° ë©”ëª¨ë¦¬ {initial_memory:.2f} MB")
        else:
            logger.warning("ğŸ’¾ psutilì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. tracemallocë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            memory_snapshots.append(("ì´ˆê¸°", 0))
    
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì¶œë ¥ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •ëœ ê²½ìš°)
    log_file_path = os.getenv("TEST_LOG_FILE", None)
    if log_file_path:
        logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file_path}")
    else:
        # ìë™ ìƒì„±ëœ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        for handler in logger.handlers:
            if isinstance(handler, logging.FileHandler):
                logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {handler.baseFilename}")
                break
    
    try:
        # python-dotenv ê²½ê³  ì–µì œë¥¼ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['PYTHONDONTWRITEBYTECODE'] = '1'
        
        # Import (ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•´ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìˆ˜í–‰)
        # sys.pathê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ import ê°€ëŠ¥
        try:
            from lawfirm_langgraph.config.langgraph_config import LangGraphConfig
        except ImportError:
            # Fallback: ìƒëŒ€ ê²½ë¡œ
            sys.path.insert(0, str(lawfirm_langgraph_dir))
            from config.langgraph_config import LangGraphConfig
        
        try:
            from lawfirm_langgraph.core.workflow.workflow_service import LangGraphWorkflowService
        except ImportError:
            # Fallback: ìƒëŒ€ ê²½ë¡œ
            sys.path.insert(0, str(lawfirm_langgraph_dir))
            from core.workflow.workflow_service import LangGraphWorkflowService
        
        logger.info("1ï¸âƒ£  ì„¤ì • ë¡œë“œ ì¤‘...")
        _setup_mlflow_config()
        
        config = LangGraphConfig.from_env()
        config.enable_checkpoint = False
        logger.info(f"   âœ… LangGraph í™œì„±í™”: {config.langgraph_enabled}")
        logger.info(f"   âœ… ì²´í¬í¬ì¸íŠ¸: {config.enable_checkpoint}")
        
        from lawfirm_langgraph.core.utils.config import Config
        config_obj = Config()
        _check_mlflow_index(config_obj)
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        logger.info("\n2ï¸âƒ£  LangGraphWorkflowService ì´ˆê¸°í™” ì¤‘...")
        service = LangGraphWorkflowService(config)
        logger.info("   âœ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì§ˆì˜ ì²˜ë¦¬
        logger.info("\n3ï¸âƒ£  ì§ˆì˜ ì²˜ë¦¬ ì¤‘...")
        logger.info("   (ì´ ì‘ì—…ì€ ëª‡ ì´ˆì—ì„œ ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· (ì²˜ë¦¬ ì „)
        if enable_memory_monitoring:
            if PSUTIL_AVAILABLE:
                process = psutil.Process(os.getpid())
                memory_before = process.memory_info().rss / 1024 / 1024
                memory_snapshots.append(("ì²˜ë¦¬ ì „", memory_before))
            current, peak = tracemalloc.get_traced_memory()
            traced_before = current / 1024 / 1024
            if PSUTIL_AVAILABLE:
                logger.info(f"   ğŸ’¾ ë©”ëª¨ë¦¬ (ì²˜ë¦¬ ì „): {memory_before:.2f} MB (traced: {traced_before:.2f} MB)")
            else:
                logger.info(f"   ğŸ’¾ ë©”ëª¨ë¦¬ (ì²˜ë¦¬ ì „): traced: {traced_before:.2f} MB")
        
        try:
            result = await service.process_query(
                query=query,
                session_id="query_test",
                enable_checkpoint=False,
                use_astream_events=True
            )
        except asyncio.CancelledError:
            logger.warning("\nâš ï¸  ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (CancelledError)")
            if enable_profiling and profiler:
                profiler.disable()
            if enable_memory_monitoring:
                tracemalloc.stop()
            raise
        except KeyboardInterrupt:
            logger.warning("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (KeyboardInterrupt)")
            if enable_profiling and profiler:
                profiler.disable()
            if enable_memory_monitoring:
                tracemalloc.stop()
            raise
        finally:
            # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· (ì²˜ë¦¬ í›„) - í•­ìƒ ì‹¤í–‰ë˜ë„ë¡ finally ë¸”ë¡ì— ë°°ì¹˜
            if enable_memory_monitoring:
                try:
                    if PSUTIL_AVAILABLE:
                        process = psutil.Process(os.getpid())
                        memory_after = process.memory_info().rss / 1024 / 1024
                        memory_snapshots.append(("ì²˜ë¦¬ í›„", memory_after))
                    current, peak = tracemalloc.get_traced_memory()
                    traced_after = current / 1024 / 1024
                    traced_peak = peak / 1024 / 1024
                    if PSUTIL_AVAILABLE:
                        logger.info(f"   ğŸ’¾ ë©”ëª¨ë¦¬ (ì²˜ë¦¬ í›„): {memory_after:.2f} MB (traced: {traced_after:.2f} MB, peak: {traced_peak:.2f} MB)")
                    else:
                        logger.info(f"   ğŸ’¾ ë©”ëª¨ë¦¬ (ì²˜ë¦¬ í›„): traced: {traced_after:.2f} MB, peak: {traced_peak:.2f} MB")
                except Exception as e:
                    logger.warning(f"   âš ï¸  ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info("\n4ï¸âƒ£  ê²°ê³¼:")
        logger.info("="*80)
        
        answer_raw = result.get("answer", "")
        answer = _extract_and_normalize_answer(result)
        answer_length = len(answer) if isinstance(answer, str) else 0
        answer_is_valid = answer_length >= MIN_ANSWER_LENGTH
        has_error_message = any(pattern in answer for pattern in ERROR_PATTERNS) if isinstance(answer, str) else False
        
        if answer and answer_length > 0:
            quality_status = "âœ…" if answer_is_valid else "âš ï¸"
            logger.info(f"\nğŸ“ ë‹µë³€ ({answer_length}ì) {quality_status}:")
            logger.info("-" * 80)
            logger.info(str(answer))
            
            if not answer_is_valid:
                logger.warning(f"\nâš ï¸  ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤! (ìµœì†Œ {MIN_ANSWER_LENGTH}ì í•„ìš”, í˜„ì¬ {answer_length}ì)")
                logger.warning("   ê°€ëŠ¥í•œ ì›ì¸:")
                logger.warning("   1. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
                logger.warning("   2. ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
                logger.warning("   3. LLM ì‘ë‹µì´ ì œëŒ€ë¡œ ì²˜ë¦¬ë˜ì§€ ì•ŠìŒ")
            
            if has_error_message:
                logger.warning("\nâš ï¸  ë‹µë³€ì— ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
                logger.warning("   ë‹µë³€ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            logger.error("\nâŒ ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤!")
            logger.error("   ê°€ëŠ¥í•œ ì›ì¸:")
            logger.error("   1. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            logger.error("   2. ë‹µë³€ ìƒì„± ë…¸ë“œê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ")
            logger.error("   3. stateì—ì„œ answerê°€ ì†ì‹¤ë¨")
            
            errors = result.get("errors", [])
            if errors:
                logger.error(f"\n   ë°œê²¬ëœ ì˜¤ë¥˜ ({len(errors)}ê°œ):")
                for i, error in enumerate(errors[:5], 1):
                    logger.error(f"   {i}. {error}")
        
        retrieved_docs = result.get("retrieved_docs", [])
        sources = result.get("sources", [])
        _log_retrieved_docs(retrieved_docs)
        
        if sources:
            logger.info(f"\nğŸ“š ì†ŒìŠ¤ (sources) ({len(sources)}ê°œ):")
            for i, source in enumerate(sources[:10], 1):
                if isinstance(source, dict):
                    source_id = source.get("id") or source.get("doc_id") or source.get("_id") or f"source_{i}"
                    source_name = source.get("name") or source.get("title") or source.get("content", "")[:50] or "ì œëª© ì—†ìŒ"
                    logger.info(f"   {i}. {source_name} (ID: {source_id})")
                else:
                    logger.info(f"   {i}. {source}")
            if len(sources) > 10:
                logger.info(f"   ... (ì´ {len(sources)}ê°œ)")
        else:
            logger.warning("\nâš ï¸  ì†ŒìŠ¤ (sources)ê°€ ì—†ìŠµë‹ˆë‹¤!")
            if retrieved_docs:
                logger.warning(f"   - retrieved_docsëŠ” {len(retrieved_docs)}ê°œ ìˆì§€ë§Œ sourcesë¡œ ë³€í™˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.warning("   - prepare_final_response_partì—ì„œ sources ìƒì„± ê³¼ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            else:
                logger.warning("   - retrieved_docsë„ ì—†ì–´ sourcesë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # sources_detail
        sources_detail = result.get("sources_detail", [])
        if sources_detail:
            logger.info(f"\nğŸ“‹ ì†ŒìŠ¤ ìƒì„¸ (sources_detail) ({len(sources_detail)}ê°œ):")
            for i, detail in enumerate(sources_detail[:5], 1):
                if isinstance(detail, dict):
                    name = detail.get("name") or detail.get("title") or "ì œëª© ì—†ìŒ"
                    doc_id = detail.get("id") or detail.get("doc_id") or f"detail_{i}"
                    source_type = detail.get("type") or detail.get("source_type") or "unknown"
                    logger.info(f"   {i}. [{source_type}] {name} (ID: {doc_id})")
                else:
                    logger.info(f"   {i}. {detail}")
            if len(sources_detail) > 5:
                logger.info(f"   ... (ì´ {len(sources_detail)}ê°œ)")
        
        # ë²•ë¥  ì°¸ì¡°
        legal_references = result.get("legal_references", [])
        if legal_references:
            logger.info(f"\nâš–ï¸  ë²•ë¥  ì°¸ì¡° ({len(legal_references)}ê°œ):")
            for i, ref in enumerate(legal_references[:5], 1):
                logger.info(f"   {i}. {ref}")
            if len(legal_references) > 5:
                logger.info(f"   ... (ì´ {len(legal_references)}ê°œ)")
        else:
            logger.warning("\nâš ï¸  ë²•ë¥  ì°¸ì¡° (legal_references)ê°€ ì—†ìŠµë‹ˆë‹¤!")
            if retrieved_docs:
                # statute_article íƒ€ì… ë¬¸ì„œ í™•ì¸
                statute_articles = [doc for doc in retrieved_docs if isinstance(doc, dict) and (doc.get("type") == "statute_article" or doc.get("source_type") == "statute_article" or doc.get("metadata", {}).get("source_type") == "statute_article")]
                if statute_articles:
                    logger.warning(f"   - retrieved_docsì— statute_article íƒ€ì… ë¬¸ì„œê°€ {len(statute_articles)}ê°œ ìˆì§€ë§Œ legal_referencesë¡œ ë³€í™˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    logger.info("\n   statute_article ë¬¸ì„œ ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
                    for i, doc in enumerate(statute_articles[:3], 1):
                        logger.info(f"   {i}. type: {doc.get('type')}, statute_name: {doc.get('statute_name')}, law_name: {doc.get('law_name')}, article_no: {doc.get('article_no')}, metadata: {doc.get('metadata', {})}")
                else:
                    logger.warning("   - retrieved_docsì— statute_article íƒ€ì… ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    logger.info("\n   retrieved_docs íƒ€ì… ë¶„í¬:")
                    type_counts = {}
                    for doc in retrieved_docs:
                        if isinstance(doc, dict):
                            doc_type = doc.get("type") or doc.get("source_type") or doc.get("metadata", {}).get("source_type", "unknown")
                            type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
                    for doc_type, count in type_counts.items():
                        logger.info(f"      - {doc_type}: {count}ê°œ")
        
        # ê´€ë ¨ ì§ˆë¬¸ (related_questions)
        related_questions = result.get("metadata", {}).get("related_questions", [])
        if related_questions:
            logger.info(f"\nâ“ ê´€ë ¨ ì§ˆë¬¸ (related_questions) ({len(related_questions)}ê°œ):")
            for i, question in enumerate(related_questions[:5], 1):
                logger.info(f"   {i}. {question}")
            if len(related_questions) > 5:
                logger.info(f"   ... (ì´ {len(related_questions)}ê°œ)")
        else:
            logger.warning("\nâš ï¸  ê´€ë ¨ ì§ˆë¬¸ (related_questions)ê°€ ì—†ìŠµë‹ˆë‹¤!")
            logger.warning("   ê°€ëŠ¥í•œ ì›ì¸:")
            logger.warning("   1. phase_infoì— suggested_questionsê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            logger.warning("   2. conversation_flow_trackerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            logger.warning("   3. metadataì— ì €ì¥ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        metadata = result.get("metadata", {})
        if metadata:
            logger.info("\nğŸ“Š ë©”íƒ€ë°ì´í„°:")
            for key, value in list(metadata.items())[:10]:
                if key == "related_questions":
                    logger.info(f"   {key}: {value} ({len(value) if isinstance(value, list) else 'N/A'}ê°œ)")
                else:
                    logger.info(f"   {key}: {value}")
        
        # ì‹ ë¢°ë„
        confidence = result.get("confidence", 0.0)
        if confidence:
            logger.info(f"\nğŸ¯ ì‹ ë¢°ë„: {confidence:.2f}")
        
        processing_time = result.get("processing_time", 0.0)
        if processing_time:
            logger.info(f"\nâ±ï¸  ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        
        answer_quality_score = _evaluate_answer_quality(
            answer, answer_length, answer_is_valid, has_error_message, retrieved_docs, sources
        )
        
        # ë””ë²„ê¹…: retrieved_docsì™€ sources ê´€ê³„ ë¶„ì„
        logger.info("\n" + "="*80)
        logger.info("ğŸ” ë””ë²„ê¹… ì •ë³´:")
        logger.info("="*80)
        
        if retrieved_docs and not sources:
            logger.warning("âš ï¸  retrieved_docsëŠ” ìˆì§€ë§Œ sourcesê°€ ì—†ìŠµë‹ˆë‹¤!")
            logger.warning("   ê°€ëŠ¥í•œ ì›ì¸:")
            logger.warning("   1. prepare_final_response_partê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            logger.warning("   2. retrieved_docsì˜ í˜•ì‹ì´ sources ìƒì„± ë¡œì§ê³¼ ë§ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            logger.warning("   3. source_typeì´ ì—†ê±°ë‚˜ ì¸ì‹ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            logger.info("\n   retrieved_docs ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
            for i, doc in enumerate(retrieved_docs[:3], 1):
                logger.info(f"   {i}. {doc}")
        elif not retrieved_docs and not sources:
            logger.warning("âš ï¸  retrieved_docsì™€ sources ëª¨ë‘ ì—†ìŠµë‹ˆë‹¤!")
            logger.warning("   ê°€ëŠ¥í•œ ì›ì¸:")
            logger.warning("   1. ê²€ìƒ‰ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (direct_answer ë…¸ë“œ ì‚¬ìš©).")
            logger.warning("   2. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            logger.warning("   3. retrieved_docsê°€ stateì—ì„œ ì†ì‹¤ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif retrieved_docs and sources:
            logger.info(f"âœ… retrieved_docs ({len(retrieved_docs)}ê°œ) â†’ sources ({len(sources)}ê°œ) ë³€í™˜ ì„±ê³µ")
            if len(retrieved_docs) > len(sources):
                logger.warning("   âš ï¸  ì¼ë¶€ retrieved_docsê°€ sourcesë¡œ ë³€í™˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.warning(f"   ({len(retrieved_docs) - len(sources)}ê°œ ëˆ„ë½)")
        
        # legal_references ë””ë²„ê¹…
        if retrieved_docs and not legal_references:
            statute_articles = [doc for doc in retrieved_docs if isinstance(doc, dict) and (doc.get("type") == "statute_article" or doc.get("source_type") == "statute_article" or doc.get("metadata", {}).get("source_type") == "statute_article")]
            if statute_articles:
                logger.warning(f"\nâš ï¸  retrieved_docsì— statute_article íƒ€ì… ë¬¸ì„œê°€ {len(statute_articles)}ê°œ ìˆì§€ë§Œ legal_referencesë¡œ ë³€í™˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                logger.warning("   ê°€ëŠ¥í•œ ì›ì¸:")
                logger.warning("   1. prepare_final_response_partê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                logger.warning("   2. statute_nameì´ë‚˜ article_no í•„ë“œê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                logger.warning("   3. legal_references ìƒì„± ë¡œì§ì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                logger.info("\n   statute_article ë¬¸ì„œ ìƒì„¸ (ì²˜ìŒ 3ê°œ):")
                for i, doc in enumerate(statute_articles[:3], 1):
                    logger.info(f"   {i}. ì „ì²´ êµ¬ì¡°:")
                    logger.info(f"      {doc}")
        
        if not related_questions:
            logger.warning("\nâš ï¸  related_questionsê°€ ì—†ìŠµë‹ˆë‹¤!")
            logger.warning("   ê°€ëŠ¥í•œ ì›ì¸:")
            logger.warning("   1. phase_infoì— suggested_questionsê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            logger.warning("   2. conversation_flow_trackerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            logger.warning("   3. metadataì— ì €ì¥ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            # phase_info í™•ì¸
            if "phase_info" in result:
                phase_info = result.get("phase_info", {})
                logger.info("\n   phase_info í™•ì¸:")
                logger.info(f"      phase_info keys: {list(phase_info.keys()) if isinstance(phase_info, dict) else 'N/A'}")
                if isinstance(phase_info, dict) and "phase2" in phase_info:
                    phase2 = phase_info.get("phase2", {})
                    if isinstance(phase2, dict) and "flow_tracking_info" in phase2:
                        flow_tracking = phase2.get("flow_tracking_info", {})
                        if isinstance(flow_tracking, dict) and "suggested_questions" in flow_tracking:
                            suggested_questions = flow_tracking.get("suggested_questions", [])
                            logger.info(f"      suggested_questions in phase_info: {len(suggested_questions)}ê°œ")
                        else:
                            logger.warning("      suggested_questionsê°€ phase_infoì— ì—†ìŠµë‹ˆë‹¤.")
        
        # needs_search í™•ì¸
        needs_search = result.get("needs_search", True)
        logger.info(f"\n   needs_search: {needs_search}")
        if not needs_search:
            logger.info("   â†’ direct_answer ë…¸ë“œê°€ ì‚¬ìš©ë˜ì–´ ê²€ìƒ‰ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        _log_performance_metrics(service)
        
        # í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
        if enable_profiling and profiler:
            try:
                profiler.disable()
                profiler.dump_stats(profile_file)
                logger.info("\n" + "="*80)
                logger.info("ğŸ“Š í”„ë¡œíŒŒì¼ë§ ê²°ê³¼")
                logger.info("="*80)
                logger.info(f"í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥: {profile_file}")
                
                stats = pstats.Stats(profiler)
                stats.sort_stats('cumulative')
                logger.info("\nìƒìœ„ 20ê°œ í•¨ìˆ˜ (cumulative time):")
                stats.print_stats(20)
                
                logger.info("\nìƒìœ„ 20ê°œ í•¨ìˆ˜ (tottime):")
                stats.sort_stats('tottime')
                stats.print_stats(20)
            except Exception as e:
                logger.error(f"âš ï¸  í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ì¶œë ¥
        if enable_memory_monitoring:
            try:
                logger.info("\n" + "="*80)
                logger.info("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ê²°ê³¼")
                logger.info("="*80)
                
                if PSUTIL_AVAILABLE:
                    for label, memory_mb in memory_snapshots:
                        if memory_mb > 0:
                            logger.info(f"   {label}: {memory_mb:.2f} MB")
                    
                    if len(memory_snapshots) >= 2 and memory_snapshots[0][1] > 0:
                        initial_memory = memory_snapshots[0][1]
                        final_memory = memory_snapshots[-1][1]
                        memory_increase = final_memory - initial_memory
                        logger.info(f"\n   í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰: {memory_increase:.2f} MB ({memory_increase / initial_memory * 100:.1f}%)")
                
                # tracemalloc ìƒì„¸ ì •ë³´
                snapshot = tracemalloc.take_snapshot()
                top_stats = snapshot.statistics('lineno')
                
                logger.info("\n   Python ë©”ëª¨ë¦¬ í• ë‹¹ ìƒìœ„ 10ê°œ (tracemalloc):")
                total_size = 0
                for index, stat in enumerate(top_stats[:10], 1):
                    total_size += stat.size
                    logger.info(f"   {index}. {stat}")
                
                logger.info(f"\n   ì´ ì¶”ì ëœ Python ë©”ëª¨ë¦¬: {total_size / 1024 / 1024:.2f} MB")
            except Exception as e:
                logger.error(f"âš ï¸  ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ì¶œë ¥ ì‹¤íŒ¨: {e}")
            finally:
                try:
                    tracemalloc.stop()
                except Exception:
                    pass
        
        # ìµœì¢… ê²€ì¦ ë° ìš”ì•½
        test_passed = True
        critical_issues = []
        warnings = []
        
        # 1. ë‹µë³€ ì¡´ì¬ ë° í’ˆì§ˆ í™•ì¸ (ê°•í™”ëœ ê²€ì¦)
        if not answer or answer_length == 0:
            test_passed = False
            critical_issues.append("ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤ (0ì)")
            # ìƒì„¸ ë””ë²„ê¹… ì •ë³´
            logger.error("\n   ğŸ“‹ ë‹µë³€ ì—†ìŒ ìƒì„¸ ë¶„ì„:")
            logger.error(f"      - answer íƒ€ì…: {type(answer_raw).__name__}")
            logger.error(f"      - answer_raw ê°’: {repr(answer_raw[:200]) if answer_raw else 'None'}")
            logger.error(f"      - result['answer'] ì¡´ì¬: {'answer' in result}")
            logger.error(f"      - result keys: {list(result.keys())[:20]}")
            if "errors" in result:
                logger.error(f"      - errors: {result.get('errors', [])[:5]}")
            if "processing_steps" in result:
                logger.error(f"      - ë§ˆì§€ë§‰ processing_steps: {result.get('processing_steps', [])[-3:]}")
        elif not answer_is_valid:
            test_passed = False
            critical_issues.append(f"ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({answer_length}ì < {MIN_ANSWER_LENGTH}ì)")
            # ìƒì„¸ ë””ë²„ê¹… ì •ë³´
            logger.warning("\n   ğŸ“‹ ë‹µë³€ ì§§ìŒ ìƒì„¸ ë¶„ì„:")
            logger.warning(f"      - ë‹µë³€ ë‚´ìš© (ì²˜ìŒ 200ì): {answer[:200]}")
            logger.warning(f"      - ë‹µë³€ ê¸¸ì´: {answer_length}ì")
            logger.warning(f"      - ìµœì†Œ ìš”êµ¬ ê¸¸ì´: {MIN_ANSWER_LENGTH}ì")
        elif has_error_message:
            test_passed = False
            critical_issues.append("ë‹µë³€ì— ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            # ìƒì„¸ ë””ë²„ê¹… ì •ë³´
            logger.error("\n   ğŸ“‹ ì˜¤ë¥˜ ë©”ì‹œì§€ ìƒì„¸ ë¶„ì„:")
            logger.error(f"      - ë‹µë³€ ë‚´ìš©: {answer[:500]}")
            for pattern in ERROR_PATTERNS:
                if pattern in answer:
                    logger.error(f"      - ë°œê²¬ëœ íŒ¨í„´: '{pattern}'")
        
        # 2. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ í™•ì¸ (ê°•í™”ëœ ë¡œê¹…)
        errors = result.get("errors", [])
        if errors:
            test_passed = False
            critical_issues.append(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ {len(errors)}ê°œ ì˜¤ë¥˜ ë°œìƒ")
            # ìƒì„¸ ë””ë²„ê¹… ì •ë³´
            logger.error("\n   ğŸ“‹ ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜ ìƒì„¸:")
            for i, error in enumerate(errors[:10], 1):
                logger.error(f"      {i}. {error}")
            if len(errors) > 10:
                logger.error(f"      ... (ì´ {len(errors)}ê°œ ì˜¤ë¥˜, ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ)")
        
        if processing_time > MAX_PROCESSING_TIME_WARNING:
            warnings.append(f"ì²˜ë¦¬ ì‹œê°„ì´ ë§¤ìš° ê¹ë‹ˆë‹¤ ({processing_time:.2f}ì´ˆ)")
            logger.warning(f"âš ï¸  ì²˜ë¦¬ ì‹œê°„ì´ ë§¤ìš° ê¹ë‹ˆë‹¤ ({processing_time:.2f}ì´ˆ)")
        
        # 4. ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ (ë¡œê¹… ê°œì„ )
        if not retrieved_docs and not sources:
            warnings.append("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            logger.warning("\n   ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ìƒì„¸ ë¶„ì„:")
            logger.warning(f"      - needs_search: {result.get('needs_search', 'N/A')}")
            logger.warning(f"      - query_type: {result.get('query_type', 'N/A')}")
            logger.warning(f"      - complexity_level: {result.get('complexity_level', 'N/A')}")
            if "metadata" in result:
                metadata = result.get("metadata", {})
                logger.warning(f"      - metadata keys: {list(metadata.keys())[:10]}")
        
        # 5. State êµ¬ì¡° ë””ë²„ê¹… ì •ë³´ (ì˜¤ë¥˜ ë°œìƒ ì‹œ)
        if not test_passed or warnings:
            logger.info("\n   ğŸ“‹ State êµ¬ì¡° ë””ë²„ê¹… ì •ë³´:")
            logger.info(f"      - result keys: {list(result.keys())}")
            logger.info(f"      - answer ì¡´ì¬: {'answer' in result}")
            logger.info(f"      - retrieved_docs ì¡´ì¬: {'retrieved_docs' in result}")
            logger.info(f"      - sources ì¡´ì¬: {'sources' in result}")
            logger.info(f"      - errors ì¡´ì¬: {'errors' in result}")
            logger.info(f"      - metadata ì¡´ì¬: {'metadata' in result}")
            if "metadata" in result:
                metadata = result.get("metadata", {})
                logger.info(f"      - metadata keys: {list(metadata.keys())[:15]}")
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        if test_passed and not warnings:
            logger.info("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (ëª¨ë“  ê²€ì¦ í†µê³¼)")
        elif test_passed and warnings:
            logger.warning("âš ï¸  í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (ê²½ê³  ì‚¬í•­ ìˆìŒ)")
            logger.warning("\n   ê²½ê³  ì‚¬í•­:")
            for i, warning in enumerate(warnings, 1):
                logger.warning(f"   {i}. {warning}")
        else:
            logger.error("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨! (ì¤‘ìš” ë¬¸ì œ ë°œê²¬)")
            logger.error("\n   ë°œê²¬ëœ ë¬¸ì œ:")
            for i, issue in enumerate(critical_issues, 1):
                logger.error(f"   {i}. {issue}")
            if warnings:
                logger.warning("\n   ì¶”ê°€ ê²½ê³ :")
                for i, warning in enumerate(warnings, 1):
                    logger.warning(f"   {i}. {warning}")
        
        logger.info("="*80)
        
        return result
        
    except asyncio.CancelledError:
        logger.warning("\nâš ï¸  ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (CancelledError)")
        if enable_profiling and profiler:
            try:
                profiler.disable()
                if profile_file:
                    profiler.dump_stats(profile_file)
                    logger.info(f"ğŸ“Š í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥: {profile_file}")
            except Exception:
                pass
        if enable_memory_monitoring:
            try:
                tracemalloc.stop()
            except Exception:
                pass
        raise
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (KeyboardInterrupt)")
        if enable_profiling and profiler:
            try:
                profiler.disable()
                if profile_file:
                    profiler.dump_stats(profile_file)
                    logger.info(f"ğŸ“Š í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥: {profile_file}")
            except Exception:
                pass
        if enable_memory_monitoring:
            try:
                tracemalloc.stop()
            except Exception:
                pass
        raise
    except ImportError as e:
        logger.error(f"\nâŒ Import ì˜¤ë¥˜: {e}")
        logger.error("\ní•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        logger.error(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
        logger.error(f"   lawfirm_langgraph ë””ë ‰í† ë¦¬: {lawfirm_langgraph_dir}")
        import sys
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__}: {e}", exc_info=True)
        
        # í”„ë¡œíŒŒì¼ë§ ë° ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì •ë¦¬
        if enable_profiling and profiler:
            try:
                profiler.disable()
                if profile_file:
                    profiler.dump_stats(profile_file)
                    logger.info(f"ğŸ“Š í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥: {profile_file}")
            except Exception:
                pass
        if enable_memory_monitoring:
            try:
                tracemalloc.stop()
            except Exception:
                pass
        
        # ìƒì„¸ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        logger.error("\nğŸ“‹ ì˜¤ë¥˜ ìƒì„¸ ë¶„ì„:")
        logger.error(f"   - ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        logger.error(f"   - ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
        
        # State ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            if 'result' in locals():
                logger.error(f"   - result íƒ€ì…: {type(result).__name__}")
                if isinstance(result, dict):
                    logger.error(f"   - result keys: {list(result.keys())[:20]}")
                    if "answer" in result:
                        logger.error(f"   - answer ì¡´ì¬: {bool(result.get('answer'))}")
                        logger.error(f"   - answer ê¸¸ì´: {len(str(result.get('answer', '')))}")
                    if "errors" in result:
                        logger.error(f"   - errors: {result.get('errors', [])[:5]}")
        except Exception:
            pass
        
        # ì„œë¹„ìŠ¤ ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            if 'service' in locals():
                logger.error(f"   - service íƒ€ì…: {type(service).__name__}")
                if hasattr(service, 'workflow'):
                    logger.error(f"   - workflow ì¡´ì¬: {service.workflow is not None}")
        except Exception:
            pass
        
        import sys
        sys.exit(1)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í”„ë¡œíŒŒì¼ë§ ë° ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì˜µì…˜ í™•ì¸
        enable_profiling = os.getenv("ENABLE_PROFILING", "false").lower() in ("true", "1", "yes")
        enable_memory_monitoring = os.getenv("ENABLE_MEMORY_MONITORING", "false").lower() in ("true", "1", "yes")
        
        query = get_query_from_args()
        
        if not query:
            logger.error("ì§ˆì˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            logger.info("\nì‚¬ìš©ë²•:")
            logger.info("  python run_query_test.py \"ì§ˆì˜ ë‚´ìš©\"")
            logger.info("  python run_query_test.py 0  # ê¸°ë³¸ ì§ˆì˜ ì„ íƒ")
            logger.info("  $env:TEST_QUERY='ì§ˆì˜ë‚´ìš©'; python run_query_test.py")
            logger.info("\ní”„ë¡œíŒŒì¼ë§ ë° ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§:")
            logger.info("  $env:ENABLE_PROFILING='true'; python run_query_test.py \"ì§ˆì˜ ë‚´ìš©\"")
            logger.info("  $env:ENABLE_MEMORY_MONITORING='true'; python run_query_test.py \"ì§ˆì˜ ë‚´ìš©\"")
            return 1
        
        if enable_profiling:
            logger.info("ğŸ“Š í”„ë¡œíŒŒì¼ë§ ëª¨ë“œ í™œì„±í™”")
        if enable_memory_monitoring:
            logger.info("ğŸ’¾ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ëª¨ë“œ í™œì„±í™”")
        
        asyncio.run(run_query_test(query, enable_profiling, enable_memory_monitoring))
        return 0
        
    except KeyboardInterrupt:
        logger.warning("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 1
    except asyncio.CancelledError:
        logger.warning("\n\nâš ï¸  ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 1
    except Exception as e:
        logger.error(f"\n\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())

