# LawFirmAI Environment Variables
# Copy this file to .env and update the values

# ===================================
# Environment Configuration
# ===================================
# 환경 설정: local, development, production
# - local: 로컬 개발 환경 (디버그 모드, 상세 로깅)
# - development: 개발 서버 환경 (디버그 모드, 표준 로깅)
# - production: 프로덕션 환경 (디버그 비활성화, 최소 로깅)
ENVIRONMENT=development

# API Configuration
LAW_FIRM_AI_API_KEY=your-api-key-here
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=true
DEBUG_SOURCES=false
USE_RERANKING=true

# Langgarph Stream Callback
USE_STREAMING_MODE=false

# hot reload
USE_RERANKING=true

# Faiss Configuration
FAISS_AVAILABLE=true

# Model Configuration
MODEL_PATH=./models
DEVICE=cpu
MODEL_CACHE_DIR=./model_cache

# Vector Store Configuration
# CHROMA_DB_PATH=./data/chroma_db
# EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# EMBEDDING_DIMENSION=768

# Logging Configuration
LOG_LEVEL=INFO
LOG_FILE=./logs/lawfirm_ai.log
LOG_FORMAT=json

# HuggingFace Spaces Configuration
HF_SPACE_ID=your-space-id
HF_TOKEN=your-hf-token

# Performance Configuration
MAX_WORKERS=4
REQUEST_TIMEOUT=30
CACHE_TTL=3600

# Cache Configuration (for testing LLM query expansion)
# Set to "true" to disable query optimization cache (forces LLM expansion on every query)
DISABLE_QUERY_CACHE=false
# Set to "true" to disable LLM enhancement cache (forces fresh LLM calls for query expansion)
DISABLE_LLM_ENHANCEMENT_CACHE=false

# Security Configuration
SECRET_KEY=your-secret-key-here
CORS_ORIGINS=http://localhost:3000,http://localhost:7860

# Monitoring Configuration
ENABLE_METRICS=true
METRICS_PORT=9090

LANGCHAIN_TRACING_V2=true
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_API_KEY=<api_key>
LANGSMITH_PROJECT=<project_name>
OPENAI_API_KEY=<your-openai-api-key>
GOOGLE_API_KEY=<api-key>

# LLM Model Configuration
# 최종 답변 생성에 사용할 LLM 모델 (기본값: gemini-2.5-flash)
# 다른 작업(쿼리 확장, 키워드 추출 등)에는 GOOGLE_MODEL 환경 변수 사용
ANSWER_LLM_MODEL=gemini-2.5-flash

LAW_OPEN_API_OC=<id>

# MLflow 인덱스 사용
USE_MLFLOW_INDEX=true
# MLflow Tracking URI (선택사항, 기본값: ./mlflow/mlruns)
MLFLOW_TRACKING_URI=file:///D:/project/LawFirmAI/LawFirmAI/mlflow/mlruns
# MLflow Run ID (선택사항, None이면 프로덕션 run 자동 조회)
MLFLOW_RUN_ID=
# MLflow 실험 이름 (기본값: faiss_index_versions)
MLFLOW_EXPERIMENT_NAME=faiss_index_versions

# 최적 파라미터 경로 (선택사항, 기본값 사용 시 생략 가능)
# 절대 경로 사용 (Windows)
OPTIMIZED_SEARCH_PARAMS_PATH=D:/project/LawFirmAI/LawFirmAI/data/ml_config/optimized_search_params.json

# .env 파일
ENABLE_SEARCH_IMPROVEMENTS=true
USE_ADAPTIVE_WEIGHTS=true
USE_ADAPTIVE_THRESHOLD=true
USE_DIVERSITY_RANKING=true
USE_METADATA_ENHANCEMENT=true
USE_QUALITY_SCORING=true

# 성능 모니터링 임계값 (초)
SLOW_NODE_THRESHOLD=10.0          # 일반 노드 경고 임계값
SLOW_LLM_NODE_THRESHOLD=15.0      # LLM 호출 노드 경고 임계값
SLOW_SEARCH_NODE_THRESHOLD=30.0   # 검색/처리 노드 경고 임계값

# ==================================
# Cross-Encoder
# ===================================
# 재랭킹 가중치 설정 (v-Encoder 재랭킹)
# 기존 점수와 Cross-Encoder 점수의 가중 평균 계산에 사용
# 합이 1.0이 되도록 자동 정규화됨
RERANK_ORIGINAL_WEIGHT=0.6        # 기존 점수 가중치 (기본값: 0.6)
RERANK_CROSS_ENCODER_WEIGHT=0.4   # Cross-Encoder 점수 가중치 (기본값: 0.4)
# 예시: Cross-Encoder 점수를 더 중요하게 하려면
# RERANK_ORIGINAL_WEIGHT=0.5
# RERANK_CROSS_ENCODER_WEIGHT=0.5
CROSS_ENCODER_MODEL_NAME="dragonkue/bge-reranker-v2-m3-ko"

# ================================
# LLM Model Configuration
# ================================
ANSWER_LLM_MODEL=gemini-2.5-flash-lite
LONG_TEXT_ANSWER_LLM_MODEL=gemini-2.5-flash

# ================================
# Postgre
# ================================
DB_POOL_MIN_SIZE=1
DB_POOL_MAX_SIZE=20