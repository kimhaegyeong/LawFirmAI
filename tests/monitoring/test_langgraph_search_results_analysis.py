#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LangGraph ì›Œí¬í”Œë¡œìš°ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨ ë¶„ì„
ì‹¤ì œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë° ë¡œê·¸ ë¶„ì„
"""

import logging
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from core.agents.workflow_service import LangGraphWorkflowService
from infrastructure.utils.langgraph_config import LangGraphConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_langgraph_search_results_flow():
    """LangGraph ì›Œí¬í”Œë¡œìš°ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ì „ë‹¬ í™•ì¸"""
    print("\n" + "="*80)
    print("LangGraph ì›Œí¬í”Œë¡œìš° ê²€ìƒ‰ ê²°ê³¼ í¬í•¨ ë¶„ì„")
    print("="*80 + "\n")

    try:
        # ì„¤ì • ë¡œë“œ
        config = LangGraphConfig.from_env()
        workflow_service = LangGraphWorkflowService(config)

        print("âœ… ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ\n")

        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        test_query = "ì†í•´ë°°ìƒ ì²­êµ¬ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"
        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_query}\n")
        print("ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...\n")

        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = await workflow_service.process_query(test_query)

        # ê²°ê³¼ ë¶„ì„
        print("\n" + "="*80)
        print("ê²°ê³¼ ë¶„ì„")
        print("="*80 + "\n")

        answer = result.get("answer", "")
        sources = result.get("sources", [])
        confidence = result.get("confidence", 0.0)
        processing_steps = result.get("processing_steps", [])

        print(f"ğŸ“ ë‹µë³€ ê¸¸ì´: {len(answer)}ì")
        print(f"ğŸ“š ì¶œì²˜ ìˆ˜: {len(sources)}ê°œ")
        print(f"ğŸ¯ ì‹ ë¢°ë„: {confidence:.2f}")
        print(f"â±ï¸ ì²˜ë¦¬ ë‹¨ê³„: {len(processing_steps)}ê°œ")

        # ê²€ìƒ‰ ê´€ë ¨ ë‹¨ê³„ í™•ì¸
        search_steps = [step for step in processing_steps if "ê²€ìƒ‰" in step or "search" in step.lower()]
        print(f"\nğŸ” ê²€ìƒ‰ ê´€ë ¨ ë‹¨ê³„: {len(search_steps)}ê°œ")
        for step in search_steps[:10]:
            print(f"   - {step}")

        # sources í™•ì¸
        if sources:
            print(f"\nğŸ“š ì¶œì²˜ ìƒì„¸:")
            for i, source in enumerate(sources[:5], 1):
                print(f"   {i}. {source}")
        else:
            print("\nâš ï¸ ì¶œì²˜ê°€ ì—†ìŠµë‹ˆë‹¤!")

        # ë‹µë³€ì— ê²€ìƒ‰ ê²°ê³¼ ì¸ìš© í™•ì¸
        answer_lower = answer.lower()
        citation_keywords = ["ì œ", "ì¡°", "ë²•", "íŒë¡€", "ëŒ€ë²•ì›"]
        has_citations = any(kw in answer_lower for kw in citation_keywords)

        print(f"\nğŸ“– ë‹µë³€ ì¸ìš© í™•ì¸:")
        print(f"   - ë²•ë¥  ì¡°ë¬¸/íŒë¡€ í‚¤ì›Œë“œ í¬í•¨: {'âœ…' if has_citations else 'âŒ'}")

        # ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°
        print(f"\nğŸ“‹ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸° (ì²« 500ì):")
        print("-" * 80)
        print(answer[:500])
        print("-" * 80)

        return {
            "answer_length": len(answer),
            "sources_count": len(sources),
            "confidence": confidence,
            "has_citations": has_citations,
            "processing_steps_count": len(processing_steps)
        }

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    import asyncio

    result = asyncio.run(test_langgraph_search_results_flow())

    if result:
        print("\n" + "="*80)
        print("ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        print(f"ë‹µë³€ ê¸¸ì´: {result['answer_length']}ì")
        print(f"ì¶œì²˜ ìˆ˜: {result['sources_count']}ê°œ")
        print(f"ì‹ ë¢°ë„: {result['confidence']:.2f}")
        print(f"ë²•ë¥  ì¸ìš©: {'âœ…' if result['has_citations'] else 'âŒ'}")
        print(f"ì²˜ë¦¬ ë‹¨ê³„: {result['processing_steps_count']}ê°œ")
        print("="*80 + "\n")
