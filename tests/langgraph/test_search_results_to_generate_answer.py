# -*- coding: utf-8 -*-
"""
ê²€ìƒ‰ ê²°ê³¼ê°€ generate_answer_enhancedê¹Œì§€ ì˜ ì „ë‹¬ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
"""

import asyncio
import logging
import sys
import time
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì • (ì•ˆì „í•œ í•¸ë“¤ëŸ¬ ì‚¬ìš©)
class SafeStreamHandler(logging.StreamHandler):
    """ì•ˆì „í•œ ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ - detached ë²„í¼ ì—ëŸ¬ ë°©ì§€"""
    def emit(self, record):
        try:
            super().emit(record)
        except (ValueError, OSError, AttributeError):
            pass

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[SafeStreamHandler()],
    force=True
)

# ë¡œê¹… ì˜ˆì™¸ ë¹„í™œì„±í™”
logging.raiseExceptions = False

logger = logging.getLogger(__name__)


async def test_search_results_to_generate_answer():
    """ê²€ìƒ‰ ê²°ê³¼ê°€ generate_answer_enhancedê¹Œì§€ ì „ë‹¬ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
    try:
        from core.agents.workflow_service import LangGraphWorkflowService
        from infrastructure.utils.langgraph_config import LangGraphConfig

        logger.info("=" * 80)
        logger.info("ê²€ìƒ‰ ê²°ê³¼ ì „ë‹¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info("=" * 80)

        # ì„¤ì • ë¡œë“œ
        logger.info("1. LangGraph ì„¤ì • ë¡œë“œ ì¤‘...")
        config = LangGraphConfig.from_env()
        logger.info(f"   âœ… LangGraph ì„¤ì • ë¡œë“œ ì™„ë£Œ")

        # ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        logger.info("2. ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        workflow_service = LangGraphWorkflowService(config)
        logger.info(f"   âœ… ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

        # í…ŒìŠ¤íŠ¸ ì§ˆì˜ (ê²€ìƒ‰ ê²°ê³¼ê°€ í™•ì‹¤íˆ ë‚˜ì˜¬ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì§ˆì˜)
        test_queries = [
            "ê³„ì•½ í•´ì§€ ìš”ê±´",
            "ì†í•´ë°°ìƒ ì±…ì„",
            "ë¯¼ë²• ì œ543ì¡°"
        ]

        logger.info("3. ê²€ìƒ‰ ê²°ê³¼ ì „ë‹¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")

        for i, query in enumerate(test_queries, 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"í…ŒìŠ¤íŠ¸ {i}/{len(test_queries)}: {query}")
            logger.info(f"{'='*80}")

            try:
                # ì„¸ì…˜ ID ìƒì„±
                session_id = f"test_search_{int(time.time())}_{i}"

                # ì§ˆì˜ ì²˜ë¦¬ (ê²€ìƒ‰ ë‹¨ê³„ê¹Œì§€ ì‹¤í–‰)
                logger.info(f"ì§ˆì˜ ì²˜ë¦¬ ì‹œì‘: {query}")
                start_time = time.time()

                # ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰í•˜ê³  ê²€ìƒ‰ ê²°ê³¼ ì „ë‹¬ í™•ì¸
                logger.info("ğŸ”„ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
                result = await workflow_service.process_query(query, session_id, enable_checkpoint=False)

                # ê²°ê³¼ì—ì„œ ê²€ìƒ‰ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ
                retrieved_docs = result.get("retrieved_docs", [])
                metadata = result.get("metadata", {})
                search_meta = metadata.get("search", {}) if isinstance(metadata, dict) else {}

                semantic_count = search_meta.get("semantic_results_count", 0)
                keyword_count = search_meta.get("keyword_results_count", 0)
                final_count = search_meta.get("final_count", len(retrieved_docs))

                logger.info(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼:")
                logger.info(f"   âœ… ì˜ë¯¸ì  ê²€ìƒ‰: {semantic_count}ê°œ")
                logger.info(f"   âœ… í‚¤ì›Œë“œ ê²€ìƒ‰: {keyword_count}ê°œ")
                logger.info(f"   âœ… ìµœì¢… í†µí•© ê²€ìƒ‰ ê²°ê³¼: {final_count}ê°œ")

                if final_count > 0:
                    logger.info(f"   ğŸ“„ ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ:")
                    first_doc = retrieved_docs[0] if isinstance(retrieved_docs, list) and len(retrieved_docs) > 0 else {}
                    if isinstance(first_doc, dict):
                        logger.info(f"      - Type: {first_doc.get('type', 'unknown')}")
                        logger.info(f"      - Source: {str(first_doc.get('source', 'unknown'))[:50]}")
                        content = first_doc.get('content', '') or first_doc.get('text', '')
                        logger.info(f"      - Content preview: {str(content)[:100]}...")
                else:
                    logger.warning("   âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")

                # ë‹µë³€ í™•ì¸
                answer = result.get("answer", "")
                logger.info(f"\nâœï¸ generate_answer_enhanced ê²°ê³¼:")
                logger.info(f"   âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
                logger.info(f"   ğŸ“Š ë‹µë³€ì—ì„œ ë°›ì€ ê²€ìƒ‰ ê²°ê³¼: {len(retrieved_docs)}ê°œ")

                if answer:
                    answer_preview = answer[:200] if len(answer) > 200 else answer
                    logger.info(f"   ğŸ“ ìƒì„±ëœ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {answer_preview}...")

                    # ê²€ìƒ‰ ê²°ê³¼ì˜ ë‚´ìš©ì´ ë‹µë³€ì— í¬í•¨ë˜ì—ˆëŠ”ì§€ ê°„ë‹¨íˆ í™•ì¸
                    doc_mentioned = False
                    if isinstance(retrieved_docs, list) and len(retrieved_docs) > 0:
                        for doc in retrieved_docs[:3]:
                            if isinstance(doc, dict):
                                source = doc.get("source", "")
                                if source and len(str(source)) > 10 and str(source)[:20] in answer:
                                    doc_mentioned = True
                                    logger.info(f"   âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ë‹µë³€ì— í¬í•¨ë¨: {str(source)[:50]}")
                                    break

                    if not doc_mentioned and len(retrieved_docs) > 0:
                        logger.warning("   âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ë‹µë³€ì— ëª…ì‹œì ìœ¼ë¡œ í¬í•¨ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ")
                else:
                    logger.warning("   âš ï¸ ìƒì„±ëœ ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤!")

                logger.info(f"\nğŸ“Š ê²€ìƒ‰ ë©”íƒ€ë°ì´í„°:")
                logger.info(f"   - ì˜ë¯¸ì  ê²€ìƒ‰: {semantic_count}ê°œ")
                logger.info(f"   - í‚¤ì›Œë“œ ê²€ìƒ‰: {keyword_count}ê°œ")
                logger.info(f"   - ìµœì¢… ê²°ê³¼: {final_count}ê°œ")
                logger.info(f"   - ê²€ìƒ‰ ì‹œê°„: {search_meta.get('search_time', 0):.3f}ì´ˆ")

                processing_time = time.time() - start_time
                logger.info(f"\nâœ… í…ŒìŠ¤íŠ¸ {i} ì™„ë£Œ (ì´ {processing_time:.2f}ì´ˆ)")

                # ê²€ì¦
                assert final_count > 0, f"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤! (semantic: {semantic_count}, keyword: {keyword_count})"
                assert len(retrieved_docs) > 0, f"ê²€ìƒ‰ ê²°ê³¼ê°€ retrieved_docsì— ì—†ìŠµë‹ˆë‹¤!"
                assert answer is not None and len(str(answer)) > 0, "ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!"

                # ê²€ìƒ‰ ê²°ê³¼ê°€ generate_answer_enhancedê¹Œì§€ ì „ë‹¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
                assert retrieved_docs is not None, "retrieved_docsê°€ Noneì…ë‹ˆë‹¤!"
                assert isinstance(retrieved_docs, list), f"retrieved_docsê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(retrieved_docs)}"

                logger.info(f"   âœ… ëª¨ë“  ê²€ì¦ í†µê³¼!")

            except Exception as e:
                logger.error(f"í…ŒìŠ¤íŠ¸ {i} ì‹¤íŒ¨: {e}", exc_info=True)
                continue

        logger.info("\n" + "=" * 80)
        logger.info("ê²€ìƒ‰ ê²°ê³¼ ì „ë‹¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        logger.info("=" * 80)

    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    asyncio.run(test_search_results_to_generate_answer())
