# -*- coding: utf-8 -*-
"""
State Reduction ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì²˜ë¦¬ ì†ë„, ë°ì´í„° ì „ì†¡ëŸ‰ ë¹„êµ
"""

import asyncio
import logging
import sys
import time
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    import psutil
except ImportError:
    psutil = None

try:
    import pytest
except ImportError:
    pytest = None

from core.agents.node_input_output_spec import get_all_node_names
from core.agents.state_adapter import StateAdapter
from core.agents.state_reduction import (
    StateReducer,
    reduce_state_for_node,
    reduce_state_size,
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_test_state() -> dict:
    """í…ŒìŠ¤íŠ¸ìš© ëŒ€ìš©ëŸ‰ State ìƒì„±"""
    return {
        "query": "ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­ì€?",
        "session_id": "test_session_123",
        "query_type": "general_question",
        "confidence": 0.85,
        "legal_field": "civil_law",
        "legal_domain": "ì¼ë°˜",
        "urgency_level": "medium",
        "urgency_reasoning": "ê¸´ê¸‰ë„ í‰ê°€ ê²°ê³¼",
        "emergency_type": None,
        "complexity_level": "simple",
        "requires_expert": False,
        "expert_subgraph": None,
        "is_multi_turn": False,
        "multi_turn_confidence": 1.0,
        "conversation_history": [],
        "conversation_context": None,
        "search_query": "ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­",
        "extracted_keywords": ["ê³„ì•½ì„œ", "ì£¼ì˜ì‚¬í•­", "ë²•ë¥ "],
        "ai_keyword_expansion": {
            "keywords": ["ê³„ì•½ì„œ", "ì£¼ì˜ì‚¬í•­", "ë²•ë¥ "],
            "expanded": ["ê³„ì•½", "ê³„ì•½ì„œ", "ì£¼ì˜ì‚¬í•­", "ë²•ë¥ ", "ë²•ë ¹"],
            "confidence": 0.9
        },
        "retrieved_docs": [
            {"content": "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ë‚´ìš© " * 500, "source": f"doc_{i}", "metadata": {"title": f"ë¬¸ì„œ {i}"}}
            for i in range(20)
        ],
        "analysis": "ë²•ë¥  ë¶„ì„ ê²°ê³¼",
        "legal_references": ["ë¯¼ë²•", "ê³„ì•½ë²•"],
        "legal_citations": [{"law": "ë¯¼ë²•", "article": "ì œ1ì¡°"}],
        "answer": "ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ìš” ì£¼ì˜ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤...",
        "sources": ["doc_1", "doc_2"],
        "enhanced_answer": "ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ìš” ì£¼ì˜ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤...",
        "structure_confidence": 0.95,
        "document_type": None,
        "document_analysis": None,
        "key_clauses": [],
        "potential_issues": [],
        "legal_validity_check": True,
        "legal_basis_validation": {"confidence": 0.9},
        "outdated_laws": [],
        "quality_check_passed": True,
        "quality_score": 0.85,
        "retry_count": 0,
        "needs_enhancement": False,
        "processing_steps": [f"ë‹¨ê³„ {i}" for i in range(50)],
        "errors": [],
        "metadata": {"version": "1.0"},
        "processing_time": 0.0,
        "tokens_used": 1000
    }


def estimate_size(obj) -> int:
    """ê°ì²´ í¬ê¸° ì¶”ì • (bytes)"""
    return sys.getsizeof(str(obj))


class TestStateReductionPerformance:
    """State Reduction ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""

    def test_memory_usage_reduction(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ í…ŒìŠ¤íŠ¸"""
        full_state = create_test_state()
        full_size = estimate_size(full_state)

        reducer = StateReducer(aggressive_reduction=True)

        # ê° ë…¸ë“œë³„ë¡œ State Reduction ì ìš©
        nodes = [
            "classify_query",
            "assess_urgency",
            "resolve_multi_turn",
            "route_expert",
            "retrieve_documents",
            "generate_answer_enhanced"
        ]

        total_reduced_size = 0
        for node_name in nodes:
            reduced = reducer.reduce_state_for_node(full_state, node_name)
            reduced_size = estimate_size(reduced)
            reduction_pct = (1 - reduced_size / full_size) * 100 if full_size > 0 else 0

            logger.info(
                f"{node_name}: {reduction_pct:.1f}% reduction "
                f"({full_size:.0f} â†’ {reduced_size:.0f} bytes)"
            )

            total_reduced_size += reduced_size

            # ê°ì†Œìœ¨ ê²€ì¦
            assert reduction_pct > 0, f"{node_name}ì—ì„œ ê°ì†Œê°€ ë°œìƒí•˜ì§€ ì•ŠìŒ"

        # í‰ê·  ê°ì†Œìœ¨ ê³„ì‚°
        avg_reduction = sum([
            (1 - estimate_size(reducer.reduce_state_for_node(full_state, node)) / full_size) * 100
            for node in nodes
        ]) / len(nodes)

        logger.info(f"í‰ê·  State Reduction: {avg_reduction:.1f}%")
        assert avg_reduction > 50, f"í‰ê·  ê°ì†Œìœ¨ì´ 50% ë¯¸ë§Œ: {avg_reduction:.1f}%"

    def test_processing_speed(self):
        """ì²˜ë¦¬ ì†ë„ ê°œì„  í…ŒìŠ¤íŠ¸"""
        full_state = create_test_state()
        reducer = StateReducer(aggressive_reduction=True)

        nodes = get_all_node_names()

        # State Reduction ì—†ì´ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
        start = time.time()
        for node_name in nodes[:5]:
            _ = full_state  # ëª¨ì˜ ì²˜ë¦¬
        time_without_reduction = time.time() - start

        # State Reduction ì ìš© í›„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
        start = time.time()
        for node_name in nodes[:5]:
            reduced = reducer.reduce_state_for_node(full_state, node_name)
            _ = reduced  # ëª¨ì˜ ì²˜ë¦¬
        time_with_reduction = time.time() - start

        logger.info(
            f"ì²˜ë¦¬ ì‹œê°„: Reduction ì—†ì´ {time_without_reduction:.4f}s, "
            f"Reduction ì ìš© {time_with_reduction:.4f}s"
        )

    def test_state_size_reduction(self):
        """State í¬ê¸° ì œí•œ í…ŒìŠ¤íŠ¸"""
        large_state = {
            "retrieved_docs": [
                {"content": "test " * 1000} for _ in range(50)
            ],
            "conversation_history": [f"turn_{i}" for i in range(20)]
        }

        reduced = reduce_state_size(large_state, max_docs=10, max_content_per_doc=500)

        assert len(reduced["retrieved_docs"]) <= 10
        for doc in reduced["retrieved_docs"]:
            assert len(doc.get("content", "")) <= 503  # 500 + "..."

        assert len(reduced["conversation_history"]) <= 5

    def test_flat_vs_nested_conversion(self):
        """Flat â†” Nested ë³€í™˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        flat_state = create_test_state()

        start = time.time()
        nested_state = StateAdapter.to_nested(flat_state)
        to_nested_time = time.time() - start

        start = time.time()
        flat_again = StateAdapter.to_flat(nested_state)
        to_flat_time = time.time() - start

        logger.info(f"Flat â†’ Nested: {to_nested_time*1000:.2f}ms")
        logger.info(f"Nested â†’ Flat: {to_flat_time*1000:.2f}ms")

        # ì£¼ìš” í•„ë“œ ë™ì¼ì„± í™•ì¸
        assert flat_again["query"] == flat_state["query"]
        assert flat_again["query_type"] == flat_state["query_type"]


class TestWorkflowIntegration:
    """ì›Œí¬í”Œë¡œìš° í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""

    def test_full_workflow_performance(self):
        """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # ì‹œë®¬ë ˆì´ì…˜: ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        initial_state = create_test_state()
        reducer = StateReducer(aggressive_reduction=True)

        workflow_nodes = [
            "classify_query",
            "assess_urgency",
            "resolve_multi_turn",
            "route_expert",
            "expand_keywords_ai",
            "retrieve_documents",
            "process_legal_terms",
            "generate_answer_enhanced",
            "validate_answer_quality"
        ]

        start_time = time.time()
        total_memory_reduction = 0

        for node_name in workflow_nodes:
            reduced = reducer.reduce_state_for_node(initial_state, node_name)
            reduced_size = estimate_size(reduced)
            original_size = estimate_size(initial_state)
            reduction = (1 - reduced_size / original_size) * 100 if original_size > 0 else 0

            total_memory_reduction += reduction

        total_time = time.time() - start_time
        avg_reduction = total_memory_reduction / len(workflow_nodes)

        logger.info(f"ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œê°„: {total_time:.4f}s")
        logger.info(f"í‰ê·  ë©”ëª¨ë¦¬ ê°ì†Œìœ¨: {avg_reduction:.1f}%")

        assert total_time < 1.0, "ì›¨í¬í”Œë¡œìš° ì‹¤í–‰ ì‹œê°„ì´ ë„ˆë¬´ ê¹€"
        assert avg_reduction > 40, f"í‰ê·  ê°ì†Œìœ¨ì´ ë‚®ìŒ: {avg_reduction:.1f}%"


def benchmark_state_operations():
    """State ì—°ì‚° ë²¤ì¹˜ë§ˆí¬"""
    logger.info("ğŸ” State Reduction ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")

    state = create_test_state()
    reducer = StateReducer(aggressive_reduction=True)

    results = []

    for node_name in get_all_node_names():
        start = time.time()
        reduced = reducer.reduce_state_for_node(state, node_name)
        elapsed = time.time() - start

        original_size = estimate_size(state)
        reduced_size = estimate_size(reduced)
        reduction_pct = (1 - reduced_size / original_size) * 100 if original_size > 0 else 0

        results.append({
            "node": node_name,
            "time_ms": elapsed * 1000,
            "original_size": original_size,
            "reduced_size": reduced_size,
            "reduction_pct": reduction_pct
        })

    # ê²°ê³¼ ì¶œë ¥
    logger.info("=" * 80)
    logger.info(f"{'Node':<30} {'Time(ms)':<12} {'Size(bytes)':<15} {'Reduction(%)':<15}")
    logger.info("-" * 80)

    for r in sorted(results, key=lambda x: x["reduction_pct"], reverse=True):
        logger.info(
            f"{r['node']:<30} {r['time_ms']:<12.2f} "
            f"{r['original_size']}â†’{r['reduced_size']:<4} {r['reduction_pct']:<15.1f}"
        )

    # í†µê³„
    avg_time = sum(r["time_ms"] for r in results) / len(results)
    avg_reduction = sum(r["reduction_pct"] for r in results) / len(results)
    total_original = sum(r["original_size"] for r in results)
    total_reduced = sum(r["reduced_size"] for r in results)

    logger.info("-" * 80)
    logger.info(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ms")
    logger.info(f"í‰ê·  ê°ì†Œìœ¨: {avg_reduction:.1f}%")
    logger.info(f"ì „ì²´ í¬ê¸°: {total_original:,} â†’ {total_reduced:,} bytes")

    return results


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    benchmark_state_operations()
