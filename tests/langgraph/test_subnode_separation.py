# -*- coding: utf-8 -*-
"""
ì„œë¸Œë…¸ë“œ ë¶„ë¦¬ í…ŒìŠ¤íŠ¸ (expand_keywords + prepare_search_query)
"""
import asyncio
import logging
import sys
import time
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from core.agents.workflow_service import LangGraphWorkflowService
from infrastructure.utils.langgraph_config import LangGraphConfig


async def test_expand_keywords_node():
    """expand_keywords ë…¸ë“œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ 1: expand_keywords ë…¸ë“œ ë™ì‘ í™•ì¸")
    print("=" * 80)
    
    config = LangGraphConfig.from_env()
    workflow_service = LangGraphWorkflowService(config)
    
    query = "ê³„ì•½ í•´ì§€ ìš”ê±´"
    print(f"\nì§ˆë¬¸: {query}")
    
    start = time.time()
    result = await workflow_service.process_query(query, "test_session_expand_keywords")
    elapsed = time.time() - start
    
    # processing_steps í™•ì¸
    processing_steps = result.get("processing_steps", [])
    step_texts = []
    for step in processing_steps:
        if isinstance(step, dict):
            step_texts.append(step.get("step", "") or step.get("message", "") or str(step))
        elif isinstance(step, str):
            step_texts.append(step)
        else:
            step_texts.append(str(step))
    
    # expand_keywords ë…¸ë“œ ì‹¤í–‰ í™•ì¸ (ë” ë„“ì€ ë²”ìœ„ë¡œ ê²€ìƒ‰)
    has_keyword_expansion = any(
        "í‚¤ì›Œë“œ í™•ì¥" in step or 
        "í‚¤ì›Œë“œ" in step and "í™•ì¥" in step or
        "expand_keywords" in step.lower()
        for step in step_texts
    )
    
    # extracted_keywords í™•ì¸
    extracted_keywords = result.get("extracted_keywords", [])
    
    print(f"\n[ê²°ê³¼]")
    print(f"  ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"  í‚¤ì›Œë“œ í™•ì¥ ë‹¨ê³„ í¬í•¨: {has_keyword_expansion}")
    print(f"  ì¶”ì¶œëœ í‚¤ì›Œë“œ ìˆ˜: {len(extracted_keywords)}ê°œ")
    print(f"  ì²˜ë¦¬ ë‹¨ê³„ ìˆ˜: {len(processing_steps)}ê°œ")
    
    if extracted_keywords:
        print(f"  í‚¤ì›Œë“œ ì˜ˆì‹œ: {extracted_keywords[:5]}")
    
    success = has_keyword_expansion and len(extracted_keywords) > 0
    
    if success:
        print("  âœ… [PASS] expand_keywords ë…¸ë“œ ì •ìƒ ì‘ë™")
    else:
        print("  âŒ [FAIL] expand_keywords ë…¸ë“œ í™•ì¸ ì‹¤íŒ¨")
        print(f"        í‚¤ì›Œë“œ í™•ì¥ í¬í•¨: {has_keyword_expansion}, í‚¤ì›Œë“œ ìˆ˜: {len(extracted_keywords)}")
    
    return success


async def test_prepare_search_query_node():
    """prepare_search_query ë…¸ë“œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ 2: prepare_search_query ë…¸ë“œ ë™ì‘ í™•ì¸")
    print("=" * 80)
    
    config = LangGraphConfig.from_env()
    workflow_service = LangGraphWorkflowService(config)
    
    query = "ë¯¼ë²• ì œ111ì¡°ì˜ ë‚´ìš©ì„ ì•Œë ¤ì£¼ì„¸ìš”"
    print(f"\nì§ˆë¬¸: {query}")
    
    start = time.time()
    result = await workflow_service.process_query(query, "test_session_prepare_query")
    elapsed = time.time() - start
    
    # processing_steps í™•ì¸
    processing_steps = result.get("processing_steps", [])
    step_texts = []
    for step in processing_steps:
        if isinstance(step, dict):
            step_texts.append(step.get("step", "") or step.get("message", "") or str(step))
        elif isinstance(step, str):
            step_texts.append(step)
        else:
            step_texts.append(str(step))
    
    # prepare_search_query ë…¸ë“œ ì‹¤í–‰ í™•ì¸ (ë” ë„“ì€ ë²”ìœ„ë¡œ ê²€ìƒ‰)
    has_search_query_prep = any(
        "ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„" in step or 
        "ì¿¼ë¦¬ ì¤€ë¹„" in step or
        "search_query" in step.lower() or
        "ìµœì í™”ëœ ì¿¼ë¦¬" in step
        for step in step_texts
    )
    
    # optimized_queries í™•ì¸
    optimized_queries = result.get("optimized_queries", {})
    search_query = result.get("search_query", "")
    search_params = result.get("search_params", {})
    
    print(f"\n[ê²°ê³¼]")
    print(f"  ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"  ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ ë‹¨ê³„ í¬í•¨: {has_search_query_prep}")
    print(f"  ìµœì í™”ëœ ì¿¼ë¦¬ ìƒì„±: {bool(optimized_queries)}")
    print(f"  ê²€ìƒ‰ ì¿¼ë¦¬: {search_query[:50] if search_query else 'N/A'}...")
    print(f"  ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: {bool(search_params)}")
    print(f"  ì²˜ë¦¬ ë‹¨ê³„ ìˆ˜: {len(processing_steps)}ê°œ")
    
    if optimized_queries:
        semantic_query = optimized_queries.get("semantic_query", "")
        print(f"  ì˜ë¯¸ì  ì¿¼ë¦¬: {semantic_query[:50] if semantic_query else 'N/A'}...")
    
    success = has_search_query_prep and bool(optimized_queries) and bool(search_params)
    
    if success:
        print("  âœ… [PASS] prepare_search_query ë…¸ë“œ ì •ìƒ ì‘ë™")
    else:
        print("  âŒ [FAIL] prepare_search_query ë…¸ë“œ í™•ì¸ ì‹¤íŒ¨")
        print(f"        ì¿¼ë¦¬ ì¤€ë¹„ í¬í•¨: {has_search_query_prep}, "
              f"ìµœì í™”ëœ ì¿¼ë¦¬: {bool(optimized_queries)}, "
              f"ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: {bool(search_params)}")
    
    return success


async def test_unified_classification():
    """í†µí•© LLM ë¶„ë¥˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ 3: í†µí•© LLM ë¶„ë¥˜ (ì§ˆë¬¸ ìœ í˜• + ë³µì¡ë„) ë™ì‘ í™•ì¸")
    print("=" * 80)
    
    config = LangGraphConfig.from_env()
    workflow_service = LangGraphWorkflowService(config)
    
    test_cases = [
        ("ì•ˆë…•í•˜ì„¸ìš”", "simple"),
        ("ë¯¼ë²• ì œ111ì¡°", "moderate"),
        ("ê³„ì•½ í•´ì§€ì™€ í•´ì œì˜ ì°¨ì´", "complex"),
    ]
    
    passed = 0
    failed = 0
    
    for query, expected_complexity in test_cases:
        print(f"\nğŸ“ ì§ˆë¬¸: {query}")
        print(f"  ì˜ˆìƒ ë³µì¡ë„: {expected_complexity}")
        
        try:
            start = time.time()
            result = await workflow_service.process_query(query, f"test_session_{passed}")
            elapsed = time.time() - start
            
            actual_complexity = result.get("query_complexity", "unknown")
            needs_search = result.get("needs_search", True)
            query_type = result.get("query_type", "unknown")
            
            print(f"  ì‹¤ì œ ë³µì¡ë„: {actual_complexity}")
            print(f"  ì§ˆë¬¸ ìœ í˜•: {query_type}")
            print(f"  ê²€ìƒ‰ í•„ìš”: {needs_search}")
            print(f"  ì‘ë‹µ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            
            # ë³µì¡ë„ ì¼ì¹˜ í™•ì¸
            if actual_complexity == expected_complexity:
                print("  âœ… [PASS] ë³µì¡ë„ ì¼ì¹˜")
                passed += 1
            else:
                print(f"  âš ï¸  ë³µì¡ë„ ë¶ˆì¼ì¹˜ (ì˜ˆìƒ: {expected_complexity}, ì‹¤ì œ: {actual_complexity})")
                failed += 1
            
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.exception(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {query}")
            failed += 1
    
    print(f"\nğŸ“Š ê²°ê³¼: {passed}ê°œ í†µê³¼, {failed}ê°œ ì‹¤íŒ¨")
    return failed == 0


async def test_subnode_sequence():
    """ì„œë¸Œë…¸ë“œ ìˆœì°¨ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ 4: ì„œë¸Œë…¸ë“œ ìˆœì°¨ ì‹¤í–‰ í™•ì¸ (expand_keywords â†’ prepare_search_query)")
    print("=" * 80)
    
    config = LangGraphConfig.from_env()
    workflow_service = LangGraphWorkflowService(config)
    
    query = "ê³„ì•½ í•´ì§€ ìš”ê±´ê³¼ ì ˆì°¨"
    print(f"\nì§ˆë¬¸: {query}")
    
    start = time.time()
    result = await workflow_service.process_query(query, "test_session_sequence")
    elapsed = time.time() - start
    
    # processing_steps í™•ì¸
    processing_steps = result.get("processing_steps", [])
    step_texts = []
    for step in processing_steps:
        if isinstance(step, dict):
            step_texts.append(step.get("step", "") or step.get("message", "") or str(step))
        elif isinstance(step, str):
            step_texts.append(step)
        else:
            step_texts.append(str(step))
    
    # ë‹¨ê³„ ìˆœì„œ í™•ì¸ (ë” ë„“ì€ ë²”ìœ„ë¡œ ê²€ìƒ‰)
    keyword_expansion_idx = -1
    search_query_prep_idx = -1
    
    for i, step in enumerate(step_texts):
        step_lower = step.lower()
        if "í‚¤ì›Œë“œ" in step and ("í™•ì¥" in step or "expansion" in step_lower):
            keyword_expansion_idx = i
        if ("ê²€ìƒ‰ ì¿¼ë¦¬" in step or "ì¿¼ë¦¬ ì¤€ë¹„" in step or 
            "search_query" in step_lower or "ìµœì í™”ëœ ì¿¼ë¦¬" in step):
            search_query_prep_idx = i
    
    print(f"\n[ê²°ê³¼]")
    print(f"  ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"  í‚¤ì›Œë“œ í™•ì¥ ë‹¨ê³„ ì¸ë±ìŠ¤: {keyword_expansion_idx}")
    print(f"  ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ ë‹¨ê³„ ì¸ë±ìŠ¤: {search_query_prep_idx}")
    print(f"  ì´ ì²˜ë¦¬ ë‹¨ê³„ ìˆ˜: {len(processing_steps)}ê°œ")
    
    # ìˆœì„œ í™•ì¸: í‚¤ì›Œë“œ í™•ì¥ì´ ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„ë³´ë‹¤ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•¨
    correct_sequence = (
        keyword_expansion_idx >= 0 and 
        search_query_prep_idx >= 0 and 
        keyword_expansion_idx < search_query_prep_idx
    )
    
    # ê²°ê³¼ í™•ì¸
    extracted_keywords = result.get("extracted_keywords", [])
    optimized_queries = result.get("optimized_queries", {})
    search_query = result.get("search_query", "")
    
    has_keywords = len(extracted_keywords) > 0
    has_optimized = bool(optimized_queries) and bool(search_query)
    
    print(f"  í‚¤ì›Œë“œ í™•ì¥ ê²°ê³¼: {has_keywords} (í‚¤ì›Œë“œ {len(extracted_keywords)}ê°œ)")
    print(f"  ì¿¼ë¦¬ ìµœì í™” ê²°ê³¼: {has_optimized}")
    
    success = correct_sequence and has_keywords and has_optimized
    
    if success:
        print("  âœ… [PASS] ì„œë¸Œë…¸ë“œ ìˆœì°¨ ì‹¤í–‰ ì •ìƒ")
    else:
        print("  âŒ [FAIL] ì„œë¸Œë…¸ë“œ ìˆœì°¨ ì‹¤í–‰ í™•ì¸ ì‹¤íŒ¨")
        print(f"        ì˜¬ë°”ë¥¸ ìˆœì„œ: {correct_sequence}, "
              f"í‚¤ì›Œë“œ ìˆìŒ: {has_keywords}, "
              f"ìµœì í™”ë¨: {has_optimized}")
    
    return success


async def test_end_to_end_workflow():
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì—”ë“œ-íˆ¬-ì—”ë“œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ 5: ì „ì²´ ì›Œí¬í”Œë¡œìš° ì—”ë“œ-íˆ¬-ì—”ë“œ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    config = LangGraphConfig.from_env()
    workflow_service = LangGraphWorkflowService(config)
    
    test_queries = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ë¯¼ë²• ì œ111ì¡°ì˜ ë‚´ìš©ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ê³„ì•½ í•´ì§€ì™€ í•´ì œì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    ]
    
    passed = 0
    failed = 0
    
    for query in test_queries:
        print(f"\nğŸ“ ì§ˆë¬¸: {query}")
        
        try:
            start = time.time()
            result = await workflow_service.process_query(query, f"test_session_e2e_{passed}")
            elapsed = time.time() - start
            
            answer = result.get("answer", "")
            query_complexity = result.get("query_complexity", "unknown")
            needs_search = result.get("needs_search", True)
            extracted_keywords = result.get("extracted_keywords", [])
            optimized_queries = result.get("optimized_queries", {})
            
            print(f"  â±ï¸  ì‘ë‹µ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            print(f"  ğŸ“Š ë³µì¡ë„: {query_complexity}")
            print(f"  ğŸ” ê²€ìƒ‰ í•„ìš”: {needs_search}")
            print(f"  ğŸ“ ë‹µë³€ ê¸¸ì´: {len(answer)}ì")
            print(f"  ğŸ”‘ í‚¤ì›Œë“œ ìˆ˜: {len(extracted_keywords)}ê°œ")
            print(f"  ğŸ” ìµœì í™”ëœ ì¿¼ë¦¬: {bool(optimized_queries)}")
            
            # ê¸°ë³¸ ê²€ì¦
            has_answer = len(answer) > 0
            has_complexity = query_complexity != "unknown"
            
            if has_answer and has_complexity:
                print("  âœ… [PASS] ì „ì²´ ì›Œí¬í”Œë¡œìš° ì •ìƒ ì‘ë™")
                passed += 1
            else:
                print("  âŒ [FAIL] ì „ì²´ ì›Œí¬í”Œë¡œìš° ê²€ì¦ ì‹¤íŒ¨")
                failed += 1
                
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.exception(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {query}")
            failed += 1
    
    print(f"\nğŸ“Š ê²°ê³¼: {passed}ê°œ í†µê³¼, {failed}ê°œ ì‹¤íŒ¨")
    return failed == 0


async def main():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("=" * 80)
    print("ì„œë¸Œë…¸ë“œ ë¶„ë¦¬ ë° í†µí•© LLM ë¶„ë¥˜ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    results = {}
    
    # í…ŒìŠ¤íŠ¸ 1: expand_keywords ë…¸ë“œ
    try:
        results["expand_keywords"] = await test_expand_keywords_node()
    except Exception as e:
        print(f"âŒ expand_keywords í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        logger.exception("expand_keywords í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        results["expand_keywords"] = False
    
    # í…ŒìŠ¤íŠ¸ 2: prepare_search_query ë…¸ë“œ
    try:
        results["prepare_search_query"] = await test_prepare_search_query_node()
    except Exception as e:
        print(f"âŒ prepare_search_query í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        logger.exception("prepare_search_query í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        results["prepare_search_query"] = False
    
    # í…ŒìŠ¤íŠ¸ 3: í†µí•© LLM ë¶„ë¥˜
    try:
        results["unified_classification"] = await test_unified_classification()
    except Exception as e:
        print(f"âŒ í†µí•© LLM ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        logger.exception("í†µí•© LLM ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        results["unified_classification"] = False
    
    # í…ŒìŠ¤íŠ¸ 4: ì„œë¸Œë…¸ë“œ ìˆœì°¨ ì‹¤í–‰
    try:
        results["subnode_sequence"] = await test_subnode_sequence()
    except Exception as e:
        print(f"âŒ ì„œë¸Œë…¸ë“œ ìˆœì°¨ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        logger.exception("ì„œë¸Œë…¸ë“œ ìˆœì°¨ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        results["subnode_sequence"] = False
    
    # í…ŒìŠ¤íŠ¸ 5: ì—”ë“œ-íˆ¬-ì—”ë“œ
    try:
        results["end_to_end"] = await test_end_to_end_workflow()
    except Exception as e:
        print(f"âŒ ì—”ë“œ-íˆ¬-ì—”ë“œ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        logger.exception("ì—”ë“œ-íˆ¬-ì—”ë“œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        results["end_to_end"] = False
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    
    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {test_name}: {status}")
    
    total_passed = sum(1 for v in results.values() if v)
    total_tests = len(results)
    
    print(f"\nğŸ“Š ì´ê³„: {total_passed}/{total_tests} í…ŒìŠ¤íŠ¸ í†µê³¼")
    
    if total_passed == total_tests:
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return 0
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

