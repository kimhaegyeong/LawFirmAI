# -*- coding: utf-8 -*-
"""
LangGraph ë™ì‘ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (íŒŒì¼ ë¡œê¹… í¬í•¨)
ë¦¬íŒ©í† ë§ í›„ LangGraphê°€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìƒì„¸ ë¡œê·¸ ì €ì¥
"""

import asyncio
import logging
import sys
import time
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

# íŒŒì¼ ë¡œê±° ì„¤ì •
log_file = log_dir / f"test_langgraph_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setLevel(logging.DEBUG)
file_formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
file_handler.setFormatter(file_formatter)

# ì½˜ì†” í•¸ë“¤ëŸ¬ ì„¤ì •
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter('%(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)

# ë£¨íŠ¸ ë¡œê±° ì„¤ì •
root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)
root_logger.addHandler(file_handler)
root_logger.addHandler(console_handler)

logger = logging.getLogger(__name__)

# Windows ë¹„ë™ê¸° í™˜ê²½ì—ì„œ ë¡œê¹… ë²„í¼ ì—ëŸ¬ ë°©ì§€
class SafeStreamHandler(logging.StreamHandler):
    """ì•ˆì „í•œ ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ - detached ë²„í¼ ì—ëŸ¬ ë°©ì§€"""
    def emit(self, record):
        try:
            super().emit(record)
        except (ValueError, OSError, AttributeError):
            pass

logging.raiseExceptions = False


async def test_langgraph_workflow_with_logging():
    """LangGraph ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ (ìƒì„¸ ë¡œê¹… í¬í•¨)"""
    try:
        # Import ê²½ë¡œ í™•ì¸ ë° ì¡°ì •
        # core/agents/workflow_service.pyë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
        from core.agents.workflow_service import LangGraphWorkflowService
        from infrastructure.utils.langgraph_config import LangGraphConfig

        logger.info("=" * 80)
        logger.info("LangGraph ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘ (íŒŒì¼ ë¡œê¹… í¬í•¨)")
        logger.info("=" * 80)
        logger.info(f"ë¡œê·¸ íŒŒì¼: {log_file}")

        # ì„¤ì • ë¡œë“œ
        logger.info("1. LangGraph ì„¤ì • ë¡œë“œ ì¤‘...")
        config = LangGraphConfig.from_env()
        logger.info(f"   âœ… LangGraph ì„¤ì • ë¡œë“œ ì™„ë£Œ (enabled={config.langgraph_enabled})")

        # ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (íŒŒì¼ ë¡œê¹… í™œì„±í™”)
        logger.info("2. ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        start_time = time.time()
        workflow_service = LangGraphWorkflowService(config, enable_file_logging=True)
        init_time = time.time() - start_time
        logger.info(f"   âœ… ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ ({init_time:.2f}ì´ˆ)")

        # í…ŒìŠ¤íŠ¸ ì§ˆì˜ (ë¯¼ì‚¬ë²• ê´€ë ¨, 1ê°œ)
        test_queries = [
            "ë¯¼ì‚¬ë²•ì—ì„œ ê³„ì•½ í•´ì§€ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]

        logger.info("3. í…ŒìŠ¤íŠ¸ ì§ˆì˜ ì‹¤í–‰ ì¤‘...")

        results = []
        for i, query in enumerate(test_queries, 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"í…ŒìŠ¤íŠ¸ ì§ˆì˜ {i}/{len(test_queries)}: {query}")
            logger.info(f"{'='*80}")

            try:
                # ì„¸ì…˜ ID ìƒì„±
                session_id = f"test_session_{int(time.time())}_{i}"

                # ì§ˆì˜ ì²˜ë¦¬
                start_time = time.time()
                result = await workflow_service.process_query(query, session_id, enable_checkpoint=False)
                processing_time = time.time() - start_time

                # ê²°ê³¼ ê²€ì¦
                answer = result.get("answer", "") if isinstance(result, dict) else ""

                # ì¤‘ì²© ë”•ì…”ë„ˆë¦¬ì—ì„œ ë¬¸ìì—´ ì¶”ì¶œ
                if isinstance(answer, dict):
                    depth = 0
                    max_depth = 20
                    while isinstance(answer, dict) and depth < max_depth:
                        if "answer" in answer:
                            answer = answer["answer"]
                        elif "content" in answer:
                            answer = answer["content"]
                        elif "text" in answer:
                            answer = answer["text"]
                        else:
                            answer = str(answer)
                            break
                        depth += 1
                    if isinstance(answer, dict):
                        answer = str(answer)

                answer = str(answer) if not isinstance(answer, str) else answer
                has_answer = bool(answer) and len(answer) > 0

                # Sources í™•ì¸
                sources = result.get("sources", []) if isinstance(result, dict) else []
                retrieved_docs = result.get("retrieved_docs", []) if isinstance(result, dict) else []

                has_sources = len(sources) > 0
                sources_count = len(sources)
                retrieved_docs_count = len(retrieved_docs)

                confidence = result.get("confidence", 0.0) if isinstance(result, dict) else 0.0
                errors = result.get("errors", []) if isinstance(result, dict) else []
                has_errors = len(errors) > 0 if isinstance(errors, list) else False

                # ì„±ê³µ ì—¬ë¶€ íŒì •
                is_success = has_answer and not has_errors
                result_status = "âœ… ì„±ê³µ" if is_success else "âŒ ì‹¤íŒ¨"

                # ê²°ê³¼ ì¶œë ¥
                logger.info(f"\n{result_status} ë‹µë³€ ìƒì„± ì™„ë£Œ (ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ)")
                logger.info(f"   - ë‹µë³€ ìœ ë¬´: {'ìˆìŒ' if has_answer else 'ì—†ìŒ'}")
                answer_length = len(answer) if isinstance(answer, str) else 0
                logger.info(f"   - ë‹µë³€ ê¸¸ì´: {answer_length}ì")
                logger.info(f"   - ì†ŒìŠ¤ ìœ ë¬´: {'ìˆìŒ' if has_sources else 'ì—†ìŒ'} ({sources_count}ê°œ)")
                logger.info(f"   - ê²€ìƒ‰ëœ ë¬¸ì„œ: {retrieved_docs_count}ê°œ")
                logger.info(f"   - ì‹ ë¢°ë„: {confidence:.2%}")
                logger.info(f"   - ì—ëŸ¬ ìœ ë¬´: {'ìˆìŒ' if has_errors else 'ì—†ìŒ'}")

                # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ í‘œì‹œ
                log_file_path = result.get("log_file", "")
                if log_file_path:
                    logger.info(f"   - ìƒì„¸ ë¡œê·¸: {log_file_path}")

                # stdoutì—ë„ ì¶œë ¥
                print(f"\n{result_status} ì§ˆì˜ {i}/{len(test_queries)}: {query} (ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ)", flush=True)
                print(f"   - ë‹µë³€ ìœ ë¬´: {'ìˆìŒ' if has_answer else 'ì—†ìŒ'}", flush=True)
                print(f"   - ë‹µë³€ ê¸¸ì´: {answer_length}ì", flush=True)
                print(f"   - ì†ŒìŠ¤ ìœ ë¬´: {'ìˆìŒ' if has_sources else 'ì—†ìŒ'} ({sources_count}ê°œ)", flush=True)
                print(f"   - ê²€ìƒ‰ëœ ë¬¸ì„œ: {retrieved_docs_count}ê°œ", flush=True)
                print(f"   - ì‹ ë¢°ë„: {confidence:.2%}", flush=True)
                print(f"   - ì—ëŸ¬ ìœ ë¬´: {'ìˆìŒ' if has_errors else 'ì—†ìŒ'}", flush=True)
                if log_file_path:
                    print(f"   - ìƒì„¸ ë¡œê·¸: {log_file_path}", flush=True)

                if has_answer:
                    logger.info(f"\nğŸ“ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°:")
                    if isinstance(answer, str):
                        answer_preview = answer[:200]
                        logger.info(f"   {answer_preview}{'...' if len(answer) > 200 else ''}")

                if has_errors:
                    logger.warning(f"\nâš ï¸ ì—ëŸ¬ ëª©ë¡:")
                    error_list = errors if isinstance(errors, list) else []
                    for error in error_list[:5]:
                        logger.warning(f"   - {error}")

                # ê²°ê³¼ ì €ì¥
                test_result = {
                    "query": query,
                    "success": is_success,
                    "processing_time": processing_time,
                    "confidence": confidence,
                    "answer_length": answer_length,
                    "has_answer": has_answer,
                    "has_sources": has_sources,
                    "sources_count": sources_count,
                    "retrieved_docs_count": retrieved_docs_count,
                    "has_errors": has_errors,
                    "errors": errors if isinstance(errors, list) else [],
                    "log_file": log_file_path
                }
                results.append(test_result)

            except Exception as e:
                import traceback
                error_traceback = traceback.format_exc()

                logger.error(f"\nâŒ í…ŒìŠ¤íŠ¸ ì§ˆì˜ ì‹¤íŒ¨: {query}")
                logger.error(f"ì˜¤ë¥˜ ìœ í˜•: {type(e).__name__}")
                logger.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
                logger.error(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{error_traceback}")

                results.append({
                    "query": query,
                    "success": False,
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "traceback": error_traceback
                })

        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        logger.info(f"\n{'='*80}")
        logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        logger.info(f"{'='*80}")

        total_queries = len(results)
        successful_queries = sum(1 for r in results if r.get("success", False))
        failed_queries = total_queries - successful_queries
        avg_time = sum(r.get("processing_time", 0) for r in results) / total_queries if total_queries > 0 else 0
        avg_confidence = sum(r.get("confidence", 0) for r in results) / total_queries if total_queries > 0 else 0

        logger.info(f"   ì´ ì§ˆì˜ ìˆ˜: {total_queries}")
        logger.info(f"   ì„±ê³µí•œ ì§ˆì˜: {successful_queries}")
        logger.info(f"   ì‹¤íŒ¨í•œ ì§ˆì˜: {failed_queries}")
        logger.info(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ")
        logger.info(f"   í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.2%}")
        logger.info(f"\n   ìƒì„¸ ë¡œê·¸ íŒŒì¼: {log_file}")

        print(f"\n{'='*80}", flush=True)
        print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½", flush=True)
        print(f"{'='*80}", flush=True)
        print(f"   ì´ ì§ˆì˜ ìˆ˜: {total_queries}", flush=True)
        print(f"   ì„±ê³µí•œ ì§ˆì˜: {successful_queries}", flush=True)
        print(f"   ì‹¤íŒ¨í•œ ì§ˆì˜: {failed_queries}", flush=True)
        print(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ", flush=True)
        print(f"   í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.2%}", flush=True)
        print(f"\n   ìƒì„¸ ë¡œê·¸ íŒŒì¼: {log_file}", flush=True)

        return successful_queries == total_queries

    except Exception as e:
        import traceback
        error_traceback = traceback.format_exc()

        logger.error(f"{'='*80}")
        logger.error("í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ")
        logger.error(f"{'='*80}")
        logger.error(f"ì˜¤ë¥˜ ìœ í˜•: {type(e).__name__}")
        logger.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
        logger.error(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{error_traceback}")

        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        result = asyncio.run(test_langgraph_workflow_with_logging())

        print(f"\n{'='*80}")
        print(f"ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'âœ… ì„±ê³µ' if result else 'âŒ ì‹¤íŒ¨'}")
        print(f"{'='*80}\n")

        sys.exit(0 if result else 1)

    except KeyboardInterrupt:
        logger.info("\ní…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()
