#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google Gemini 2.0 Flash Exp ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
ìµœì‹  Gemini ëª¨ë¸ë¡œ ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸
"""

import os
import sys
import asyncio
import tempfile
import time
from pathlib import Path
from typing import List, Dict, Any

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == "win32":
    import codecs
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())
    sys.stderr = codecs.getwriter("utf-8")(sys.stderr.detach())

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ë° .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()  # .env íŒŒì¼ ë¡œë“œ

os.environ["USE_LANGGRAPH"] = "true"
os.environ["LANGGRAPH_ENABLED"] = "true"
os.environ["LLM_PROVIDER"] = "google"

# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤ (ê°„ì†Œí™”)
TEST_QUESTIONS = [
    "ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­ì€?",
    "ì´í˜¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
    "ì ˆë„ì£„ì˜ êµ¬ì„±ìš”ê±´ì€?",
    "ì†í•´ë°°ìƒ ì²­êµ¬ ë°©ë²•ì€?",
    "ë¶€ë‹¹í•´ê³  êµ¬ì œ ì ˆì°¨ëŠ”?",
    "ë¶€ë™ì‚° ë§¤ë§¤ê³„ì•½ì„œ í•„ìˆ˜ ì¡°í•­ì€?",
    "íŠ¹í—ˆê¶Œ ì¹¨í•´ ì‹œ êµ¬ì œ ë°©ë²•ì€?",
    "ì†Œë“ì„¸ ì‹ ê³  ëˆ„ë½ ì‹œ ê°€ì‚°ì„¸ëŠ”?",
    "ë²•ì •ëŒ€ë¦¬ì¸ì˜ ê¶Œí•œì€?",
    "ì†Œì†¡ ì œê¸° ì‹œ ê´€í•  ë²•ì›ì€?"
]

async def test_gemini_performance():
    """Google Gemini ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("=" * 80)
    print("Google Gemini 2.5 Flash Lite ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print("ë³€ê²½ì‚¬í•­:")
    print("- ëª¨ë¸: qwen2.5:3b â†’ gemini-2.5-flash-lite")
    print("- ì œê³µì: Ollama â†’ Google AI")
    print("- ì‘ë‹µ ê¸¸ì´: 100 í† í°")
    print("- íƒ€ì„ì•„ì›ƒ: 10ì´ˆ")
    print("- Temperature: 0.3 (ì¼ê´€ì„± í–¥ìƒ)")
    print("=" * 80)
    
    # Google API í‚¤ í™•ì¸
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key or google_api_key == "your_google_api_key_here":
        print("âŒ Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("âš ï¸ API í‚¤ ì—†ì´ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. (ë°±ì—… ëª¨ë¸ ì‚¬ìš©)")
        print("ğŸ’¡ Google API í‚¤ë¥¼ ì„¤ì •í•˜ë©´ Gemini ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   í™˜ê²½ ë³€ìˆ˜: GOOGLE_API_KEY=your_api_key")
        print("   ë˜ëŠ” .env íŒŒì¼ì— GOOGLE_API_KEY=your_api_key ì¶”ê°€")
    else:
        print("âœ… Google API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    try:
        from source.services.langgraph.workflow_service import LangGraphWorkflowService
        from source.utils.langgraph_config import LangGraphConfig
        
        # ì„ì‹œ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì‚¬ìš©
        with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as f:
            db_path = f.name
        
        # ì„¤ì • ìƒì„±
        config = LangGraphConfig.from_env()
        config.checkpoint_db_path = db_path
        
        # ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        service = LangGraphWorkflowService(config)
        print("âœ… Gemini ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = []
        total_start_time = time.time()
        
        print(f"\nğŸš€ {len(TEST_QUESTIONS)}ê°œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        for i, question in enumerate(TEST_QUESTIONS, 1):
            print(f"\n[{i:2d}/{len(TEST_QUESTIONS)}] {question}")
            
            start_time = time.time()
            result = await service.process_query(question)
            processing_time = time.time() - start_time
            
            results.append({
                "question_id": i,
                "question": question,
                "processing_time": processing_time,
                "response_length": len(result.get("answer", "")),
                "confidence": result.get("confidence", 0),
                "query_type": result.get("query_type", "unknown"),
                "errors": result.get("errors", [])
            })
            
            print(f"    âš¡ ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"    ğŸ“ ë‹µë³€ ê¸¸ì´: {len(result.get('answer', ''))}ì")
            print(f"    ğŸ¯ ì‹ ë¢°ë„: {result.get('confidence', 0):.2f}")
            print(f"    ğŸ“‹ ì§ˆë¬¸ ìœ í˜•: {result.get('query_type', 'unknown')}")
            
            if result.get("errors"):
                print(f"    âŒ ì˜¤ë¥˜: {result['errors']}")
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if i % 3 == 0:
                avg_time = sum(r["processing_time"] for r in results) / len(results)
                print(f"\nğŸ“Š ì§„í–‰ë¥ : {i}/{len(TEST_QUESTIONS)} - í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_time:.2f}ì´ˆ")
        
        total_time = time.time() - total_start_time
        
        # ê²°ê³¼ ë¶„ì„
        analyze_gemini_performance_results(results, total_time)
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        os.unlink(db_path)
        
        return results
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []

def analyze_gemini_performance_results(results: List[Dict[str, Any]], total_time: float):
    """Gemini ì„±ëŠ¥ ê²°ê³¼ ë¶„ì„"""
    print("\n" + "=" * 80)
    print("Google Gemini ì„±ëŠ¥ ê²°ê³¼ ë¶„ì„")
    print("=" * 80)
    
    if not results:
        print("âŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ë³¸ í†µê³„
    total_questions = len(results)
    avg_processing_time = sum(r["processing_time"] for r in results) / total_questions
    min_time = min(r["processing_time"] for r in results)
    max_time = max(r["processing_time"] for r in results)
    avg_response_length = sum(r["response_length"] for r in results) / total_questions
    avg_confidence = sum(r["confidence"] for r in results) / total_questions
    
    print(f"ğŸ“Š ì„±ëŠ¥ í†µê³„:")
    print(f"   - ì´ ì§ˆë¬¸ ìˆ˜: {total_questions}")
    print(f"   - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_processing_time:.2f}ì´ˆ")
    print(f"   - ìµœë‹¨ ì²˜ë¦¬ ì‹œê°„: {min_time:.2f}ì´ˆ")
    print(f"   - ìµœì¥ ì²˜ë¦¬ ì‹œê°„: {max_time:.2f}ì´ˆ")
    print(f"   - í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_response_length:.0f}ì")
    print(f"   - í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
    print(f"   - ì´ˆë‹¹ ì²˜ë¦¬ ì§ˆë¬¸ ìˆ˜: {total_questions/total_time:.2f}ê°œ")
    
    # ì²˜ë¦¬ ì‹œê°„ ë¶„í¬
    very_fast_tests = [r for r in results if r["processing_time"] < 2]
    fast_tests = [r for r in results if 2 <= r["processing_time"] < 5]
    medium_tests = [r for r in results if 5 <= r["processing_time"] < 10]
    slow_tests = [r for r in results if r["processing_time"] >= 10]
    
    print(f"\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„ ë¶„í¬:")
    print(f"   - ë§¤ìš° ë¹ ë¦„ (<2ì´ˆ): {len(very_fast_tests)}ê°œ ({len(very_fast_tests)/total_questions*100:.1f}%)")
    print(f"   - ë¹ ë¦„ (2-5ì´ˆ): {len(fast_tests)}ê°œ ({len(fast_tests)/total_questions*100:.1f}%)")
    print(f"   - ë³´í†µ (5-10ì´ˆ): {len(medium_tests)}ê°œ ({len(medium_tests)/total_questions*100:.1f}%)")
    print(f"   - ëŠë¦¼ (â‰¥10ì´ˆ): {len(slow_tests)}ê°œ ({len(slow_tests)/total_questions*100:.1f}%)")
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥
    query_types = {}
    for r in results:
        qtype = r["query_type"]
        if qtype not in query_types:
            query_types[qtype] = []
        query_types[qtype].append(r["processing_time"])
    
    print(f"\nğŸ“‹ ì§ˆë¬¸ ìœ í˜•ë³„ í‰ê·  ì²˜ë¦¬ ì‹œê°„:")
    for qtype, times in query_types.items():
        avg_time = sum(times) / len(times)
        print(f"   - {qtype}: {avg_time:.2f}ì´ˆ ({len(times)}ê°œ)")
    
    # ì„±ëŠ¥ í‰ê°€
    print(f"\nğŸ† ì„±ëŠ¥ í‰ê°€:")
    if avg_processing_time < 2:
        print("   ğŸš€ ìš°ìˆ˜ - í‰ê·  2ì´ˆ ë¯¸ë§Œ (ë§¤ìš° ë¹ ë¦„)")
    elif avg_processing_time < 5:
        print("   âœ… ì–‘í˜¸ - í‰ê·  5ì´ˆ ë¯¸ë§Œ (ë¹ ë¦„)")
    elif avg_processing_time < 10:
        print("   âš ï¸ ë³´í†µ - í‰ê·  10ì´ˆ ë¯¸ë§Œ")
    else:
        print("   âŒ ê°œì„  í•„ìš” - í‰ê·  10ì´ˆ ì´ìƒ")
    
    # ì´ì „ ì„±ëŠ¥ê³¼ ë¹„êµ
    previous_avg = 20.65  # ì´ì „ Ollama í…ŒìŠ¤íŠ¸ ê²°ê³¼
    improvement = ((previous_avg - avg_processing_time) / previous_avg) * 100
    
    print(f"\nğŸ“ˆ ì´ì „ ì„±ëŠ¥ ëŒ€ë¹„ ê°œì„ :")
    print(f"   - ì´ì „ í‰ê·  (Ollama): {previous_avg:.2f}ì´ˆ")
    print(f"   - í˜„ì¬ í‰ê·  (Gemini): {avg_processing_time:.2f}ì´ˆ")
    print(f"   - ê°œì„ ìœ¨: {improvement:.1f}%")
    
    if improvement > 0:
        print(f"   âœ… ì„±ëŠ¥ì´ {improvement:.1f}% í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        if improvement > 50:
            print("   ğŸ‰ ëŒ€í­ ê°œì„ ! Gemini ëª¨ë¸ì´ ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤.")
        elif improvement > 25:
            print("   ğŸš€ ìƒë‹¹í•œ ê°œì„ ! Gemini ëª¨ë¸ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.")
        else:
            print("   âœ… ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"   âš ï¸ ì„±ëŠ¥ì´ {abs(improvement):.1f}% ì €í•˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # Gemini íŠ¹ì¥ì  ë¶„ì„
    print(f"\nğŸŒŸ Gemini ëª¨ë¸ íŠ¹ì¥ì :")
    if avg_processing_time < 5:
        print("   âœ… ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µ ì†ë„")
    if avg_response_length > 100:
        print("   âœ… ì ì ˆí•œ ë‹µë³€ ê¸¸ì´")
    if avg_confidence > 0.7:
        print("   âœ… ë†’ì€ ì‹ ë¢°ë„")
    
    # ì¶”ê°€ ìµœì í™” ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ’¡ ì¶”ê°€ ìµœì í™” ê¶Œì¥ì‚¬í•­:")
    if avg_processing_time > 5:
        print("   1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìµœì í™”")
        print("   2. ìºì‹± ì‹œìŠ¤í…œ ë„ì…")
        print("   3. ì‘ë‹µ ê¸¸ì´ ë” ì œí•œ (max_output_tokens=50)")
    
    if len(slow_tests) > total_questions * 0.2:
        print("   4. ëŠë¦° ì§ˆë¬¸ë“¤ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬ ë¡œì§ í•„ìš”")
    
    print("   5. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬í˜„")
    print("   6. ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ë„ì…")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    results = await test_gemini_performance()
    
    if results:
        print(f"\nğŸ‰ Gemini ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        avg_time = sum(r["processing_time"] for r in results) / len(results)
        print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ")
        
        if avg_time < 5:
            print("ğŸš€ Gemini ëª¨ë¸ì´ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤!")
        elif avg_time < 10:
            print("âœ… Gemini ëª¨ë¸ì´ ë¹ ë¦…ë‹ˆë‹¤!")
        else:
            print("âš ï¸ ì¶”ê°€ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        print("âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨")

if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.WARNING)  # ë¡œê·¸ ë ˆë²¨ ë‚®ì¶¤
    
    asyncio.run(main())
