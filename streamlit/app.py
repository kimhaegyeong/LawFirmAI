# -*- coding: utf-8 -*-
"""
LawFirmAI - Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜
Phase 2ì˜ ëª¨ë“  ê°œì„ ì‚¬í•­ì„ í†µí•©í•œ ìµœì í™”ëœ ë²„ì „
"""

import gc
import json
import logging
import os
import sys
import time
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# LangGraph í™œì„±í™” ì„¤ì •
os.environ["USE_LANGGRAPH"] = os.getenv("USE_LANGGRAPH", "true")

# Streamlit ë° ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import psutil
import torch
from streamlit_chat import message

import streamlit as st

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from source.data.database import DatabaseManager
from source.data.vector_store import LegalVectorStore
from source.services.answer_formatter import AnswerFormatter
from source.services.chat_service import ChatService
from source.services.confidence_calculator import ConfidenceCalculator
from source.services.context_builder import ContextBuilder
from source.services.context_compressor import ContextCompressor
from source.services.conversation_flow_tracker import ConversationFlowTracker
from source.services.dynamic_prompt_updater import create_dynamic_prompt_updater
from source.services.emotion_intent_analyzer import (
    EmotionIntentAnalyzer,
    EmotionType,
    IntentType,
    UrgencyLevel,
)
from source.services.gemini_client import GeminiClient
from source.services.hybrid_search_engine import HybridSearchEngine
from source.services.improved_answer_generator import ImprovedAnswerGenerator

# Phase 1: ëŒ€í™” ë§¥ë½ ê°•í™” ëª¨ë“ˆ
from source.services.integrated_session_manager import IntegratedSessionManager
from source.services.langgraph.workflow_service import LangGraphWorkflowService
from source.services.legal_term_expander import LegalTermExpander

# AKLS ëª¨ë“ˆ (ì„ì‹œ ë¹„í™œì„±í™”)
from source.services.multi_turn_handler import MultiTurnQuestionHandler
from source.services.optimized_search_engine import OptimizedSearchEngine
from source.services.performance_monitor import PerformanceContext
from source.services.performance_monitor import (
    PerformanceMonitor as SourcePerformanceMonitor,
)
from source.services.prompt_templates import PromptTemplateManager
from source.services.question_classifier import QuestionClassifier, QuestionType
from source.services.unified_prompt_manager import (
    LegalDomain,
    ModelType,
    UnifiedPromptManager,
)

# Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„ ëª¨ë“ˆ
from source.services.user_profile_manager import (
    DetailLevel,
    ExpertiseLevel,
    UserProfileManager,
)
from source.utils.config import Config
from source.utils.langgraph_config import LangGraphConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/streamlit_app.log')
    ]
)
logger = logging.getLogger(__name__)

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ìµœì í™” í´ë˜ìŠ¤"""

    def __init__(self, max_memory_percent: float = 85.0):
        self.max_memory_percent = max_memory_percent
        self.logger = logging.getLogger(__name__)

    @contextmanager
    def memory_efficient_inference(self):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì¶”ë¡  ì»¨í…ìŠ¤íŠ¸"""
        # ì¶”ë¡  ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
        self._cleanup_memory()

        try:
            yield
        finally:
            # ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            self._cleanup_memory()

    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def monitor_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        memory_percent = psutil.virtual_memory().percent
        if memory_percent > self.max_memory_percent:
            self.logger.warning(f"High memory usage: {memory_percent:.1f}%")
            self._cleanup_memory()
        return memory_percent

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.response_times = []
        self.error_count = 0
        self.total_requests = 0

    def log_request(self, response_time: float, success: bool = True, operation: str = None):
        """ìš”ì²­ ë¡œê¹…"""
        self.total_requests += 1
        self.response_times.append(response_time)

        if not success:
            self.error_count += 1

        # ìµœê·¼ 100ê°œ ìš”ì²­ë§Œ ìœ ì§€
        if len(self.response_times) > 100:
            self.response_times = self.response_times[-100:]

    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.response_times:
            return {
                "avg_response_time": 0,
                "error_rate": 0,
                "total_requests": 0
            }

        return {
            "avg_response_time": sum(self.response_times) / len(self.response_times),
            "error_rate": self.error_count / self.total_requests if self.total_requests > 0 else 0,
            "total_requests": self.total_requests
        }

@st.cache_resource
def initialize_app():
    """Streamlit ìºì‹œë¥¼ ì‚¬ìš©í•œ ì•± ì´ˆê¸°í™”"""
    app = StreamlitApp()
    if not app.initialize_components():
        logger.error("Failed to initialize components")
    return app

class StreamlitApp:
    """Streamlit ì „ìš© LawFirmAI ì• í”Œë¦¬ì¼€ì´ì…˜"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.memory_optimizer = MemoryOptimizer()
        self.performance_monitor = PerformanceMonitor()

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.db_manager = None
        self.vector_store = None
        self.question_classifier = None
        self.hybrid_search_engine = None
        self.prompt_template_manager = None
        self.unified_prompt_manager = None
        self.dynamic_prompt_updater = None
        self.confidence_calculator = None
        self.legal_term_expander = None
        self.gemini_client = None
        self.improved_answer_generator = None
        self.answer_formatter = None
        self.context_builder = None

        # ChatService ì´ˆê¸°í™” (LangGraph í†µí•©)
        self.chat_service = None

        # LangGraph ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤
        self.langgraph_workflow = None

        # Phase 1: ëŒ€í™” ë§¥ë½ ê°•í™” ì»´í¬ë„ŒíŠ¸
        self.session_manager = None
        self.multi_turn_handler = None
        self.context_compressor = None

        # Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„ ì»´í¬ë„ŒíŠ¸
        self.user_profile_manager = None
        self.emotion_intent_analyzer = None
        self.conversation_flow_tracker = None

        # ì„¸ì…˜ ê´€ë¦¬
        self.current_session_id = None
        self.current_user_id = None

        # ì´ˆê¸°í™” ìƒíƒœ
        self.is_initialized = False
        self.initialization_error = None

    def initialize_components(self) -> bool:
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            self.logger.info("Initializing LawFirmAI components...")
            start_time = time.time()

            with self.memory_optimizer.memory_efficient_inference():
                # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì ì´ˆê¸°í™”
                self.db_manager = DatabaseManager("data/lawfirm.db")
                self.logger.info("Database manager initialized")

                # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” (íŒë¡€ ë°ì´í„°ìš©)
                self.vector_store = LegalVectorStore(
                    model_name='jhgan/ko-sroberta-multitask',
                    dimension=768,
                    index_type='flat'
                )
                # íŒë¡€ ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ
                if not self.vector_store.load_index('data/embeddings/ml_enhanced_ko_sroberta_precedents'):
                    self.logger.warning("Failed to load precedent vector index, using law index")
                    self.vector_store.load_index('data/embeddings/ml_enhanced_ko_sroberta')
                self.logger.info("Vector store initialized")

                # ì§ˆë¬¸ ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
                self.question_classifier = QuestionClassifier()
                self.logger.info("Question classifier initialized")

                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
                self.hybrid_search_engine = HybridSearchEngine()
                self.logger.info("Hybrid search engine initialized")

                # ìµœì í™”ëœ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
                self.optimized_search_engine = OptimizedSearchEngine(
                    vector_store=self.vector_store,
                    hybrid_engine=self.hybrid_search_engine,
                    cache_size=1000,
                    cache_ttl=3600
                )
                self.logger.info("Optimized search engine initialized")

                # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬ì ì´ˆê¸°í™”
                self.prompt_template_manager = PromptTemplateManager()
                self.logger.info("Prompt template manager initialized")

                # í†µí•© í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì
                self.unified_prompt_manager = UnifiedPromptManager()
                self.logger.info("Unified prompt manager initialized")

                # ë™ì  í”„ë¡¬í”„íŠ¸ ì—…ë°ì´í„°
                self.dynamic_prompt_updater = create_dynamic_prompt_updater(self.unified_prompt_manager)
                self.logger.info("Dynamic prompt updater initialized")

                # ì‹ ë¢°ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”
                self.confidence_calculator = ConfidenceCalculator()
                self.logger.info("Confidence calculator initialized")

                # ë²•ë¥  ìš©ì–´ í™•ì¥ê¸° ì´ˆê¸°í™”
                self.legal_term_expander = LegalTermExpander()
                self.logger.info("Legal term expander initialized")

                # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                if os.getenv('GEMINI_ENABLED', 'true').lower() == 'true':
                    self.gemini_client = GeminiClient()
                    self.logger.info("Gemini client initialized")

                # ë‹µë³€ í¬ë§·í„° ì´ˆê¸°í™”
                self.answer_formatter = AnswerFormatter()
                self.logger.info("Answer formatter initialized")

                # ì»¨í…ìŠ¤íŠ¸ ë¹Œë” ì´ˆê¸°í™”
                self.context_builder = ContextBuilder()
                self.logger.info("Context builder initialized")

                # ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸° ì´ˆê¸°í™”
                self.improved_answer_generator = ImprovedAnswerGenerator(
                    gemini_client=self.gemini_client,
                    prompt_template_manager=self.prompt_template_manager,
                    confidence_calculator=self.confidence_calculator,
                    answer_formatter=self.answer_formatter,
                    context_builder=self.context_builder
                )
                self.logger.info("Improved answer generator initialized")

                # ChatService ì´ˆê¸°í™” (LangGraph í†µí•©)
                config = Config()
                self.chat_service = ChatService(config)
                self.logger.info("ChatService initialized with LangGraph integration")

                # Phase 1: ëŒ€í™” ë§¥ë½ ê°•í™” ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
                self.session_manager = IntegratedSessionManager("data/conversations.db")
                self.logger.info("Integrated session manager initialized")

                self.multi_turn_handler = MultiTurnQuestionHandler()
                self.logger.info("Multi-turn question handler initialized")

                self.context_compressor = ContextCompressor(max_tokens=2000)
                self.logger.info("Context compressor initialized")

                # Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
                self.user_profile_manager = UserProfileManager(self.session_manager.conversation_store)
                self.logger.info("User profile manager initialized")

                self.emotion_intent_analyzer = EmotionIntentAnalyzer()
                self.logger.info("Emotion intent analyzer initialized")

                self.conversation_flow_tracker = ConversationFlowTracker()
                self.logger.info("Conversation flow tracker initialized")

                # ì„¸ì…˜ ID ìƒì„±
                self.current_session_id = f"streamlit_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                self.current_user_id = "streamlit_user"  # ê¸°ë³¸ ì‚¬ìš©ì ID
                self.logger.info(f"Session initialized: {self.current_session_id}")

            initialization_time = time.time() - start_time
            self.logger.info(f"All components initialized successfully in {initialization_time:.2f} seconds")

            self.is_initialized = True
            return True

        except Exception as e:
            self.initialization_error = str(e)
            self.logger.error(f"Failed to initialize components: {e}")
            return False

    def process_query(self, query: str, session_id: str = None, user_id: str = None) -> Dict[str, Any]:
        """ì§ˆì˜ ì²˜ë¦¬ (Phase 1 ëŒ€í™” ë§¥ë½ ê°•í™” ì ìš©)"""
        if not self.is_initialized:
            return {
                "answer": "ì‹œìŠ¤í…œì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "error": "System not initialized",
                "confidence": {"confidence": 0, "reliability_level": "VERY_LOW"}
            }

        # ì„¸ì…˜ ID ì„¤ì •
        if not session_id:
            session_id = self.current_session_id
        if not user_id:
            user_id = self.current_user_id

        start_time = time.time()

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        try:
            with self.memory_optimizer.memory_efficient_inference():
                # Phase 1: ëŒ€í™” ë§¥ë½ ì²˜ë¦¬
                context = None
                resolved_query = query
                multi_turn_info = {}

                if self.session_manager:
                    # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
                    context = self.session_manager.get_or_create_session(session_id, user_id)

                    # ë‹¤ì¤‘ í„´ ì§ˆë¬¸ ì²˜ë¦¬
                    if self.multi_turn_handler and context:
                        is_multi_turn = self.multi_turn_handler.detect_multi_turn_question(query, context)
                        if is_multi_turn:
                            multi_turn_result = self.multi_turn_handler.build_complete_query(query, context)
                            resolved_query = multi_turn_result["resolved_query"]
                            multi_turn_info = {
                                "is_multi_turn": True,
                                "original_query": query,
                                "resolved_query": resolved_query,
                                "confidence": multi_turn_result["confidence"],
                                "reasoning": multi_turn_result["reasoning"]
                            }
                            self.logger.info(f"Multi-turn question resolved: '{query}' -> '{resolved_query}'")

                # Phase 2: ê°œì¸í™” ë° ì§€ëŠ¥í˜• ë¶„ì„
                personalized_context = {}
                emotion_intent_info = {}
                flow_tracking_info = {}

                if self.user_profile_manager:
                    # ê°œì¸í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    personalized_context = self.user_profile_manager.get_personalized_context(user_id, resolved_query)
                    self.logger.info(f"Personalized context created for user {user_id}")

                if self.emotion_intent_analyzer:
                    # ê°ì • ë° ì˜ë„ ë¶„ì„
                    emotion_result = self.emotion_intent_analyzer.analyze_emotion(resolved_query)
                    intent_result = self.emotion_intent_analyzer.analyze_intent(resolved_query, context)

                    # ì‘ë‹µ í†¤ ê²°ì •
                    response_tone = self.emotion_intent_analyzer.get_contextual_response_tone(
                        emotion_result, intent_result, personalized_context
                    )

                    emotion_intent_info = {
                        "emotion": {
                            "primary_emotion": emotion_result.primary_emotion.value,
                            "confidence": emotion_result.confidence,
                            "intensity": emotion_result.intensity,
                            "reasoning": emotion_result.reasoning
                        },
                        "intent": {
                            "primary_intent": intent_result.primary_intent.value,
                            "confidence": intent_result.confidence,
                            "urgency_level": intent_result.urgency_level.value,
                            "reasoning": intent_result.reasoning
                        },
                        "response_tone": {
                            "tone_type": response_tone.tone_type,
                            "empathy_level": response_tone.empathy_level,
                            "formality_level": response_tone.formality_level,
                            "urgency_response": response_tone.urgency_response,
                            "explanation_depth": response_tone.explanation_depth
                        }
                    }
                    self.logger.info(f"Emotion: {emotion_result.primary_emotion.value}, Intent: {intent_result.primary_intent.value}")

                if self.conversation_flow_tracker and context:
                    # ëŒ€í™” íë¦„ ì¶”ì 
                    from source.services.conversation_manager import ConversationTurn
                    self.conversation_flow_tracker.track_conversation_flow(session_id,
                        ConversationTurn(
                            user_query=query,
                            bot_response="",
                            timestamp=datetime.now(),
                            question_type="general_question"
                        )
                    )

                    # ë‹¤ìŒ ì˜ë„ ì˜ˆì¸¡ ë° í›„ì† ì§ˆë¬¸ ì œì•ˆ
                    predicted_intents = self.conversation_flow_tracker.predict_next_intent(context)
                    suggested_questions = self.conversation_flow_tracker.suggest_follow_up_questions(context)
                    conversation_state = self.conversation_flow_tracker.get_conversation_state(context)

                    flow_tracking_info = {
                        "predicted_intents": predicted_intents,
                        "suggested_questions": suggested_questions,
                        "conversation_state": conversation_state
                    }
                    self.logger.info(f"Flow tracking: state={conversation_state}, suggestions={len(suggested_questions)}")

                # ChatServiceë¥¼ ì‚¬ìš©í•œ ì²˜ë¦¬ (ì‹¤ì œ RAG ì‹œìŠ¤í…œ)
                if self.chat_service and hasattr(self.chat_service, 'improved_answer_generator') and self.chat_service.improved_answer_generator:
                    import asyncio
                    result = asyncio.run(self.chat_service.process_message(resolved_query, session_id=session_id))

                    response_time = time.time() - start_time
                    self.performance_monitor.log_request(response_time, success=True, operation="process_query")

                    # Phase 1: ì„¸ì…˜ì— í„´ ì¶”ê°€
                    if self.session_manager and context:
                        self.session_manager.add_turn(
                            session_id,
                            query,
                            result.get("response", ""),
                            result.get("question_type", "general_question"),
                            user_id
                        )

                    # ì»¨í…ìŠ¤íŠ¸ ì••ì¶• í™•ì¸
                    compression_info = {}
                    if self.context_compressor and context:
                        current_tokens = self.context_compressor.calculate_tokens(context)
                        if current_tokens > 1500:
                            compression_result = self.context_compressor.compress_long_conversation(context)
                            compression_info = {
                                "compressed": True,
                                "original_tokens": compression_result.original_tokens,
                                "compressed_tokens": compression_result.compressed_tokens,
                                "compression_ratio": compression_result.compression_ratio,
                                "summary": compression_result.summary
                            }
                            self.logger.info(f"Context compressed: {compression_result.compression_ratio:.2f} ratio")

                    return {
                        "answer": result.get("response", ""),
                        "confidence": {
                            "confidence": result.get("confidence", 0.0),
                            "reliability_level": "HIGH" if result.get("confidence", 0) > 0.7 else "MEDIUM" if result.get("confidence", 0) > 0.4 else "LOW"
                        },
                        "processing_time": response_time,
                        "memory_usage": self.memory_optimizer.monitor_memory_usage(),
                        "question_type": result.get("question_type", "general_question"),
                        "session_id": session_id,
                        "user_id": user_id,
                        "multi_turn_info": multi_turn_info,
                        "compression_info": compression_info,
                        "context_stats": {
                            "total_turns": len(context.turns) if context else 0,
                            "total_entities": sum(len(entities) for entities in context.entities.values()) if context else 0,
                            "topics": list(context.topic_stack) if context else []
                        },
                        "personalized_context": personalized_context,
                        "emotion_intent_info": emotion_intent_info,
                        "flow_tracking_info": flow_tracking_info,
                        "session_id": result.get("session_id"),
                        "query_type": result.get("query_type", ""),
                        "legal_references": result.get("legal_references", []),
                        "processing_steps": result.get("processing_steps", []),
                        "metadata": result.get("metadata", {}),
                        "errors": result.get("errors", [])
                    }
                else:
                    # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
                    return self._process_query_legacy(query, start_time)

        except Exception as e:
            response_time = time.time() - start_time
            self.performance_monitor.log_request(response_time, success=False, operation="process_query_error")
            self.logger.error(f"Error processing query: {e}")

            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "error": str(e),
                "confidence": {"confidence": 0, "reliability_level": "VERY_LOW"},
                "processing_time": response_time
            }

    def _process_query_legacy(self, query: str, start_time: float) -> Dict[str, Any]:
        """ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì§ˆì˜ ì²˜ë¦¬ (í´ë°±)"""
        try:
            with self.memory_optimizer.memory_efficient_inference():
                # ì§ˆë¬¸ ë¶„ë¥˜
                question_classification = self.question_classifier.classify_question(query)

                # ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹¤í–‰
                search_results = self.hybrid_search_engine.search_with_question_type(
                    query=query,
                    question_type=question_classification,
                    max_results=10
                )

                # ë‹µë³€ ìƒì„±
                answer_result = self.improved_answer_generator.generate_answer(
                    query=query,
                    question_type=question_classification,
                    context="",
                    sources=search_results,
                    conversation_history=None
                )

                response_time = time.time() - start_time
                self.performance_monitor.log_request(response_time, success=True, operation="legacy_process_query")

                return {
                    "answer": answer_result.answer,
                    "question_type": question_classification.question_type.value,
                    "confidence": {
                        "confidence": answer_result.confidence.confidence,
                        "reliability_level": answer_result.confidence.level.value
                    },
                    "processing_time": response_time,
                    "memory_usage": self.memory_optimizer.monitor_memory_usage()
                }

        except Exception as e:
            response_time = time.time() - start_time
            self.performance_monitor.log_request(response_time, success=False, operation="legacy_process_query_error")
            self.logger.error(f"Error in legacy processing: {e}")

            return {
                "answer": "ê¸°ì¡´ ì²˜ë¦¬ ë°©ì‹ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "error": str(e),
                "confidence": {"confidence": 0, "reliability_level": "VERY_LOW"},
                "processing_time": response_time
            }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    st.set_page_config(
        page_title="LawFirmAI - ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸",
        page_icon="âš–ï¸",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("âš–ï¸ LawFirmAI - ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸")

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if 'app' not in st.session_state:
        with st.spinner('ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...'):
            st.session_state.app = initialize_app()

    app = st.session_state.app

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")

        # íƒ­ ìƒì„±
        tabs = st.tabs(["ğŸ’¬ ì±„íŒ…", "ğŸ‘¤ í”„ë¡œí•„", "ğŸ“Š ìƒíƒœ"])

        with tabs[0]:
            st.subheader("ì±„íŒ… ì„¤ì •")
            user_id = st.text_input("ì‚¬ìš©ì ID", value=app.current_user_id)

        with tabs[1]:
            st.subheader("ì‚¬ìš©ì í”„ë¡œí•„")
            expertise_level = st.selectbox("ì „ë¬¸ì„± ìˆ˜ì¤€", ["beginner", "intermediate", "advanced", "expert"])
            detail_level = st.selectbox("ë‹µë³€ ìƒì„¸ë„", ["simple", "medium", "detailed"])

        with tabs[2]:
            st.subheader("ì‹œìŠ¤í…œ ìƒíƒœ")
            if app.is_initialized:
                st.success("âœ… ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                stats = app.performance_monitor.get_stats()
                st.json(stats)
            else:
                st.error("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
                st.error(app.initialization_error)

    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.markdown("### ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    # ì±„íŒ… í‘œì‹œ
    chat_container = st.container()

    with chat_container:
        for i, msg in enumerate(st.session_state.chat_history):
            if msg["role"] == "user":
                st.chat_message("user").write(msg["content"])
            else:
                st.chat_message("assistant").write(msg["content"])

                # ì‹ ë¢°ë„ ì •ë³´ í‘œì‹œ
                if "confidence" in msg:
                    conf_info = msg["confidence"]
                    st.caption(f"ì‹ ë¢°ë„: {conf_info.get('confidence', 0):.1%} | "
                              f"ìˆ˜ì¤€: {conf_info.get('reliability_level', 'Unknown')} | "
                              f"ì²˜ë¦¬ ì‹œê°„: {msg.get('processing_time', 0):.2f}ì´ˆ")

    # ì§ˆë¬¸ ì…ë ¥
    user_input = st.chat_input("ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

    if user_input:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        st.rerun()

    # ë‹µë³€ ìƒì„±
    if st.session_state.chat_history and st.session_state.chat_history[-1]["role"] == "user":
        last_msg = st.session_state.chat_history[-1]["content"]

        with st.spinner('ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...'):
            result = app.process_query(last_msg, user_id=user_id)

            answer = result.get("answer", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ë‹µë³€ ì¶”ê°€
            st.session_state.chat_history.append({
                "role": "assistant",
                "content": answer,
                "confidence": result.get("confidence", {}),
                "processing_time": result.get("processing_time", 0)
            })

            st.rerun()

if __name__ == "__main__":
    main()
